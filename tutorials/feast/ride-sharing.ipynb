{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccebad61-e805-4c65-9db4-537156967dab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ride-Sharing\n",
    "\n",
    "In this tutorial, you explore how to leverage Feast to generate and reuse/share training features, and provide feature\n",
    "consistency/efficiency in near real-time model inference. In this use-case, your goal is to train a ride-sharing driver\n",
    "satisfaction prediction model. Feast addresses several prevalent challenges in this pipeline:\n",
    "\n",
    "1. **Training-serving skew and complex data joins**: Often, feature values are scattered across multiple tables. Merging\n",
    "   these datasets can be a tedious, time-consuming, and error-prone task. Feast simplifies this process with proven\n",
    "   logic that maintains point-in-time accuracy, preventing future feature values from leaking into your models.\n",
    "1. **Online feature availability**: During inference, models frequently require features that aren't immediately\n",
    "   accessible and must be derived from other data sources. Feast streamlines deployment to a range of online stores,\n",
    "   such as DynamoDB, Redis, and Google Cloud Datastore. It ensures that essential features are always at hand and\n",
    "   updated at the moment of inference.\n",
    "1. **Feature and model versioning**: Within larger organizations, it's common for different teams to inadvertently\n",
    "   duplicate feature creation logic because they can't reuse features from other projects. Plus, models have data\n",
    "   dependencies that demand versioning, like when executing A/B tests on various model iterations. Feast promotes the\n",
    "   discovery and sharing of previously used features and facilitates the versioning of feature groups through feature\n",
    "   services.\n",
    "1. **Feature Transformations**: Feast introduces lightweight feature transformations, empowering users to standardize\n",
    "   transformation logic for both online and offline scenarios and across different models.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Feature Repository Structure](#feature-repository-structure)\n",
    "- [Inspecting the Raw Data](#inspecting-the-raw-data)\n",
    "- [Register Feature Definitions](#register-feature-definitions)\n",
    "- [Generate a Training Dataset](#generate-training-data)\n",
    "- [Generate Features for Batch Scoring](#generate-features-for-batch-scoring)\n",
    "- [Ingest Batch Features into an Online Store](#ingest-batch-features-into-an-online-store)\n",
    "- [Fetch Online Features for Real-time Inference](#fetch-online-features-for-real-time-inference)\n",
    "- [Fetch Online Features Using a Feature Service](#fetch-online-features-using-a-feature-service)\n",
    "- [Ingest Streaming Features](#ingest-streaming-features)\n",
    "- [Train the Model](#train-the-model)\n",
    "\n",
    "To get started, let's import the libraries you need and explore the Feast feature repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95d799-fd31-4b71-9566-c68835089261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from feast import FeatureStore\n",
    "from feast.data_source import PushMode\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from definitions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12846f15-e759-4c06-94cc-874bc6fcc475",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Repository Structure\n",
    "\n",
    "The following command lists the files and directories inside the Feast feature repository:\n",
    "\n",
    "- `data/`: This directory houses the raw demo data in the Parquet format.\n",
    "- `feature_store.yaml`: This YAML file demonstrates the setup for Feast and the location of the data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a4b9f-27ab-4ea6-8fdd-16720503bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_repo_path = Path(\"/mnt/shared/feast-store\")\n",
    "os.listdir(feature_repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ffea9-2bb5-413a-94e8-025afcc9089e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's take a closer look to the `feature_store.yaml` configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65309177-98a7-4159-a385-255639e8af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(feature_repo_path/\"feature_store.yaml\", \"r\") as file:\n",
    "    for line in file:\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72d632-076d-4416-82c5-d73c63abcce4",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `feature_store.yaml` file is pivotal for defining the overarching architecture of the feature store in Feast. The\n",
    "provider value determines the default offline and online stores to be used.\n",
    "\n",
    "- **Offline Store**: Serves as the compute layer to process historical data. This is crucial for the generation of\n",
    "  training data and calculating feature values for serving.\n",
    "- **Online Store**: Acts as a low-latency repository for the most recent feature values, facilitating real-time\n",
    "  inference.\n",
    "\n",
    "It's noteworthy that Feast is compatible with a plethora of offline and online stores. Some of these include Spark,\n",
    "Azure, Hive, Trino, and PostgreSQL. Integration with these platforms is achieved via community plugins. In this case,\n",
    "you use a file to keep the offline state and SQLite to implement the online store.\n",
    "\n",
    "Let's now take a closer look to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008ce9a-ec7a-4543-b1cd-1a669224309f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inspecting the Raw Data\n",
    "\n",
    "In this demo, the raw feature data is stored in a local Parquet file. This dataset chronicles the hourly statistics\n",
    "related to a driver's activity on a ride-sharing platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51434937-ec33-4b15-89b0-5090ab178825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"data/driver_stats.parquet\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a641c0a3-f6a1-4c4d-8c51-95113e61743e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Following, the next steps in this tutorial you go through a comprehensive and typical Feast workflow. Here are the\n",
    "primary steps:\n",
    "\n",
    "1. [Register feature definitions](#register-feature-definitions)\n",
    "1. [Generate a Training Dataset](#generate-training-data)\n",
    "1. [Generate Features for Batch Scoring](#generate-features-for-batch-scoring)\n",
    "1. [Ingest Batch Features into an Online Store](#ingest-batch-features-into-an-online-store)\n",
    "1. [Fetch Online Features for Real-time Inference](#fetch-online-features-for-real-time-inference)\n",
    "1. [Fetch Online Features Using a Feature Service](#fetch-online-features-using-a-feature-service)\n",
    "1. [Ingest Streaming Features](#ingest-streaming-features)\n",
    "1. [Train the Model](#train-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b1814-5017-408e-a688-51d0a3ef28be",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Register Feature Definitions\n",
    "\n",
    "In the next cell, you create a new `FeatureStore` object, by pointing it to the location of Feast feature repository.\n",
    "Then, the `apply` method examines the variables you are passing to identify feature view and entity definitions.\n",
    "\n",
    "Once found, it registers these objects and initiates the deployment of the necessary infrastructure. In your case, the\n",
    "command processes the improted variables from the `definitions.py` file inside the demo's directory. It then establishes\n",
    "SQLite tables for the online store. It's important to note that you had previously chosen SQLite as the default online\n",
    "store by configuring the `online_store` parameter in `feature_store.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2ab8e-6135-4f70-8bf0-1234a6ed4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureStore(repo_path=feature_repo_path)\n",
    "\n",
    "fs.apply([driver, driver_stats_source, driver_stats_feature_view, driver_stats_push_source,\n",
    "          driver_activity, transformed_stats, driver_stats_fresh_feature_view])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630cd32-1416-4779-9383-e65326fa1d7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that you have registered the feature views and entities in your store, you can view the result in the Feast UI, by\n",
    "navigating to it from your EzUA dashboard:\n",
    "\n",
    "![feast-ui](images/feast-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a930f-afc1-4e76-81eb-f9e2d2376fb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate Training Data\n",
    "\n",
    "To train a model, both features and labels are essential. Frequently, labels are stored separately from the features.\n",
    "For instance, while one table might store user survey results, another set of tables may contain the feature values.\n",
    "Feast simplifies the process of mapping these features to the corresponding labels.\n",
    "\n",
    "To achieve this, Feast requires a list of entities (such as driver IDs) accompanied by timestamps. With this\n",
    "information, Feast intelligently joins the pertinent tables to produce the appropriate feature vectors. There are two\n",
    "primary methods to generate this list:\n",
    "\n",
    "1. Users can query the labels table, which contains timestamps, and then feed this data into Feast as an entity\n",
    "   dataframe. This approach is useful for generating training data.\n",
    "1. Alternatively, users can utilize a SQL query to extract entities from the table.\n",
    "\n",
    "> It's crucial to incorporate timestamps since the objective is to leverage features corresponding to the same driver\n",
    "> at different moments for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd03e7-a7f2-49e2-ad0c-8f3cc35103e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = pd.DataFrame.from_dict({\n",
    "        # entity's join key -> entity values\n",
    "        \"driver_id\": [1001, 1002, 1003],\n",
    "        # \"event_timestamp\" (reserved key) -> timestamps\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "        ],\n",
    "        # (optional) label name -> label values. Feast does not process these.\n",
    "        \"label_driver_reported_satisfaction\": [1, 5, 3],\n",
    "    }\n",
    ")\n",
    "\n",
    "# on demand transformations give us the last two features\n",
    "# `conv_plus_trips` and `acc_plus_trips`\n",
    "training_df = fs.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_hourly_stats:avg_daily_trips\",\n",
    "        \"transformed_stats:conv_plus_trips\",\n",
    "        \"transformed_stats:acc_plus_trips\",\n",
    "    ],\n",
    ").to_df()\n",
    "\n",
    "print(\"----- Feature schema -----\\n\")\n",
    "print(training_df.info())\n",
    "\n",
    "print()\n",
    "print(\"----- Example features -----\\n\")\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33215b6f-ab8b-4a18-bfa1-a6ef9ce990ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate Features for Batch Scoring\n",
    "\n",
    "To power a batch model, the primary requirement is to generate features. This is achieved using the\n",
    "`get_historical_features` call. However, it's essential to use the current timestamp for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ff580-7e97-49cb-a04e-df9e3cc9f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_df[\"event_timestamp\"] = pd.to_datetime(\"now\", utc=True)\n",
    "training_df = fs.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_hourly_stats:avg_daily_trips\",\n",
    "        \"transformed_stats:conv_plus_trips\",\n",
    "        \"transformed_stats:acc_plus_trips\",\n",
    "    ],\n",
    ").to_df()\n",
    "\n",
    "print(\"\\n----- Example features -----\\n\")\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb0fd3-5a41-4f5d-8b5c-7461bfd0dc5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ingest Batch Features into an Online Store\n",
    "\n",
    "To prepare for serving, you serialize the most recent feature values spanning back to their inception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8ce93-6913-4185-ad92-b71077aff0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.materialize(\n",
    "    datetime(2021, 4, 12, 10, 59, 42), datetime.now(), [\"driver_hourly_stats\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605b436-a773-4cc7-aaf4-7f9ece20bc27",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fetch Online Features for Real-time Inference\n",
    "\n",
    "During inference, it's essential to rapidly access the latest feature values for various drivers. Without an online\n",
    "feature store, these values might solely reside in batch sources. By utilizing the `get_online_features()` function, you\n",
    "can swiftly retrieve these values from the online feature store. Once obtained, these feature vectors are ready to be\n",
    "fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9ac1d-fe7e-4fdd-b497-f00804cec1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = fs.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_hourly_stats:avg_daily_trips\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        # {join_key: entity_value}\n",
    "        {\"driver_id\": 1004},\n",
    "        {\"driver_id\": 1005},\n",
    "    ],\n",
    ").to_dict()\n",
    "\n",
    "print(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaced7e1-4c9f-4f3e-a370-90a08fc29ce0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fetch Online Features Using a Feature Service\n",
    "\n",
    "Feature services offer a way to manage multiple features, allowing for a decoupling between feature view definitions and\n",
    "the features required by end applications. Additionally, the feature store provides the flexibility to fetch either\n",
    "online or historical features using the API outlined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf17d5-6016-4bfb-99d3-b132da6ff528",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_service = fs.get_feature_service(\"driver_activity\")\n",
    "\n",
    "feature_vector = fs.get_online_features(\n",
    "    features=feature_service,\n",
    "    entity_rows=[\n",
    "        {\"driver_id\": 1004},\n",
    "        {\"driver_id\": 1005},\n",
    "    ],\n",
    ").to_dict()\n",
    "\n",
    "print(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40df3a1-1547-437e-8914-6283dc3c1bc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ingest Streaming Features\n",
    "\n",
    "Feast doesn't natively support ingestion from streaming sources. Instead, it operates on a push-based model where\n",
    "features are actively pushed into Feast. You can create a streaming pipeline dedicated to feature generation, and\n",
    "subsequently push these features to either the offline store, the online store, or both, based on your requirements.\n",
    "\n",
    "This approach is contingent on the `PushSource` defined earlier. Pushing data to this source ensures that all connected\n",
    "feature views are populated with the newly pushed feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f826fed3-fdb1-4fb1-81fb-5dafecc9007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"driver_id\": [1001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2023, 5, 13, 10, 59, 42),\n",
    "        ],\n",
    "        \"created\": [\n",
    "            datetime(2023, 5, 13, 10, 59, 42),\n",
    "        ],\n",
    "        \"conv_rate\": [1.0],\n",
    "        \"acc_rate\": [1.0],\n",
    "        \"avg_daily_trips\": [1000],\n",
    "    }\n",
    ")\n",
    "\n",
    "fs.push(\"driver_stats_push_source\", event_df, to=PushMode.ONLINE_AND_OFFLINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d4f230-ba5d-4e16-bfcb-1fef0825aee1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Retrieve the data you pushed to the sreaming source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70275ee-44ef-4e8d-8b10-8f289eb3ee7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_entity = pd.DataFrame.from_dict({\n",
    "        # entity's join key -> entity values\n",
    "        \"driver_id\": [1001],\n",
    "        # \"event_timestamp\" (reserved key) -> timestamps\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2023, 5, 13, 10, 59, 42)\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# on demand transformations give us the last two features\n",
    "# `conv_plus_trips` and `acc_plus_trips`\n",
    "temp_df = fs.get_historical_features(\n",
    "    entity_df=temp_entity,\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_hourly_stats:avg_daily_trips\",\n",
    "        \"transformed_stats:conv_plus_trips\",\n",
    "        \"transformed_stats:acc_plus_trips\",\n",
    "    ],\n",
    ").to_df()\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7cb62b-4945-4ca6-a2fd-d465ee7ec5cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train the Model\n",
    "\n",
    "Finally, you can use the training dataset you created before to train a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b4c8d-4849-4a8d-bb26-db48ee5b2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"label_driver_reported_satisfaction\"\n",
    "\n",
    "train_X = training_df[training_df.columns.drop(target).drop(\"event_timestamp\")]\n",
    "train_Y = training_df.loc[:, target]\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(train_X[sorted(train_X)], train_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ride-sharing",
   "language": "python",
   "name": "ride-sharing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
