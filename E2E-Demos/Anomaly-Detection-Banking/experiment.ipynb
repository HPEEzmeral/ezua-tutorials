{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fraud Detection\n",
    "\n",
    "Fraudulent activity has permeated multiple sectors, from e-commerce and healthcare to banking and payment systems. This illicit industry amasses billions every year and is on an upward trajectory. The 2018 global economic crime survey by PwC verifies this assertion, revealing that 49 percent of the 7,200 enterprises surveyed had fallen prey to some form of fraudulent conduct.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/artboard.png\" alt=\"fraud-detection-banking\" style=\"width:100%\">\n",
    "</figure>\n",
    "\n",
    "Despite the perceived peril of fraud to businesses, the advent of sophisticated systems, such as rule engines or machine learning, equips us with the tools to detect and prevent such behaviors. In this notebook, we demonstrate how a machine learning system helps us achieve this.\n",
    "\n",
    "At its core, a rules engine is a sophisticated software system that enforces one or more business rules in a real-time production environment. More often than not, these rules are the crystallization of hard-earned insights gleaned from domain experts. For instance, we could establish rules limiting the number of transactions in a given time frame, and blocking transactions that originate from previously identified fraudulent IPs and/or domains. Such rules prove highly effective in detecting certain types of fraud, yet they are not without their limitations. Rules with predefined threshold values may give rise to false positives or false negatives. To illustrate, imagine a rule that rejects any transaction exceeding \\\\$10,000 for a particular user. A seasoned fraudster might exploit this by staying one step ahead, consciously making a transaction slightly below this threshold (for instance, \\\\$9,999), thereby evading detection.\n",
    "\n",
    "This is where machine learning comes to the rescue: By reducing both the risk of fraud and potential financial losses to businesses, machine learning fortifies the efficacy of the detection system. Combining this technology with rules-based systems ensures that fraud detection becomes a more precise and reliable endeavor. In our exploration, we will be inspecting fraudulent transactions using the Banksim dataset. This synthetically created dataset is an combination of various customer payments, made at different intervals and in varying amounts. Through this, we aim to provide a comprehensive understanding of how we can detect and curtail fraudulent activities with high accuracy.\n",
    "\n",
    "In this notebook, you will go through the steps below:\n",
    "\n",
    "1. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "1. [Data Preprocessing](#Data-Preprocessing)\n",
    "1. [Oversampling with SMOTE](#Oversampling-with-SMOTE)\n",
    "1. [K-Neighbours Classifier](#K-Neighbours-Classifier)\n",
    "1. [Random Forest Classifier](#Random-Forest-Classifier)\n",
    "1. [XGBoost Classifier](#XGBoost-Classifier)\n",
    "1. [Logistic Regression Classifier](#Logistic-Regression-Classifier)\n",
    "1. [Model Deployment](#Model-Deployment)\n",
    "1. [Prediction](#Prediction)\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment Overview\n",
    "\n",
    "First, let's go through the overview of this experiment. Below we can see the steps involved in this experiment:\n",
    "\n",
    "- Import all required packages, define helper functions, and initialize global variables.\n",
    "- Process and validate the data.\n",
    "- Initiate the model training process and retrieve the best performing model.\n",
    "- Deploy the model using KServe.\n",
    "- Transform the Notebook in a Kubeflow pipeline using Kale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports & Initialization\n",
    "\n",
    "In this section, you import all the necessary packages that are required for our analysis. These packages provide the tools and functionalities needed to effectively process the data, train our machine learning model, and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import pickle\n",
    "import requests\n",
    "import urllib3\n",
    "import subprocess\n",
    "\n",
    "import joblib\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import (accuracy_score, auc, classification_report,\n",
    "                             confusion_matrix, roc_curve)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "from kubernetes import client\n",
    "\n",
    "# Suppress warnings\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:initialize"
    ]
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"MINIO_HOST_URL\": os.environ[\"MLFLOW_S3_ENDPOINT_URL\"],\n",
    "    \"MINIO_ACCESS_KEY\": os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    \"MINIO_SECRET_KEY\": os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    \"KSERVE_MODEL_NAME\": \"fraud-detection\",\n",
    "    \"NAMESPACE\": open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\", \"r\").read(),\n",
    "    \"BUCKET\": \"experiments\",\n",
    "    \"SOURCE_PATH\": \"dataset/feed.csv\",\n",
    "    \"SERVICE_ACCOUNT\": \"kserve-minio-sa\",\n",
    "    \"PROTOCOL_VERSION\": \"v2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\n",
    "    service_name=\"s3\",\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    endpoint_url=os.environ[\"MLFLOW_S3_ENDPOINT_URL\"],\n",
    "    verify=False)\n",
    "\n",
    "bucket_name = config.get(\"BUCKET\")\n",
    "buckets = client.list_buckets()\n",
    "\n",
    "if not any(bucket['Name'] == bucket_name for bucket in buckets['Buckets']):\n",
    "    client.create_bucket(Bucket=config.get(\"BUCKET\"))\n",
    "\n",
    "if os.path.exists(\"dataset\"):\n",
    "    train_dataset = os.path.join(\"dataset\", \"feed.csv\")\n",
    "\n",
    "    client.upload_file(Filename=train_dataset, \n",
    "                       Bucket=config.get(\"BUCKET\"), \n",
    "                       Key=f\"{train_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define Helper Functions\n",
    "\n",
    "In this section, you define the function for plotting the Receiver Operating Characteristic Area Under the Curve (ROC_AUC). The ROC curve is a powerful diagnostic tool used for assessing the performance of binary classification models. It is a plot with the True Positive Rate (TPR) or sensitivity on the $y$-axis, and the False Positive Rate (FPR) or 1-specificity on the $x$-axis. Both rates range between `0` and `1`. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold.\n",
    "\n",
    "The Area Under the ROC Curve (AUC-ROC) is a single scalar value that aggregates the performance of the classifier over all possible thresholds, providing a measure of the model's ability to distinguish between positive and negative classes. Here's a breakdown of what the AUC-ROC value means:\n",
    "\n",
    "- An AUC of `1.0` indicates that the classifier is perfect - it has a 100% true positive rate and a 0% false positive rate.\n",
    "- An AUC of `0.5` suggests that the classifier is no better than random guessing - it has an equal chance of classifying a positive sample as negative, and vice versa.\n",
    "- An AUC of less than `0.5` implies that the classifier is worse than random guessing - it's as if the model is \"learning\" to make the wrong predictions.\n",
    "\n",
    "By visualizing the ROC curve, you can better understand the trade-off between sensitivity (the ability to correctly classify true positives) and specificity (the ability to correctly classify true negatives). This helps in selecting the optimal threshold that balances both metrics according to the specific needs of a given application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "def init_minio_client():\n",
    "    client = boto3.client(\n",
    "        service_name=\"s3\",\n",
    "        aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        endpoint_url=os.environ[\"MLFLOW_S3_ENDPOINT_URL\"],\n",
    "        verify=False)\n",
    "    return client\n",
    "\n",
    "\n",
    "def plot_roc_auc(y_test, preds):\n",
    "    \"\"\"Plot the Receiver Operating Characteristic (ROC) curve.\"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "In this section, you undertake a detailed Exploratory Data Analysis (EDA) of the dataset, aiming to uncover critical insights that could inform your subsequent analysis. The initial snapshot of the dataset, as seen below, reveals nine feature columns and a target column. The feature columns are as follows:\n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <th style=\"text-align:left\">Feature</th>\n",
    "    <th style=\"text-align:left\">Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Step</td>\n",
    "    <td style=\"text-align:left\">Represents the simulation day. The simulation ran for a total of 180 steps, equivalent to six months.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Customer</td>\n",
    "    <td style=\"text-align:left\">Denotes the unique ID assigned to each customer.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">zipCodeOrigin</td>\n",
    "    <td style=\"text-align:left\">Represents the originating or source zip code.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Merchant</td>\n",
    "    <td style=\"text-align:left\">Specifies the unique ID of the merchant.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">zipMerchant</td>\n",
    "    <td style=\"text-align:left\">Represents the zip code associated with the merchant.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Age</td>\n",
    "    <td style=\"text-align:left\">\n",
    "        Classifies the age of customers:\n",
    "        <ul>\n",
    "            <li>0: <= 18 years</li>\n",
    "            <li>1: 19-25 years</li>\n",
    "            <li>2: 26-35 years</li>\n",
    "            <li>3: 36-45 years</li>\n",
    "            <li>4: 46-55 years</li>\n",
    "            <li>5: 56-65 years</li>\n",
    "            <li>6: > 65 years</li>\n",
    "            <li>U: Unknown</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Gender</td>\n",
    "    <td style=\"text-align:left\">\n",
    "        Indicates the gender of the customer:\n",
    "        <ul>\n",
    "            <li>E : Enterprise</li>\n",
    "            <li>F: Female</li>\n",
    "            <li>M: Male</li>\n",
    "            <li>U: Unknown</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Category</td>\n",
    "    <td style=\"text-align:left\">Denotes the category of the purchase.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Amount</td>\n",
    "    <td style=\"text-align:left\">Specifies the purchase amount.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Fraud</td>\n",
    "    <td style=\"text-align:left\">The target variable, indicates whether the transaction was fraudulent (1) or benign (0).</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "So, let's fetch the dataset from object storage and read it in a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:fraud_detection_dataset",
     "prev:initialize"
    ]
   },
   "outputs": [],
   "source": [
    "file_name = config.get(\"SOURCE_PATH\")\n",
    "\n",
    "client = init_minio_client()\n",
    "client.download_file(Bucket=config.get(\"BUCKET\"), Key=config.get(\"SOURCE_PATH\"), Filename=\"feed.csv\")\n",
    "\n",
    "data = pd.read_csv(\"feed.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, let's take a closer look at the types of data stored in each column and assess whether there are any missing values present in our dataset. Fortunately, this dataset does not contain any missing values, which means you don't have to include any imputation strategies in your preprocessing steps. The absence of missing values simplifies the data cleaning process and allows you to proceed with the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Synthetic Minority Over-sampling TEchnique (SMOTE)\n",
    "\n",
    "Fraudulent data, as you can also observe from the plot below, tends to be imbalanced. This imbalance can lead to a bias towards the majority class during the training of a machine learning model. To counteract this issue, you can utilize techniques such as oversampling or undersampling.\n",
    "\n",
    "- Oversampling refers to the process of augmenting the number of instances in the minority class by generating similar instances.\n",
    "- Undersampling involves reducing the number of instances in the majority class by randomly selecting data points until the count aligns with the minority class.\n",
    "\n",
    "Each of these strategies has associated risks. For instance, oversampling may result in the creation of duplicate or highly similar data points, which may not be beneficial for fraud detection given that fraudulent transactions often exhibit unique characteristics. Undersampling, however, implies a loss of data points, and consequently, valuable information.\n",
    "\n",
    "To balance the dataset without introducing excessive redundancy or losing crucial information, you could employ an oversampling technique known as SMOTE (Synthetic Minority Over-sampling TEchnique). Unlike naive oversampling methods, SMOTE generates synthetic instances of the minority class using neighboring instances, ensuring that the new samples are not exact copies but closely resemble existing instances. This technique can potentially improve the performance of your model by providing it with a more representative and balanced view of the different classes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:data_preprocessing",
     "prev:fraud_detection_dataset"
    ]
   },
   "outputs": [],
   "source": [
    "# Create two dataframes with fraud and non-fraud data \n",
    "df_fraud = data.loc[data.fraud == 1] \n",
    "df_non_fraud = data.loc[data.fraud == 0]\n",
    "\n",
    "sns.countplot(x=\"fraud\",data=data)\n",
    "plt.title(\"Count of Fraudulent Payments\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of normal examples:\", df_non_fraud.fraud.count())\n",
    "print(\"Number of fradulent examples:\", df_fraud.fraud.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Moreover, by examining the data below, it becomes apparent that the 'leisure' and 'travel' categories seem to be most frequently targeted by fraudsters. It appears that these perpetrators might be strategically choosing categories where people typically spend more on average.\n",
    "\n",
    "To validate this hypothesis, you need to delve deeper into the transaction data, specifically comparing the amounts transacted in fraudulent and non-fraudulent cases. By doing this, you could gain a better understanding of the patterns and behaviors of fraudsters, and thus improve the effectiveness of our detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Mean feature values per category:\")\n",
    "data.groupby('category')[['amount', 'fraud']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Upon further analysis, the initial hypothesis—that fraudsters predominantly target categories where average spending is higher—holds true only to a certain extent. However, a clear trend emerges when you examine the transaction values associated with fraudulent activities.\n",
    "\n",
    "As illustrated in the table below, you can confidently assert that a fraudulent transaction is typically significantly larger—about four times or more—than the average transaction within a given category. This significant deviation in transaction amounts may provide a useful indicator when identifying potential fraudulent activities in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Create two dataframes with fraud and non-fraud data \n",
    "pd.concat([df_fraud.groupby('category')['amount'].mean(),\n",
    "           df_non_fraud.groupby('category')['amount'].mean(),\n",
    "           data.groupby('category')['fraud'].mean()*100],\n",
    "           keys=[\"Fraudulent\",\"Non-Fraudulent\",\"Percent(%)\"], axis=1,sort=False).sort_values(by=['Non-Fraudulent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Upon examining the average amount spent across various categories, you can see that spending is typically similar, generally ranging from `0` to `500`, once you exclude the outliers. However, the 'travel' category stands as an exception, with spending reaching significantly higher levels.\n",
    "\n",
    "This deviation in the 'travel' category could be due to a variety of factors, such as the inherent high cost associated with travel and tourism activities. Such information is crucial, not only for understanding the spending behavior of customers but also for improving our fraud detection strategies, as categories with higher average spending might attract more fraudulent activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot histograms of the amounts in fraud and non-fraud data \n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.boxplot(x=data.category, y=data.amount)\n",
    "plt.title(\"Boxplot for the Amount spend in category\")\n",
    "plt.ylim(0, 4000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Reinforcing previous observations, the histogram below presents a striking representation of the relationship between the number and amount of fraudulent transactions. While the count of fraudulent transactions is relatively low, the monetary value they represent is disproportionately high. This pattern underscores the serious financial implications of fraud, even when the number of incidents might appear relatively minor at first glance. It's precisely this disparity that makes effective and precise fraud detection systems crucial for the integrity of any financial system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot histograms of the amounts in fraud and non-fraud data \n",
    "plt.hist(df_fraud.amount, alpha=0.5, label='fraud',bins=100)\n",
    "plt.hist(df_non_fraud.amount, alpha=0.5, label='nonfraud',bins=100)\n",
    "plt.title(\"Histogram for fraudulent and nonfraudulent payments\")\n",
    "plt.ylim(0, 10000)\n",
    "plt.xlim(0, 1000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Examining the table below, you can observe the percentage of fraudulent transactions within each age category. Among known age categories, the group '0' (representing ages 18 and under) exhibits the highest fraud percent, standing at `1.957586`. This data is crucial for enhancing our understanding of the demographics most vulnerable to fraudulent activities and can be instrumental in tailoring our fraud detection algorithms and preventive measures accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "((data.groupby('age')['fraud'].mean() * 100).reset_index()\n",
    "                                            .rename(columns={'age':'Age','fraud' : 'Fraud Percent'})\n",
    "                                            .sort_values(by='Fraud Percent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Data Preprocessing**\n",
    "\n",
    "In this section, you will focus on preprocessing the data and preparing it for the training phase. Upon investigating our data, you can see that there are two columns with only one unique zip code value. In terms of machine learning, a feature with a single value adds no predictive power, since it remains constant for all observations. Therefore, you could drop this column from the dataset to streamline the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Unique 'zipCodeOri' values:\", data.zipcodeOri.nunique())\n",
    "print(\"Unique 'zipMerchant' values:\", data.zipMerchant.nunique())\n",
    "# dropping zipcodeori and zipMerchant since they have only one unique value\n",
    "data_reduced = data.drop([\"zipcodeOri\", \"zipMerchant\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, you will convert the categorical features into numerical values. One efficient way to transform categorical values into numerical representations is by utilizing the pandas library's `cat.codes` property. This method allows us to encode categorical variables into numerical codes without significantly increasing the dimensionality of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turning object columns type to categorical for easing the transformation process\n",
    "col_categorical = data_reduced.select_dtypes(include= ['object']).columns\n",
    "for col in col_categorical:\n",
    "    data_reduced[col] = data_reduced[col].astype('category')\n",
    "# categorical values ==> numeric values\n",
    "data_reduced[col_categorical] = data_reduced[col_categorical].apply(lambda x: x.cat.codes)\n",
    "data_reduced.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To proceed with our model training, you need to define our independent variable ($X$) and dependent/target variable ($y$). In this context, the independent variable ($X$) refers to the set of features or attributes that you will use to predict the dependent/target variable ($y$). By properly defining $X$ and $y$, you can establish the foundation for training your machine learning model and exploring the relationships between the independent variables and the dependent/target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:"
    ]
   },
   "outputs": [],
   "source": [
    "X = data_reduced.drop(['fraud'], axis=1)\n",
    "y = data['fraud']\n",
    "\n",
    "print(X.head(), \"\\n\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Oversampling with SMOTE\n",
    "\n",
    "To address the issue of class imbalance in the dataset, you will apply the Synthetic Minority Over-sampling TEchnique (SMOTE). As you saw earlier, this oversampling technique generates synthetic instances of the minority class (fraudulent transactions) by interpolating between existing instances. By applying SMOTE, you will effectively increase the number of instances in the minority class to match the number of instances in the majority class (non-fraudulent transactions). As a result, you will have an equal number of instances for both classes, which helps to alleviate the potential bias and improve the performance of your machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:oversampling_with_smote",
     "prev:data_preprocessing"
    ]
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "y_res = pd.DataFrame(y_res)\n",
    "\n",
    "print(y_res.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train-Test Split for Model Performance Measurement\n",
    "\n",
    "To assess the performance of your machine learning model, you should split the data into two sets: a training set and a testing set. The training set will be used to train the model, allowing it to learn patterns and relationships within the data. The testing set, on the other hand, will be used to evaluate the model's performance on unseen data. By measuring the model's performance on the testing set, you can gain insights into how well it generalizes to new and unseen instances.\n",
    "\n",
    "While cross-validation is a commonly recommended practice for model evaluation, in this case, due to the large number of instances in the dataset, you could opt for a simple train-test split. However, it is important to note that cross-validation should be used whenever feasible, as it provides a more comprehensive evaluation of the model's performance and helps to mitigate potential biases introduced by a single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:"
    ]
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As mentioned earlier, fraud datasets often suffer from severe class imbalance, where the majority of instances are non-fraudulent transactions. If you were to naively predict non-fraudulent for all instances in such imbalanced datasets, we would achieve a high accuracy score, typically around 99%. However, this misleading accuracy score does not indicate a successful fraud detection system. In reality, the goal of a fraud detection classifier is to identify the fraudulent transactions accurately, which are the minority class in the dataset.\n",
    "\n",
    "To accurately evaluate the performance of a fraud detection classifier, it is essential to consider metrics that are sensitive to both the minority (fraudulent) and majority (non-fraudulent) classes. Metrics such as precision, recall, F1-score, and the area under the Receiver Operating Characteristic (ROC) curve provide a more comprehensive assessment of the model's effectiveness in detecting fraud.\n",
    "\n",
    "In addition, it is important to establish a baseline accuracy score that exceeds the accuracy achieved by simply predicting the majority class (non-fraudulent). This baseline accuracy serves as a benchmark for evaluating the performance of the fraud detection model, ensuring that it performs significantly better than a simplistic approach and demonstrates its ability to accurately detect fraudulent transactions.\n",
    "\n",
    "Therefore, when evaluating the performance of a fraud detection model, it is imperative to consider multiple metrics that provide a comprehensive understanding of its effectiveness in detecting both fraudulent and non-fraudulent instances, rather than relying solely on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# The base score should be better than predicting always non-fraduelent\n",
    "print(\"Base accuracy score we must beat is: \", \n",
    "      df_non_fraud.fraud.count() / np.add(df_non_fraud.fraud.count(),df_fraud.fraud.count()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# K-Neighbours Classifier\n",
    "\n",
    "The K-Nearest Neighbors (KNN) classifier is a popular machine learning algorithm used for classification. In this section, you will train a KNN classifier as a potential approach for the fraud detection problem.\n",
    "\n",
    "The KNN classifier works based on the principle that instances with similar feature values tend to belong to the same class. It classifies new instances by finding the K nearest neighbors in the training set and assigning the majority class label among those neighbors to the new instance.\n",
    "\n",
    "Key features of the KNN classifier include:\n",
    "\n",
    "- K value: The value of K represents the number of neighbors to consider for classification. It is an important parameter that needs to be carefully chosen to achieve optimal performance.\n",
    "- Distance metric: The choice of distance metric, such as Euclidean distance or Manhattan distance, determines the similarity between instances and influences the classification process.\n",
    "- Computational cost: The KNN classifier can be computationally expensive, especially for large datasets, as it requires calculating distances between the new instance and all training instances. Therefore, it is essential to consider the computational trade-offs when working with KNN.\n",
    "\n",
    "By employing the KNN classifier, we can leverage the proximity-based nature of the algorithm to detect potential fraud patterns in our data. However, it is important to experiment with different values of K and evaluate the model's performance using appropriate evaluation metrics to ensure its effectiveness in detecting fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:knn_classifier",
     "prev:oversampling_with_smote"
    ]
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, p=1)\n",
    "knn.fit(X_train, np.ravel(y_train, order='C'))\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Classification Report for K-Nearest Neighbours: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix of K-Nearest Neigbours: \\n\", confusion_matrix(y_test, y_pred))\n",
    "plot_roc_auc(y_test, knn.predict_proba(X_test)[:,1])\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# save the model to minio s3 object\n",
    "filename = \"model.pkl\"\n",
    "pickle.dump(knn, open(f\"{filename}\", \"wb\"))\n",
    "\n",
    "minio_client = init_minio_client()\n",
    "minio_client.upload_file(Filename=filename, \n",
    "                         Bucket=config.get(\"BUCKET\"), \n",
    "                         Key=f\"banking/pickles/k-neighbors/model/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "The Random Forest classifier is a powerful and versatile machine learning algorithm widely used for classification. In the context of our fraud detection problem, let's explore the Random Forest classifier as a potential approach.\n",
    "\n",
    "The Random Forest algorithm is an ensemble method that works by constructing a multitude of decision trees and aggregating their predictions to make the final classification. Each decision tree in the Random Forest is trained on a different subset of the data, using a random selection of features. This randomness helps to reduce overfitting and improve the generalization ability of the model.\n",
    "\n",
    "Key features of the Random Forest classifier include:\n",
    "\n",
    "- Ensemble learning: The Random Forest classifier combines the predictions of multiple decision trees to make a more robust and accurate prediction. The ensemble approach helps to mitigate the risk of individual decision trees making errors.\n",
    "- Feature importance: The Random Forest classifier provides a measure of feature importance, indicating the relative importance of each feature in making predictions. This information can be valuable for understanding the key factors contributing to fraudulent transactions.\n",
    "- Parallelization: The Random Forest algorithm lends itself well to parallelization, as each decision tree in the forest can be trained independently. This makes it suitable for large datasets and can lead to faster training times.\n",
    "\n",
    "When applying the Random Forest classifier to our fraud detection problem, it is crucial to tune hyperparameters, such as the number of decision trees in the forest and the maximum depth of each tree, to optimize performance. Additionally, evaluating the model's performance using appropriate metrics and considering feature importance can provide valuable insights for fraud detection and prevention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:random_forest_classifier",
     "prev:oversampling_with_smote"
    ]
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=50,\n",
    "                                max_depth=8,\n",
    "                                random_state=42,\n",
    "                                verbose=1,\n",
    "                                class_weight=\"balanced\")\n",
    "rf_clf.fit(X_train, y_train.values.ravel())\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "print(\"Classification Report for Random Forest Classifier: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix of Random Forest Classifier: \\n\", confusion_matrix(y_test, y_pred))\n",
    "plot_roc_auc(y_test, rf_clf.predict_proba(X_test)[:,1])\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# save the model to minio s3 object\n",
    "filename = \"model.pkl\"\n",
    "pickle.dump(rf_clf, open(f\"{filename}\", \"wb\"))\n",
    "\n",
    "minio_client = init_minio_client()\n",
    "minio_client.upload_file(Filename=filename, \n",
    "                         Bucket=config.get(\"BUCKET\"), \n",
    "                         Key=f\"banking/pickles/random_forest/model/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# XGBoost Classifier\n",
    "\n",
    "The XGBoost (Extreme Gradient Boosting) classifier is a state-of-the-art machine learning algorithm known for its exceptional performance and widespread use in various domains, including fraud detection. Let's delve into the XGBoost classifier and its relevance to our fraud detection problem.\n",
    "\n",
    "XGBoost is an ensemble learning method that combines the power of gradient boosting with several innovative techniques. It excels at handling large-scale datasets and effectively capturing complex relationships between features. The algorithm constructs a series of decision trees iteratively, where each subsequent tree corrects the mistakes made by the previous trees.\n",
    "\n",
    "Key features of the XGBoost classifier include:\n",
    "\n",
    "- Gradient boosting: XGBoost utilizes gradient boosting, a technique that sequentially adds decision trees to improve the model's predictive accuracy. By iteratively minimizing a specified loss function, XGBoost focuses on capturing intricate patterns and relationships in the data.\n",
    "- Feature importance: XGBoost provides valuable insights into feature importance by quantifying the impact of each feature on the model's performance. This information aids in identifying the most influential features for detecting fraudulent transactions.\n",
    "- Parallel processing: XGBoost supports parallel processing, enabling faster training times and efficient computation on large-scale datasets. It leverages the capabilities of multicore processors and distributed computing frameworks for accelerated model training.\n",
    "\n",
    "When employing the XGBoost classifier for fraud detection, it is crucial to tune hyperparameters, such as the learning rate, maximum depth of trees, and regularization parameters, to optimize performance. Evaluating the model's performance using appropriate metrics and considering feature importance can enhance the effectiveness of our fraud detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:xgboost_classifier",
     "prev:oversampling_with_smote"
    ]
   },
   "outputs": [],
   "source": [
    "XGBoost_CLF = xgb.XGBClassifier(silent=None, seed=42, colsample_bynode=1, max_depth=6, learning_rate=0.05, n_estimators=50, \n",
    "                                objective=\"binary:hinge\", booster='gbtree', missing=1,\n",
    "                                n_jobs=-1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, \n",
    "                                subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n",
    "                                base_score=0.5, random_state=42, verbosity=1)\n",
    "XGBoost_CLF.fit(X_train, y_train)\n",
    "y_pred = XGBoost_CLF.predict(X_test)\n",
    "\n",
    "print(\"Classification Report for XGBoost: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix of: \\n\", confusion_matrix(y_test, y_pred))\n",
    "plot_roc_auc(y_test, XGBoost_CLF.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Get accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# save the model to minio s3 object\n",
    "filename = \"model.pkl\"\n",
    "pickle.dump(XGBoost_CLF, open(f\"{filename}\", \"wb\"))\n",
    "\n",
    "minio_client = init_minio_client()\n",
    "minio_client.upload_file(Filename=filename, \n",
    "                         Bucket=config.get(\"BUCKET\"), \n",
    "                         Key=f\"banking/pickles/xgb/model/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression Classifier\n",
    "\n",
    "The Logistic Regression classifier is a well-established and widely used algorithm for binary classification tasks, making it relevant to our fraud detection problem. Let's explore the Logistic Regression classifier and its applicability to our scenario.\n",
    "\n",
    "Despite its name, Logistic Regression is a classification algorithm that models the probability of an instance belonging to a particular class. It is particularly suited for problems where the dependent variable is binary, as in our case where we aim to distinguish between fraudulent and non-fraudulent transactions.\n",
    "\n",
    "Key features of the Logistic Regression classifier include:\n",
    "\n",
    "- Probabilistic modeling: Logistic Regression models the relationship between the independent variables and the probability of belonging to a specific class. It employs the logistic function (also known as the sigmoid function) to map the output to a probability score.\n",
    "- Interpretability: Logistic Regression provides interpretable coefficients for each independent variable, which allows us to understand the impact of the features on the likelihood of fraud. These coefficients indicate the direction and magnitude of the relationship between each feature and the probability of fraudulent transactions.\n",
    "- Efficiency: Logistic Regression is computationally efficient and can handle large datasets with relative ease. It converges quickly and is less prone to overfitting, making it suitable for situations where interpretability and simplicity are important factors.\n",
    "\n",
    "When utilizing the Logistic Regression classifier for fraud detection, it is crucial to preprocess the data appropriately, handle categorical variables, and consider feature scaling if necessary. Evaluating the model's performance using appropriate metrics such as precision, recall, F1-score, and area under the ROC curve can provide a comprehensive understanding of its effectiveness in detecting fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:logistic_regression_cls",
     "prev:oversampling_with_smote"
    ]
   },
   "outputs": [],
   "source": [
    "LRmodel = LogisticRegression(max_iter=999, solver='lbfgs')\n",
    "LRmodel.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = LRmodel.predict(X_test)\n",
    "print(\"Classification Report for LogisticRegression: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix of LogisticRegression: \\n\", confusion_matrix(y_test, y_pred))\n",
    "plot_roc_auc(y_test, LRmodel.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Get accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# save the model to minio s3 object\n",
    "filename = \"model.pkl\"\n",
    "pickle.dump(LRmodel, open(f\"{filename}\", \"wb\"))\n",
    "\n",
    "minio_client = init_minio_client()\n",
    "minio_client.upload_file(Filename=filename, \n",
    "                         Bucket=config.get(\"BUCKET\"), \n",
    "                         Key=f\"banking/pickles/logisticregression/model/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Deployment\n",
    "\n",
    "After training and evaluating our fraud detection model, the next step is to deploy it into a production environment where it can be used to detect fraud in real-time transactions.\n",
    "\n",
    "Before diving into deployment, you should start by creating a secure environment for accessing the S3 endpoint. First, you define a Secret object, which securely holds the necessary credentials. Additionally, we create a ServiceAccount object, associating it with the secret to establish an identity for the deployment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "manifest = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: kserve-minio-sa\n",
    "secrets:\n",
    "- name: {os.environ['USER']}-objectstore-secret\n",
    "\n",
    "---\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"fraud-detection\"\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: kserve-minio-sa\n",
    "    sklearn:\n",
    "      protocolVersion: \"v2\"\n",
    "      storageUri: \"s3://{config['BUCKET']}/banking/pickles/logisticregression/model\"\n",
    "\"\"\"\n",
    "\n",
    "os.makedirs(\"manifests\", exist_ok=True)\n",
    "\n",
    "with open(os.path.join(\"manifests\", \"isvc.yaml\"), \"w\") as f:\n",
    "    f.write(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = subprocess.run([\"kubectl\", \"apply\", \"-f\", \"manifests/isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prediction\n",
    "\n",
    "With the deployed fraud detection model, we can now utilize it to make predictions on new, incoming transactions. The prediction process involves passing the relevant transaction data through the deployed model to obtain a prediction of whether the transaction is fraudulent or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip",
     "prev:model_deploy"
    ]
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(\"dataset\", \"generated-data.csv\"))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "data_reduced = data.drop(['zipcodeOri', 'zipMerchant'], axis=1)\n",
    "data_reduced.loc[:, ['customer', 'merchant', 'category']].astype('category')\n",
    "\n",
    "col_categorical = data_reduced.select_dtypes(include= ['object']).columns\n",
    "for col in col_categorical:\n",
    "    data_reduced[col] = data_reduced[col].astype('category')\n",
    "\n",
    "data_reduced[col_categorical] = data_reduced[col_categorical].apply(lambda x: x.cat.codes)\n",
    "data_reduced.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "DOMAIN_NAME = \"svc.cluster.local\"  # change this to your domain for external access\n",
    "NAMESPACE = config.get(\"NAMESPACE\")\n",
    "DEPLOYMENT_NAME = config.get(\"KSERVE_MODEL_NAME\")\n",
    "MODEL_NAME = DEPLOYMENT_NAME\n",
    "SVC = f'{DEPLOYMENT_NAME}-predictor-default.{NAMESPACE}.{DOMAIN_NAME}'\n",
    "URL = f\"https://{SVC}/v2/models/{MODEL_NAME}/infer\"\n",
    "\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "X = data_reduced.drop(['fraud'], axis=1)\n",
    "y = data_reduced['fraud']\n",
    "print(\"Shape:\", [len(X.values), len(X.values[0])])\n",
    "\n",
    "inference_request = {\n",
    "    \"inputs\" : [{\n",
    "        \"name\" : \"fraud-detection-infer-001\",\n",
    "        \"datatype\": \"FP32\",\n",
    "\n",
    "        \"shape\": [1, 7],\n",
    "        # Example of non-fraudulent Transaction Dtls\n",
    "        # \"data\": [list(item) for item in X.values][14],\n",
    "        # Example of a fraudulent request\n",
    "        \"data\": [list(item) for item in X.values][17],\n",
    "    }]\n",
    "}\n",
    "\n",
    "print(\"data:\", inference_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "message = {\"message\":\"\", \"value\":\"\"}\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ['AUTH_TOKEN']}\"}\n",
    "response = requests.post(URL, json=inference_request, headers=headers, verify=False)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    if json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'][0] != None and json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'][0] == 1:\n",
    "        message['message'] = \"Fraudulent Banking Transaction!\"\n",
    "        message['value'] = json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'][0]        \n",
    "        print('\\033[91m' \"Prediction Result:\", json.dumps(message))\n",
    "    elif len(json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'])>1:\n",
    "        print(\"Model-Infer-dtl:[data]:\\n\", json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'])\n",
    "    else:\n",
    "        message['message'] = \"Non-fraudulent Banking Transaction!\"\n",
    "        message['value'] = json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'][0]   \n",
    "        print('\\033[92m'  \"Prediction Result:\", json.dumps(message))\n",
    "else:\n",
    "    print(response.status_code, response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, the primary objective was to develop a fraud detection model using bank payment data. We employed various classifiers and achieved remarkable results in detecting fraudulent transactions. As fraud datasets often suffer from class imbalance, we utilized the SMOTE oversampling technique to address this issue by generating synthetic minority class examples.\n",
    "\n",
    "# References\n",
    "\n",
    "1. Lavion, Didier; et al, \"PwC's Global Economic Crime and Fraud Survey 2022\", \n",
    "2. https://www.pwc.com/gx/en/services/forensics/economic-crime-survey.html\n",
    "3. https://www.pwc.com/gx/en/services/forensics/gecs/outcomes-of-platform-fraud.svg |\n",
    "4. https://www.pwc.com/gx/en/forensics/gecsm-2022/pdf/PwC%E2%80%99s-Global-Economic-Crime-and-Fraud-Survey-2022.pdf\n",
    "5. [SMOTE: Synthetic Minority Over-sampling Technique](https://jair.org/index.php/jair/article/view/10302)\n",
    "6. [Banksim Data Set Paper](http://www.msc-les.org/proceedings/emss/2014/EMSS2014_144.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gcr.io/mapr-252711/kubeflow/notebooks/jupyter-data-science:ezaf-fy23-q3",
   "experiment": {
    "id": "bdd12faf-cf17-44ab-9df9-f12d162cac08",
    "name": "stage-experiment"
   },
   "experiment_name": "stage-experiment",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "fraud-detection-exp-14-06-2023-02",
   "pipeline_name": "fraud-detection-exp-14-06-2023-02",
   "snapshot_volumes": false,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:add-external-df-volume:true",
    "label:add-ldapcert-secret:true",
    "label:add-sssd-secret:true",
    "label:add-user-s3-secret:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "836de0bc6c3ec877b14e515fee0e932bbf60b1fe66c7ecc90fa579a75883c3a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
