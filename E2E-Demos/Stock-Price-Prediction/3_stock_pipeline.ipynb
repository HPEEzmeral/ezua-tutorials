{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5951e952-aeea-42d9-8cd5-e0701925bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import create_component_from_func,InputPath,OutputPath,func_to_container_op\n",
    "from kubernetes.client.models import V1EnvVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531e8e95-0627-4e53-a5a4-1fb1a4658e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_proxy = \"http://hpeproxy.its.hpecorp.net:443\"\n",
    "https_proxy = \"http://hpeproxy.its.hpecorp.net:443\"\n",
    "\n",
    "# Set the environment variables\n",
    "os.environ['HTTP_PROXY'] = http_proxy\n",
    "os.environ['HTTPS_PROXY'] = https_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ceff4cf1-736c-4872-8c97-6a94be7c4a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(output_csv:OutputPath(),\n",
    "              csv_url: str) -> None:\n",
    "    import pandas as pd\n",
    "    df=pd.read_csv(csv_url)\n",
    "    print(df.head())\n",
    "    with open(output_csv,\"w\") as f:\n",
    "        df.to_csv(f,index=False)\n",
    "\n",
    "\n",
    "def preprocess_data(input_data: InputPath(),\n",
    "                    output_csv:OutputPath(),\n",
    "                    scale_factor:OutputPath(),\n",
    "                    start_date: str ,\n",
    "                    end_date: str) -> None:\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import pickle\n",
    "    with open(input_data) as f:\n",
    "        df = pd.read_csv(f)\n",
    "    \n",
    "    df = df.loc[(df[\"Date\"]>=start_date) & (df[\"Date\"]<=end_date)]  \n",
    "    df['Date'] = pd.to_datetime(df['Date'],format = \"%Y-%m-%d\").dt.date\n",
    "    df=df.set_index(\"Date\")\n",
    "    df.index.freq = 'D' \n",
    "    \n",
    "    date_range = pd.DataFrame(pd.date_range(start=start_date, end=end_date),columns=[\"Date\"])\n",
    "    date_range['Date'] = date_range['Date'].dt.date\n",
    "    date_range=date_range.set_index(\"Date\")\n",
    "    date_range.index.freq = 'D'\n",
    "    \n",
    "    df = pd.merge(date_range, df, how='left',left_index=True, right_index=True).fillna(method = 'ffill')\n",
    "    df.dropna(inplace=True)\n",
    "    df=df[['Open','Close']]\n",
    "    print(df.head())\n",
    "    \n",
    "    scale = MinMaxScaler()\n",
    "    df[df.columns] = scale.fit_transform(df)\n",
    "    with open(scale_factor,'wb') as f:\n",
    "        pickle.dump(scale,f)\n",
    "    \n",
    "    with open(output_csv,\"w\") as f:\n",
    "        df.to_csv(f,index=False)\n",
    "        \n",
    "    print(\"preprocessed the data and scale object saved\")    \n",
    "    \n",
    "\n",
    "def get_train_test_data(input_data: InputPath(),\n",
    "                        train_percent:float,\n",
    "                        train_data: OutputPath(),\n",
    "                        test_data: OutputPath()) -> None:\n",
    "    \n",
    "    import pandas as pd\n",
    "    with open(input_data) as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "    training_size = round(len(df) * train_percent)\n",
    "    train, test = df[:training_size], df[training_size:]\n",
    "    print(train.head())\n",
    "    print(test.head())\n",
    "            \n",
    "    with open(train_data,\"w\") as f:\n",
    "        train.to_csv(f,index=False)  \n",
    "\n",
    "    with open(test_data,\"w\") as f:\n",
    "        test.to_csv(f,index=False) \n",
    "\n",
    "        \n",
    "def create_sequence(input_data: InputPath(),\n",
    "                    seq_data: OutputPath(),\n",
    "                    label_data: OutputPath()) -> None:\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    with open(input_data,'r') as f:\n",
    "        dataset = pd.read_csv(input_data)\n",
    "        \n",
    "    sequences = []\n",
    "    labels = []\n",
    "    start_idx = 0\n",
    "\n",
    "    for stop_idx in range(60,len(dataset)): \n",
    "        sequences.append(dataset.iloc[start_idx:stop_idx])\n",
    "        labels.append(dataset.iloc[stop_idx])\n",
    "        start_idx += 1\n",
    "    \n",
    "    with open(seq_data,'wb') as f:\n",
    "        np.save(f,np.array(sequences))\n",
    "        \n",
    "    with open(label_data,'wb') as f:\n",
    "        np.save(f,np.array(labels))    \n",
    "        \n",
    "        \n",
    "\n",
    "def model_training(\n",
    "                   train_seq_data: InputPath(),\n",
    "                   train_label_data: InputPath(),\n",
    "                   test_seq_data: InputPath(),\n",
    "                   test_label_data: InputPath(),\n",
    "                   loss: str,\n",
    "                   optimizer: str,\n",
    "                   metrics: str,\n",
    "                   epochs:int,\n",
    "                   trained_model: OutputPath()) -> None:\n",
    "    \n",
    "    print(\"loading libraries\")\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, LSTM\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"loaded libraries\")\n",
    "    \n",
    "    with open(train_seq_data,'rb') as f:\n",
    "        train_seq = np.load(f)\n",
    "    with open(train_label_data,'rb') as f:\n",
    "        train_label = np.load(f)  \n",
    "    with open(test_seq_data,'rb') as f:\n",
    "        test_seq = np.load(f)\n",
    "    with open(test_label_data,'rb') as f:\n",
    "        test_label = np.load(f)\n",
    "        \n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=60, return_sequences=True, input_shape = (train_seq.shape[1],train_seq.shape[2])))\n",
    "    model.add(Dropout(0.1)) \n",
    "    model.add(LSTM(units=60))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metrics])\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model.fit(train_seq, train_label, epochs=epochs ,validation_data=(test_seq, test_label), verbose=1)\n",
    "    print(\"Training done\")\n",
    "    \n",
    "    with open(trained_model,'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "\n",
    "        \n",
    "def model_evaluate(trained_model:InputPath(),\n",
    "                   test_seq_data:InputPath(),\n",
    "                   test_label_data: InputPath(),\n",
    "                   scale_factor:InputPath(),\n",
    "                   model_metrices: OutputPath())-> None:\n",
    "    \n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "    with open(trained_model,'rb') as f:\n",
    "        model = pickle.load(f)   \n",
    "    with open(scale_factor,'rb') as f:\n",
    "        scale = pickle.load(f) \n",
    "    with open(test_seq_data,'rb') as f:\n",
    "        test_seq = np.load(f)\n",
    "    with open(test_label_data,'rb') as f:\n",
    "        test_label = np.load(f)\n",
    "        \n",
    "     \n",
    "    pred = model.predict(test_seq)\n",
    "    pred = scale.inverse_transform(pred)\n",
    "    label = scale.inverse_transform(test_label)\n",
    "    \n",
    "    mae = mean_absolute_error(label, pred)\n",
    "    rmse = mean_squared_error(label, pred, squared=False)\n",
    "    \n",
    "    metric = {\"Mean Absolute Error\":mae,\"Root Mean Square Error\":rmse}\n",
    "    print(metric)\n",
    "\n",
    "    with open(model_metrices,'wb') as f:\n",
    "        pickle.dump(metric,f)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def logg_env_function():\n",
    "    import os\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    env_http = os.getenv('HTTP_PROXY')\n",
    "    env_https = os.getenv('HTTPS_PROXY')\n",
    "    logging.info('The environment variable is: {}'.format(env_http,env_https))\n",
    "        \n",
    "\n",
    "logg_env_function_op = create_component_from_func(logg_env_function, base_image='python:3.8')\n",
    "\n",
    "\n",
    "step_read_data=create_component_from_func(\n",
    "    func = read_data,\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=['pandas==1.2.4']\n",
    ")\n",
    "\n",
    "step_preprocess_data=create_component_from_func(\n",
    "    func = preprocess_data,\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=['pandas==1.2.4','scikit-learn==0.24.2']\n",
    ")\n",
    "\n",
    "step_get_train_test_data=create_component_from_func(\n",
    "    func = get_train_test_data,\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=['pandas==1.2.4']\n",
    ")\n",
    "\n",
    "step_create_sequence=create_component_from_func(\n",
    "    func = create_sequence,\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=['pandas==1.2.4','numpy==1.22.4']\n",
    ")\n",
    "\n",
    "step_model_training=create_component_from_func(\n",
    "    func = model_training,\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=['numpy==1.22.4','tensorflow-cpu==2.7.4']\n",
    ")\n",
    "\n",
    "step_model_evaluate=create_component_from_func(\n",
    "    func = model_evaluate,\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=['numpy==1.22.4','scikit-learn==0.24.2','tensorflow-cpu==2.7.4']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7481c945-79de-4e8c-a3d4-42b80564f107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/0eb5fc98-68f8-4d2d-a40a-d3243de673d5\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/69e07b01-c83a-4e57-b47d-57b406cf0913\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Stock Price Prediction Pipeline',\n",
    "  description='Stock Price Prediction Pipeline for POC'\n",
    ")\n",
    "def stock_pipeline(csv_url:str,\n",
    "                   start_date:str,\n",
    "                   end_date:str,\n",
    "                   train_percent:float,\n",
    "                   loss: str,\n",
    "                   optimizer: str,\n",
    "                   metrics: str,\n",
    "                   epochs:int):\n",
    "    \n",
    "\n",
    "    env_var_http = V1EnvVar(name='HTTP_PROXY', value='http://hpeproxy.its.hpecorp.net:443')\n",
    "    env_var_https = V1EnvVar(name='HTTPS_PROXY', value='http://hpeproxy.its.hpecorp.net:443')   \n",
    "    \n",
    "    \n",
    "    container_op1 = logg_env_function_op().add_env_variable(env_var_https).add_env_variable(env_var_http)\n",
    "    \n",
    "    #STEP1: Task to read the file from source\n",
    "    read_data_task = step_read_data(csv_url=csv_url).add_env_variable(env_var_https).add_env_variable(env_var_http)\n",
    "\n",
    "    \n",
    "    #STEP2: Task to preprocess the data \n",
    "    preprocess_data_task = step_preprocess_data(read_data_task.outputs['output_csv'],\n",
    "                                                start_date=start_date,\n",
    "                                                end_date=end_date).add_env_variable(env_var_https).add_env_variable(env_var_http)\n",
    "    \n",
    "    preprocess_data_task.after(read_data_task)\n",
    "    \n",
    "    \n",
    "    #STEP3: Task to split the data into training and test\n",
    "    train_test_task = step_get_train_test_data(preprocess_data_task.outputs['output_csv'],\n",
    "                                               train_percent=train_percent).add_env_variable(env_var_https).add_env_variable(env_var_http) \n",
    "    \n",
    "    train_test_task.after(preprocess_data_task)\n",
    "    \n",
    "    \n",
    "    #STEP4 - a: Task to create the input sequence and output label from training data\n",
    "    train_seq_task = step_create_sequence(train_test_task.outputs['train_data']).add_env_variable(env_var_https).add_env_variable(env_var_http)\n",
    "    train_seq_task.after(train_test_task)\n",
    "    \n",
    "    #STEP4 - b: Task to create the input sequence and output label from test data\n",
    "    test_seq_task = step_create_sequence(train_test_task.outputs['test_data']).add_env_variable(env_var_https).add_env_variable(env_var_http)\n",
    "    test_seq_task.after(train_test_task)\n",
    "    \n",
    " \n",
    "    #STEP5: Task to define and train the model\n",
    "    task_model_training = step_model_training(train_seq_task.outputs['seq_data'],\n",
    "                                              train_seq_task.outputs['label_data'],\n",
    "                                              test_seq_task.outputs['seq_data'],\n",
    "                                              test_seq_task.outputs['label_data'],\n",
    "                                              loss = loss,\n",
    "                                              optimizer = optimizer,\n",
    "                                              metrics = metrics,\n",
    "                                              epochs = epochs\n",
    "                                             ).add_env_variable(env_var_https).add_env_variable(env_var_http) \n",
    "    \n",
    "    task_model_training.after(train_seq_task)\n",
    "    task_model_training.after(test_seq_task)\n",
    "    \n",
    "    \n",
    "    #STEP6: Task to evaluate the model on different metrices\n",
    "    model_evaluate_task = step_model_evaluate(task_model_training.outputs['trained_model'],\n",
    "                                              test_seq_task.outputs['seq_data'],\n",
    "                                              test_seq_task.outputs['label_data'],\n",
    "                                              preprocess_data_task.outputs['scale_factor']\n",
    "                                             ).add_env_variable(env_var_https).add_env_variable(env_var_http) \n",
    "    \n",
    "    model_evaluate_task.after(task_model_training)\n",
    "    \n",
    "    # To disable cache\n",
    "    # read_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    # preprocess_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    # train_test_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    # train_seq_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    # test_seq_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    # task_model_training.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    # model_evaluate_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "experiment_name = \"stock-pipeline \"+str(datetime.datetime.now().date())\n",
    "run_name = stock_pipeline.__name__+'_run'\n",
    "namespace='hpedemo-user01'\n",
    "\n",
    "arguments = {\"csv_url\":\"https://raw.githubusercontent.com/snairharikrishnan/test/main/ASIANPAINT.csv\",\n",
    "             \"start_date\":\"2014-01-01\",\n",
    "             \"end_date\":\"2019-12-31\",\n",
    "             \"train_percent\":\"0.80\",\n",
    "             \"loss\":\"mean_squared_error\",\n",
    "             \"optimizer\":\"adam\",\n",
    "             \"metrics\":\"mean_absolute_error\",\n",
    "             \"epochs\":\"5\"\n",
    "            }\n",
    "\n",
    "client = kfp.Client()\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func=stock_pipeline,\n",
    "                                                  experiment_name=experiment_name,\n",
    "                                                  run_name=run_name,\n",
    "                                                  arguments=arguments\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607d7a5-1f6d-4aee-9f41-3a029972ba94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
