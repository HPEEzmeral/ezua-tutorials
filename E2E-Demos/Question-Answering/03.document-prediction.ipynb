{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972db2df-307f-4492-80c6-e84082d778f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Invoking and Testing the Vector Store Inference Service (Optional)\n",
    "\n",
    "Welcome to the third part of our tutorial series on building a question-answering application over a corpus of private documents using Large Language Models (LLMs). In our previous Notebooks, you've embarked on the journey of transforming unstructured text data into structured vector embeddings and deploying an Inference Service to serve the Vector Store that holds these embeddings.\n",
    "\n",
    "In this optional Notebook, you will focus on invoking the Vector Store Inference Service you've created and testing its performance. This is an essential step, as it allows us to verify the functionality of your service and observe how it performs in practice. Throughout this Notebook, we will guide you on how to construct suitable requests, communicate with the service, and interpret the responses.\n",
    "\n",
    "By the end of this Notebook, you will gain practical insights into the workings of the Vector Store Inference Service and will be well-prepared to integrate it into a larger system, alongside the Large Language Model Inference Service that you will create in the subsequent Notebook.\n",
    "\n",
    "Let's get started! As always, let's import the libraries you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fd850-d35a-476f-ba05-b11763ddec68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f8bb43-af00-4dae-bf22-dec236bcafe7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, you need to construct the URL you'll hit and define the payload for the POST request you'll send. For this example. you'll be using the V1 inference protocol, which is described below:\n",
    "\n",
    "| API          | Verb | Path                          | Request Payload   | Response Payload                  |\n",
    "|--------------|------|-------------------------------|-------------------|-----------------------------------|\n",
    "| List Models  | GET  | /v1/models                    |                   | {\"models\": [<model_name>]}        |\n",
    "| Model Ready  | GET  | /v1/models/<model_name>       |                   | {\"name\": <model_name>,\"ready\": $bool} |\n",
    "| Predict      | POST | /v1/models/<model_name>:predict | {\"instances\": []}* | {\"predictions\": []}              |\n",
    "| Explain      | POST | /v1/models/<model_name>:explain | {\"instances\": []}* | {\"predictions\": [], \"explanations\": []} |\n",
    "\n",
    "\\* Payload is optional\n",
    "\n",
    "You want to invoke the `predict` API. So let's use a simple query to test our service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e2ebd-5e3b-4289-8358-9406ba816921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOMAIN_NAME = \"svc.cluster.local\"  # change this to your domain for external access\n",
    "NAMESPACE = os.environ['USER']\n",
    "DEPLOYMENT_NAME = \"vectorstore\"\n",
    "MODEL_NAME = DEPLOYMENT_NAME\n",
    "SVC = f'{DEPLOYMENT_NAME}-predictor-default.{NAMESPACE}.{DOMAIN_NAME}'\n",
    "URL = f\"https://{SVC}/v1/models/{MODEL_NAME}:predict\"\n",
    "\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da091c-9fce-4f91-8382-e5c785bdf24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"instances\": [{\n",
    "      \"question\": \"Who's Ada Lovelace?\"\n",
    "  }]\n",
    "}\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ['AUTH_TOKEN']}\"}\n",
    "\n",
    "response = requests.post(URL, json=data, headers=headers, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fdac2-cacb-40cc-bf2d-d1548072bb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c9e0e-6d17-4d15-ba4e-e353cc1cd3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Well done! Through this Notebook, you've successfully interacted with and tested the Vector Store Inference Service. You've learned how to construct and send requests to the service and how to interpret the responses. This hands-on experience is crucial as it provides a practical understanding of the service's operation, preparing you for real-world applications.\n",
    "\n",
    "In the next Notebook, you will extend our question-answering system by creating an Inference Service for the Large Language Model (LLM). The LLM Inference Service will work in conjunction with the Vector Store Inference Service to provide comprehensive and accurate answers to user queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-answering",
   "language": "python",
   "name": "question-answering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
