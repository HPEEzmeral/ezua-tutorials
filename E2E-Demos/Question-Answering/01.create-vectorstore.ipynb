{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3324b7d0-515d-46a7-ac00-03e788d5e96a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vector Stores: Embedding and Storing Documents in a Latent Space\n",
    "\n",
    "In this Jupyter Notebook, you will explore a foundational element of a question-answering system: the Vector Store. The Vector Store serves as the key component that allows us to efficiently retrieve relevant context from a corpus of documents based on a user's query.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/documents.jpg\" alt=\"documents\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "      Photo by <a href=\"https://unsplash.com/@anniespratt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Annie Spratt</a> on <a href=\"https://unsplash.com/photos/5cFwQ-WMcJU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "The approach you will employ involves transforming each document in the corpus into a high-dimensional numerical representation known as an \"embedding\", using a pre-trained Transformer model. This process is sometimes referred to as \"embedding\" the document in a latent space. The latent space here is a high-dimensional space where similar documents are close to each other. The position of a document in this space is determined by the content and the semantic meaning it carries.\n",
    "\n",
    "Once we have these embeddings, we store them in a Vector Store. A Vector Store is an advanced AI-native database designed to hold these high-dimensional vectors and provide efficient search capabilities. This enables us to quickly identify documents in our corpus that are semantically similar to a given query, which will also be represented as a vector in the same latent space.\n",
    "\n",
    "The following cells in this Notebook will guide you through the process of creating such a Vector Store. You will start by generating embeddings for each document, then you will move on to storing these embeddings in a Vector Store and finally, you will see how easy it is to to retrieve documents from the Vector Store based on a query.\n",
    "\n",
    "First, let's import the libraries you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df209f-e471-45d8-ad58-ad5c34819ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea076619-2576-4154-ad50-0775b84a4359",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load the Documents\n",
    "\n",
    "The next cells contain a set of helper functions designed to load text documents from a specified directory. These functions are essential for preparing your data before embedding it into the high-dimensional latent space.\n",
    "\n",
    "The key operations performed by these functions are:\n",
    "\n",
    "- Directory Scanning: Scan the specified directory for all `.txt` files recursively.\n",
    "- Document Loading: Load the file in LangChain `Document` object, using the provided `TextLoader` object.\n",
    "\n",
    "By running this cell, you will have a list of documents ready to be processed and embedded in the latent space. This forms our corpus, which will be used for subsequent operations in the Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc826f3-6670-4fbd-87a9-ef4e67c6f30f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_doc(fn):\n",
    "    loader = TextLoader(fn)\n",
    "    doc = loader.load()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bafbff81-1b65-41fb-9d65-a26e88779d37",
   "metadata": {
    "tags": []
   },
   "source": [
    "def load_docs(source_dir: str) -> list:\n",
    "    \"\"\"Load all documents in a the given directory.\"\"\"\n",
    "    fns = glob.glob(os.path.join(source_dir, \"*.txt\"))\n",
    "    \n",
    "    docs = []\n",
    "    for i, fn in enumerate(tqdm(fns, desc=\"Loading documents...\")):\n",
    "        docs.extend(load_doc(fn))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ce613-1f5a-428b-a20f-c2d83c46c40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = load_docs(\"documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ce8b8-52d1-468d-a28c-7384b90f553f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Document Processing: Chunking Text for the Language Model\n",
    "\n",
    "In this section of the Notebook, you will process the documents by splitting them into chunks. This operation is crucial when working with Large Language Models (LLMs), as these models have a maximum limit on the number of tokens (words or pieces of words) they can process at once. This limit is often referred to as the model's \"context window.\"\n",
    "\n",
    "In this example, you are splitting each document into segments that are at most `500` tokens long. We're using the LangChain `RecursiveCharacterTextSplitter`, which, by default, splits each document when it encounters two consecutive newline characters, represented as `\\n\\n`. Furthermore, each segment is distinct, meaning there is no overlap between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389f2b4-9bde-48b7-a87b-eee356c0fae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_docs(docs: list, chunk_size: int, chunk_overlap: int) -> list:\n",
    "    \"\"\"Load the documents and split them into chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    return texts\n",
    "\n",
    "texts = process_docs(docs, chunk_size=500, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6e838-1746-4100-bd22-30e03b36c3e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generating Embeddings & Storing them in Chroma\n",
    "\n",
    "In this section of the Notebook, you will be using the HuggingFace's `all-MiniLM-L6-v2` model in conjunction with the Sentence Transformers framework to generate embeddings for our document chunks. Sentence Transformers is a Python framework that allows us to leverage the power of Transformer models to generate dense vector embeddings for sentences. These embeddings can capture the semantic meaning of the input text, making them ideal for tasks like semantic search, clustering, and information retrieval.\n",
    "\n",
    "By leveraging this framework and the Chroma database interface provided by LangChain, you can embed your documents into a latent space and subsequently store the results in a Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719391de-6f29-4234-9f40-cbb10e9da7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_model = \"all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c988d-dc14-48da-ba95-7232142f81b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "settings = Settings(anonymized_telemetry=False, chroma_db_impl=\"duckdb+parquet\",  persist_directory=f\"{os.getcwd()}/db\")\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=f\"{os.getcwd()}/db\", client_settings=settings)\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f5bcc-c6ee-416c-8893-b30a69f2f054",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, you can test the accuracy of the document retrieval mechanism by providing a simple query. Chroma will return with the four most similar documents by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5699cd9-93b3-4513-8cb6-2d492ff5a06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How can I create a cgroup?\"\n",
    "matches = db.similarity_search(query); matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb1b93-567e-4c2e-b3f9-48a3b1d43fc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have successfully traversed the journey of embedding documents into a high-dimensional latent space and storing these embeddings in a Vector Store. By accomplishing this, you've transformed unstructured text data into a structured form that can power a robust question-answering system.\n",
    "\n",
    "However, your journey doesn't end here. Now that we have our Vector Store ready, the next step is to create an Inference Service that can leverage this store to provide context to user queries. For this, you will be using KServe, a flexible, cloud-native platform for serving machine learning models.\n",
    "\n",
    "In the next Notebook, we will guide you through the process of setting up a custom Inference Service using KServe. This service will make use of our Vector Store to retrieve and rank relevant document chunks based on a user's query, providing accurate and efficient context to an LLM!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-answering",
   "language": "python",
   "name": "question-answering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
