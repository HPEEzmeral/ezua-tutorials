{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f891729d-9030-4d9e-a7be-e81386ae820f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating a Large Language Model Inference Service\n",
    "\n",
    "Welcome to the fourth part of our tutorial series on building a question-answering application over a corpus of private documents using Large Language Models (LLMs). In the previous Notebooks, we journeyed through the processes of creating vector embeddings of our documents, setting up a Vector Store Inference Service, and testing its performance.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/llm.jpg\" alt=\"llm\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "      Photo by <a href=\"https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Google DeepMind</a> on <a href=\"https://unsplash.com/photos/LaKwLAmcnBc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "Now, we're moving towards the next crucial step: creating an Inference Service for the Large Language Model (LLM). This Inference Service will be the centerpiece of our question-answering system, working in tandem with the Vector Store Inference Service to deliver comprehensive and accurate answers to user queries.\n",
    "\n",
    "In this Notebook, we'll guide you through the steps required to set up this LLM Inference Service. We'll discuss how to create a Docker image for the custom predictor, the role of the transformer component, define a KServe InferenceService YAML, and deploy the service. By the end of this Notebook, you'll have a fully functioning LLM Inference Service that can take user queries, interact with the Vector Store, and provide insightful responses.\n",
    "\n",
    "Let's dive in! Let's import the libraries you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import logging\n",
    "import warnings\n",
    "import subprocess\n",
    "\n",
    "import mlflow\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0cc08-1c00-49cc-85bd-b792383b26e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Architecture\n",
    "\n",
    "In this setup, an additional component, called a \"transformer\", plays a pivotal role in processing user queries and integrating the Vector Store Inference Service with the LLM Inference Service. The transformer's role is to intercept the user's request, extract the necessary information, and then communicate with the Vector Store Inference Service to retrieve the relevant context. The transformer then takes the response of the Vector Store Inference Service (i.e., the context), combines it with the user's query, and forwards the enriched prompt to the LLM predictor.\n",
    "\n",
    "Here's a detailed look at the process:\n",
    "\n",
    "1. Intercepting the User's Request: The transformer acts as a gateway between the user and the LLM inference service. When a user sends a query, it first reaches the transformer. The transformer extracts the query from the request.\n",
    "1. Communicating with the Vector Store Inference Service: The transformer then takes the user's query and sends a POST request to the Vector Store Inference Service including the user's query in the payload, just like you did in the previous Notebook.\n",
    "1. Receiving and Processing the Context: The Vector Store Inference Service responds by sending back the relevant context.\n",
    "1. Combining the Context with the User's Query: The transformer then combines the received context with the user's original query using a prompt template. This creates an enriched prompt that contains both the user's original question and the relevant context from our documents.\n",
    "1. Forwarding the Enriched Query to the LLM Predictor: Finally, the transformer forwards this enriched query to the LLM predictor. The predictor then processes this query and generates a response, which is sent back to the transformer. Steps 2 through 5 are transparent to the user.\n",
    "1. The transformer returns the response to the user.\n",
    "\n",
    "As such, you should build two custom Docker images at this point: one for the predictor and one for the transformer. The source code and the Dockerfiles are provided in the corresponding folders: `llm` and `transformer`. For your convenience, you can use the images we have pre-built for you:\n",
    "\n",
    "- Predictor: `gcr.io/mapr-252711/ezua-demos/llm-predictor:v0.1.0`\n",
    "- Transformer: `gcr.io/mapr-252711/ezua-demos/llm-transformer:v0.1.0`\n",
    "\n",
    "Once ready, proceed with the next steps.\n",
    "\n",
    "# Creating the Inference Service\n",
    "\n",
    "As before, you'll need to provide a few variables:\n",
    "\n",
    "1. The custom predictor image you built.\n",
    "1. The custom transfromer image you built.\n",
    "\n",
    "You can leave any field empty to use the image we provide for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b97f1-8e6b-4181-b220-60f61f77e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_IMAGE = (input(\"Enter the name of the predictor image (default: dpoulopoulos/llm-predictor:b98a87f): \")\n",
    "                   or \"dpoulopoulos/llm-predictor:b98a87f\")\n",
    "TRANSFORMER_IMAGE = (input(\"Enter the name of the transformer image (default: dpoulopoulos/llm-transformer:b98a87f): \")\n",
    "                     or \"dpoulopoulos/llm-transformer:b98a87f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c9d-72e4-491a-ad99-f6f533f0ef94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isvc = \"\"\"\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: llm\n",
    "spec:\n",
    "  predictor:\n",
    "    timeout: 600\n",
    "    containers:\n",
    "    - name: kserve-container\n",
    "      image: {0}\n",
    "      imagePullPolicy: Always\n",
    "      resources:\n",
    "        requests:\n",
    "          memory: \"8Gi\"\n",
    "          cpu: \"1000m\"\n",
    "        limits:\n",
    "          memory: \"8Gi\"\n",
    "          cpu: \"1000m\"\n",
    "  transformer:\n",
    "    timeout: 600\n",
    "    containers:\n",
    "      - image: {1}\n",
    "        imagePullPolicy: Always\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "        name: kserve-container\n",
    "        args: [\"--use_ssl\"]\n",
    "\"\"\".format(PREDICTOR_IMAGE,\n",
    "           TRANSFORMER_IMAGE)\n",
    "\n",
    "with open(\"llm/isvc.yaml\", \"w\") as f:\n",
    "    f.write(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631d06-ea76-4159-a08b-57e76850ff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"llm/isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20235af7-7b47-4b68-8f6d-f0b01b0c23e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations on completing this crucial step in this tutorial series! You've successfully built a Large Language Model (LLM) Inference Service, and you've learned about the role of a transformer in enriching user queries with relevant context from our documents. Together with the Vector Store Inference Service, these components form the backbone of our question-answering application.\n",
    "\n",
    "However, the journey doesn't stop here. The next and final step is to test our LLM Inference Service, ensuring that it's working as expected and delivering accurate responses. This will help us gain confidence in our setup and prepare us for real-world applications. In the next Notebook, we will guide you through the process of invoking the LLM Inference Service. We will show you how to construct suitable requests, communicate with the service, and interpret the responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-answering",
   "language": "python",
   "name": "question-answering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
