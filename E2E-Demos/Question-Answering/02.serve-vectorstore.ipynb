{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b72df6-4961-4128-9281-6c9634dd33fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating an Inference Service using MLFlow and KServe\n",
    "\n",
    "Welcome to the second part of our tutorial on building a question-answering application over a corpus of private documents using Large Language Models (LLMs). In the previous Notebook, you focused on embedding the documents into a high-dimensional latent space and storing these embeddings in a Vector Store using the Chroma database interface provided by LangChain.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/inference-service.jpg\" alt=\"isvc\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "    Photo by <a href=\"https://unsplash.com/@growtika?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Growtika</a> on <a href=\"https://unsplash.com/photos/GSiEeoHcNTQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "In this Notebook, you will be taking the next step in this journey. You will use MLFlow to log the Chroma DB files as artifacts of an experiment. After logging the artifacts, you will then create an Inference Service that retrieves these artifacts and uses them to provide context to user queries. For this purpose, you'll be using KServe, a Kubernetes-based platform that provides a serverless framework for serving machine learning models at scale.\n",
    "\n",
    "It's important to note that since KServe does not support serving Chroma DB files out-of-the-box, you will be using a custom predictor component. This means that you'll need to create a Docker image first, which can then be deployed as our Inference Service. This process allows for a high degree of customization, enabling you to fine-tune your service to your specific needs. You can find the code, as well as the Dockerfile for the custom predictor inside the `vectorstore` directory of this project. However, to save time, you can use the one we have pre-built for you: `gcr.io/mapr-252711/ezua-demos/vectorstore:v0.1.0`.\n",
    "\n",
    "Once you're ready, this Notebook will guide you through the necessary steps for creating a scalable Vector Store service. First, let's import the libraries you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import logging\n",
    "import warnings\n",
    "import subprocess\n",
    "\n",
    "import mlflow\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656fd38-0660-402b-bd83-72e566cd4e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_base64(message: str):\n",
    "    encoded_bytes = base64.b64encode(message.encode('ASCII'))\n",
    "    return encoded_bytes.decode('ASCII')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8b07a-15d8-44a2-89c4-266ee61d1a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logging the Vector Store as an Artifact\n",
    "\n",
    "To kick-off this process, you need to create a new experiment (or re-use and existing one) and log the Chroma DB files as an artifact of this experiment. In the end, you'll need to retrieve the URI pointing to the location of this artifact and pass it to the custom predictor component. This way, the custom predictor component will know hot to retrieve the artifact and serve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23d780-57e0-4d5c-a699-aa3046a5b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(exp_name):\n",
    "    \"\"\"Register an experiment in MLFlow.\n",
    "    \n",
    "    args:\n",
    "      exp_name (str): The name of the experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mlflow.set_experiment(exp_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to set the experiment: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f97e2-4c39-4464-b3e8-c778b96d28da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new MLFlow experiment or re-use an existing one\n",
    "get_or_create_experiment('question-answering')\n",
    "\n",
    "# Log the Chroma DB files as an artifact of the experiment\n",
    "mlflow.log_artifact(f\"{os.getcwd()}/db\")\n",
    "\n",
    "# Retrieve the URI of the artifact\n",
    "uri = mlflow.get_artifact_uri(\"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25e4d-4803-4464-80d9-6b62ad2b4d49",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating and Submitting the Inference Service\n",
    "\n",
    "In the final section of this Notebook, you will create and submit an Inference Service using a YAML template and a Python subprocess. The steps in this section include:\n",
    "\n",
    "1. Creating the YAML Template: You will create a YAML file that defines the specifications of our Inference Service. This includes details like the name of the service, the Docker image to use, and other configuration settings. You will store this YAML into a file that you can explore and submit.\n",
    "1. Submitting the YAML Template: Once your YAML template is ready, you need to submit it to KServe for deployment. To do this, you will use a Python subprocess to run a shell command that submits your YAML template to KServe.\n",
    "\n",
    "By the end of this section, you will have a running Inference Service that is ready to receive user queries and provide context for answering them using the Vector Store. This marks the completion of your journey, from transforming unstructured text data into structured vector embeddings, to creating a scalable service that can provide context based on those embeddings.\n",
    "\n",
    "Provide the name of the docker image you built at the first step in the next cell. Leave it blank to use the one we have pre-built for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be7670-5333-41ec-a79b-fa3ae88f81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_image = (input(\"Enter the name of the predictor image (default: gcr.io/mapr-252711/ezua-demos/vectorstore:v0.1.0): \")\n",
    "                   or \"gcr.io/mapr-252711/ezua-demos/vectorstore:v0.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c9d-72e4-491a-ad99-f6f533f0ef94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isvc = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: minio-secret\n",
    "type: Opaque\n",
    "data:\n",
    "  MINIO_ACCESS_KEY: {0}\n",
    "  MINIO_SECRET_KEY: {1}\n",
    "\n",
    "---\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: vectorstore\n",
    "spec:\n",
    "  predictor:\n",
    "    containers:\n",
    "    - name: kserve-container\n",
    "      image: {2}\n",
    "      imagePullPolicy: Always\n",
    "      resources:\n",
    "        requests:\n",
    "          memory: \"2Gi\"\n",
    "          cpu: \"500m\"\n",
    "        limits:\n",
    "          memory: \"2Gi\"\n",
    "          cpu: \"500m\"\n",
    "      args:\n",
    "      - --persist-uri\n",
    "      - {3}\n",
    "      env:\n",
    "      - name: MLFLOW_S3_ENDPOINT_URL\n",
    "        value: {4}\n",
    "      - name: TRANSFORMERS_CACHE\n",
    "        value: /src\n",
    "      - name: SENTENCE_TRANSFORMERS_HOME\n",
    "        value: /src\n",
    "      - name: MINIO_ACCESS_KEY\n",
    "        valueFrom:\n",
    "          secretKeyRef:\n",
    "            key: MINIO_ACCESS_KEY\n",
    "            name: minio-secret\n",
    "      - name: MINIO_SECRET_KEY\n",
    "        valueFrom:\n",
    "          secretKeyRef:\n",
    "            key: MINIO_SECRET_KEY\n",
    "            name: minio-secret\n",
    "\"\"\".format(encode_base64(os.environ[\"AWS_ACCESS_KEY_ID\"]),\n",
    "           encode_base64(os.environ[\"AWS_SECRET_ACCESS_KEY\"]),\n",
    "           predictor_image,\n",
    "           uri,\n",
    "           os.environ[\"MLFLOW_S3_ENDPOINT_URL\"])\n",
    "\n",
    "with open(\"vectorstore/isvc.yaml\", \"w\") as f:\n",
    "    f.write(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631d06-ea76-4159-a08b-57e76850ff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"vectorstore/isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e2d08-8e09-4c9e-826c-0f0dfdc2d3f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully navigated through the process of logging the Chroma DB files as artifacts using MLFlow, creating a custom Docker image, and setting up an Inference Service with KServe that retrieves these artifacts to serve your Vector Store. This Inference Service forms the backbone of our question-answering application, enabling us to efficiently answer queries based on the document embeddings we generated previously.\n",
    "\n",
    "From here, there are two paths you can choose:\n",
    "\n",
    "- Testing the Vector Store Inference Service: If you'd like to test the Vector Store Inference Service that you've just created, you can proceed to our third (optional) Notebook. This Notebook provides a step-by-step guide on how to invoke the Inference Service and validate its performance.\n",
    "- Creating the LLM Inference Service: Alternatively, if you're ready to move on to the next stage of the project, you can jump straight to our fourth Notebook. In this Notebook, we'll guide you through the process of creating an Inference Service for the Large Language Model (LLM), which will work in conjunction with the Vector Store Inference Service to provide answers to user queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-answering",
   "language": "python",
   "name": "question-answering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
