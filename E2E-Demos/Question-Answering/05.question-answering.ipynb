{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286558bc-9780-4399-ae75-8055b8e0a9d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Invoking and Testing the Large Language Model Inference Service\n",
    "\n",
    "Welcome to the fifth and final part of our tutorial series on building a question-answering application over a corpus of private documents using Large Language Models (LLMs). In the previous Notebooks, we've covered the processes of creating vector embeddings of our documents, deploying a Vector Store Inference Service, creating a Large Language Model Inference Service, and enriching user queries with relevant context using an Inference service Transformer component.\n",
    "\n",
    "In this Notebook, you focus on the crucial task of invoking and testing the LLM Inference Service you've created. This is an important step in the development process as it allows us to validate the functionality and performance of your service in a practical setting.\n",
    "\n",
    "Throughout this Notebook, we'll guide you on how to construct and send requests to the LLM Inference Service, interpret the responses, and handle potential issues that might arise. By the end of this Notebook, you will have gained practical experience in working with the LLM Inference Service, preparing you to integrate it into larger systems or applications.\n",
    "\n",
    "Let's get started by importing the libraries you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fd850-d35a-476f-ba05-b11763ddec68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23caa9-d988-4ae0-afd6-094a5a09c82a",
   "metadata": {
    "tags": []
   },
   "source": [
    "You are now ready to test your service. Provide your question and get back the answer from the LLM inference service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e2ebd-5e3b-4289-8358-9406ba816921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOMAIN_NAME = \"svc.cluster.local\"  # change this to your domain for external access\n",
    "NAMESPACE = os.environ['USER']\n",
    "DEPLOYMENT_NAME = \"llm\"\n",
    "MODEL_NAME = DEPLOYMENT_NAME\n",
    "SVC = f'{DEPLOYMENT_NAME}-transformer-default.{NAMESPACE}.{DOMAIN_NAME}'\n",
    "URL = f\"https://{SVC}/v1/models/{MODEL_NAME}:predict\"\n",
    "\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da091c-9fce-4f91-8382-e5c785bdf24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"instances\": [{\n",
    "      \"question\": \"What are modern CPUs made of?\"\n",
    "  }]\n",
    "}\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ['AUTH_TOKEN']}\"}\n",
    "\n",
    "response = requests.post(URL, json=data, headers=headers, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fdac2-cacb-40cc-bf2d-d1548072bb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed71c82-a009-4b87-8e85-79fa218999b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you're executing this tutorial in an environment without access to a GPU device, the inference step might require more time than usual. Please exercise patience and allow for approximately 7-8 minutes. In the unlikely event that you encounter a time-out error, please attempt the process again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501663ea-9a56-4fca-a63a-69fabe51ec34",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations on reaching the finish line of this comprehensive tutorial! You've successfully developed an application capable of delivering responses to user queries in a natural language format. The journey has not only enhanced your understanding but also allowed you to acquire hands-on experience in various facets of Large Language Models.\n",
    "\n",
    "Throughout this process, you've demystified the concept of a Vector Store, created custom predictor and transformer components, and learned to log artifacts with MLflow. Moreover, all these tasks have been accomplished within the comfortable and familiar confines of your JupyterLab environment.\n",
    "\n",
    "In conclusion, you've taken significant strides in your journey of mastering Large Language Models, and how to create real-world applications using the EzUA platform. Happy coding, and until our next tutorial, keep learning and experimenting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-answering",
   "language": "python",
   "name": "question-answering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
