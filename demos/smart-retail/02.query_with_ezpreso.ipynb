{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be01229-0819-4586-8d10-32c2a0513ed9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806b008-9c4f-42f5-838a-5b42b0d5188e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 2:** Connecting and Querying Data Sources with EzPresto\n",
    "\n",
    "This exercise will be introduce **EzPresto**, HPE Ezmeral's supercharged Presto distribution on **HPE Ezmeral Unified Analytics Software**. EzPresto is an SQL query engine based on the open-source query engine PrestoDB that is optimized to run federated queries across various data sources. Enterprise applications such as Supersets, Tableau, Power BI, and data processing engines (such as Spark), can leverage EzPresto for rapid query performance and prompt insights through federated data access. \n",
    "\n",
    "EzPresto is the underlying driver behind all of what you will learn in this exercise, in which you will:\n",
    "\n",
    "- Connect Data Sources to HPE Ezmeral Unified Analytics.\n",
    "- Run SQL queries using the Unified Analytics Query Editor.\n",
    "- Learn about HPE's custom Jupyter Magic commands.\n",
    "- Run SQL queries on connected Data Sources directly from within a notebook cell.\n",
    "\n",
    "By the end of this exercise, you will be proficient in directly or indirectly leveraging EzPresto to streamline your data analytics workflows. \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1787d-8a4d-4b65-b751-8ed9483b0e3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Contents:**\n",
    "insert ToC here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7ab9d-1899-4fee-a27d-7d6f1d6ccc9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> This exercise requires the completion of Exercise 1: Exploring Retail Data with Apache Spark.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f71c84-02cf-4d3e-9f4f-c47e6ac2dbb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. Connecting a Data Source in Unified Analytics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0fe25-896d-4c44-a64b-2ae98df81012",
   "metadata": {
    "tags": []
   },
   "source": [
    "HPE Ezmeral Unified Analytics allows users to connect multiple types of internal and external data sources - from SQL servers to Snowflake, Terradata and Oracle databases - and make the files, objects and tables within them available to any tool or application running on Unified Analytics.\n",
    "\n",
    "In this section, you will learn how to make a data connection using the Delta Tables you created in Exercise 1. \n",
    "\n",
    "### Connect Delta Tables as Data Source using Hive.\n",
    "\n",
    "Let's take those Delta Tables you created in Exercise 1 and make them available to other applications in Unified Analytics by making them a **Data Source**. This will allow you to use EzPresto to turn them into datasets later in the exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162d9b1-1b1e-4170-8590-6410c8ce5c22",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Navigate back to the Unified Analytics dashboard.\n",
    "1. In the sidebar navigation menu, select `Data Engineering` > `Data Sources`.\n",
    "1. Under the `Structured Data` tab, click `Add New Data Source`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8c89c-fe10-419e-b5c4-dd152fd13549",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise2/datasources.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115054b-7f09-4e94-a658-805ab59cbd41",
   "metadata": {
    "tags": []
   },
   "source": [
    "4. Click `Create Connection` under **Hive**. \n",
    "- **Name**: `retail`\n",
    "- **Hive Metastore:** `discovery`\n",
    "- **Data Dir:** `file:/data/shared/retail-delta/data`\n",
    "- **File Type:** `PARQUET`\n",
    "5. Click `Connect`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eef0ef-596b-49d8-aaea-0fb356b214d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "![image.png](./images/exercise2/connect-dl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81c4e6-e737-4a72-be42-b2797b4f0864",
   "metadata": {
    "tags": []
   },
   "source": [
    "6. Under the `Structured Data` tab, you will now see your connected data source. \n",
    "\n",
    "### Viewing and Querying Data from Data Sources\n",
    "\n",
    "Now that our Delta Tables are available via a Data Source, we can leverage the native data tools within Unified Analytics to run queries and create datasets.\n",
    "\n",
    "1. Click `Query using Data Catalog` under the newly created data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f721c4a-07a1-4c2f-b26a-8f0b9a0da9a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "![image.png](./images/exercise2/tile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03eaf7-2b56-487b-9853-01edbdfcaedd",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. Under `Connected Data Sources`, look for the `data` group.\n",
    "3. Under the `data` group, check the `default` box. \n",
    "4. Select the datasets for all three countries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48139022-3815-40ea-bb82-a0ab84b1fc0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "![image.png](./images/exercise2/datacatalog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355dcac6-8f33-4c6d-813c-357c0afb1f69",
   "metadata": {
    "tags": []
   },
   "source": [
    "5. Click `Selected Datasets` in the top left corner.\n",
    "6. Click `Query Editor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a74bc-bb74-4e46-9f63-1cebc4990d27",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, you are introduced to the the **Unified Analytics Query Editor** where you can directly query data sources from specific datasets and data tables - all from within the Unified Analytics user interface! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de14a6-4cad-4d43-adf0-d6d5452c3429",
   "metadata": {
    "tags": []
   },
   "source": [
    "![image.png](./images/exercise2/QueryEditor.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f9fe6-6057-4d7b-ab9b-a940f6b3f8fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "7. Next, we're going to run an SQL query which will combine the data from our three tables (czech, germany, and swiss) in the retail.default schema. This will merge all columns from the czech and germany tables, and select specific columns from the swiss table whilst also applying a transformation to the country column. We'll also limit our final result set to 1000 rows. And all in a nice UI!\n",
    "\n",
    "    Paste the following SQL Query into the `SQL Query` field and \n",
    "    click `Run`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13bcea-7bb7-4f4f-a216-c0a42f4f6d1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "```sql\n",
    "SELECT * FROM retail.default.czech UNION ALL SELECT * FROM retail.default.germany UNION ALL ( SELECT PRODUCTID , PRODUCT , TYPE , UNITPRICE , UNIT , QTY , TOTALSALES , CURRENCY , STORE , (CASE WHEN (country = 'Swiss') THEN 'Switzerland' ELSE country END) COUNTRY , YEAR FROM retail.default.swiss ) LIMIT 1000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d258af-ff6e-47b6-9fcb-adb98593bb69",
   "metadata": {
    "tags": []
   },
   "source": [
    "8. Expand the resulting query to visually validate it.\n",
    "9. Under `Actions` (top-left corner of the table), click `Save as View`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3bade-212e-40ad-9bbc-c9836cc63370",
   "metadata": {
    "tags": []
   },
   "source": [
    "![image.png](./images/exercise2/query-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cdd3d-f65a-4cec-a360-b52e6b93b377",
   "metadata": {
    "tags": []
   },
   "source": [
    "10. Name the View `retail`. \n",
    "11. We'll want to save the schema of this new table as a Custom Schema. Under `Schema`, select `+ Add new schema` and name it `retailschema`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785fbea-7da1-498a-abcd-75a10f07472b",
   "metadata": {
    "tags": []
   },
   "source": [
    "![image.png](./images/exercise2/save-as-view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc431cae-33d3-4f9f-bedb-d61b4b43b166",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2. Jupyter Magic Commands on HPE Ezmeral Unified Analytics**\n",
    "\n",
    "Jupyter Notebooks Magic functions, also known as magic commands or magics, are commands that you can execute within a code cell.   \n",
    "Magics are not code of any language, but are shortcuts that extends the capabilities of a notebook. \n",
    "\n",
    "There are two types of magic commands - **Line** and **Cell** magic commands:\n",
    "\n",
    "**Line magic** commands do not require a cell body and start with a single % character.  \n",
    "**Cell magic** commands start with %% and require additional lines of input (a cell body). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05b8ca-96c8-4232-b1c2-b66b3d197a7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HPE Ezmeral Magic Commands \n",
    "**HPE Ezmeral Unified Analytics Software** supports both Line and Cell magic commands and includes custom commands that allow for users to interact with other tools native to Unified Analytics directly within notebooks.\n",
    "\n",
    "We can check out the full list of custom HPE magic commands by running `%command`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb82bf08-370c-4d5f-b866-e34c1c28623e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a06495993a43fbafaebcdec36a83cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n    <table style=\"border-collapse: collapse; border: 1px solid black; width: 100%;\">\\n        <tâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The %commands command lists the magic commands and SDKs that are customized by Hewlett Packard Enterprise and are available in this notebook.\n",
    "%commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d19dd-815b-43ca-92bb-b2bf74ce12a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Updating the Cached JWT Token\n",
    "\n",
    "In a Jupyter notebook, a JWT token (JSON Web Token) is a compact and URL-safe means of representing authentication information to be transferred between other notebook servers or external applications. It is commonly used for securely authenticating and authorizing users within Jupyter environments, allowing them to access resources and execute code while ensuring their identity and permissions are properly validated.\n",
    "\n",
    "When working in Jupyter notebooks for long durations, particularly when making calls to other applications, the JWT token can expire and result in an error when attempting to make calls. This is particularly relevant for working on a Jupyter notebook within **HPE Ezmeral Unified Analytics**, which provides users to leverage a plethora of external tools within the notebook (Such as Spark, Livy and Presto).\n",
    "\n",
    "If you encounter a JWT token expiration error while running cells in a Jupyter notebook, you can resolve it by running the `%update_token` magic command.  \n",
    "This function updates the JWT in environment variables and any other locations where the token is utilized. \n",
    "  \n",
    "Ideally, it is good practice to refresh the token prior to making external connections. Some examples relevant to the Smart Retail Experience demo include:\n",
    "\n",
    "- Authentication when establishing a connection with PrestoDB.\n",
    "- Authentication with local s3 minio object storage.  \n",
    "- Authentication with KServe external API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe7b6e8-d5a4-4244-9edf-9d9425355d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Switching to connection presto://ezpresto-svc-locator.ezpresto.svc.cluster.local:8080</span>"
      ],
      "text/plain": [
       "Switching to connection presto://ezpresto-svc-locator.ezpresto.svc.cluster.local:8080"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token suseccfully refreshed.\n"
     ]
    }
   ],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b59107-9c22-430e-b49e-a41e29245916",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Directly interacting with connected SQL databases using the SQL Magic Command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c053e-e835-4527-b02c-769fc26b65c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using the `%sql` magic command, you can directly query SQL databases from Data Sources you have connected to **HPE Ezmeral Unified Analytics Software** from within Jupyter notebook cells! When you run the notebook cell containing `%sql` and your SQL query, the magic command sends the query to the database, runs the query, and retrieves the result.\n",
    "\n",
    "This is made possible by the native integration of EzPresto into Unified Analytics. **However, the Data Source must be made publicly available.**\n",
    "\n",
    "To change the access of a Data Source from `private` to `public`:\n",
    "\n",
    "1. Navigate back to the Unified Analytics dashboard.\n",
    "1. In the sidebar navigation menu, select `Data Engineering` > `Data Sources`.\n",
    "1. Under the three dots in the top corner of the Data Source of interest, click `Change to public access`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba0a707-50fc-4849-9434-7a100d2fbe6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "![image.png](./images/exercise2/public-access.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341b8e2-56f7-415f-b88a-cfa64c6b32f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> Wait until a confirmation message appears stating that the source is publicly available.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c2cd9-c591-433e-9150-ef4d83edf3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's try the `%sql` magic command to interact with our Delta Tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f552d93-de1e-4af8-9c2c-5d0ac2c960cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;presto://ezpresto-svc-locator.ezpresto.svc.cluster.local:8080&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'presto://ezpresto-svc-locator.ezpresto.svc.cluster.local:8080'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "select * from retail.default.czech limit 10\n",
      "RuntimeError: (pyhive.exc.DatabaseError) {'message': 'The column type of table default.czech is declared as type string, but the Parquet file (file:/data/shared/retail-delta/data/czech/part-00000-7a9f285f-a3ea-4a9a-932b-e45f3de1a4ca-c000.snappy.parquet) declares the column as type INT32', 'errorCode': 16777224, 'errorName': 'HIVE_PARTITION_SCHEMA_MISMATCH', 'errorType': 'EXTERNAL', 'boolean': False, 'failureInfo': {'type': 'com.facebook.presto.spi.PrestoException', 'message': 'The column type of table default.czech is declared as type string, but the Parquet file (file:/data/shared/retail-delta/data/czech/part-00000-7a9f285f-a3ea-4a9a-932b-e45f3de1a4ca-c000.snappy.parquet) declares the column as type INT32', 'suppressed': [], 'stack': ['com.facebook.presto.hive.parquet.ParquetPageSourceFactory.getParquetType(ParquetPageSourceFactory.java:415)', 'com.facebook.presto.hive.parquet.ParquetPageSourceFactory.getColumnType(ParquetPageSourceFactory.java:516)', 'com.facebook.presto.hive.parquet.ParquetPageSourceFactory.lambda$createParquetPageSource$3(ParquetPageSourceFactory.java:225)', 'java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)', 'java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)', 'java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)', 'java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)', 'java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)', 'java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)', 'java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)', 'java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558)', 'com.facebook.presto.hive.parquet.ParquetPageSourceFactory.createParquetPageSource(ParquetPageSourceFactory.java:229)', 'com.facebook.presto.hive.parquet.ParquetPageSourceFactory.createPageSource(ParquetPageSourceFactory.java:557)', 'com.facebook.presto.hive.HivePageSourceProvider.createHivePageSource(HivePageSourceProvider.java:433)', 'com.facebook.presto.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:187)', 'com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:63)', 'com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:80)', 'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:263)', 'com.facebook.presto.operator.Driver.processInternal(Driver.java:428)', 'com.facebook.presto.operator.Driver.lambda$processFor$9(Driver.java:311)', 'com.facebook.presto.operator.Driver.tryWithLock(Driver.java:732)', 'com.facebook.presto.operator.Driver.processFor(Driver.java:304)', 'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1079)', 'com.facebook.presto.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:165)', 'com.facebook.presto.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:603)', 'com.facebook.presto.$gen.Presto_0_281_fy24_q1_d2019f7____20240222_222600_1.run(Unknown Source)', 'java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)', 'java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)', 'java.base/java.lang.Thread.run(Thread.java:829)']}}\n",
      "[SQL: select * from retail.default.czech limit 10]\n",
      "(Background on this error at: https://sqlalche.me/e/14/4xp6)\n",
      "If you need help solving this issue, send us a message: https://ploomber.io/community\n"
     ]
    }
   ],
   "source": [
    "%sql select * from retail.retail.czech limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fc7b3-da34-43bd-8a6c-5c23ddbb377b",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can also save the output of our command as a Python variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cece6e3-372a-414f-accb-8dd82b4841ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;presto://ezpresto-svc-locator.ezpresto.svc.cluster.local:8080&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'presto://ezpresto-svc-locator.ezpresto.svc.cluster.local:8080'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "select * from retail.retail.czech limit 10\n",
      "RuntimeError: (pyhive.exc.DatabaseError) {'message': 'The column type of table retail.czech is declared as type string, but the Parquet file (file:/data/shared/retail-delta/data/czech/part-00000-90e5182a-79c2-4717-a5aa-6973936a287e-c000.snappy.parquet) declares the column as type INT32', 'errorCode': 16777224, 'errorName': 'HIVE_PARTITION_SCHEMA_MISMATCH', 'errorType': 'EXTERNAL', 'boolean': False, 'failureInfo': {'type': 'com.facebook.presto.spi.PrestoException', 'message': 'The column type of table retail.czech is declared as type string, but the Parquet file (file:/data/shared/retail-delta/data/czech/part-00000-90e5182a-79c2-4717-a5aa-6973936a287e-c000.snappy.parquet) declares the column as type INT32', 'suppressed': [], 'stack': ['com.facebook.presto.hive.parquet.ParquetPageSourceFactory.getParquetType(ParquetPageSourceFactory.java:415)', 'com.facebook.presto.hive.parquet.ParquetPageSourceFactory.getColumnType(ParquetPageSourceFactory.java:516)', 'com.facebook.presto.hive.parquet.ParquetPageSourceFactory.lambda$createParquetPageSource$3(ParquetPageSourceFactory.java:225)', 'java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)', 'java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)', 'java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)', 'java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)', 'java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)', 'java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)', 'java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)', 'java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558)', 'com.facebook.presto.hive.parquet.ParquetPageSourceFactory.createParquetPageSource(ParquetPageSourceFactory.java:229)', 'com.facebook.presto.hive.parquet.ParquetPageSourceFactory.createPageSource(ParquetPageSourceFactory.java:557)', 'com.facebook.presto.hive.HivePageSourceProvider.createHivePageSource(HivePageSourceProvider.java:433)', 'com.facebook.presto.hive.HivePageSourceProvider.createPageSource(HivePageSourceProvider.java:187)', 'com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:63)', 'com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:80)', 'com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:263)', 'com.facebook.presto.operator.Driver.processInternal(Driver.java:428)', 'com.facebook.presto.operator.Driver.lambda$processFor$9(Driver.java:311)', 'com.facebook.presto.operator.Driver.tryWithLock(Driver.java:732)', 'com.facebook.presto.operator.Driver.processFor(Driver.java:304)', 'com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1079)', 'com.facebook.presto.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:165)', 'com.facebook.presto.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:603)', 'com.facebook.presto.$gen.Presto_0_281_fy24_q1_d2019f7____20240222_222057_1.run(Unknown Source)', 'java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)', 'java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)', 'java.base/java.lang.Thread.run(Thread.java:829)']}}\n",
      "[SQL: select * from retail.retail.czech limit 10]\n",
      "(Background on this error at: https://sqlalche.me/e/14/4xp6)\n",
      "If you need help solving this issue, send us a message: https://ploomber.io/community\n"
     ]
    }
   ],
   "source": [
    "result = %sql select * from retail.retail.czech limit 10\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dfac0-f760-45ed-9c7f-ecc7d86535a4",
   "metadata": {
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
