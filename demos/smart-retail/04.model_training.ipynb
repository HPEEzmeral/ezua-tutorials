{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009417ca-1757-4559-ada8-d4e746b730e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f226d-0889-4fdf-9211-3dc036f51a76",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 4:** Building an Image Classification Model with Tensorflow and MLflow\n",
    "\n",
    "You presented your Superset Dashboard to your retail company executives. They like the numbers and are raving about the insights you have gathered and visually presented in such an clean, intuitive manner.\n",
    "\n",
    "You're about to conclude when suddenly, someone sidetracks the conversation. Since the introduction of self-serve checkouts to your retail stores, your customers have had to manually search up fresh produce items through a clunky UI. \"What a pain!\" every executive cries. They've all experienced this during their own shopping.\n",
    "\n",
    "\"What about all of those self-serve checkouts that have cameras above the scanners?\" someone asks. \"Could we use them to **automatically detect** what produce a customer has just placed on the scale and deliver a wonderful, futurisitc user experience?\". They turn to you. You've now been tasked with building that very feature into all the self-serve checkouts across your company's stores.\n",
    "\n",
    "Seeing as video cameras are just a stream of static pictures, you realize that the first thing you will need to do is create a model to **detect produce in images**.  \n",
    "\n",
    "In this exercise, you will do just that, through learning how to:\n",
    "\n",
    "- Load and preprocess your image datasets.\n",
    "- Prepare a model for training on your dataset.\n",
    "- Learn about parameters and neural network layers.\n",
    "- Train a model using Tensorflow.\n",
    "- Store the modela and its training metrics in MLflow.\n",
    "- Test the model and validate its accuracy.\n",
    "\n",
    "By the end of this exercise, you will have trained your own **classification model** using Tensorflow and MLflow to detect and classify produce (*\"That's a carrot!\"*) in any given image!\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0307bdd-087c-4c61-ade0-6603c108cc62",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Prerequisites**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751a1f8-82b5-450b-80cc-ab280e0cfa1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "As instructed in the [Introductory notebook](./00.introduction.ipynb), ensure that you have run `pip install -r requirements.txt` in a Terminal window, located in the same working directory, prior to running this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490016b4-5cb1-4699-9c00-ecfc8a19cd23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. Importing Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a908eaf5-dfda-43df-96aa-6e3ea9198b9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's import some of the libraries we will use to train a model, most of which are common to most machine learning workflows:\n",
    "\n",
    "**NumPy (np)**:\n",
    "   - **Purpose**: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and mathematical functions, making it essential for numerical operations.\n",
    "   - **Usage**: NumPy arrays are used extensively for data manipulation and mathematical operations. It offers efficient data structures for handling large datasets, which is crucial for training machine learning models.\n",
    "\n",
    "\n",
    "**Pandas (pd)**:\n",
    "   - **Purpose**: Pandas is a powerful data manipulation and analysis library. It provides easy-to-use data structures and data manipulation tools, making it ideal for data preprocessing and cleaning.\n",
    "   - **Usage**: Pandas DataFrames are used for handling structured data. It facilitates tasks such as data loading, cleaning, filtering, and transformation, which are essential steps in preparing data for model training.\n",
    "\n",
    "\n",
    "**TensorFlow (tf)**:\n",
    "   - **Purpose**: TensorFlow is an open-source machine learning framework developed by Google. It provides comprehensive tools and libraries for building and deploying machine learning models, including neural networks.\n",
    "   - **Usage**: TensorFlow offers a high-level API for building and training deep learning models efficiently. It supports both high-level and low-level operations, allowing for flexibility in model design and optimization. It also provides tools for model deployment and serving, making it suitable for both research and production use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f8bf7-ed2c-4c87-934c-9c79e6a63a93",
   "metadata": {
    "papermill": {
     "duration": 7.613981,
     "end_time": "2023-03-13T20:40:28.950010",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.336029",
     "status": "completed"
    },
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import mlflow.tensorflow  \n",
    "from mlflow.tracking.client import MlflowClient\n",
    "from mlflow.models import signature\n",
    "import os\n",
    "import urllib3\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad7df29-37d5-4674-a03d-f701242ec5da",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ignore any warnings that appear above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ce1dd-6302-4237-8c50-ffb7d65c504f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682fa0d-b993-4606-a7e8-f03bd4487355",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2. Loading and Preparing the Image Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77c161-dbba-4342-ade6-c61f414eb41e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let's load the three image datasets that have been supplied to you by your data engineering team. Wait, why more than one?\n",
    "\n",
    "In general, any single dataset you wish to build, train and fine-tune models on should be split up into three datasets (containing percentage of dataset): **training** (80%), **testing** (10%) and **validation** (10%). The **validation** dataset will be used to validate the accuracy of the model **during** training, whilst the **testing** dataset will be used validate the accuracy of the model **after** it has been trained. \n",
    "\n",
    "We're first going to declare the paths of where these datasets currently live. Run the following command to get the full working path of this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145ffd3d-409d-4e3b-9935-23fb795b3f37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/shared/end2end-main-exercises/exercises'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877da471-9996-440d-8e3e-10a170f35f06",
   "metadata": {
    "tags": []
   },
   "source": [
    "Copy the output (without the surrounding apostrophes) into the root_path variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c129f625-a3e9-4b4b-a95e-ee05fbe6ba0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets/quick/\n"
     ]
    }
   ],
   "source": [
    "root_path = '' #Insert the output of the %pwd command here\n",
    "\n",
    "\n",
    "#For faster training times (<5 minutes) rather than the whole dataset. \n",
    "#Do not change. Full dataset download link coming soon.\n",
    "quick_training = True\n",
    "\n",
    "#Declare dataset path\n",
    "if quick_training:\n",
    "    path = root_path + '/datasets/quick/'\n",
    "else:\n",
    "    path = root_path + '/datasets/full/'\n",
    "    \n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4024d5-fed5-476c-80bc-bf3968b25975",
   "metadata": {
    "papermill": {
     "duration": 0.005834,
     "end_time": "2023-03-13T20:40:21.330048",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.324214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We'll now create lists of all the image paths in each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c3757-40d3-4525-ac44-eaaeadb934e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Path object for the training directory and get a list of all .jpg files in the directory\n",
    "train_dir = Path(path + 'train')\n",
    "train_filepaths = [p for p in train_dir.glob('**/*') if p.suffix.lower() in ['.jpg', '.jpeg'] and not p.name.startswith('.')]\n",
    "\n",
    "# Create a Path object for the testing directory and get a list of all .jpg files in the directory\n",
    "test_dir = Path(path + 'test')\n",
    "test_filepaths = [p for p in test_dir.glob('**/*') if p.suffix.lower() in ['.jpg', '.jpeg'] and not p.name.startswith('.')]\n",
    "\n",
    "# Create a Path object for the validation directory and get a list of all .jpg files in the directory\n",
    "val_dir = Path(path + 'validation')\n",
    "val_filepaths = [p for p in val_dir.glob('**/*') if p.suffix.lower() in ['.jpg', '.jpeg'] and not p.name.startswith('.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21163556-216f-4924-ba1a-d42ea8828934",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll define a function that takes in all of the images from a given dataset and determines the **labels** for each of the images. It will then return and a shuffled list of images (rather than sorted alphabetically or by date, to ensure that we are training the model on unordered images) and their labels as a Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27258877-e5ae-4814-b175-15f24e260302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a DataFrame with filepaths and labels for image data\n",
    "def proc_img(filepath):\n",
    "    \"\"\"\n",
    "    This function takes a list of image file paths as input and creates a DataFrame containing:\n",
    "    - The original file paths\n",
    "    - Extracted labels from the file path structure\n",
    "    - Shuffled rows for random presentation\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Filter out unwanted subfolders:\n",
    "    # Exclude Jupyter's checkpoint folders to avoid processing unnecessary files\n",
    "    filepath = [x for x in filepath if \".ipynb_checkpoints\" not in str(x)]\n",
    "\n",
    "    # 2. Extract labels from file paths:\n",
    "    # Derive labels from the second-to-last segment of each file path, typically indicating categories\n",
    "    labels = [\n",
    "        str(filepath[i]).split(\"/\")[-2]\n",
    "        for i in range(len(filepath))\n",
    "        if not str(filepath[i]).split(\"/\")[-2].startswith(\".\")  # Exclude hidden files/folders\n",
    "    ]\n",
    "\n",
    "    # 3. Create pandas Series for filepaths and labels:\n",
    "    # Organize filepaths and labels into structured Series for DataFrame creation\n",
    "    filepath = pd.Series(filepath, name=\"Filepath\", dtype=str)\n",
    "    labels = pd.Series(labels, name=\"Label\", dtype=str)\n",
    "\n",
    "    # 4. Construct the DataFrame:\n",
    "    # Combine filepaths and labels into a single DataFrame for convenient analysis\n",
    "    df = pd.concat([filepath, labels], axis=1)\n",
    "\n",
    "    # 5. Shuffle and reset index:\n",
    "    # Randomize the order of rows for unbiased processing and training\n",
    "    # Reset the index to start from 0 for consistency\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375d79f-fb9e-4401-86ff-edf3311a9ac2",
   "metadata": {
    "tags": []
   },
   "source": [
    "We'll create those shuffled lists for the training, testing and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2a3e2-4daf-4a54-8c2b-e430310eec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the proc_img function on the training filepaths to create a DataFrame for training\n",
    "train_df = proc_img(train_filepaths)\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# Call the proc_img function on the testing filepaths to create a DataFrame for testing\n",
    "test_df = proc_img(test_filepaths)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# Call the proc_img function on the validation filepaths to create a DataFrame for validation\n",
    "val_df = proc_img(val_filepaths)\n",
    "val_df.dropna(inplace=True)\n",
    "\n",
    "if {len(train_df.Label.unique())} != {len(test_df.Label.unique())} != {len(val_df.Label.unique())}:\n",
    "    print('incorrect amount of Labels, please do not continue...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca00ba8-49e1-4e08-b83f-83b220495eda",
   "metadata": {
    "tags": []
   },
   "source": [
    "And here's how the datasets look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549269f5-ea2e-4ee3-8b0a-e933f28e60c1",
   "metadata": {
    "tags": [
     "block:load_preprocess_images"
    ]
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('#### Training set ####')\n",
    "print()\n",
    "print('-- Training set --\\n')\n",
    "print(f'Number of pictures: {train_df.shape[0]}\\n')\n",
    "print(f'Number of different labels: {len(train_df.Label.unique())}\\n')\n",
    "print(f'Labels: {train_df.Label.unique()}')\n",
    "print()\n",
    "print('#### Test set ####')\n",
    "print()\n",
    "print('-- Test set --\\n')\n",
    "print(f'Number of pictures: {test_df.shape[0]}\\n')\n",
    "print(f'Number of different labels: {len(test_df.Label.unique())}\\n')\n",
    "print(f'Labels: {test_df.Label.unique()}')\n",
    "print()\n",
    "print('#### Validation set ####')\n",
    "print()\n",
    "print('-- Validate set --\\n')\n",
    "print(f'Number of pictures: {val_df.shape[0]}\\n')\n",
    "print(f'Number of different labels: {len(val_df.Label.unique())}\\n')\n",
    "print(f'Labels: {val_df.Label.unique()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493e33c",
   "metadata": {
    "papermill": {
     "duration": 0.031338,
     "end_time": "2023-03-13T20:40:29.032984",
     "exception": false,
     "start_time": "2023-03-13T20:40:29.001646",
     "status": "completed"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# The DataFrame with the filepaths in one column and the labels in the other one\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0dad0a-5661-4bef-beea-92bb9853b6cc",
   "metadata": {
    "papermill": {
     "duration": 0.005834,
     "end_time": "2023-03-13T20:40:21.330048",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.324214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's see what some of the training dataset looks like. We'll pull out a set of images (one per category) from the dataset with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22f07f",
   "metadata": {
    "papermill": {
     "duration": 12.274186,
     "end_time": "2023-03-13T20:40:41.317059",
     "exception": false,
     "start_time": "2023-03-13T20:40:29.042873",
     "status": "completed"
    },
    "tags": [
     "block:"
    ]
   },
   "outputs": [],
   "source": [
    "# 1. Select a diverse set of images for visualization:\n",
    "# Create a DataFrame that includes only one image from each category to showcase variety\n",
    "df_unique = train_df.copy().drop_duplicates(subset=[\"Label\"]).reset_index()\n",
    "\n",
    "# 2. Prepare a visually appealing display:\n",
    "# Create a figure with 5 subplots arranged horizontally to display multiple images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15, 10),\n",
    "                            subplot_kw={'xticks': [], 'yticks': []})  # Hide unnecessary axes\n",
    "\n",
    "# 3. Iterate through selected images and visualize them:\n",
    "for i, ax in enumerate(axes.flat):  # Flatten the axes for easier iteration\n",
    "    if i < len(df_unique):  # Ensure we don't exceed available images\n",
    "        # Load and display the image\n",
    "        ax.imshow(plt.imread(df_unique.iloc[i]['Filepath']))\n",
    "        # Add a clear label for context\n",
    "        ax.set_title(df_unique.iloc[i]['Label'], fontsize=12)\n",
    "\n",
    "# 4. Refine layout for better viewing:\n",
    "plt.tight_layout(pad=0.5)  # Adjust spacing for visual clarity\n",
    "\n",
    "# 5. Display the plot:\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c86e6f-8584-41b8-818e-3da66c530cf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "And with that, our data is ready for it's preperation for model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22622444",
   "metadata": {
    "papermill": {
     "duration": 0.028726,
     "end_time": "2023-03-13T20:40:41.374706",
     "exception": false,
     "start_time": "2023-03-13T20:40:41.345980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **3. Preparing the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69182377-1c96-48e9-b170-36f4ca74e2a5",
   "metadata": {
    "papermill": {
     "duration": 0.005834,
     "end_time": "2023-03-13T20:40:21.330048",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.324214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To detect produce as it is scanned at the checkout, we are going to train a **MobileNetV2** object recognition model. MobileNetV2 is a lightweight convolutional neural network (CNN) architecture designed for smaller devices, such as mobile and embedded devices. MobileNetV2 excels at image classification tasks like identifying objects in pictures while requiring minimal computing power and memory, making it perfect for applications such as this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88ebc8-7de3-4d38-be4d-fad36839ba5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "To train a MobileNetV2 model, we'll first declare the **image data generators** specifically written for MobileNetV2 for each of the training, testing and validation datasets. The `ImageDataGenerator` class efficiently prepares and delivers batches of images during training. It utilizes a custom preprocessing function (`tf.keras.applications.mobilenet_v2.preprocess_input`) that prepares each image to MobileNetV2's requirements, typically including scaling pixel values and subtracting specific means from each color channel. This streamlined approach ensures images are formatted correctly for optimal performance within the MobileNetV2 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee102dd6-cdbc-4cfa-bd42-9adec37f17dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **Click to expand for a further deep dive into each of the parameters used to configure the data generators below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c9b91-4404-474a-87c8-9c4c388d4715",
   "metadata": {
    "tags": []
   },
   "source": [
    "A few things our data generators will perform on our dataset include:\n",
    "- Utilizes tf.keras.preprocessing.image.ImageDataGenerator for real-time data augmentation during training.\n",
    "- The training images are loaded from a Pandas DataFrame using the specified 'Filepath' column for input data and 'Label' column for output data.\n",
    "- Images are resized to (224, 224) pixels, and the MobileNetV2 preprocessing function is applied.\n",
    "- Data augmentation techniques like rotation, zoom, shift, shear, and horizontal flip are employed to enhance the diversity of the training dataset.\n",
    "- Batches of 32 images are generated, and the order is shuffled for each epoch.\n",
    "\n",
    "They will be defined by the following parameters:\n",
    "\n",
    "1. **flow_from_dataframe:** This is a method provided by the `ImageDataGenerator` class in TensorFlow Keras. It specifically creates a data generator that reads data from a Pandas DataFrame. \n",
    "\n",
    "2. **dataframe (train_df):** This refers to the Pandas DataFrame you provide. It's expected to contain two columns:\n",
    "   - **x_col ('Filepath')**: This column should contain the file paths of the images you want to train on.\n",
    "   - **y_col ('Label')**: This column should hold the corresponding labels for each image, likely representing the category it belongs to (e.g., \"cat\", \"dog\", \"car\").\n",
    "\n",
    "3. **target_size=(224, 224):** This specifies the desired size for the resized images. In this case, all images will be resized to 224 pixels wide and 224 pixels tall. Resizing images ensures consistency and reduces computational complexity during training.\n",
    "\n",
    "4. **color_mode='rgb':** This indicates that the images are assumed to be in RGB color mode, which uses three channels (red, green, blue) to represent color information.\n",
    "\n",
    "5. **class_mode='categorical':** This signifies that the task is categorical classification. The model will learn to classify images into predefined categories based on the labels provided.\n",
    "\n",
    "6. **batch_size=32:** This defines the number of images that will be processed together as a single batch during training.  A batch size of 32 means the model will receive 32 images and their labels at a time to update its internal parameters.\n",
    "\n",
    "7. **shuffle=True:** This setting enables shuffling of the image order within each batch. Shuffling helps prevent the model from overfitting to the training data by ensuring it sees images in a random order during each training epoch (iteration).\n",
    "\n",
    "8. **seed=0:**  This sets a seed value for the random number generator used for shuffling and data augmentation.  Using a fixed seed ensures that the data order and augmentations are reproducible, meaning you'll get the same results when re-running the code. \n",
    "\n",
    "9. **Data Augmentation:** These are the following techniques applied to randomly modify the training images:\n",
    "    - **rotation_range=30:** Images are randomly rotated up to 30 degrees in either direction.\n",
    "    - **zoom_range=0.15:** Images are randomly zoomed in or out by up to 15%.\n",
    "    - **width_shift_range=0.2, height_shift_range=0.2:** Images are randomly shifted horizontally or vertically by up to 20% of their width or height, respectively.\n",
    "    - **shear_range=0.15:** A shearing transformation is applied to distort the image slightly.\n",
    "    - **horizontal_flip=True:** Images are randomly flipped horizontally (mirrored).\n",
    "\n",
    "10. **fill_mode=\"nearest\":** This setting determines how empty spaces created by image transformations (e.g., zooming) are filled. Here, the \"nearest\" neighbor pixel filling is used, meaning the value of the closest pixel is copied to fill the empty space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8827b44-2baa-47c5-9cd7-9347649cf1c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abfb275-3052-483b-b6f5-5140e1c2a493",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, let's leverage the pre-configured scripts from the Tensorflow library to declare an instance of MobileNetV2 model (stored in the repo for your convenience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdb4f7",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 4.031283,
     "end_time": "2023-03-13T20:40:46.611646",
     "exception": false,
     "start_time": "2023-03-13T20:40:42.580363",
     "status": "completed"
    },
    "tags": [
     "block:"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the pretained model\n",
    "pretrained_model = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "    input_shape=(224, 224, 3), \n",
    "    include_top=False, \n",
    "    weights=None,\n",
    "    pooling='avg')\n",
    "    \n",
    "pretrained_model.load_weights('resources/mobilenet_v2.h5')\n",
    "    \n",
    "pretrained_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e909289-56fd-4c4a-9712-88de981f308e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll set up the data generator for **training** dataset, applying the appropriate transforms. **Confirm** that the numbers match what we know from above about these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59685a-bbf0-413a-82da-3de8b32e4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image data generator for preprocessing train images using MobileNetV2\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56393d-273a-4a14-aaa0-2cdc628e8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a flow of images and labels from a Pandas dataframe for training\n",
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=train_df, # Use the specified Pandas dataframe\n",
    "    x_col='Filepath', # Use the 'Filepath' column as the input (x) data\n",
    "    y_col='Label', # Use the 'Label' column as the output (y) data\n",
    "    target_size=(224, 224), # Resize the images to the specified dimensions\n",
    "    color_mode='rgb', # Use RGB color mode\n",
    "    class_mode='categorical', # Use categorical classification\n",
    "    batch_size=32, # Generate batches of 32 images at a time\n",
    "    shuffle=True, # Shuffle the order of the images\n",
    "    seed=0, # Use a fixed seed for reproducibility\n",
    "    rotation_range=30, # Randomly rotate images up to 30 degrees\n",
    "    zoom_range=0.15, # Randomly zoom images up to 15%\n",
    "    width_shift_range=0.2, # Randomly shift images horizontally up to 20%\n",
    "    height_shift_range=0.2, # Randomly shift images vertically up to 20%\n",
    "    shear_range=0.15, # Randomly apply shearing transformations to images\n",
    "    horizontal_flip=True, # Randomly flip images horizontally\n",
    "    fill_mode=\"nearest\", # Use the nearest pixel to fill any empty spaces created by image transformations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197ed18-937d-42b9-9ba0-bbe53e732856",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then, the **testing** dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015458dc-9ec2-49e6-8182-c051fcbfd60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image data generator for preprocessing test images using MobileNetV2\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c69e91-0867-4891-affc-ede2dd1fd886",
   "metadata": {
    "papermill": {
     "duration": 1.1483,
     "end_time": "2023-03-13T20:40:42.551388",
     "exception": false,
     "start_time": "2023-03-13T20:40:41.403088",
     "status": "completed"
    },
    "tags": [
     "block:loadimages",
     "prev:load_preprocess_images"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate a flow of images and labels from a Pandas dataframe\n",
    "test_images = test_generator.flow_from_dataframe(\n",
    "    dataframe=test_df, # Use the specified Pandas dataframe\n",
    "    x_col='Filepath', # Use the 'Filepath' column as the input (x) data\n",
    "    y_col='Label', # Use the 'Label' column as the output (y) data\n",
    "    target_size=(224, 224), # Resize the images to the specified dimensions\n",
    "    color_mode='rgb', # Use RGB color mode\n",
    "    class_mode='categorical', # Use categorical classification\n",
    "    batch_size=32, # Generate batches of 32 images at a time\n",
    "    shuffle=False # Do not shuffle the order of the images\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc400eaa-9b3a-42f7-b7bd-66b0e156e44d",
   "metadata": {
    "tags": []
   },
   "source": [
    "And finally, the **validation** dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32ff4c-db62-4610-972b-d0c9368ec2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image data generator for preprocessing validation images using MobileNetV2\n",
    "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc65ca-4583-40b4-adf3-15e7bc462b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a flow of images and labels from a Pandas dataframe for validation\n",
    "val_images = val_generator.flow_from_dataframe(\n",
    "    dataframe=val_df, # Use the specified Pandas dataframe\n",
    "    x_col='Filepath', # Use the 'Filepath' column as the input (x) data\n",
    "    y_col='Label', # Use the 'Label' column as the output (y) data\n",
    "    target_size=(224, 224), # Resize the images to the specified dimensions\n",
    "    color_mode='rgb', # Use RGB color mode\n",
    "    class_mode='categorical', # Use categorical classification\n",
    "    batch_size=32, # Generate batches of 32 images at a time\n",
    "    shuffle=True, # Shuffle the order of the images\n",
    "    seed=0, # Use a fixed seed for reproducibility\n",
    "    rotation_range=30, # Randomly rotate images up to 30 degrees\n",
    "    zoom_range=0.15, # Randomly zoom images up to 15%\n",
    "    width_shift_range=0.2, # Randomly shift images horizontally up to 20%\n",
    "    height_shift_range=0.2, # Randomly shift images vertically up to 20%\n",
    "    shear_range=0.15, # Randomly apply shearing transformations to images\n",
    "    horizontal_flip=True, # Randomly flip images horizontally\n",
    "    fill_mode=\"nearest\" # Use the nearest pixel to fill any empty spaces created by image transformations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc7ff4",
   "metadata": {
    "papermill": {
     "duration": 0.028545,
     "end_time": "2023-03-13T20:40:46.668800",
     "exception": false,
     "start_time": "2023-03-13T20:40:46.640255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **4. Preparing for Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911128fd-6d5e-4fc3-afda-2a8c9914ecfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that we've prepared our datasets and created an instance of the base model that we will train, our last step is to declare important machine learning parameters to our training run. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9244865f-69e0-443f-8a12-0cc35658e6df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A Brief Introduction to Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1563319-276e-4afd-bee9-cd522d11a2ff",
   "metadata": {
    "papermill": {
     "duration": 0.005834,
     "end_time": "2023-03-13T20:40:21.330048",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.324214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The ways in which machines learn concepts from new information is not dissimilar to the way humans do. Like humans, the method by which a machine learns is to be repeatedly exposed to data that reinforces that concept. Additionally, for any given person and any given concept, there is a better or best method to learn that concept (and of course, there are certainly suboptimal ways to learn that concept also!). In machine learning, these methods are defined by a set of **parameters** that will define how and when a model will be exposed to new data, such that it can best extrapolate patterns from it. \n",
    "\n",
    "Three important parameters that we will introduce to you to facilitate this training run are **epochs**, **batch sizes** and **patience**. \n",
    "\n",
    "\n",
    "Imagine you're studying for a history exam using the course textbook made up of 10 chapters.\n",
    "\n",
    "#### **Epochs:** \n",
    "\n",
    "A single epoch is going through all 10 chapters **once**. Each time you re-read the textbook, you change your understanding of the content changes - some concepts are strengthened, some negated, others changed entirely. The number of **epochs** is the total amount of times you read the whole textbook through all 15 chapters. Read the textbook too few times and you will not be able to identify answers in the final quiz (known as **underfitting**). Read the book too many times, however, and you may memorize the content so well that any question that you will struggle with any question that does not strictly adhere to what you read in the book (known as **overfitting**). \n",
    "\n",
    "For our model, instead of a textbook of 10 chapters, we have a **training** dataset of hundreds of images. Setting the epoch parameter to **15** will result in the training run iterating over the entire dataset of images **15 times**.\n",
    "\n",
    "#### **Batch Size:**  \n",
    "\n",
    "A batch size is how many chapters you can read in a single sitting. If you want to speed read to get the whole book read in 30 minutes, you probably haven't properly absorbed any of the content. To properly engage with the content, you read a chapter or even sections of chapters at a time in **batches**. Too few chapters at a time will be slow, and you'll likely forget how concepts in chapters correlated to one another, affecting your learning. Read too fast and you'll miss key historic details.\n",
    "\n",
    "For our training run, we will process **32 images at a time**. \n",
    "\n",
    "#### **Patience:** \n",
    "\n",
    "To validate your understanding, after each textbook readthrough, you quiz yourself on questions that are relevant but were **not** in the example questions of the textbook. If by the fourth time you've read through the textbook, your are unable to answer the new questions correctly (that is to say, you cannot **abstract** your understanding to questions or content beyond what was in the textbook), you will stop studying and go talk with your professor - clearly there is something you're not getting, and there might be a better way to learn it. Do this too early and your professor will tell you that you didn't spend enough time learning the content to properly grasp it. Do this too late and you waste valuable study time trying to grasp a concept with an incorrect methodolgy. \n",
    "\n",
    "For our training run, we will use random images from the **validation** dataset to validate our model's learning as it continues to pass through each epoch. If after 4 epochs, the objects the models detect in the random test images are not what is labelled, then the training run will halt. In these instances, usually the dataset or the model parameters will need to be tweaked.  \n",
    "\n",
    "To summarize this training run: The model will run through the **training** dataset **15** times, **32 photos at a time**. Using the **validation** dataset, we will validate the model's detection accuracy every runthrough of the dataset - and halt the training run if the accuracy is not adequate after **4** consecutive runs.\n",
    "\n",
    "Let's define those parameters for our run now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5546f-df6c-4268-afe1-0c260f27848f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of epochs to train the model\n",
    "param_epoch = 2\n",
    "\n",
    "# Define the batch size to use for training and validation\n",
    "param_batch_size = 32\n",
    "\n",
    "# Define the number of epochs to wait before early stopping if the validation loss does not improve\n",
    "param_patience = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fae775-f2ad-42d2-8816-89fa72f84a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What's in a layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d079e-036e-4229-a171-b645524ae198",
   "metadata": {
    "tags": []
   },
   "source": [
    "In neural networks, a **layer** is a group of interconnected processing units (known as nodes or neurons) that processes and transforms input data to produce an output. By combining multiple layers in a neural network, complex relationships and patterns within the data can be learned and utilized for various machine learning tasks - such as being able to recognise pixel patterns that make up specific objects, like fruits or vegetables! \n",
    "\n",
    "To describe the types of layers, let's consider an example related to our produce recognition model. Each produce item will have different colors, shapes, and sizes that make it uniquely identifyable. Each layer is a friend helping you determine what any given piece of produce is. How will they all work together? In layers!\n",
    "\n",
    "1. **Input Layer**: You start by showing your friend the fruits and vegetables, one by one. They take a good look and notice all the details - like whether they're round, long, green, or yellow.\n",
    "\n",
    "2. **Hidden Layers**: Your friend has a few friends of their own. They pass the items along to them, along with the information they observed about that particlar produce item, to those friends - each of whom will each focuses on different features. One friend might pay attention to the color, another to the shape, and so on. They all talk to each other and combine their observations to present the information to one more person...\n",
    "\n",
    "3. **Output Layer**: Finally, after all this analyzing, your best friend recieves all this analysis and makes a final guess. They say, \"I think this is an apple,\" or \"This looks like a carrot.\" This is the output - the conclusion your best friend reached after all the thinking and analyzing between your other friends.\n",
    "\n",
    "So, each layer in the neural network helps to break down the information about the input image of a fruit or vegetable, step by step, until it can make a statistically probable guess about what is present in the image. The neural network we are building to do this is known as a **classification model**. \n",
    "\n",
    "We will now adds some new layers to the MobileNetV2 model we declared earlier to make the model better at recognizing patterns in the data. After that, we will figure out how many different categories our dataset has based on the labels. We'll then set up the final layer of the neural network to give an answer for each category. Finally, we'll set up the rules for training the model, like how it should learn and how it should measure its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacde793-1e71-4a6c-8ea4-9d76e6faaabc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Get the input layer of the pre-trained model\n",
    "inputs = pretrained_model.input\n",
    "\n",
    "# Add new layers to improve the pre-trained model for our specific task\n",
    "# First new layer with 128 units and ReLU activation\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n",
    "# Second new layer with 128 units and ReLU activation\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "# Figure out how many categories (labels) our data has\n",
    "labels = len(train_df.Label.unique())\n",
    "\n",
    "# Add the final layer to give answers for each category\n",
    "# This layer has softmax activation to output probabilities for each category\n",
    "outputs = tf.keras.layers.Dense(labels, activation='softmax')(x)\n",
    "\n",
    "# Create the neural network model with input and output layers\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model with training rules\n",
    "# Using Adam optimizer, categorical cross-entropy loss, and accuracy as the metric\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6593cc5-86b9-4791-9243-57f8b1436a5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Lastly, we'll confirm the identified labels for our training are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490c41c-e1e0-4073-82cb-711e7f282bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = (train_images.class_indices)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030cb390-da89-4b12-9445-583e94baa24b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **5. It's time to train!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0fbc14-ce4f-41df-99eb-618fb4b06b0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that we have prepared our data and our model, it is time to begin a **training experiment**. We *could* just run a notebook cell or a single Python script, but doing so would result in limitations when it comes to managing and scaling our experiments, especially as the complexity of our models and datasets grow as our retail stores gather more data and model feedback. Enter MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b96d1b-f324-4ba4-a7c7-745cea06d98b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introducing MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc33434-e41d-42fc-9302-ff71f53f2b6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "MLflow is an open-source machine learning platform that is designed to help data scientists and engineers simplify the intricacies inherent in machine learning endeavors, rendering their projects more manageable, collaborative traceable, and reproducible.\n",
    "\n",
    "Before the world was introduced to MLflow, engineers managed machine learning tasks using a mix of tools and methods, which lacked consistency and efficiency. They manually explored data, developed models with libraries like scikit-learn or TensorFlow, and tracked experiments in spreadsheets. Model management relied on conventional version control like Git, while deployment and monitoring required custom scripts. Collaboration and reproducibility suffered due to ad-hoc sharing (such as sharing large model files over networks) and difficulties in replicating experiments across environments (often due to confliciting libraries or dependencies).\n",
    "\n",
    "Today, MLflow is the most utilized and contributed machine learning platform and comes natively installed with **HPE Ezmeral Unified Analytics**.\n",
    "\n",
    "Over the course of the remainder of this exercise and the next, we will learn to leverage the key features of MLflow to build our produce recognition model, including:\n",
    "\n",
    "1. **Experiment Tracking**: MLflow enables users to meticulously track experiment details such as parameters and metrics. This functionality facilitates the comparison of different runs and aids in analyzing results, empowering data scientists to discern which models yield superior performance and allowing for fine-tuning of their approaches.\n",
    "\n",
    "2. **Model Management**: The platform provides tools for packaging and storing trained models in a reusable format. This ensures that models can be effortlessly applied to new data or shared with collaborators.\n",
    "\n",
    "3. **Model Registry**: MLflow offers a Model Registry, allowing for centralized management of models and their various versions. This feature proves invaluable in production environments where model governance and monitoring are paramount.\n",
    "\n",
    "4. **Project Management**: MLflow facilitates the packaging of entire machine learning projects, encompassing code and environment settings, thereby streamlining sharing and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0c8cd-f3b0-4ed7-9441-a9a47c75c8e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating an MLflow Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b2f290-1c84-421a-83be-e0efb8d76389",
   "metadata": {
    "tags": []
   },
   "source": [
    "To start, let's create an MLflow **experiment**, which is a container for running individual executions of a **machine learning training script** (below) with specific **parameters** (above!). Having an Experiment will help organize and manage different iterations of our model's training, allowing for easy comparison, reproducibility, and progress tracking in machine learning projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e07b9-be28-41fc-93f7-cafe2e439fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caefea22-c14d-401d-9c72-a46513b6d854",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's give both our **experiment** and our **model** a name and declare the path in the artifacts associated with this Experiment as to where our model is to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c838ed8-6382-448f-a252-97a296bbd914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment variables for MLflow\n",
    "experiment_name = \"smart-retail\"\n",
    "model_name = \"retail-demo\"\n",
    "artifact_path = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64fa792-80fa-42f0-915c-939471880102",
   "metadata": {
    "tags": []
   },
   "source": [
    "We'll confirm that no experiments with the same name currently exist. If not, we'll create that experiment in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f26196-bc00-4a67-afad-5d9c1e8086a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Search for existing experiments \n",
    "experiments = mlflow.search_experiments(view_type=3, order_by=[\"experiment_id\"], filter_string=\"name = '\" + experiment_name + \"'\")\n",
    "\n",
    "# Check if the existing experiments list is not empty before accessing its elements\n",
    "if experiments and experiments[0].lifecycle_stage == 'deleted':\n",
    "    MlflowClient().restore_experiment(experiments[0].experiment_id)\n",
    "\n",
    "# Create MLflow experiment and generate MLflow run name\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Define time-stamped name for this specific run.\n",
    "run_name = \"retail-demo-\" + time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36f5176-fc15-4d8f-a20d-0f00e3f21057",
   "metadata": {
    "tags": []
   },
   "source": [
    "And with that, we've connected to the native MLflow instance running on Unified Analytics and created an experiment - no setup of or connection to MLflow from this notebook required. This is the power of running applications such as MLflow on **HPE Ezmeral Unified Analytics**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41aa2e-0316-4e60-abcd-06b5ae37c255",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Beginning the training run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d31d9-dc1b-4c52-a280-eab59fa243e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our training begins with the `model.fit()` function in the middle of the code block. Before that, we'll **log** the **parameters** we declared earlier, such that we know what they were for this run and we can **compare** them with the **accuracy** of the model. Tweaking these parameters between runs on the same dataset is known as **tuning** a model.\n",
    "\n",
    "The training of the model will then run on the specified hyperparameters and incorporate an early stopping callback to prevent overfitting (determined by our **patience** parameter). Once training is complete, the trained model is saved as an **MLflow artifact**, along with metrics such as training and validation accuracy. Notably, the model is saved with a specific version, such that we can choose what **version** we want to test new data on or even **serve** to a production application. Throughout the run, the **validation** dataset is employed to evaluate the model's performance on unseen data, aiding in preventing overfitting and ensuring the model's generalizability.\n",
    "\n",
    "Let's begin our training run! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047576f5-727f-4811-a9d1-6a0ddf697a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start a new MLflow run\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "    \n",
    "# Start a new MLflow run\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    \n",
    "    # Log the model architecture as a Keras summary\n",
    "    mlflow.autolog()  \n",
    "    \n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_param(\"batch_size\", param_batch_size)\n",
    "    mlflow.log_param(\"epochs\", param_epoch)\n",
    "    mlflow.log_param(\"patience\", param_patience)\n",
    "    \n",
    "    # Get the full model path\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    artifact_uri = mlflow.get_artifact_uri(run_id)\n",
    "    #artifact_path = \"model\"\n",
    "    test_uri = \"s3://mlflow/3/{run_id}/artifacts/{artifact_path}\".format(run_id=run_id, artifact_path=artifact_path)\n",
    "\n",
    "    # Train the model with the specified hyperparameters and callbacks\n",
    "    history = model.fit(\n",
    "        train_images,\n",
    "        validation_data=val_images,\n",
    "        batch_size=param_batch_size,\n",
    "        epochs=param_epoch,\n",
    "        verbose='auto',\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=param_patience,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Save the model in TensorFlow Serving format\n",
    "    tf.saved_model.save(model, \"tf_serving_model/1\")\n",
    "\n",
    "    # Log the saved model as an artifact\n",
    "    mlflow.log_artifact(\"tf_serving_model\")\n",
    "    \n",
    "    # Log values/metric from all epochs in mlflow\n",
    "    for item in history.history.items():\n",
    "        for value in item[1]:\n",
    "            mlflow.log_metric(item[0], value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b370b9d-7227-4166-9c59-32992856f1f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "And with that, we have our own produce **classification model**!\n",
    "\n",
    "But the question begs... how good is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d5c3f3-af34-4f35-a4c0-b0758b741acc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **6. Model Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de113b8b-81b7-4597-a34c-f08f0eaebcc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "This code segment predicts the labels of test images using a pre-trained model. It first predicts the probabilities for each class using the model's `predict` function and then selects the class with the highest probability for each image. Next, it maps the numeric labels to their corresponding class names using a dictionary created from the training data's class indices. This step converts the numeric predictions into human-readable class names. The true labels for the test images are also mapped to class names. Then, the code calculates the accuracy of the model's predictions on the test set using the `accuracy_score` function from scikit-learn, comparing the predicted labels with the true labels, and prints the accuracy percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59124e",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 106.90135,
     "end_time": "2023-03-13T21:24:09.563300",
     "exception": false,
     "start_time": "2023-03-13T21:22:22.661950",
     "status": "completed"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Predict labels for the test images using the trained model\n",
    "pred = model.predict(test_images)\n",
    "# Convert predicted probabilities into class labels by selecting the class with the highest probability for each image\n",
    "pred = np.argmax(pred, axis=1)\n",
    "\n",
    "# Map numeric labels to their corresponding class names\n",
    "labels = train_images.class_indices\n",
    "labels = dict((v, k) for k, v in labels.items())\n",
    "pred = [labels[k] for k in pred]\n",
    "\n",
    "# Map true labels of test images to their corresponding class names\n",
    "y_test = [labels[k] for k in test_images.classes]\n",
    "\n",
    "# Compute accuracy of the model's predictions on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, pred)\n",
    "\n",
    "# Print the accuracy percentage\n",
    "print(f'Accuracy on the test set: {100*acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd11ad2-a841-40f1-98f6-539dbaa46e79",
   "metadata": {
    "tags": []
   },
   "source": [
    "For a **first run**, that is pretty good! We will use this model for the remainder of our exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c25ec-a82f-4a5b-b5e8-070d0c5f4eb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validating with your own image!\n",
    "\n",
    "We'll set up a little `predict` function which will take any image we give it, prepare the image for the way the model likes it, then parse it to the model for it to predict what, if any, produce is in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b3b92-545b-4c29-956d-e8fb2bfa6583",
   "metadata": {
    "papermill": {
     "duration": 0.168369,
     "end_time": "2023-03-13T21:24:26.545710",
     "exception": false,
     "start_time": "2023-03-13T21:24:26.377341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(location):\n",
    "    \n",
    "    #Check to see if a web URL or a local file is being parsed \n",
    "    if \"http\" in location:\n",
    "        response = requests.get(location)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "        img=load_img(location,target_size=(224,224,3))\n",
    "    \n",
    "    #Convert the image into the dimensions of the model's input layer.\n",
    "    img=img_to_array(img)\n",
    "    img=img/255\n",
    "    img=np.expand_dims(img,[0])\n",
    "    \n",
    "    # Infer the model with the image\n",
    "    answer=model.predict(img)\n",
    "\n",
    "    # Format the answer\n",
    "    y_class = answer.argmax(axis=-1)\n",
    "    y = \" \".join(str(x) for x in y_class)\n",
    "    y = int(y)\n",
    "    res = labels[y]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd4ca5-9ae5-4cdc-b74f-cd78f343a3bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, **go out** onto Google Images and find any image with a **orange**, an **apple** or a **lemon**. Copy the link into the `online_url` variable.\n",
    "\n",
    "If you are in an offline/proxy environment, upload your file to the same directory as this notebook and call it **test_image.jpg**. An has been supplied if you just wish to run the cell without finding your own image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac911ac6-c330-42b1-8b72-381f27df8e3d",
   "metadata": {
    "papermill": {
     "duration": 0.168369,
     "end_time": "2023-03-13T21:24:26.545710",
     "exception": false,
     "start_time": "2023-03-13T21:24:26.377341",
     "status": "completed"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the path for our image, be it a web URL or a local file\n",
    "online_url = \"\"\n",
    "local_url = os.getcwd() + \"/images/test_image.jpg\"\n",
    "\n",
    "if online_url:\n",
    "    image_url = online_url\n",
    "else:\n",
    "    image_url = local_url\n",
    "\n",
    "#Infer our model with the image to make a prediction\n",
    "predicted_label = predict(image_url)\n",
    "print(\"The model predicts: \" + str(predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599540f-2b07-4d22-b1e2-f8c1ac438ad5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Did the model correctly guess what was in your image? If so, great! If not... also great!\n",
    "\n",
    "**Incorrect predictions** from models provide vital feedback that can help you to understand the model's behaviour and improve either the training dataset or the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c65778b-3b0d-4f5f-9626-f9e9cd05c945",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9ad36-7cb9-472f-9300-ded9f7f7b87c",
   "metadata": {
    "tags": []
   },
   "source": [
    "In a single exercise, you have learned the basics of machine learning - including how to import and prepare a dataset, define training parameters, trained a model using Tensorflow and set up an MLflow experiment to monitor the training run.\n",
    "\n",
    "Now, we have a model file that we can run `model.precict` on any new image to detect any fresh produce our retail customers are scanning at the checkout! \n",
    "\n",
    "In the next exercise, we will run through MLflow in more detail. Training and fine tuning models is an interative process, so ensuring that our models are appropriately registered and training experiments are tracked not only provides us with new, identifyable base models for which to train new data on, but when our model is deployed to hundreds of retail stores across several countries, we can be sure we're using the right one! \n",
    "\n",
    "This will put you in good sted for the final two exercises, where you will learn to manage several versions of the model you just created and **serve** it to our in-store retail application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gcr.io/mapr-252711/kubeflow/notebooks/jupyter-tensorflow-cuda-full:ezaf-fy23-q2",
   "experiment": {
    "id": "new",
    "name": "jk-fruit-demo"
   },
   "experiment_name": "jk-fruit-demo",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "random",
     "algorithmSettings": [
      {
       "name": "random_state",
       "value": "10"
      },
      {
       "name": "acq_optimizer",
       "value": "auto"
      },
      {
       "name": "acq_func",
       "value": "gp_hedge"
      },
      {
       "name": "base_estimator",
       "value": "GP"
      }
     ]
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "additionalMetricNames": [],
     "goal": 1,
     "objectiveMetricName": "stage",
     "type": "maximize"
    },
    "parallelTrialCount": 3,
    "parameters": [
     {
      "feasibleSpace": {
       "max": "50",
       "min": "1",
       "step": "1"
      },
      "name": "param_epoch",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "list": [
        "32",
        "64"
       ]
      },
      "name": "param_batch_size",
      "parameterType": "categorical"
     },
     {
      "feasibleSpace": {
       "max": "10",
       "min": "1",
       "step": "1"
      },
      "name": "param_patience",
      "parameterType": "int"
     }
    ]
   },
   "katib_run": false,
   "pipeline_description": "fruit-veg",
   "pipeline_name": "fruit-veg",
   "snapshot_volumes": false,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:add-ldapcert-secret:true",
    "label:add-sssd-secret:true",
    "label:add-user-s3-secret:true"
   ],
   "storage_class_name": "dataplatform",
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/mnt/shared/",
     "name": "kubeflow-pipeline",
     "size": 1,
     "size_type": "Gi",
     "snapshot": false,
     "snapshot_name": "",
     "type": "pvc"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2679.282234,
   "end_time": "2023-03-13T21:24:51.692744",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-13T20:40:12.410510",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
