{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801896dd-7519-4344-8cac-2a9e6ce22a74",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bac2dc-3599-441e-ab23-d952d84d1e24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 1:** Exploring Sales Data with Apache Spark\n",
    "\n",
    "This exercise will introduce **Apache Spark on HPE Ezmeral Unified Analytics**. We'll leverage Spark's powerful distributed processing capabilities to analyze sales information and uncover insights of sales data across three major European grocery stores.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Set up a Spark session for interacting with data.\n",
    "- Generate sample sales data for different countries and currencies.\n",
    "- Explore techniques for data loading, transformation, and analysis using Spark SQL and DataFrames.\n",
    "- Create Delta Tables and perform version control.\n",
    "\n",
    "Feel free to modify and extend the code examples to suit your specific data analysis needs.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf107c4-4241-4b59-968e-c0cd1ea13f8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Prerequisites:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9744912-6776-4d76-a074-770fe2be4750",
   "metadata": {
    "tags": []
   },
   "source": [
    "As instructed in the [Introductory notebook](./00.introduction.ipynb), ensure that you have run `pip install -r requirements.txt` in a Terminal window, located in the same working directory, prior to running this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34b061-c46d-4864-ad93-36367fd76695",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Important:</b> Make sure you selected <b>PySpark</b> for your notebook kernel - check the top right corner!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbeea39-73eb-4ed7-9ab8-c6bd3c5aa8c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. Create Spark Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366089e-a3b4-4c9e-ba53-fcbad54f0034",
   "metadata": {
    "tags": []
   },
   "source": [
    "Think about the most recent Excel spreadsheet you edited. It probably had tens or even hundreds of rows across tens of columns. When you run an Excel command, such as a *SUM()* or a *VLOOKUP()*, you may have noticed that it took a far bit of time to process. Maybe, even the fans of your laptop sped up a bit as your computer worked to crunch the numbers. \n",
    "\n",
    "Now, scale that same command out to a spreadsheet with tens of **millions** of rows across **thousands** of columns. That is the Big Data that companies must work with on a daily basis, and no single PC is going to run any *VLOOKUP* command on data of that size.\n",
    "\n",
    "Instead of spreadsheets, the enterprise world is largely built upon **tables** in a variety of formats. To query these tables to retrieve certain data takes a **mammoth** amount of compute. It makes no sense to have a single **compute server** executing these queries - it would be far faster to parallelize queries across several computers. Enter **Apache Spark**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce431084-dbf6-46c0-9378-d31139a83d2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introduction to Apache Spark on HPE Ezmeral Unified Analytics\n",
    "\n",
    "Apache Spark is a popular open-source big data framework that **distributes the computations** required to perform queries on large sets of data. This distribution, along with working with data in-memory rather than directly from storage disks, drastically brings down the time usually taken to query and index data. The combination of speed, versatility, and ease of use made Spark the go-to framework when working with big data. \n",
    "\n",
    "Apache Spark comes pre-installed with **HPE Ezmeral Unified Analytics** and can leverage as much or as little of the compute available in a Unified Analytics cluster as a user desired. The core components of an Apache Spark deployment include:\n",
    "\n",
    "<img src=\"./images/exercise1/spark_archi.PNG\" alt=\"Drawing\" style=\"width: 60%;\"/>\n",
    "\n",
    "**Driver:** The driver program coordinates the execution of Spark jobs. It submits tasks to executors, schedules operations, and manages communication between various components.\n",
    "\n",
    "**Workers:** These are machines in the Spark cluster that manage executors. Each worker runs one or more executors. When running Spark on a HPE Ezmeral Unified Analytics deployment, Spark Workers are Kubernetes pods distributed among worker nodes of the Unified Analytics cluster, allowing them to scale across multiple machines as required. \n",
    "\n",
    "**Executors:** Executors reside on worker nodes and carry out the actual computations assigned by the driver program. They partition and distribute the workload across machines in the cluster.\n",
    "\n",
    "**JVM:**  Spark utilizes the Java Virtual Machine (JVM) on each worker node to execute executors.\n",
    "\n",
    "On **HPE Ezmeral Unified Analytics**, you will use Apache Spark to analyze large datasets at high speed with a unified platform for batch processing, streaming, and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be59379-6d08-426b-b759-80942d845b76",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a Spark Interactive Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07e337-f157-4cc2-9160-6f7cbaa69aaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's begin using Spark! Here, you use Unified Analytics' native integration of **Apache Livy** to create and manage an interactive Spark session. Livy is an open-source REST service that enables remote and interactive analytics on Apache Spark clusters. It provides a way to interact with Spark clusters programmatically using a REST API, allowing you to submit Spark jobs, run interactive queries, and manage Sparksessions from web applications without the need for a specific Spark client. As a result, multiple Unified Analytics users can interact with your Spark cluster concurrently and reliably!\n",
    "\n",
    "First, let's connect to the Livy endpoint and create a new Spark interactive session. The Spark interactive\n",
    "session is particularly useful for exploratory data analysis, prototyping, and iterative development. It allows you to\n",
    "interactively work with large datasets, perform transformations, apply analytical operations, and build ML models using\n",
    "Spark's distributed computing capabilities. \n",
    "\n",
    "To communicate with Livy and manage your sessions you use Sparkmagic, an open-source tool that provides a Jupyter kernel\n",
    "extension. Sparkmagic integrates with Livy, to provide the underlying communication layer between the Jupyter kernel and\n",
    "the Spark cluster.\n",
    "\n",
    "**Execute the cell below**, then:\n",
    "\n",
    "1. Select the `Add Endpoint` tab.\n",
    "1. Select `Single Sign-on` and ensure there is a Livy address in the `Address` field. \n",
    "1. Click `Add Endpoint`.\n",
    "1. Select the `Create Session` tab.\n",
    "1. Provide a name (e.g. `retail-demo`).\n",
    "1. Select `python` under the Language field.\n",
    "1. Click `Create Session` (right side).\n",
    "\n",
    "The session will take a few minutes for your session to initialize. \n",
    "\n",
    "Once ready, the Manage Sessions pane will activate, displaying\n",
    "your session ID. When the session state turns to idle, you're all set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b30ae-c564-42d3-8872-3559afa39b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf2946-32f2-40b6-88cc-0af78fed48f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let's check the status of the session.\n",
    "\n",
    "1. Navigate back to the Unified Analytics dashboard.\n",
    "1. In the sidebar navigation menu, select `Spark Interactive Sessions`.\n",
    "\n",
    "![image.png](./images/exercise1/menu.PNG)\n",
    "\n",
    "3. Here, you can check the status of your session. It will take 2-3 minutes to start. When the `State` says `Idle`, the session is ready. \n",
    "\n",
    "![image.png](./images/exercise1/session.PNG)\n",
    "\n",
    "4. Scroll back up to the Notebook cell of the session (%manage_spark command). Confirm under the `Manage Sessions` tab that the session should now be visible as `Idle` too. \n",
    "\n",
    "![image.png](./images/exercise1/session2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172e937-fb30-4e50-ac72-c94d4c66ba5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure Spark Interactive Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1b1ae-2d62-420d-bf6d-f5dc43a63111",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Run the `%config_spark` magic command.\n",
    "2. Leave the settings as they are. Click `Submit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89506a-93b1-4673-8064-3f51470ce265",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Important:</b> Ignore the resulting message and <b>do not</b> restart the kernel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43012d3a-51e1-40ba-8827-c1ef21175533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79c95e-d669-4575-adba-c2ce7553a120",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, let's import the required libraries for working with Spark in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d03e4-d6ea-4a42-85cf-8f75731209d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef16f3c-7a82-4812-974f-239136c6c8fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will also define the paths for where Spark will pull files from and save files to. These paths are specific to the Unified Analytics directory structure and to be left as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087bdc8e-44d1-48ab-9471-cccb2b875e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_root = \"file:///mounts/shared-volume/shared/retail-data/raw-data\"\n",
    "delta_root = \"file:///mounts/shared-volume/shared/retail-data/delta-tables/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68571ddd-0bbb-4112-8d86-f431d3b5d0be",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can now instantiate the Spark session. We'll add delta extensions to the configuration to be able to interact with the delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6811656-d54e-466e-89a1-f7e9719d4b61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataCleaningWithSpark\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.local.dir\", \"/mnt/shared/end2end-main-exercises/exercises\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Pyspark session started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a7dbb-003b-424d-bb37-302ce6493c34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2. Generating and Preparing Sales Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a8d980-01a8-4536-a653-c0ec087d4fbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this section, we are going to synthetically generate several years of sales data from our three retail stores located in three countries: Switzerland, Germany and the Czech Republic. This sales data will provide the basis for the remaining exercises, where we will learn to analyze, graph and build dashboards to gather insights between and across regions. \n",
    "\n",
    "**Optional:** To use `Data Sources` connected through Unified Analytics (such as MySQL, MariaDB and PostgresSQL databases), follow **this**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b5e7a-c316-482b-a980-975ffdce573b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generating Sales Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9b9d8-c9b8-42eb-80db-04479c8e8d8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "A Python script has been provided which can generate the sales data for the three given country locations. \n",
    "\n",
    "The parameters for this script are:\n",
    "\n",
    "- cu: Currency, to account for conversions between stores.\n",
    "- s: Number of stores in that region.\n",
    "- sy: Start Year\n",
    "- ey: End Year\n",
    "- csv: Resulting File Name\n",
    "\n",
    "We'll see the first 10 rows of the newly created table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9ef1d-e4d4-4d42-acb6-c9abae749939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run resources/create_csv.py -c \"Germany\" -cu EUR -s 5 -sy 2019 -ey 2023 -csv \"germany_sales_data_2019_2023.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671a32f-fb8e-46b5-aa4d-52220cb0cb22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run resources/create_csv.py -c \"Czech Republic\" -cu CZK -s 5 -sy 2019 -ey 2023 -csv \"czech_sales_data_2019_2023.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc93d56-0fd2-4c33-b77d-d709c228597a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run resources/create_csv.py -c \"Swiss\" -cu CHF -s 5 -sy 2019 -ey 2023 -csv \"swiss_sales_data_2019_2023.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf50a54e-8337-49bf-b692-21a6ed7dca49",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll ensure that our Spark Interactive session can access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a51cc5-8f76-49ef-bf2d-793e4da95f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "data_path = file_root\n",
    "\n",
    "# List files in the directory\n",
    "files = spark.sparkContext.wholeTextFiles(data_path)\n",
    "\n",
    "# Display the list of files\n",
    "for file_path, _ in files.collect():\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922dfac2-2f13-4b17-9e01-cd7cee98f55a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **3. Create Delta Tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acafe4dd-7c94-4416-a505-fb25558a614b",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this section, we will create Delta Tables from our CSV files that we can query using Unified Analytics. Delta Tables are a type of table that can be created in Delta Lake, which is an extension of Apache Parquet file format.\n",
    "\n",
    "### Define an ETL Pipeline to create Delta Tables \n",
    "\n",
    "First, let's define some functions that will:\n",
    "\n",
    "1. Load the data in from a CSV and return a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d95d3-5fa2-4eb4-a15c-1c1467f73713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def load_data(spark, country, data_path):\n",
    "    # Define the path to the CSV file\n",
    "    csv_path = f\"{data_path}/{country}_sales_data_2019_2023.csv\"\n",
    "\n",
    "    # Define the schema with specific data types\n",
    "    schema = StructType([\n",
    "        StructField(\"PRODUCTID\", IntegerType(), True),\n",
    "        StructField(\"PRODUCT\", StringType(), True),\n",
    "        StructField(\"TYPE\", StringType(), True),\n",
    "        StructField(\"UNITPRICE\", DoubleType(), True),\n",
    "        StructField(\"UNIT\", StringType(), True),\n",
    "        StructField(\"QTY\", IntegerType(), True),\n",
    "        StructField(\"TOTALSALES\", DoubleType(), True),\n",
    "        StructField(\"CURRENCY\", StringType(), True),\n",
    "        StructField(\"STORE\", StringType(), True),\n",
    "        StructField(\"COUNTRY\", StringType(), True),\n",
    "        StructField(\"YEAR\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Read data from the CSV file with the specified schema\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(csv_path)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c9954-ac26-41a4-abd3-18f7b04b00ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. Clean the data, in this case by ensuring the currency of each item is standardized in Euros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb38a7-7fe3-4c0a-a467-4b45d9ec64f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(df, spark, country):\n",
    "    # Define a UDF to convert currencies to EUR\n",
    "    convert_udf = udf(lambda currency, amount: amount / CZK_TO_EUR_RATE if currency == \"CZK\" else amount / CHF_TO_EUR_RATE if currency == \"CHF\" else amount, DoubleType())\n",
    "\n",
    "    # Apply the UDFs to the DataFrame\n",
    "    corrected_df = df.withColumn(\"totalsales\", convert_udf(col(\"currency\"), col(\"totalsales\"))) \\\n",
    "                     .withColumn(\"currency\", lit(\"EUR\"))\n",
    "\n",
    "    # Show the results\n",
    "    corrected_df.show()\n",
    "\n",
    "    return corrected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506a081-3198-4387-972b-d93770235000",
   "metadata": {
    "tags": []
   },
   "source": [
    "3. Save the data as parquet files (Delta Tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d436fd8-3f8b-4193-8db9-08f6804f79fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_data(df, country):\n",
    "    delta_path = delta_root + country\n",
    "\n",
    "    # Check if the directory exists, and create it if it doesn't\n",
    "    if not os.path.exists(delta_path):\n",
    "        os.makedirs(delta_path)\n",
    "        \n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e149b-dbac-4d5b-9e0b-dbcee401d81b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Great! We've just created functions that will **extract** the data from our generated CSV files, **transform** them into Delta Tables with the currency standardized, then **load** them into a new directory.\n",
    "\n",
    "You guessed it! We have just created an **ETL pipeline!** \n",
    "\n",
    "After declaring our country list and our currency conversion rates, we can run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1df26-63c3-4c65-852f-6ad6159dfbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "COUNTRY_LIST = [\"czech\", \"germany\", \"swiss\"]\n",
    "CZK_TO_EUR_RATE = 25\n",
    "CHF_TO_EUR_RATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde60bbb-22de-4407-ab8f-3c34590e1e8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Hint:</b> As you can tell by the parameters to the create_csv.py functions in Section 2, we can synthetically generate data for as many stores in as many European countries as we want! Feel free to experiment, so long as the countries are declared in the cell above <b>and the countries that are already there remain.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509607bd-a86f-4252-9c6e-354709521775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for country in COUNTRY_LIST:\n",
    "    # Load data from the DBs\n",
    "    df = load_data(spark, country, data_path)\n",
    "    df.show()\n",
    "    \n",
    "    # Clean the data\n",
    "    cleaned_df = clean_data(df, spark, country)\n",
    "    cleaned_df.printSchema()\n",
    "    \n",
    "    # Write the cleaned data back to the Delta Table\n",
    "    write_data(cleaned_df, country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca77167-5306-4515-aa1c-1178a42f593a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we'll confirm the Delta Tables were create correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61cd90-dfe5-42cd-ad23-9fcc25739f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for country in COUNTRY_LIST:\n",
    "    # List files in a directory\n",
    "    selected_country_path = delta_root + country\n",
    "    files = os.listdir(selected_country_path)\n",
    "    print(\"Table:\", country)\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith(\".parquet\"):\n",
    "            full_path = os.path.join(selected_country_path, file)\n",
    "            print(\"Saved in:\", full_path)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d64969c-d434-47ea-813e-1f9d8df35610",
   "metadata": {},
   "source": [
    "## **4. Exploring Dataset Version Control**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f553d7-1a2d-4777-8383-9225b2ed1b28",
   "metadata": {
    "tags": []
   },
   "source": [
    "For the last part of this exercise, we'll explore how to best leverage the Delta Table format to clean and manipulate our datasets using our Spark Interative session. \n",
    "\n",
    "To \"clean\" data involves identifying and correcting errors, inconsistencies, and inaccuracies within datasets to ensure their reliability and usability for any given analytics use case. In today's data-driven world, this is **crucial** step of any analytics, modelling or AI workflow.\n",
    "\n",
    "This process typically includes tasks such as handling missing values, removing duplicates, standardizing formats, and resolving discrepancies, ultimately aiming to improve the quality and integrity of the data to ensure any insight generated through analysis is accurate and sound.\n",
    "\n",
    "Cleaning data will often take several iterations. If you make a modification on a dataset that you want to roll back, doing so manually can often be a nightmare. This is where the Apache Parquet format and Delta Tables come into their own. \n",
    "\n",
    "Let's explore how to use Delta Tables as our own dataset version control!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2adbe3c-fb8e-43d2-a39b-e9f4b33da067",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "For the smoothest experience, remove all previous versions of Delta Tables that may exist from previous runnings of this exercise.<br><br> Open a <b> Terminal </b> window and run: <i>rm -r /mnt/shared/retail-data<i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc2daf-bd72-44ca-99a1-f1b213e23f01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ruining a perfectly good Delta Table.\n",
    "\n",
    "First, let's load the `czech` Delta Table as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3bb3a-7c4f-4053-98b1-02452c20ff8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "#Disable Vectorized Reader to ensure data types remain consistent\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n",
    "\n",
    "# Set the parameters\n",
    "country = \"czech\"\n",
    "delta_path = delta_root + country\n",
    "\n",
    "# Read the Delta table using the load method\n",
    "read_df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Show the contents of the DataFrame\n",
    "read_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724dfdb-8f0c-4393-ae04-cde3901a003a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll modify the Delta Table by changing the data values in all of the columns **aside from**  `TYPE`,  `UNIT PRICE`,  `UNIT` and  `QTY`, to  `NULL`. \n",
    "\n",
    "This will result in the creation of a new Delta Table (a Parquet file in the `czech` Delta Table path) that will be set as the default when retrieving the `czech` Delta Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314d2b2-0494-4cb0-8537-c1a8027eddd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the new Delta Table with the same schema, but only the selected columns data.\n",
    "df_select = df.select(\"type\", \"unitprice\", \"unit\", \"qty\")\n",
    "df_select.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e35ea-4acd-44f3-bb22-41fbe805d10a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's load the  `czech` Delta Table now and see see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33efef9-815a-4203-9eed-b5d20b5404ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the Delta table using the load method\n",
    "read_df_select = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Show the contents of the DataFrame\n",
    "read_df_select.show()\n",
    "\n",
    "# Display the schema of the version 0 DataFrame\n",
    "read_df_select.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf4cc2-a772-4a34-a045-c3077fd60a63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time Warp!\n",
    "\n",
    "Whilst it's fun to ruin data intentionally, it is very common in data engineering practice to make the occasional mistake. Thankfully, by saving our datasets as Delta Tables, we now have a \"version control\" for any datasets that we manipulate. To ensure we have a complete table with no `NULL` for the remaining exercises, let's roll the `czech` Delta Table back to before we messed with it.\n",
    "\n",
    "Let's first observe the two versions of the `czech` Delta Table - before and after our column manipulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da2e54-d1bb-4e6b-bb87-dcdb22a5707b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DeltaTable object\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Get the history of the Delta table\n",
    "history_df = delta_table.history()\n",
    "\n",
    "# List all versions with timestamp\n",
    "versions_with_timestamp = history_df.select(\"version\", \"timestamp\").distinct().collect()\n",
    "\n",
    "# Display the list of versions with timestamp\n",
    "print(\"List of Delta Table Versions with Timestamp:\")\n",
    "for version_info in versions_with_timestamp:\n",
    "    version = version_info[\"version\"]\n",
    "    timestamp = version_info[\"timestamp\"]\n",
    "    print(f\"Version: {version}, Timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98729c2d-b560-4fff-8679-1a92b9c34609",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "As expected, two versions - timestamped! Let's print them out in full. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5d8be-de4f-424a-b0b8-31357723db06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read a specific version (e.g., version 0) of the Delta table\n",
    "print(\"Before Manipulation:\")\n",
    "read_df_version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(delta_path)\n",
    "read_df_version_0.show()\n",
    "\n",
    "# Read a specific version (e.g., version 1) of the Delta table\n",
    "print(\"After Manipulation:\")\n",
    "read_df_version_1 = spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(delta_path)\n",
    "read_df_version_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47730dea-6c5d-42de-90ad-e180dfb81672",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let's set the default version for the `czech` Delta Table as the original dataset (Version 0). We'll do this by overwriting the current Delta Table (Version 1) with the data from original (Version 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b42536-d830-4940-b2ef-7914fb670b47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read a specific version (e.g., version 0) of the Delta table\n",
    "read_df_version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(delta_path)\n",
    "\n",
    "# If you want to perform further actions or overwrite the current Delta table:\n",
    "# Overwrite the current Delta table with version 0 data\n",
    "read_df_version_0.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27781d28-8213-4bf1-8d9e-cb210fcab1b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "We'll load the default `czech` Delta Table in and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6a93e-5494-4e44-a103-3e4c8ca9c19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the Delta table using the load method\n",
    "read_df_select = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Show the contents of the DataFrame\n",
    "read_df_select.show()\n",
    "\n",
    "# Display the schema of the version 0 DataFrame\n",
    "read_df_select.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ceda3-dba8-4695-9dfb-8309730e5e94",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our original data is back! As you will recall, whenever a Delta Table is modified, a new Parquet file (Delta Table Version) is created. We can see this when we observe all of the versions once again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bbcc34-5d49-4d0c-94ff-ebc929ae2133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DeltaTable object\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Get the history of the Delta table\n",
    "history_df = delta_table.history()\n",
    "\n",
    "# List all versions with timestamp\n",
    "versions_with_timestamp = history_df.select(\"version\", \"timestamp\").distinct().collect()\n",
    "\n",
    "# Display the list of versions with timestamp\n",
    "print(\"List of Delta Table Versions with Timestamp:\")\n",
    "for version_info in versions_with_timestamp:\n",
    "    version = version_info[\"version\"]\n",
    "    timestamp = version_info[\"timestamp\"]\n",
    "    print(f\"Version: {version}, Timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318187ca-e5ee-4846-b0a9-293efe1f59d9",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e027a2b-d412-4621-b26f-00e1ddebeda8",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this exercise, you learned to perform the basics of data engineering - all within a single notebook! \n",
    "\n",
    "**HPE Ezmeral Unified Analytics** makes this possible by natively supporting and including the most widely used open-source data tools and frameworks and making them readily available out-of-the-box, such that you spent this time performing invaluable data preperation for upcoming exercises instead of hours installing and connecting them all!\n",
    "\n",
    "In the next exercise, you will learn how to use EzPresto on HPE Ezmeral Unified Analytics to prepare these datasets for visualization and modelling. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
