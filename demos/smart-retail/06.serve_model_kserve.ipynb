{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995e629d-b140-4654-8000-c5d2d2982027",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c186f6-e95f-463b-980c-1b8c83503285",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 6:** Serving your model with KServe\n",
    "\n",
    "By this stage, you have a model that is saved in a Model Regsitry as a 'Production' model. Now, it's time to make this model available to every self-serve checkout in every one of our retail stores.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "- Be briefly introduced to containers, Kubernetes, Kubeflow and KServe.\n",
    "- Create an InferenceService configuration.\n",
    "- Serve the model through an InferenceService using KServe.\n",
    "- Learn how to manage Kubeflow Endpoints.\n",
    "\n",
    "By the end of this exercise, you will have learned how to serve a model at scale using KServe and Kubeflow on **HPE Ezmeral Unified Analytics**.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b852c8-2216-4bf9-bee9-432aa4c7987c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Prerequsites**\n",
    "\n",
    "As instructed in the [Introductory notebook](./00.introduction.ipynb), ensure that you have run `pip install -r requirements.txt` in a Terminal window, located in the same working directory, prior to running this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82a37e-7723-46fa-b22f-ac40fe013c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> This exercise requires the completion of Exercises 5: Tracking, Registering and Inferencing Models in MLflow.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7535e-9c43-4dc5-9efb-6e7c4e6d64b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1. Introduction to Kubeflow and KServe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ba67d-c7bc-4ba3-9868-ee779aaa04c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you were just looking to serve your model locally, such as to a single notebook, script or application, you would now be in a position to simply use the MLflow URI to get a \"copy\" of it each time. \n",
    "\n",
    "You can probably guess that whilst this may work for a one or a few instances, it is hardly a **scalable** solution. What if we want to deploy this model such that it can be remotely inferenced by every self-serve checkout in every retail store across multiple countries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5fb20-44cf-4b6a-8ec9-adc11fa41124",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A quick word about Kubernetes and containers\n",
    "\n",
    "This problem of scaling applications (such as a model inferencing server) based on user demand is one that has plagued infrastructure managers since the birth of the modern computer. How do I deploy an application onto infrastructure such that it is available to 100 users during the evening and 100,000 users during the day? Making it available.\n",
    "\n",
    "This problem brought about the idea of the **container**, whereby a lightweight instance of an application could be easily spun up and powered down near-instantaneously. By deploying applications in containers, we could better manage the allocation of our compute infrastructure resources at any given time. \n",
    "\n",
    "However, containerization brought about another problem: when there are hundreds of thousands or even **millions** of containers, who (or *what*) is going to spin up and power down these containers based on demand? Enter **Kubernetes**. \n",
    "\n",
    "**Kubernetes**, also known as K8s, is an open-source system designed to automate the deployment, scaling, and management of containerized applications. Kubernetes is like a conductor for a container orchestra*, ensuring everything runs smoothly and efficiently on top of several resource notes (compute and storage servers). Kubernetes groups containers, which are self-contained units of software, into rapidly replicatable logical units for easier management and discovery.\n",
    "\n",
    "**A fitting analogy, seeing as Kubernetes is, by definition, a container orchestration platform!*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97116932-8e48-48a4-9643-7760e0da1d3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What is KServe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8443e-6b38-4237-882b-55ac3c4fae1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "KServe is a framework for deploying and serving machine learning (ML) models in production on Kubernetes. It simplifies the process of serving models by providing a Kubernetes Custom Resource Definition (CRD) that lets you easily define **how** your models should be served.\n",
    "\n",
    "KServe is **incredibly powerful** for model inferencing, as it can scale your model serving instances up or down based on real-time traffic. This allows for what is known as \"scale-to-zero\" functionality on CPUs and GPUs for efficient resource utilization.\n",
    "\n",
    "KServe is not a standalone platform but instead core add-on component of **Kubeflow**, specifically addressing the model serving aspect of the ML pipeline. Kubeflow is an open-source platform focused on machine learning operations (MLOps) on Kubernetes. It offers a collection of tools that cover the entire ML lifecycle, from model building, training, and deployment to monitoring and management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a5512-b8a3-4c07-9a2c-9e557a629f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Wait, Kubeflow sounds familiar..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51ba28-74f1-46f4-bb4c-5ceea61107ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "... to another machine learning platform we've already been introduced to in MLflow. \n",
    "\n",
    "Kubeflow and MLflow serve different purposes in the machine learning workflow:\n",
    "\n",
    "* **Kubeflow** is an open-source platform designed to facilitate the end-to-end **orchestration and management** of machine learning workflows on **Kubernetes**, providing capabilities for model training, **deployment**, **serving**, and **monitoring**.\n",
    "\n",
    "* **MLflow**, on the other hand, is a light **platform-agnostic** open-source tool that excels at **experiment tracking**, **version control** for models through a Model Registry, and facilitates **collaboration** among data scientists by keeping everything organized and reproducible.  \n",
    "\n",
    "The latest versions of Kubeflow and MLflow come natively installed with **HPE Ezmeral Unified Analytics**, which sits on top of a Kubernetes distribution - taking away all of the pain of deploying, connecting and managing these applications on top of Kubernetes yourself. \n",
    "\n",
    "Today, data scientists and engineers leverage both Kubeflow and MLflow to address distinct needs within the machine learning lifecycle. Together with Unified Analytics, they provide the complete MLOps solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490016b4-5cb1-4699-9c00-ecfc8a19cd23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2. Declaring Variables and Importing Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa7970-549d-4f24-a048-5ebdf1e3cb9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's re-declare the variables related to our MLflow experiement such that we can access them in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129f625-a3e9-4b4b-a95e-ee05fbe6ba0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experiment variables for MLflow\n",
    "experiment_name = \"retail-experiment\"\n",
    "model_name = \"produce-detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9589dae-75c1-49fd-8ea8-72dc42c2c429",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll import the necessary libraries. \n",
    "\n",
    "Ignore any warnings that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f8bf7-ed2c-4c87-934c-9c79e6a63a93",
   "metadata": {
    "papermill": {
     "duration": 7.613981,
     "end_time": "2023-03-13T20:40:28.950010",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.336029",
     "status": "completed"
    },
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "from kubernetes import client \n",
    "from kubernetes.client import V1EnvVar\n",
    "from kubernetes.client.models import V1ObjectMeta\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import utils\n",
    "from kserve import V1beta1InferenceService\n",
    "from kserve import V1beta1InferenceServiceSpec\n",
    "from kserve import V1beta1PredictorSpec\n",
    "from kserve import V1beta1TFServingSpec\n",
    "import urllib3\n",
    "import mlflow\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f91cdf-3363-4e18-8a00-4b47067899ca",
   "metadata": {
    "papermill": {
     "duration": 0.074875,
     "end_time": "2023-03-13T21:22:22.585971",
     "exception": false,
     "start_time": "2023-03-13T21:22:22.511096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **3. Model Serving with KServe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4bdadb-cf16-4796-a2ad-0037c4cdd238",
   "metadata": {
    "tags": []
   },
   "source": [
    "It's time to leverage the power of KServe to make our produce detection model available wherever we may need it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8f9206-fe7f-4eca-8e10-c362a5555039",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Model details from MLflow\n",
    "\n",
    "First, let's connect this notebook to MLflow and get the URI for the 'Production' version of our model. We confirmed this URI was accessible in our Testing section of <a href=\"./05.working_in_mlflow.ipynb\" style=\"color: black\">Exercise 5</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879fc0fa-91f8-4f05-abb3-e43c55a2e9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad2cc0-5394-47cb-9f66-a748090a2049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create an instance of the MlflowClient\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Get the latest model version in Production\n",
    "latest_versions = client.get_latest_versions(name=model_name, stages=[\"Production\"])\n",
    "latest_version = latest_versions[0]\n",
    "\n",
    "# Get the model uri\n",
    "model_uri = latest_version.source.replace(\"model\", \"tf_serving_model\")\n",
    "\n",
    "print(\"Model Storage Path in S3 (as shown in MLflow): \" + str(model_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86510ed0-768f-42ec-95f4-4fec04238247",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create the InferenceService config file (YAML)\n",
    "\n",
    "Next, we'll create the configuration file (YAML) that tells KServe to serve our model as an **InferenceService**. An InferenceService is a Kubernetes Custom Resource (CR) provided by KServe. In the context of KServe, an InferenceService represents a scalable and load-balanced service that hosts one or more machine learning models for real-time inference or prediction.\n",
    "\n",
    "A YAML file for defining an InferenceService in KServe typically specifies the configuration for deploying and serving a machine learning model. Like most KServe YAMLs, our InferenceService YAML will require:\n",
    "\n",
    "- `apiVersion` and `kind` to specify the Kubernetes API version and kind of resource being defined, respectively.\n",
    "- `metadata` to provide metadata, such as the name of the InferenceService.\n",
    "- `spec` to define the specifications for the InferenceService, including the `predictor` and `transformer`.\n",
    "- `predictor` section specifies the details of the model to be served, such as the model's location (`modelUri`), runtime version (`runtimeVersion`), and resource requirements (`resources`).\n",
    "- `transformer` section specifies any pre-processing or post-processing steps required before or after making predictions. In this example, it includes a container image (`image`) and environment variables (`env`).\n",
    "\n",
    "For our YAML file, we set up the necessary resources in Kubernetes to deploy an InferenceService with KServe, including secrets for authentication with the Unified Analytics internal S3 storage that MLflow uses to store models in the Model Registry, service account authentication details, and the configuration for the InferenceService itself (where we parse the model URI and declare Tensorflow as the `predictor`). \n",
    "\n",
    "We'll declare the secret and service name parameters that we will parse into the YAML text, then create a YAML file and store it in the local directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c184d0-e715-4266-9e0d-974aa86d66ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set paramentes\n",
    "isvc_name = experiment_name\n",
    "secret_name = 's3-proxy-kserve-secret'\n",
    "sa_name = 's3-proxy-kserve-sa'\n",
    "\n",
    "#Set name of YAML file\n",
    "yaml_name = './model-kserve.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffa909-99c2-461f-99d7-ce26bbe4ed58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create YAML configuration file\n",
    "with open(yaml_name, 'w') as file:\n",
    "    text = f\"\"\"---\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: \"{secret_name}\"\n",
    "  annotations:\n",
    "    serving.kserve.io/s3-cabundle: \"\"\n",
    "    serving.kserve.io/s3-endpoint: \"local-s3-service.ezdata-system.svc.cluster.local:30000/\"\n",
    "    serving.kserve.io/s3-useanoncredential: \"false\"\n",
    "    serving.kserve.io/s3-usehttps: \"0\"\n",
    "    serving.kserve.io/s3-verifyssl: \"0\"\n",
    "stringData:\n",
    "  AWS_ACCESS_KEY_ID: \"{os.environ['AUTH_TOKEN']}\"\n",
    "  AWS_SECRET_ACCESS_KEY: \"s3\"\n",
    "type: Opaque\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: \"{sa_name}\"\n",
    "secrets:\n",
    "  - name: \"access-token\"\n",
    "\n",
    "---\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: \"{isvc_name}\"\n",
    "  annotations:\n",
    "    \"sidecar.istio.io/inject\": \"false\"\n",
    "spec:\n",
    "  predictor:\n",
    "    tensorflow:\n",
    "      storageUri: \"{model_uri}\"\n",
    "    serviceAccountName: \"{sa_name}\"\n",
    "\"\"\"\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65a202-da63-4237-b97f-a9950b852aa5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we run the Kubernetes command `apply` to deploy our InferenceService!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57361cd2-819f-4901-a800-ea72c8a30ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the container\n",
    "!kubectl apply -f {yaml_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071c5d4-b707-4e83-9aec-b67a373e2224",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can confirm that it's up, running and ready to inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17d8fd-2f82-43ee-825e-2c33cfcdee9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wait until the ISvc is ready\n",
    "KServe_client = KServeClient()\n",
    "KServe_client.wait_isvc_ready(isvc_name, watch=True, timeout_seconds=120)\n",
    "print(f\"\\nInferenceService {isvc_name} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a530e1d-1ad4-4a9e-b899-f658a3746323",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **4. Managing Kubeflow Endpoints**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb00d4-dc0a-4787-849e-71e4dab53751",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can observe the status of our InferenceService through the **Endpoints** pane in Kubeflow.\n",
    "\n",
    "To access the Endpoints pane:\n",
    "\n",
    "1. Navigate back to the Unified Analytics dashboard.\n",
    "1. In the sidebar navigation menu, select `Data Science` > `Model Serving`.\n",
    "1. The **Kubeflow Endpoints** pane will open in a new tab.\n",
    "\n",
    "Here, we will see the complete list of applications (in our case, an InferenceService instance) currently being served using KServe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5095b51-0f7f-4591-b3d6-82b04a165272",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise6/kserve1.png\" alt=\"Drawing\" style=\"width: 90%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb024006-4c79-4b20-af12-11ce92f320fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Let's explore the InferenceService further.\n",
    "\n",
    "4. Click on the `retail-experiment` Endpoint.\n",
    "\n",
    "Here, we can see details about our InferenceService, including the **serving endpoints**, the MLflow URI for the model and our choice of predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78455700-e1bb-42ff-a17e-fd1182f6bb09",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/exercise6/kserve2.png\" alt=\"Drawing\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e499a06-6594-493f-a726-b81c7f8d3999",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Tip:</b> Should you ever encounter issues with an InferenceService, you can diagnose the issue via the logs located under the `Logs` tab.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61e46c-cc4e-4691-8c4a-4b5c66150fd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `URL internal` link is what can be used to inference the link from any application within **HPE Ezmeral Unified Analytics**, including notebooks and custom applications hosted within Unified Analytics. For applications hosted outside of our Unified Analytics cluster that have the right authentication, you would use the `URL external` link. \n",
    "\n",
    "5. Copy the `URL internal` link to the clipboard and paste it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ba023-4a1a-4ece-896b-efe47cb76f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "internal_url = \"http://retail-experiment-predictor.admin-9dbd466f.svc.cluster.local\" #paste the URL internal endpoint for your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a8fec-becf-4bda-a5db-b4076333da45",
   "metadata": {
    "papermill": {
     "duration": 0.074875,
     "end_time": "2023-03-13T21:22:22.585971",
     "exception": false,
     "start_time": "2023-03-13T21:22:22.511096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **5. Inferencing our Model using an Endpoint**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2ef76-6906-40c6-8cfe-8a0847fa025d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Your produce recognition model is scalably served and is accessible via an endpoint URL. Let's learn how we use this endpoint to send the model an image of a fruit or vegetable, and for it to send back what it detects*.\n",
    "\n",
    "**Calling an model to make a detection or prediction is known as **inferencing** a model.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95cb9b7-1b9a-434d-8c7b-4d21ff41827d",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we'll build out the full **endpoint URL** using the `URL internal` from our InferenceService."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37d6bb-deec-4cac-907d-8942c107524b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298806c-5d0f-4746-8267-67e7791c3e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the Serving URL\n",
    "serving_url = internal_url + \"/v1/models/\" + isvc_name + \":predict\"\n",
    "\n",
    "print(\"Serving URL: \" + serving_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a306316-8590-4167-b3d9-228a91ad63c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll define some functions. Similar to previous exercises, we'll prepare any image that we want to infer the model on. We'll also define a function that will convert our preprocessed image into a JSON `REST` package that we can `POST` to the endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33dccaa-6807-420b-a661-5cb19418255a",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess_image(location):\n",
    "    # Load the image\n",
    "    if \"http\" in location:\n",
    "        response = requests.get(location)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "        img=load_img(location,target_size=(224,224,3))\n",
    "    \n",
    "    img = img.resize((224, 224))\n",
    "    print(type(img))\n",
    "    img_array = img_to_array(img)\n",
    "    print(img_array.shape)\n",
    "    print(type(img_array))\n",
    "    img_array = img_array / 255.0\n",
    "    print(img_array.shape)\n",
    "    print(type(img_array))\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    print(img_array.shape)\n",
    "    print(type(img_array))\n",
    "\n",
    "    return img_array\n",
    "\n",
    "def format_data(data):\n",
    "    # Convert the NumPy array to a list\n",
    "    data_list = data.tolist()\n",
    "    \n",
    "    # Format the list as a JSON string\n",
    "    data_formatted = json.dumps(data_list)\n",
    "    \n",
    "    # Create a JSON request string with the formatted data\n",
    "    json_request = '{{ \"instances\" : {} }}'.format(data_formatted)\n",
    "    \n",
    "    return json_request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeac32f-c210-4df2-9d61-9c2171529b81",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we're ready to inference our model with a supplied image. \n",
    "\n",
    "For the final time, **go out** onto Google Images and find a **new** image with a fruit or vegetable in it. Replace the `image_url` variable with the link to your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240cfb5f-485f-4512-81a7-5039f5e8c45c",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Define your labels\n",
    "labels = {'apple': 0, 'banana': 1, 'carrot': 2, 'cucumber': 3, 'lemon': 4, 'orange': 5}\n",
    "labels = dict((v, k) for k, v in labels.items())\n",
    "\n",
    "online_url = \"\"\n",
    "local_url = os.getcwd() + \"/images/test_image.jpg\"\n",
    "\n",
    "if online_url:\n",
    "    image_url = online_url\n",
    "else:\n",
    "    image_url = local_url\n",
    "\n",
    "preprocessed_image = preprocess_image(image_url)\n",
    "\n",
    "json_request = format_data(preprocessed_image)\n",
    "\n",
    "headers = headers = {\"Authorization\": f\"Bearer {os.environ['AUTH_TOKEN']}\"}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(serving_url, data=json_request, headers=headers, verify=False)\n",
    "print(response)\n",
    "print(\"Request successfully made.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7361c8-8ac5-45af-8339-0a08cfd818ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's see what we got back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f837d-ab95-4b9a-bdd6-ead2c9684f15",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Print the raw response content\n",
    "print(\"Raw Response Content:\")\n",
    "print(response.content.decode('utf-8'))\n",
    "\n",
    "# Decode the JSON response\n",
    "if response.headers.get(\"Content-Type\") == \"application/json\":\n",
    "    response_data = response.json()\n",
    "    predictions = response_data['predictions']\n",
    "\n",
    "    formatted_predictions = [[round(pred * 100, 2) for pred in prediction] for prediction in predictions]\n",
    "\n",
    "    print(\"\\nTranslated Predictions:\")\n",
    "    for label, prob in zip(labels, formatted_predictions[0]):\n",
    "        print(f\"- {label}: \\t{prob}%\")\n",
    "    \n",
    "    # Get the predicted label\n",
    "    predicted_label_index = np.argmax(formatted_predictions)\n",
    "    predicted_label = labels[predicted_label_index]\n",
    "\n",
    "    print(\"\\nPredicted class label:\", predicted_label, \"with\", formatted_predictions[0][predicted_label_index], \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0aff05-4880-4198-b4d9-e6ae90c4016c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Did the model correctly guess what was in your image? If so, great! If not... still great! We've successfully validated that our model is being served with an endpoint using KServe!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33620aa3-f065-4c09-9362-749c1274fcee",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0c656-2af4-4225-aaff-8252d5c68c35",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this exercise, you have learned the underlying theory behind scaling the serving of a model - including containers, Kubernetes, Kubeflow and KServe. You took the latest version of your produce detection model from the MLflow Model Repository, created the configuration YAML for a KServe InferenceService, and deployed it on the Kubernetes cluster that powers Unified Analytics.\n",
    "\n",
    "Now, we have an endpoint URL that we can call to inference our model from any application within **HPE Ezmeral Unified Analytics**!\n",
    "\n",
    "In the next exercise, we will leverage this endpoint to make **real-time** detections of produce within a custom self-checkout application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gcr.io/mapr-252711/kubeflow/notebooks/jupyter-tensorflow-cuda-full:ezaf-fy23-q2",
   "experiment": {
    "id": "new",
    "name": "jk-fruit-demo"
   },
   "experiment_name": "jk-fruit-demo",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "random",
     "algorithmSettings": [
      {
       "name": "random_state",
       "value": "10"
      },
      {
       "name": "acq_optimizer",
       "value": "auto"
      },
      {
       "name": "acq_func",
       "value": "gp_hedge"
      },
      {
       "name": "base_estimator",
       "value": "GP"
      }
     ]
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "additionalMetricNames": [],
     "goal": 1,
     "objectiveMetricName": "stage",
     "type": "maximize"
    },
    "parallelTrialCount": 3,
    "parameters": [
     {
      "feasibleSpace": {
       "max": "50",
       "min": "1",
       "step": "1"
      },
      "name": "param_epoch",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "list": [
        "32",
        "64"
       ]
      },
      "name": "param_batch_size",
      "parameterType": "categorical"
     },
     {
      "feasibleSpace": {
       "max": "10",
       "min": "1",
       "step": "1"
      },
      "name": "param_patience",
      "parameterType": "int"
     }
    ]
   },
   "katib_run": false,
   "pipeline_description": "fruit-veg",
   "pipeline_name": "fruit-veg",
   "snapshot_volumes": false,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:add-ldapcert-secret:true",
    "label:add-sssd-secret:true",
    "label:add-user-s3-secret:true"
   ],
   "storage_class_name": "dataplatform",
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/mnt/shared/",
     "name": "kubeflow-pipeline",
     "size": 1,
     "size_type": "Gi",
     "snapshot": false,
     "snapshot_name": "",
     "type": "pvc"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2679.282234,
   "end_time": "2023-03-13T21:24:51.692744",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-13T20:40:12.410510",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
