{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7wSVnqQ7xZB"
   },
   "source": [
    "# Bike Sharing (MLFlow - KServe)\n",
    "\n",
    "This notebook provides a detailed walkthrough of a comprehensive Machine Learning (ML) workflow, encompassing data\n",
    "preprocessing, model training and evaluation, hyperparameter tuning, experiment tracking via MLFlow, and model\n",
    "deployment using Seldon and KServe. The use case under consideration is the well-known bike sharing dataset, sourced\n",
    "from the UCI ML Repository.\n",
    "\n",
    "![bike-sharing](images/bike-sharing.jpg)\n",
    "(Photo by <a href=\"https://unsplash.com/@zaccastravels?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">ZACHARY STAINES</a> on <a href=\"https://unsplash.com/photos/KEhNcoCldbk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>)\n",
    "\n",
    "The dataset records the hourly and daily count of rental bikes between 2011 and 2012 in the Capital Bikeshare system,\n",
    "supplemented with corresponding weather and seasonal data. The primary objective of this dataset is to foster research\n",
    "into bike sharing systems, which are gaining significant attention due to their implications on traffic management,\n",
    "environmental sustainability, and public health.\n",
    "\n",
    "The task associated with this dataset is regression, with 17,389 instances. The overarching goal is to construct a\n",
    "predictive model capable of forecasting bike rental demand. The primary target variable for prediction is the `cnt`\n",
    "attribute, representing the total count of rental bikes, inclusive of both casual and registered users.\n",
    "\n",
    "By leveraging other features in the dataset (such as date, season, year, month, hour, holiday, weekday, working day,\n",
    "weather conditions, temperature, perceived temperature, humidity, and wind speed), you can train a model to predict this\n",
    "count with high accuracy.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setting Up the Environment](#setting-up-the-environment)\n",
    "1. [Set an MLflow Experiment](#set-an-mlflow-experiment)\n",
    "1. [Load the Dataset](#load-the-dataset)\n",
    "1. [Data Preprocessing](#data-preprocessing)\n",
    "1. [Data Visualization](#data-visualization)\n",
    "1. [Prepare the Training and Test Datasets](#prepare-the-training-and-test-datasets)\n",
    "1. [Establish the Evaluation Metrics](#establish-the-evaluation-metrics)\n",
    "1. [Feature Importance](#feature-importance)\n",
    "1. [Permutation Importance](#permutation-importance)\n",
    "1. [MLflow Tracking](#mlflow-tracking)\n",
    "1. [Model Training and Hyperparameter Tuning](#model-training-and-hyperparameter-tuning)\n",
    "1. [Best Model Identification](#best-model-identification)\n",
    "1. [Model Testing](#model-testing)\n",
    "1. [Model Deployment](#model-deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "The subsequent code cells are dedicated to importing the requisite dependencies. Additionally, it's recommended to\n",
    "establish a local directory for preserving the training artifacts generated during your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import warnings\n",
    "import subprocess\n",
    "\n",
    "import sklearn\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>MLflow Credentials</h2>\")\n",
    "display(heading)\n",
    "\n",
    "domain_input = widgets.Text(description='Username:', placeholder=\"i001ua.tryezmeral.com\")\n",
    "username_input = widgets.Text(description='Username:')\n",
    "password_input = widgets.Password(description='Password:')\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "success_message = widgets.Output()\n",
    "\n",
    "domain = None\n",
    "mlflow_username = None\n",
    "mlflow_password = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global domain, mlflow_username, mlflow_password\n",
    "    domain = domain_input.value\n",
    "    mlflow_username = username_input.value\n",
    "    mlflow_password = password_input.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        print(\"Credentials submitted successfully!\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(domain_input, username_input, password_input, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_url = f\"https://keycloak.{domain}/realms/UA/protocol/openid-connect/token\"\n",
    "\n",
    "data = {\n",
    "    \"username\" : mlflow_username,\n",
    "    \"password\" : mlflow_password,\n",
    "    \"grant_type\" : \"password\",\n",
    "    \"client_id\" : \"ua-grant\",\n",
    "}\n",
    "\n",
    "token_responce = requests.post(token_url, data=data, allow_redirects=True, verify=False)\n",
    "\n",
    "token = token_responce.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_TOKEN'] = token\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.environ['MLFLOW_TRACKING_TOKEN']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"s3\"\n",
    "os.environ[\"AWS_ENDPOINT_URL\"] = 'http://local-s3-service.ezdata-system.svc.cluster.local:30000'\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = os.environ[\"AWS_ENDPOINT_URL\"]\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"true\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://mlflow.mlflow.svc.cluster.local:5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"model_artifacts\"):\n",
    "    os.system(\"rm -rf model_artifacts\")\n",
    "os.mkdir(\"model_artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set an MLflow Experiment\n",
    "\n",
    "To set an experiment as active in MLflow, you can specify it either by its name using the `experiment_name` parameter or\n",
    "by its ID using the `experiment_id` parameter. It's important to note that you can't specify both the experiment name\n",
    "and ID simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_experiment(exp_name):\n",
    "    \"\"\"Register an experiment in MLFlow.\n",
    "    \n",
    "    args:\n",
    "      exp_name (str): The name of the experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mlflow.set_experiment(exp_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to set the experiment: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up an experiment with set_exp from ezmllib.mlflow\n",
    "experiment_name = \"bike-sharing\"\n",
    "set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwKZC40S-e0R"
   },
   "source": [
    "## Load the Dataset\n",
    "\n",
    "With the preliminary setup complete, you can now proceed to load the dataset. The data is provided in a CSV format,\n",
    "which can be conveniently loaded using the Pandas library in Python. To get a glimpse of the dataset, you display the\n",
    "first five rows using the `head()` method of the DataFrame. This initial exploration provides a snapshot of the data you\n",
    "work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "mFGzYdKCCNiK",
    "outputId": "8783bf81-d46a-4958-d2dc-59a324868a64",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load input data into pandas dataframe\n",
    "bike_sharing = pd.read_csv(\"dataset/bike-sharing.csv\")\n",
    "bike_sharing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQk3RQt2FB8x"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "In this phase, you prepare the data for the subsequent stages of the analysis. This involves cleaning, transforming, and\n",
    "structuring the data to ensure it is in the optimal format for your ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "vyS5Ru5aE5Y7",
    "outputId": "5d0b2528-9664-437d-8e3f-61119fdaad5e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove unused columns\n",
    "bike_sharing.drop(columns=[\"instant\", \"dteday\", \"registered\", \"casual\"],\n",
    "                  inplace=True)\n",
    "\n",
    "# Use better names\n",
    "bike_sharing.rename(\n",
    "    columns={\n",
    "        \"yr\": \"year\",\n",
    "        \"mnth\": \"month\",\n",
    "        \"hr\": \"hour_of_day\",\n",
    "        \"holiday\": \"is_holiday\",\n",
    "        \"workingday\": \"is_workingday\",\n",
    "        \"weathersit\": \"weather_situation\",\n",
    "        \"temp\": \"temperature\",\n",
    "        \"atemp\": \"feels_like_temperature\",\n",
    "        \"hum\": \"humidity\",\n",
    "        \"cnt\": \"rented_bikes\",\n",
    "    }, inplace=True)\n",
    "\n",
    "# Convert every data point to `float64`\n",
    "cols = bike_sharing.select_dtypes(exclude=['float64']).columns\n",
    "for i in ['season', 'year', 'month', 'hour_of_day', 'is_holiday',\n",
    "          'weekday', 'is_workingday', 'weather_situation', 'rented_bikes']:\n",
    "    bike_sharing[i] = bike_sharing[i].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40MGTHbNFKTP"
   },
   "source": [
    "## Data Visualization\n",
    "\n",
    "In this section, you employ various visualization techniques to better understand the data. By creating graphical\n",
    "representations of the data, you can identify patterns, trends, and correlations that might not be evident from the raw\n",
    "data alone. This step is crucial in guiding the subsequent analysis and model building process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "bNZOegwGHzUR",
    "outputId": "45a00d75-c019-4c39-96c4-08e3995fb381",
    "tags": []
   },
   "outputs": [],
   "source": [
    "hour_of_day_agg = bike_sharing.groupby([\"hour_of_day\"])[\"rented_bikes\"].sum()\n",
    "\n",
    "hour_of_day_agg.plot(\n",
    "    kind=\"line\", \n",
    "    title=\"Total rented bikes by hour of day\",\n",
    "    xticks=hour_of_day_agg.index,\n",
    "    figsize=(10, 5),\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMg_JKoUKq9j"
   },
   "source": [
    "## Prepare the Training and Test Datasets\n",
    "\n",
    "In this section, you partition the data into training and test datasets. This is a crucial step in the ML workflow,\n",
    "allowing you to train the model on a subset of the data (the training set), and then evaluate its performance on unseen\n",
    "data (the test set). This process helps ensure that your model generalizes well to new data and is not simply memorizing\n",
    "the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "ZwtDgaZ9Ktie",
    "outputId": "4971f3d6-5e99-4583-acb6-4ef73892a633",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset randomly into 70% for training and 30% for testing.\n",
    "X = bike_sharing.drop(\"rented_bikes\", axis=1)\n",
    "y = bike_sharing.rented_bikes\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.7, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.size}\")\n",
    "print(f\"Test samples: {X_test.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN0w6zFJSb87"
   },
   "source": [
    "## Establish the Evaluation Metrics\n",
    "\n",
    "Before proceeding to the training stage, you define the evaluation metrics that you will use to assess the performance\n",
    "of your model. These metrics provide quantitative measures of the model's accuracy, helping you understand how well the\n",
    "model is performing and where improvements can be made. This step is crucial in ensuring that your model meets the\n",
    "desired performance standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC1wzz_T_tSA"
   },
   "source": [
    "### Root Mean Square Error (RMSE)\n",
    "\n",
    "One of the evaluation metrics you use is the Root Mean Square Error (RMSE). This metric provides a measure of the\n",
    "differences between the values predicted by the model and the actual values. By taking the square root of the average of\n",
    "these squared differences, RMSE can give you a sense of the magnitude of the prediction errors. Lower RMSE values\n",
    "indicate a better fit of the model to the data.\n",
    "\n",
    "References: \n",
    "- https://medium.com/@xaviergeerinck/artificial-intelligence-how-to-measure-performance-accuracy-precision-recall-f1-roc-rmse-611d10e4caac\n",
    "- https://www.kaggle.com/residentmario/model-fit-metrics#Root-mean-squared-error-(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhPcLCteQy6j",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def rmse_score(y, y_pred):\n",
    "    score = rmse(y, y_pred)\n",
    "    message = \"RMSE score: {:.4f}\".format(score)\n",
    "    return score, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ3nr3D_AE85"
   },
   "source": [
    "### Cross-Validation RMSLE score\n",
    "\n",
    "Another evaluation metric you employ is the Root Mean Squared Logarithmic Error (RMSLE) score, calculated through\n",
    "cross-validation. Cross-validation is a robust technique that averages measures of prediction accuracy to derive a more\n",
    "precise estimate of model performance.\n",
    "\n",
    "The RMSLE score is especially valuable in your situation as it penalizes underestimates more than overestimates.\n",
    "Therefore, it is an essential metric for a bike sharing demand prediction model, ensuring that you avoid scenarios\n",
    "where the available number of bikes falls short of the demand.\n",
    "\n",
    "References: \n",
    "- https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "- https://www.kaggle.com/carlolepelaars/understanding-the-metric-rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9H9CZAP2ASe6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmsle_clip(estimator, x, y):\n",
    "    \"\"\"Clip negative prediction numbers before calculating RMSLE.\"\"\"\n",
    "    y_pred = estimator.predict(x)\n",
    "    y_pred_clipped = np.clip(y_pred, a_min=0, a_max=None)\n",
    "    return sklearn.metrics.mean_squared_log_error(\n",
    "        y, y_pred_clipped, squared=False)\n",
    "\n",
    "def rmsle_cv(model, X_train, y_train):\n",
    "    kf = KFold(\n",
    "        n_splits=4, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "    # Evaluate RMSLE score by cross-validation\n",
    "    rmsle = cross_val_score(model, X_train.values, y_train,\n",
    "                            scoring=rmsle_clip, cv=kf, error_score=\"raise\")\n",
    "    return rmsle\n",
    "\n",
    "def rmsle_cv_score(model, X_train, y_train):\n",
    "    score = rmsle_cv(model, X_train, y_train)\n",
    "    message = (\"Cross-Validation RMSLE score: {:.4f}\"\n",
    "               \" (std = {:.4f})\").format(score.mean(), score.std())\n",
    "\n",
    "    return score, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad0mABWEarsA"
   },
   "source": [
    "## Feature Importance\n",
    "\n",
    "In this section, you analyze the importance of each feature in the dataset. Feature importance refers to techniques\n",
    "that assign a score to input features based on how useful they are at predicting a target variable.\n",
    "\n",
    "Understanding which features are most influential in predicting the target variable can provide valuable insights into\n",
    "the dataset and the underlying model. This can help you interpret the model's predictions, and can guide further data\n",
    "collection and feature engineering efforts.\n",
    "\n",
    "References:\n",
    "- https://medium.com/bigdatarepublic/feature-importance-whats-in-a-name-79532e59eea3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZ7kzjbOWae8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_feature_importance(model):\n",
    "    feature_importance = pd.DataFrame(\n",
    "        model.feature_importances_,\n",
    "        index=X_train.columns,\n",
    "        columns=[\"Importance\"])\n",
    "\n",
    "    # sort by importance\n",
    "    feature_importance.sort_values(\n",
    "        by=\"Importance\", ascending=False, inplace=True)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.barplot(\n",
    "        data=feature_importance.reset_index(),\n",
    "        y=\"index\",\n",
    "        x=\"Importance\",\n",
    "    ).set_title(\"Feature Importance\")\n",
    "\n",
    "    # save image\n",
    "    plt.savefig(\"model_artifacts/feature_importance.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYfCxPo8w-Gn"
   },
   "source": [
    "## Permutation Importance\n",
    "\n",
    "Permutation Importance is a technique used to measure feature importance. It works by randomly shuffling a single\n",
    "feature in the validation data and measuring the decrease in the model's performance. The features that cause the most\n",
    "significant drop in performance are considered the most important.\n",
    "\n",
    "This method provides a straightforward way to interpret the influence of each feature on the model's predictions. It can\n",
    "help you understand which features are driving the model's decisions and where you might focus your attention for\n",
    "further data analysis or feature engineering.\n",
    "\n",
    "References:\n",
    "- https://www.kaggle.com/dansbecker/permutation-importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_vzVVbGcS6M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_permutation_importance(model):\n",
    "    p_importance = permutation_importance(\n",
    "        model, X_test, y_test, random_state=42, n_jobs=-1)\n",
    "\n",
    "\n",
    "    # sort by importance\n",
    "    sorted_idx = p_importance.importances_mean.argsort()[::-1]\n",
    "    p_importance = pd.DataFrame(\n",
    "        data=p_importance.importances[sorted_idx].T,\n",
    "        columns=X_train.columns[sorted_idx]\n",
    "    )\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.barplot(\n",
    "        data=p_importance,\n",
    "        orient=\"h\"\n",
    "    ).set_title(\"Permutation Importance\")\n",
    "\n",
    "    # save image\n",
    "    plt.savefig(\n",
    "        \"model_artifacts/permutation_importance.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "warpAv8RFSOI"
   },
   "source": [
    "## MLflow Tracking\n",
    "\n",
    "In this phase, you use MLflow Tracking, a component of MLflow that logs and tracks experiment data. This includes\n",
    "parameters, metrics, and artifacts of ML models during the training process.\n",
    "\n",
    "MLflow Tracking provides a centralized repository for metadata associated with your experiments, making it easier to\n",
    "compare different runs, reproduce results, and share findings with your team. This is a crucial step in maintaining an\n",
    "organized and efficient ML workflow.\n",
    "\n",
    "First, let's setup the logger.\n",
    "\n",
    "References:\n",
    "- https://www.mlflow.org/docs/latest/cli.html#mlflow-ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyQRcKslAwv-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Track params and metrics\n",
    "def log_mlflow_run(model, signature):\n",
    "    # Auto-logging for scikit-learn estimators\n",
    "    # mlflow.sklearn.autolog()\n",
    "\n",
    "    # log estimator_name name\n",
    "    name = model.__class__.__name__\n",
    "    mlflow.set_tag(\"estimator_name\", name)\n",
    "\n",
    "    # log input features\n",
    "    mlflow.set_tag(\"features\", str(X_train.columns.values.tolist()))\n",
    "\n",
    "    # Log tracked parameters only\n",
    "    mlflow.log_params({key: model.get_params()[key] for key in parameters})\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        'RMSLE_CV': score_cv.mean(),\n",
    "        'RMSE': score})\n",
    "\n",
    "    # log training loss\n",
    "    for s in model.train_score_:\n",
    "        mlflow.log_metric(\"Train Loss\", s)\n",
    "\n",
    "    # Save model to artifacts\n",
    "    mlflow.sklearn.log_model(model, \"model\")#, signature=signature)\n",
    "\n",
    "    # log charts\n",
    "    mlflow.log_artifacts(\"model_artifacts\")\n",
    "\n",
    "    # misc\n",
    "    # Log all model parameters\n",
    "    # mlflow.log_params(model.get_params())\n",
    "    mlflow.log_param(\"Training size\", X_test.size) \n",
    "    mlflow.log_param(\"Test size\", y_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDAdPTeDTjr1"
   },
   "source": [
    "## Model Training and Hyperparameter Tuning\n",
    "\n",
    "In this section, you focus on training the model and tuning its hyperparameters. For this particular use case, you\n",
    "employ the following approach:\n",
    "\n",
    "- Approach: You use a Supervised Learning method, specifically a Decision Tree model. Decision Trees are intuitive\n",
    "  and easy-to-interpret models that make decisions based on a set of rules inferred from the features.\n",
    "- Tree Type: Given that the task is to predict a continuous target variable (the count of total rental bikes), you will\n",
    "  use a Regression Tree.\n",
    "- Technique/Ensemble Method: To improve the performance of your Decision Tree model, you use an ensemble method known as\n",
    "  Gradient Boosting. Gradient Boosting combines several weak learners (in this case, Decision Trees) to create a robust\n",
    "  predictive model. It trains models in a gradual, additive, and sequential manner, with each new model correcting the\n",
    "  errors made by the previous ones.\n",
    "\n",
    "By carefully tuning the hyperparameters of your Gradient Boosting model, you can optimize its performance and ensure it\n",
    "generalizes well to new data.\n",
    "\n",
    "References:\n",
    "- GBRT (Gradient Boosted Regression Tree): https://orbi.uliege.be/bitstream/2268/163521/1/slides.pdf\n",
    "- Choosing a model: https://scikit-learn.org/stable/tutorial/machine_learning_map\n",
    "- Machine Learning Models Explained\n",
    ": https://docs.paperspace.com/machine-learning/wiki/machine-learning-models-explained\n",
    "- Gradient Boosted Regression Trees: https://orbi.uliege.be/bitstream/2268/163521/1/slides.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSbcPvkBThXV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GBRT (Gradient Boosted Regression Tree) scikit-learn implementation \n",
    "model_class = GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7BYFTSRzLk2"
   },
   "source": [
    "Set the training's process hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Mu88JOkMiJF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"learning_rate\": [0.1, 0.05],\n",
    "    \"max_depth\": [4, 5],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnUDX2p2j9p_",
    "tags": []
   },
   "source": [
    "To optimize the performance of your model, you tune its hyperparameters using a method known as Grid Search.\n",
    "\n",
    "Grid Search is a traditional method for hyperparameter tuning. It works by defining a grid of hyperparameters and then\n",
    "evaluating the model performance for each point on the grid. You can think of this as an exhaustive search through a\n",
    "manually specified subset of the hyperparameter space of the chosen algorithm.\n",
    "\n",
    "By using Grid Search, you can systematically work through multiple combinations of hyperparameters to determine the\n",
    "optimal values that improve the performance of the model. This process can significantly enhance the predictive accuracy\n",
    "of your model.\n",
    "\n",
    "References:\n",
    "- More advanced tuning techniques: https://research.fb.com/efficient-tuning-of-online-systems-using-bayesian-optimization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CybsVlgCw6n9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate parameters combinations\n",
    "params_keys = parameters.keys()\n",
    "params_values = [\n",
    "    parameters[key] if isinstance(parameters[key], list) else [parameters[key]]\n",
    "    for key in params_keys]\n",
    "\n",
    "runs_parameters = [\n",
    "    dict(zip(params_keys, combination))\n",
    "         for combination in itertools.product(*params_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u23-Tpn_0X7d"
   },
   "source": [
    "Now that you have prepared the data and set up the model, the next step is to train the model. During this process, the\n",
    "model learns from the features of the training data to predict the target variable.\n",
    "\n",
    "Model training involves adjusting the model to minimize the difference between the predicted and actual values, a\n",
    "process guided by a specific learning algorithm. In your case, you are using a Gradient Boosting model, which learns to\n",
    "correct its errors in a gradual, additive, and sequential manner.\n",
    "\n",
    "This is a crucial step in the ML workflow, as the quality of the model's predictions heavily depends on the\n",
    "effectiveness of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "Le6sa7jjg37v",
    "outputId": "7e8c3e45-e75a-45ce-8157-38b097d623cc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "for i, run_parameters in enumerate(runs_parameters):\n",
    "    # mlflow: stop active runs if any\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "    # mlflow:track run\n",
    "    mlflow.start_run(run_name=f\"Run {i}\")\n",
    "\n",
    "    # create model instance\n",
    "    model = model_class(**run_parameters)\n",
    "\n",
    "    # train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # get evaluations scores\n",
    "    ypred = model.predict(X_test)\n",
    "    score, message = rmse_score(y_test, model.predict(X_test))\n",
    "    score_cv, message_cv = rmsle_cv_score(model, X_train, y_train)\n",
    "\n",
    "    # get model signature\n",
    "    signature = infer_signature(model_input=X_train,\n",
    "                                model_output=model.predict(X_train))\n",
    "\n",
    "    # mlflow: log metrics\n",
    "    log_mlflow_run(model, signature)\n",
    "\n",
    "    # mlflow: end tracking\n",
    "    mlflow.end_run()\n",
    "\n",
    "    print(f\"Learning Rate: {run_parameters['learning_rate']}\\n\"\n",
    "          f\"Max Depth: {run_parameters['max_depth']}\\n\"\n",
    "          f\"{message}\\n\"\n",
    "          f\"{message_cv}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOHX6U3ABTSE"
   },
   "source": [
    "## Best Model Identification\n",
    "\n",
    "After training several models and tuning their hyperparameters, you identify the model that performs the best according\n",
    "to the chosen evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_df = mlflow.search_runs(order_by=['metrics.RMSLE_CV ASC'],\n",
    "                                 max_results=1)\n",
    "if len(best_run_df.index) == 0:\n",
    "    raise Exception(f\"Found no runs for experiment '{experiment_name}'\")\n",
    "\n",
    "best_run = mlflow.get_run(best_run_df.at[0, 'run_id'])\n",
    "best_model_uri = f\"{best_run.info.artifact_uri}/model\"\n",
    "best_model = mlflow.sklearn.load_model(best_model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "wHVM74A--4-C",
    "outputId": "b28470ff-aa99-4f19-c124-d4b9c3a88233",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print best run info\n",
    "print(\"Best run info:\")\n",
    "print(f\"Run id: {best_run.info.run_id}\")\n",
    "print(f\"Run parameters: {best_run.data.params}\")\n",
    "print(f\"Run score: RMSLE_CV = {best_run.data.metrics['RMSLE_CV']:.4f}\")\n",
    "print(f\"Run model URI: {best_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "WmjSO3vhCP7u",
    "outputId": "52e2c4ac-4aeb-44b8-d65d-7d2de8122fb8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_feature_importance(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "LQRJKFuJCSBZ",
    "outputId": "3a60cd14-f402-4304-ee8c-7f3647be4721",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_permutation_importance(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDhu91aa8vuw"
   },
   "source": [
    "## Model Testing\n",
    "\n",
    "Once you have identified the best model, the next step is to test its predictive performance on unseen data. This is\n",
    "done using the test dataset, which has been set aside specifically for this purpose.\n",
    "\n",
    "Testing the model's predictions allows you to evaluate how well the model generalizes to new data. This is a crucial\n",
    "step in the machine learning process, as it provides a realistic estimate of the model's performance in a real-world\n",
    "setting.\n",
    "\n",
    "You compare the model's predictions with the actual values in the test dataset and calculate your chosen evaluation\n",
    "metrics. These results provide a clear indication of the model's predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "EiQwrb7TK40n",
    "outputId": "709d749f-bc0d-4b68-c2c4-8c1a0197eca6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions = X_test.copy()\n",
    "# real output (rented_bikes) from test dataset\n",
    "test_predictions[\"rented_bikes\"] = y_test\n",
    "\n",
    "# add \"predicted_rented_bikes\" from test dataset\n",
    "test_predictions[\"predicted_rented_bikes\"] = (\n",
    "    best_model.predict(X_test).astype(int)\n",
    ")\n",
    "\n",
    "# show results\n",
    "test_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 911
    },
    "id": "SwfQEr_NGlDa",
    "outputId": "e153d67b-b13f-4b13-bbe9-542eed7442a8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot truth vs prediction values\n",
    "test_predictions.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"rented_bikes\",\n",
    "    y=\"predicted_rented_bikes\",\n",
    "    title=\"Rented bikes vs predicted rented bikes\",\n",
    "    figsize=(10, 10)\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "In this section of the notebook, you focus on deploying the trained model and bridge the gap between insightful data\n",
    "analysis and tangible real-world impact.\n",
    "\n",
    "For this, you use KServe, an open-source platform that facilitates the deployment and management of ML models at scale.\n",
    "It provides a robust and scalable infrastructure to serve predictions from trained models in production environments.\n",
    "The backend that you use for KServe is Seldon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: s3creds\n",
    "  annotations:\n",
    "     serving.kserve.io/s3-endpoint: {os.environ[\"AWS_ENDPOINT_URL\"].replace(\"http://\", \"\")}\n",
    "     serving.kserve.io/s3-usehttps: \"0\"\n",
    "     serving.kserve.io/s3-useanoncredential: \"false\"\n",
    "     serving.kserve.io/s3-cabundle: \"\"\n",
    "type: Opaque\n",
    "stringData:\n",
    "  AWS_ACCESS_KEY_ID: {os.environ[\"AWS_ACCESS_KEY_ID\"]}\n",
    "  AWS_SECRET_ACCESS_KEY: {os.environ[\"AWS_SECRET_ACCESS_KEY\"]}\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: kserve-minio-sa\n",
    "secrets:\n",
    "- name: s3creds\n",
    "\n",
    "---\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"bike-sharing\"\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: kserve-minio-sa\n",
    "    sklearn:\n",
    "      protocolVersion: \"v2\"\n",
    "      storageUri: \"{best_model_uri}/model\"\n",
    "\"\"\"\n",
    "\n",
    "os.makedirs(\"manifests\", exist_ok=True)\n",
    "\n",
    "with open(os.path.join(\"manifests\", \"isvc.yaml\"), \"w\") as f:\n",
    "    f.write(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = subprocess.run([\"kubectl\", \"apply\", \"-f\", \"manifests/isvc.yaml\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMUyEIXKPIvKiU5I2T//pwx",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MLflow-example-notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gcr.io/mapr-252711/kubeflow/notebooks/jupyter-data-science:ezaf-fy23-q3",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": false,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:add-external-df-volume:true",
    "label:add-ldapcert-secret:true",
    "label:add-sssd-secret:true",
    "label:add-user-s3-secret:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
