{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3324b7d0-515d-46a7-ac00-03e788d5e96a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vector Stores: Embedding and Storing Documents in a Latent Space\n",
    "\n",
    "In this Jupyter Notebook, you explore a foundational element of a question-answering system: the Vector Store. The\n",
    "Vector Store serves as the key component that allows us to efficiently retrieve relevant context from a corpus of\n",
    "documents based on a user's query.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/documents.jpg\" alt=\"documents\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "      Photo by <a href=\"https://unsplash.com/@anniespratt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Annie Spratt</a> on <a href=\"https://unsplash.com/photos/5cFwQ-WMcJU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "The approach you employ involves transforming each document in the corpus into a high-dimensional numerical\n",
    "representation known as an \"embedding\", using a pre-trained Transformer model. This process is sometimes referred to as\n",
    "\"embedding\" the document in a latent space. The latent space here is a high-dimensional space where similar documents\n",
    "are close to each other. The position of a document in this space is determined by the content and the semantic meaning\n",
    "it carries.\n",
    "\n",
    "Once you have these embeddings, you store them in a Vector Store. A Vector Store is an advanced AI-native database\n",
    "designed to hold these high-dimensional vectors and provide efficient search capabilities. This enables you to quickly\n",
    "identify documents in your corpus that are semantically similar to a given query, which will also be represented as a\n",
    "vector in the same latent space.\n",
    "\n",
    "The following cells in this Notebook guides you through the process of creating such a Vector Store. You start by\n",
    "generating embeddings for each document, then you move on to storing these embeddings in a Vector Store and finally,\n",
    "you see how easy it is to to retrieve documents from the Vector Store based on a query.\n",
    "\n",
    "First, let's import the libraries you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df209f-e471-45d8-ad58-ad5c34819ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea076619-2576-4154-ad50-0775b84a4359",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load the Documents\n",
    "\n",
    "The next cells contain a set of helper functions designed to load text documents from a specified directory. These\n",
    "functions are essential for preparing your data before embedding it into the high-dimensional latent space.\n",
    "\n",
    "The key operations performed by these functions are:\n",
    "\n",
    "- Directory Scanning: Scan the specified directory for all `.txt` files recursively.\n",
    "- Document Loading: Load the file in LangChain `Document` object, using the provided `TextLoader` object.\n",
    "\n",
    "By running this cell, you have a list of documents ready to be processed and embedded in the latent space. This forms\n",
    "your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc826f3-6670-4fbd-87a9-ef4e67c6f30f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_doc(fn):\n",
    "    loader = TextLoader(fn)\n",
    "    doc = loader.load()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbff81-1b65-41fb-9d65-a26e88779d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_docs(source_dir: str) -> list:\n",
    "    \"\"\"Load all documents in a the given directory.\"\"\"\n",
    "    fns = glob.glob(os.path.join(source_dir, \"*.txt\"))\n",
    "    \n",
    "    docs = []\n",
    "    for i, fn in enumerate(tqdm(fns, desc=\"Loading documents...\")):\n",
    "        docs.extend(load_doc(fn))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ce613-1f5a-428b-a20f-c2d83c46c40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = load_docs(\"documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ce8b8-52d1-468d-a28c-7384b90f553f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Document Processing: Chunking Text for the Language Model\n",
    "\n",
    "In this section of the Notebook, you process the documents by splitting them into chunks. This operation is crucial when\n",
    "working with Large Language Models (LLMs), as these models have a maximum limit on the number of tokens (words or pieces\n",
    "of words) they can process at once. This limit is often referred to as the model's \"context window\".\n",
    "\n",
    "In this example, you split each document into segments that are at most `500` tokens long. You use the LangChain's\n",
    "`RecursiveCharacterTextSplitter`, which, by default, splits each document when it encounters two consecutive newline\n",
    "characters, represented as `\\n\\n`. Furthermore, each segment is distinct, meaning there is no overlap between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389f2b4-9bde-48b7-a87b-eee356c0fae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_docs(docs: list, chunk_size: int, chunk_overlap: int) -> list:\n",
    "    \"\"\"Load the documents and split them into chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    return texts\n",
    "\n",
    "texts = process_docs(docs, chunk_size=500, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6e838-1746-4100-bd22-30e03b36c3e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generating Embeddings & Storing them in Chroma\n",
    "\n",
    "In this section of the Notebook, you use HuggingFace's `all-MiniLM-L6-v2` model in conjunction with the Sentence\n",
    "Transformers framework to generate embeddings for your document chunks. Sentence Transformers is a Python framework that\n",
    "allows you to leverage the power of Transformer models to generate dense vector embeddings for sentences. These\n",
    "embeddings can capture the semantic meaning of the input text, making them ideal for tasks like semantic search,\n",
    "clustering, and information retrieval.\n",
    "\n",
    "By leveraging this framework and the Chroma database interface provided by LangChain, you can embed your documents into\n",
    "a latent space and subsequently store the results in a Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719391de-6f29-4234-9f40-cbb10e9da7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_model = \"all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c988d-dc14-48da-ba95-7232142f81b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "settings = Settings(anonymized_telemetry=False, chroma_db_impl=\"duckdb+parquet\",  persist_directory=f\"{os.getcwd()}/db\")\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=f\"{os.getcwd()}/db\", client_settings=settings)\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f5bcc-c6ee-416c-8893-b30a69f2f054",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, you can test the accuracy of the document retrieval mechanism by providing a simple query. Chroma will return\n",
    "with the four most similar documents by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5699cd9-93b3-4513-8cb6-2d492ff5a06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How can I create a cgroup?\"\n",
    "matches = db.similarity_search(query); matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb1b93-567e-4c2e-b3f9-48a3b1d43fc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have successfully traversed the journey of embedding documents into a high-dimensional latent space\n",
    "and storing these embeddings in a Vector Store. By accomplishing this, you've transformed unstructured text data into a\n",
    "structured form that can power a robust question-answering system.\n",
    "\n",
    "However, your journey doesn't end here. Now that you have the Vector Store ready, the next step is to create an\n",
    "Inference Service that can leverage this store to provide context to user queries. For this, you use KServe, a flexible,\n",
    "cloud-native platform for serving Machine Learning models.\n",
    "\n",
    "In the next Notebook, you set up a custom Inference Service using KServe. This service uses the Vector Store to retrieve\n",
    "and rank relevant document chunks based on a user's query, providing accurate and efficient context to an LLM!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-answering",
   "language": "python",
   "name": "question-answering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
