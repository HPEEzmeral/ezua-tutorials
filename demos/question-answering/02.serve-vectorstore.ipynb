{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b72df6-4961-4128-9281-6c9634dd33fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating an Inference Service using MLFlow and KServe\n",
    "\n",
    "Welcome to part two of the tutorial on building a question-answering application over a private document corpus with\n",
    "Large Language Models (LLMs). In the previous Notebook, you transformed the documents into a high-dimensional latent\n",
    "space and saved these embeddings in a Vector Store using the Chroma database interface from LangChain.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/inference-service.jpg\" alt=\"isvc\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "    Photo by <a href=\"https://unsplash.com/@growtika?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Growtika</a> on <a href=\"https://unsplash.com/photos/GSiEeoHcNTQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "In this Notebook, you delve deeper. You use MLFlow to log the Chroma DB files as experiment artifacts. Once logged, you\n",
    "then set up an Inference Service that fetches these artifacts and leverages them to provide context to user inquiries.\n",
    "For this task, you work with KServe, a Kubernetes-centric platform that offers a serverless blueprint for scaling\n",
    "Machine Learning models seamlessly.\n",
    "\n",
    "A crucial point to remember: KServe doesn't support Chroma DB files natively. Because of this, you integrate a custom\n",
    "predictor component. This involves creating a Docker image, which then serves as your Inference Service. This approach\n",
    "grants you a high level of customization, ensuring the service fits your requirements. You can find the necessary code\n",
    "and the Dockerfile for this custom predictor in the `dockerfiles/vectorstore` directory. But for a quicker setup,\n",
    "there's a pre-built option available: `dpoulopoulos/qna-vectorstore:v0.1.0`.\n",
    "\n",
    "Once you're ready, this Notebook guides you through the necessary steps for creating a scalable Vector Store service.\n",
    "First, let's import the libraries you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import subprocess\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656fd38-0660-402b-bd83-72e566cd4e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_base64(message: str):\n",
    "    encoded_bytes = base64.b64encode(message.encode('ASCII'))\n",
    "    return encoded_bytes.decode('ASCII')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8b07a-15d8-44a2-89c4-266ee61d1a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logging the Vector Store as an Artifact\n",
    "\n",
    "To begin, you create a new experiment or utilize an existing one and log the Chroma DB files as an artifact of this\n",
    "experiment. Ultimately, you retrieve the URI that points to this artifact's location and provide it to the custom\n",
    "predictor component. By doing this, the custom predictor component understands how to fetch the artifact and serve it\n",
    "effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23d780-57e0-4d5c-a699-aa3046a5b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(exp_name):\n",
    "    \"\"\"Register an experiment in MLFlow.\n",
    "    \n",
    "    args:\n",
    "      exp_name (str): The name of the experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mlflow.set_experiment(exp_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to set the experiment: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f97e2-4c39-4464-b3e8-c778b96d28da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new MLFlow experiment or re-use an existing one\n",
    "get_or_create_experiment('question-answering')\n",
    "\n",
    "# Log the Chroma DB files as an artifact of the experiment\n",
    "mlflow.log_artifact(f\"{os.getcwd()}/db\")\n",
    "\n",
    "# Retrieve the URI of the artifact\n",
    "uri = mlflow.get_artifact_uri(\"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25e4d-4803-4464-80d9-6b62ad2b4d49",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating and Submitting the Inference Service\n",
    "\n",
    "In the final segment of this Notebook, you create and submit an Inference Service via a YAML template and a Python\n",
    "subprocess. This process unfolds as follows:\n",
    "\n",
    "1. Drafting the YAML Template: Here, you craft a YAML file that outlines the Inference Service's specifics. This\n",
    "   captures elements like the service's name, the chosen Docker image, and additional configurations. After drafting,\n",
    "   you save this YAML to a file for inspection and later submission.\n",
    "1. Pushing the YAML Template: With your YAML template prepped, the next step is to present it to KServe for deployment.\n",
    "   You accomplish this by leveraging a Python subprocess to execute a shell command.\n",
    "\n",
    "By the end of this section, you will have a running Inference Service that is ready to receive user queries and provide\n",
    "context for answering them using the Vector Store. This marks the completion of your journey, from transforming\n",
    "unstructured text data into structured vector embeddings, to creating a scalable service that can provide context based\n",
    "on those embeddings.\n",
    "\n",
    "In the upcoming cell, input the name of the Docker image you constructed in the initial phase. If you wish to utilize\n",
    "the pre-fabricated one, simply leave the field untouched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be7670-5333-41ec-a79b-fa3ae88f81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_image = (input(\"Enter the name of the predictor image (default: dpoulopoulos/qna-vectorstore:v0.1.0): \")\n",
    "                   or \"dpoulopoulos/qna-vectorstore:v0.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c9d-72e4-491a-ad99-f6f533f0ef94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isvc = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: minio-secret\n",
    "type: Opaque\n",
    "data:\n",
    "  MINIO_ACCESS_KEY: {0}\n",
    "  MINIO_SECRET_KEY: {1}\n",
    "\n",
    "---\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: vectorstore\n",
    "spec:\n",
    "  predictor:\n",
    "    containers:\n",
    "    - name: kserve-container\n",
    "      image: {2}\n",
    "      imagePullPolicy: Always\n",
    "      resources:\n",
    "        requests:\n",
    "          memory: \"2Gi\"\n",
    "          cpu: \"500m\"\n",
    "        limits:\n",
    "          memory: \"2Gi\"\n",
    "          cpu: \"500m\"\n",
    "      args:\n",
    "      - --persist-uri\n",
    "      - {3}\n",
    "      env:\n",
    "      - name: MLFLOW_S3_ENDPOINT_URL\n",
    "        value: {4}\n",
    "      - name: TRANSFORMERS_CACHE\n",
    "        value: /src\n",
    "      - name: SENTENCE_TRANSFORMERS_HOME\n",
    "        value: /src\n",
    "      - name: MINIO_ACCESS_KEY\n",
    "        valueFrom:\n",
    "          secretKeyRef:\n",
    "            key: MINIO_ACCESS_KEY\n",
    "            name: minio-secret\n",
    "      - name: MINIO_SECRET_KEY\n",
    "        valueFrom:\n",
    "          secretKeyRef:\n",
    "            key: MINIO_SECRET_KEY\n",
    "            name: minio-secret\n",
    "\"\"\".format(encode_base64(os.environ[\"AWS_ACCESS_KEY_ID\"]),\n",
    "           encode_base64(os.environ[\"AWS_SECRET_ACCESS_KEY\"]),\n",
    "           predictor_image, uri, os.environ[\"MLFLOW_S3_ENDPOINT_URL\"])\n",
    "\n",
    "with open(\"vectorstore/isvc.yaml\", \"w\") as f:\n",
    "    f.write(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631d06-ea76-4159-a08b-57e76850ff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"vectorstore/isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e2d08-8e09-4c9e-826c-0f0dfdc2d3f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully navigated through the process of logging the Chroma DB files as artifacts using\n",
    "MLFlow, creating a custom Docker image, and setting up an Inference Service with KServe that retrieves these artifacts\n",
    "to serve your Vector Store. This Inference Service forms the backbone of your question-answering application, enabling\n",
    "you to efficiently answer queries based on the document embeddings we generated previously.\n",
    "\n",
    "From here, there are two paths you can choose:\n",
    "\n",
    "- Testing the Vector Store Inference Service: If you'd like to test the Vector Store Inference Service that you've just\n",
    "  created, you can proceed to our third (optional) Notebook. This Notebook provides a step-by-step guide on how to\n",
    "  invoke the Inference Service and validate its performance.\n",
    "- Creating the LLM Inference Service: Alternatively, if you're ready to move on to the next stage of the project, you\n",
    "  can jump straight to our fourth Notebook. In this Notebook, you create an Inference Service for the Large Language\n",
    "  Model (LLM), which will work in conjunction with the Vector Store Inference Service to provide answers to user\n",
    "  queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-answering",
   "language": "python",
   "name": "question-answering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
