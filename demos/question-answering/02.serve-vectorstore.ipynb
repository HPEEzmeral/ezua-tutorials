{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b72df6-4961-4128-9281-6c9634dd33fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating an Inference Service using MLFlow and KServe\n",
    "\n",
    "Welcome to part two of the tutorial on building a question-answering application over a private document corpus with\n",
    "Large Language Models (LLMs). In the previous Notebook, you transformed the documents into a high-dimensional latent\n",
    "space and saved these embeddings in a Vector Store using the Chroma database interface from LangChain.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/inference-service.jpg\" alt=\"isvc\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "    Photo by <a href=\"https://unsplash.com/@growtika?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Growtika</a> on <a href=\"https://unsplash.com/photos/GSiEeoHcNTQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "In this Notebook, you delve deeper. You use MLflow to log the Chroma DB files as experiment artifacts. Once logged, you\n",
    "then set up an Inference Service (ISVC) that fetches these artifacts and leverages them to provide context to user\n",
    "inquiries. For this task, you work with KServe, a Kubernetes-centric platform that offers a serverless blueprint for\n",
    "scaling Machine Learning (ML) models seamlessly.\n",
    "\n",
    "A crucial point to remember: KServe doesn't support Chroma DB files natively. Because of this, you integrate a custom\n",
    "predictor component. This involves creating a Docker image, which then serves as your ISVC endpoint. This approach\n",
    "grants you a high level of customization, ensuring the service fits your requirements. You can find the necessary code\n",
    "and the Dockerfile for this custom predictor in the `dockerfiles/vectorstore` directory. But for a quicker setup,\n",
    "there's a pre-built option available: `dpoulopoulos/qna-vectorstore:v0.1.0`.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Logging the Vector Store as an Artifact](#logging-the-vector-store-as-an-artifact)\n",
    "1. [Creating and Submitting the Inference Service](#creating-and-submitting-the-inference-service)\n",
    "1. [Conclusion and Next Steps](#conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import subprocess\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d136311-4035-46b6-b17b-f946dd3d7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.environ['MLFLOW_TRACKING_TOKEN']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"s3\"\n",
    "os.environ[\"AWS_ENDPOINT_URL\"] = 'http://local-s3-service.ezdata-system.svc.cluster.local:30000'\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = os.environ[\"AWS_ENDPOINT_URL\"]\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656fd38-0660-402b-bd83-72e566cd4e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_base64(message: str):\n",
    "    encoded_bytes = base64.b64encode(message.encode('ASCII'))\n",
    "    return encoded_bytes.decode('ASCII')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8b07a-15d8-44a2-89c4-266ee61d1a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logging the Vector Store as an Artifact\n",
    "\n",
    "To begin, you create a new experiment or utilize an existing one and log the Chroma DB files as an artifact of this\n",
    "experiment. Ultimately, you retrieve the URI that points to this artifact's location and provide it to the custom\n",
    "predictor component. By doing this, the custom predictor component understands how to fetch the artifact and serve it\n",
    "effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23d780-57e0-4d5c-a699-aa3046a5b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(exp_name):\n",
    "    \"\"\"Register an experiment in MLFlow.\n",
    "    \n",
    "    args:\n",
    "      exp_name (str): The name of the experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mlflow.set_experiment(exp_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to set the experiment: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f97e2-4c39-4464-b3e8-c778b96d28da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new MLFlow experiment or re-use an existing one\n",
    "get_or_create_experiment('question-answering')\n",
    "\n",
    "# Log the Chroma DB files as an artifact of the experiment\n",
    "mlflow.log_artifact(f\"{os.getcwd()}/db\")\n",
    "\n",
    "# Retrieve the URI of the artifact\n",
    "uri = mlflow.get_artifact_uri(\"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25e4d-4803-4464-80d9-6b62ad2b4d49",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating and Submitting the Inference Service\n",
    "\n",
    "In the final segment of this Notebook, you create and submit an ISVC via a YAML template and a Python subprocess. This\n",
    "process unfolds as follows:\n",
    "\n",
    "1. Drafting the YAML Template: Here, you craft a YAML file that outlines the ISVC's specifics. This captures elements\n",
    "   like the service's name, the chosen Docker image, and additional configurations. After drafting, you save this YAML\n",
    "   to a file for inspection and later submission.\n",
    "1. Pushing the YAML Template: With your YAML template prepped, the next step is to present it to KServe for deployment.\n",
    "   You accomplish this by leveraging a Python subprocess to execute a shell command.\n",
    "\n",
    "By the end of this section, you will have a running ISVC that is ready to receive user queries and provide context for\n",
    "answering them using the Vector Store. This marks the completion of your journey, from transforming unstructured text\n",
    "data into structured vector embeddings, to creating a scalable service that can provide context based on those\n",
    "embeddings.\n",
    "\n",
    "In the upcoming cell, input the name of the Docker image you constructed in the initial phase. If you wish to utilize\n",
    "the pre-fabricated one, simply leave the field untouched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be7670-5333-41ec-a79b-fa3ae88f81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_image = (input(\"Enter the name of the predictor image (default: dpoulopoulos/qna-vectorstore:v0.1.0): \")\n",
    "                   or \"dpoulopoulos/qna-vectorstore:v0.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c9d-72e4-491a-ad99-f6f533f0ef94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isvc = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: minio-secret\n",
    "type: Opaque\n",
    "data:\n",
    "  MINIO_ACCESS_KEY: {0}\n",
    "  MINIO_SECRET_KEY: {1}\n",
    "\n",
    "---\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: vectorstore\n",
    "spec:\n",
    "  predictor:\n",
    "    containers:\n",
    "    - name: kserve-container\n",
    "      image: {2}\n",
    "      imagePullPolicy: Always\n",
    "      resources:\n",
    "        requests:\n",
    "          memory: \"2Gi\"\n",
    "          cpu: \"500m\"\n",
    "        limits:\n",
    "          memory: \"2Gi\"\n",
    "          cpu: \"500m\"\n",
    "      args:\n",
    "      - --persist-uri\n",
    "      - {3}\n",
    "      # If you are running behind a proxy, uncomment the following lines and replace the values with your proxy URLs.\n",
    "      # env:\n",
    "      # - name: HTTP_PROXY\n",
    "      #   value: <yout http proxy URL>\n",
    "      # - name: HTTPS_PROXY\n",
    "      #   value: <yout https proxy URL>\n",
    "      # - name: NO_PROXY\n",
    "      #   value: .local\n",
    "      - name: MLFLOW_S3_ENDPOINT_URL\n",
    "        value: {4}\n",
    "      - name: TRANSFORMERS_CACHE\n",
    "        value: /src\n",
    "      - name: SENTENCE_TRANSFORMERS_HOME\n",
    "        value: /src\n",
    "      - name: MINIO_ACCESS_KEY\n",
    "        valueFrom:\n",
    "          secretKeyRef:\n",
    "            key: MINIO_ACCESS_KEY\n",
    "            name: minio-secret\n",
    "      - name: MINIO_SECRET_KEY\n",
    "        valueFrom:\n",
    "          secretKeyRef:\n",
    "            key: MINIO_SECRET_KEY\n",
    "            name: minio-secret\n",
    "\"\"\".format(encode_base64(os.environ[\"AWS_ACCESS_KEY_ID\"]),\n",
    "           encode_base64(os.environ[\"AWS_SECRET_ACCESS_KEY\"]),\n",
    "           predictor_image, uri, os.environ[\"MLFLOW_S3_ENDPOINT_URL\"])\n",
    "\n",
    "with open(\"vectorstore-isvc.yaml\", \"w\") as f:\n",
    "    f.write(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631d06-ea76-4159-a08b-57e76850ff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"vectorstore-isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e2d08-8e09-4c9e-826c-0f0dfdc2d3f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully navigated through the process of logging the Chroma DB files as artifacts using\n",
    "MLflow, creating a custom Docker image, and setting up an ISVC with KServe that retrieves these artifacts to serve your\n",
    "Vector Store. This ISVC forms the backbone of your question-answering application, enabling you to efficiently answer\n",
    "queries based on the document embeddings we generated previously.\n",
    "\n",
    "From here, there are two paths you can choose:\n",
    "\n",
    "- Testing the Vector Store ISVC: If you'd like to test the Vector Store ISVC that you've just created, you can proceed\n",
    "  to the third (optional) Notebook. This Notebook provides a step-by-step guide on how to invoke the ISVC and validate\n",
    "  its performance.\n",
    "- Creating the LLM ISVC: Alternatively, if you're ready to move on to the next stage of the project, you\n",
    "  can jump straight to our fourth Notebook. In this Notebook, you create an ISVC for the Large Language Model (LLM),\n",
    "  which will work in conjunction with the Vector Store ISVC to provide answers to user queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-answering",
   "language": "python",
   "name": "question-answering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
