{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286558bc-9780-4399-ae75-8055b8e0a9d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Invoking and Testing the Large Language Model Inference Service\n",
    "\n",
    "Welcome to the fifth and final part of the tutorial series on building a question-answering application over a corpus of\n",
    "private documents using Large Language Models (LLMs). In the previous Notebooks, you've covered the processes of\n",
    "creating vector embeddings of our documents, deploying a Vector Store Inference Service, creating a Large Language Model\n",
    "Inference Service, and enriching user queries with relevant context using an Inference service Transformer component.\n",
    "\n",
    "In this Notebook, you focus on the crucial task of invoking and testing the LLM Inference Service you've created. This\n",
    "is an important step in the development process as it allows you to validate the functionality and performance of your\n",
    "service in a practical setting.\n",
    "\n",
    "Throughout this Notebook, you see how to construct and send requests to the LLM Inference Service, interpret the\n",
    "responses, and handle potential issues that might arise. By the end of this Notebook, you will have gained practical\n",
    "experience in working with the LLM Inference Service, preparing you to integrate it into larger systems or applications.\n",
    "\n",
    "Let's get started by importing the libraries you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fd850-d35a-476f-ba05-b11763ddec68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23caa9-d988-4ae0-afd6-094a5a09c82a",
   "metadata": {
    "tags": []
   },
   "source": [
    "You are now ready to test your service. Provide your question and get back the answer from the LLM inference service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e2ebd-5e3b-4289-8358-9406ba816921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOMAIN_NAME = \"svc.cluster.local\"  # change this to your domain for external access\n",
    "NAMESPACE = os.environ['USER']\n",
    "DEPLOYMENT_NAME = \"llm\"\n",
    "MODEL_NAME = DEPLOYMENT_NAME\n",
    "SVC = f'{DEPLOYMENT_NAME}-transformer-default.{NAMESPACE}.{DOMAIN_NAME}'\n",
    "URL = f\"https://{SVC}/v1/models/{MODEL_NAME}:predict\"\n",
    "\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da091c-9fce-4f91-8382-e5c785bdf24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"instances\": [{\n",
    "      \"system\": \"You are an AI assistant. You will be given a task. You must generate a detailed answer.\",\n",
    "      \"instruction\": \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\",\n",
    "      \"input\": \"Can a process running in a Linux Namespace communicate with another process running in another Namespace?\",\n",
    "      \"max_tokens\": 50,\n",
    "      \"top_k\": 100,\n",
    "      \"top_p\": 0.4,\n",
    "      \"num_docs\": 1,\n",
    "      \"temperature\": 0.2\n",
    "  }]\n",
    "}\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ['AUTH_TOKEN']}\"}\n",
    "\n",
    "response = requests.post(URL, json=data, headers=headers, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fdac2-cacb-40cc-bf2d-d1548072bb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed71c82-a009-4b87-8e85-79fa218999b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you're executing this tutorial in an environment without access to a GPU device, the inference step might require\n",
    "more time than usual. Please exercise patience and allow for approximately 5 minutes. In the unlikely event that you\n",
    "encounter a time-out error, please attempt the process again.\n",
    "\n",
    "Furthermore, increasing the number of retrieved documents (`num_docs`) increases the latency. For testing purposes just\n",
    "keep this number as low as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501663ea-9a56-4fca-a63a-69fabe51ec34",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations on reaching the finish line of this comprehensive tutorial! You've successfully developed an application\n",
    "capable of delivering responses to user queries in a natural language format. The journey has not only enhanced your\n",
    "understanding but also allowed you to acquire hands-on experience in various facets of Large Language Models.\n",
    "\n",
    "Throughout this process, you've demystified the concept of a Vector Store, created custom predictor and transformer\n",
    "components, and learned to log artifacts with MLflow. Moreover, all these tasks have been accomplished within the\n",
    "comfortable and familiar confines of your JupyterLab environment.\n",
    "\n",
    "In conclusion, you've taken significant strides in your journey of mastering Large Language Models, and how to\n",
    "create real-world applications using the EzUA platform. Happy coding, and until the next tutorial, keep learning and\n",
    "experimenting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-answering",
   "language": "python",
   "name": "question-answering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
