{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286558bc-9780-4399-ae75-8055b8e0a9d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Invoking and Testing the Large Language Model Inference Service\n",
    "\n",
    "Welcome to the fifth and final part of the tutorial series on building a question-answering application over a corpus of\n",
    "private documents using Large Language Models (LLMs). In the previous Notebooks, you've covered the processes of\n",
    "creating vector embeddings of our documents, deploying a Vector Store Inference Service (ISVC), creating an LLM ISVC,\n",
    "and enriching user queries with relevant context using an ISVC Transformer component.\n",
    "\n",
    "In this Notebook, you focus on the crucial task of invoking and testing the LLM ISVC you've created. This is an\n",
    "important step in the development process as it allows you to validate the functionality and performance of your\n",
    "service in a practical setting.\n",
    "\n",
    "Throughout this Notebook, you see how to construct and send requests to the LLM ISVC, interpret the responses, and\n",
    "handle potential issues that might arise. By the end of this Notebook, you will have gained practical experience in\n",
    "working with the LLM ISVC, preparing you to integrate it into larger systems or applications.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Invoking the LLM Inference Service](#invoking-the-llm-inference-service)\n",
    "1. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fd850-d35a-476f-ba05-b11763ddec68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23caa9-d988-4ae0-afd6-094a5a09c82a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Invoking the LLM Inference Service\n",
    "\n",
    "You are now ready to test your service. Provide your question and get back the answer from the LLM inference service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e2ebd-5e3b-4289-8358-9406ba816921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOMAIN_NAME = \"svc.cluster.local\"  # change this to your domain for external access\n",
    "NAMESPACE = os.environ['USER']\n",
    "DEPLOYMENT_NAME = \"llm\"\n",
    "MODEL_NAME = DEPLOYMENT_NAME\n",
    "SVC = f'{DEPLOYMENT_NAME}-transformer-default.{NAMESPACE}.{DOMAIN_NAME}'\n",
    "URL = f\"https://{SVC}/v1/models/{MODEL_NAME}:predict\"\n",
    "\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da091c-9fce-4f91-8382-e5c785bdf24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"instances\": [{\n",
    "      \"system\": \"You are an AI assistant. You will be given a task. You must generate a detailed answer.\",\n",
    "      \"instruction\": \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\",\n",
    "      \"input\": \"Can a process running in a Linux Namespace communicate with another process running in another Namespace?\",\n",
    "      \"max_tokens\": 50,\n",
    "      \"top_k\": 100,\n",
    "      \"top_p\": 0.4,\n",
    "      \"num_docs\": 1,\n",
    "      \"temperature\": 0.2\n",
    "  }]\n",
    "}\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ['AUTH_TOKEN']}\"}\n",
    "\n",
    "response = requests.post(URL, json=data, headers=headers, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fdac2-cacb-40cc-bf2d-d1548072bb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed71c82-a009-4b87-8e85-79fa218999b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you're executing this tutorial in an environment without access to a GPU device, the inference step might require\n",
    "more time than usual. Please exercise patience and allow for approximately 5 minutes. In the unlikely event that you\n",
    "encounter a time-out error, please attempt the process again.\n",
    "\n",
    "Furthermore, increasing the number of retrieved documents (`num_docs`) increases the latency. For testing purposes keep\n",
    "this number as low as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501663ea-9a56-4fca-a63a-69fabe51ec34",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations on reaching the finish line of this comprehensive tutorial! You've successfully developed an application\n",
    "capable of delivering responses to user queries in a natural language format. The journey has not only enhanced your\n",
    "understanding but also allowed you to acquire hands-on experience in various facets of LLMs.\n",
    "\n",
    "Throughout this process, you've demystified the concept of a Vector Store, created custom predictor and transformer\n",
    "components, and learned to log artifacts with MLflow. Moreover, all these tasks have been accomplished within the\n",
    "comfortable and familiar confines of your JupyterLab environment.\n",
    "\n",
    "In conclusion, you've taken significant strides in your journey of mastering LLMs, and how to create real-world\n",
    "applications using the EzUA platform. As a next step, you can follow the instructions in the README file and deploy the\n",
    "front-end application of this service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-answering",
   "language": "python",
   "name": "question-answering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
