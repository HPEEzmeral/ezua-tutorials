{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f891729d-9030-4d9e-a7be-e81386ae820f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating a Large Language Model Inference Service\n",
    "\n",
    "Welcome to the fourth part of the tutorial series on building a question-answering application over a corpus of private\n",
    "documents using Large Language Models (LLMs). In the previous Notebooks, you journeyed through the processes of creating\n",
    "vector embeddings of our documents, setting up a Vector Store Inference Service, and testing its performance.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/llm.jpg\" alt=\"llm\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "      Photo by <a href=\"https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Google DeepMind</a> on <a href=\"https://unsplash.com/photos/LaKwLAmcnBc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "Now, you're moving towards the next crucial step: creating an Inference Service for the Large Language Model (LLM). This\n",
    "Inference Service is the centerpiece of the question-answering system, working in tandem with the Vector Store Inference\n",
    "Service to deliver comprehensive and accurate answers to user queries.\n",
    "\n",
    "In this Notebook, you set up this LLM Inference Service. You see how to a Docker image for the custom predictor, the\n",
    "role of the transformer component, define a KServe InferenceService YAML, and deploy the service. By the end of this\n",
    "Notebook, you'll have a fully functioning LLM Inference Service that can accept user queries, interact with the Vector\n",
    "Store, and provide insightful responses.\n",
    "\n",
    "Let's dive in! Let's import the libraries you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0cc08-1c00-49cc-85bd-b792383b26e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Architecture\n",
    "\n",
    "In this setup, an additional component, called a \"transformer\", plays a pivotal role in processing user queries and\n",
    "integrating the Vector Store Inference Service with the LLM Inference Service. The transformer's role is to intercept\n",
    "the user's request, extract the necessary information, and then communicate with the Vector Store Inference Service to\n",
    "retrieve the relevant context. The transformer then takes the response of the Vector Store Inference Service (i.e., the\n",
    "context), combines it with the user's query, and forwards the enriched prompt to the LLM predictor.\n",
    "\n",
    "Here's a detailed look at the process:\n",
    "\n",
    "1. Intercepting the User's Request: The transformer acts as a gateway between the user and the LLM inference service.\n",
    "   When a user sends a query, it first reaches the transformer. The transformer extracts the query from the request.\n",
    "1. Communicating with the Vector Store Inference Service: The transformer then takes the user's query and sends a POST\n",
    "   request to the Vector Store Inference Service including the user's query in the payload, just like you did in the\n",
    "   previous Notebook.\n",
    "1. Receiving and Processing the Context: The Vector Store Inference Service responds by sending back the relevant\n",
    "   context.\n",
    "1. Combining the Context with the User's Query: The transformer then combines the received context with the user's\n",
    "   original query using a prompt template. This creates an enriched prompt that contains both the user's original\n",
    "   question and the relevant context from our documents.\n",
    "1. Forwarding the Enriched Query to the LLM Predictor: Finally, the transformer forwards this enriched query to the LLM\n",
    "   predictor. The predictor then processes this query and generates a response, which is sent back to the transformer.\n",
    "   Steps 2 through 5 are transparent to the user.\n",
    "1. The transformer returns the response to the user.\n",
    "\n",
    "As such, you should build two custom Docker images at this point: one for the predictor and one for the transformer. The\n",
    "source code and the Dockerfiles are provided in the corresponding folders: `dockerfiles/llm` and\n",
    "`dockerfiles/transformer`. For your convenience, you can use the images we have pre-built for you:\n",
    "\n",
    "- Predictor: `dpoulopoulos/qna-predictor:v0.1.0`\n",
    "- Transformer: `dpoulopoulos/qna-transformer:v0.1.0`\n",
    "\n",
    "Once ready, proceed with the next steps.\n",
    "\n",
    "# Creating the Inference Service\n",
    "\n",
    "As before, you need to provide a few variables:\n",
    "\n",
    "1. The custom predictor image you built.\n",
    "1. The custom transfromer image you built.\n",
    "\n",
    "You can leave any field empty to use the image we provide for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b97f1-8e6b-4181-b220-60f61f77e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_IMAGE = (input(\"Enter the name of the predictor image (default: dpoulopoulos/qna-predictor:v0.1.0): \")\n",
    "                   or \"dpoulopoulos/qna-predictor:v0.1.0\")\n",
    "TRANSFORMER_IMAGE = (input(\"Enter the name of the transformer image (default: dpoulopoulos/qna-transformer:v0.1.0): \")\n",
    "                     or \"dpoulopoulos/qna-transformer:v0.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c9d-72e4-491a-ad99-f6f533f0ef94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isvc = \"\"\"\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: llm\n",
    "spec:\n",
    "  predictor:\n",
    "    timeout: 600\n",
    "    containers:\n",
    "      - name: kserve-container\n",
    "        image: {0}\n",
    "        imagePullPolicy: Always\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"8Gi\"\n",
    "            cpu: \"1000m\"\n",
    "          limits:\n",
    "            memory: \"8Gi\"\n",
    "            cpu: \"1000m\"\n",
    "  transformer:\n",
    "    timeout: 600\n",
    "    containers:\n",
    "      - image: {1}\n",
    "        imagePullPolicy: Always\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "        name: kserve-container\n",
    "        args: [\"--use_ssl\"]\n",
    "\"\"\".format(PREDICTOR_IMAGE,\n",
    "           TRANSFORMER_IMAGE)\n",
    "\n",
    "with open(\"llm-isvc.yaml\", \"w\") as f:\n",
    "    f.write(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631d06-ea76-4159-a08b-57e76850ff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"llm-isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20235af7-7b47-4b68-8f6d-f0b01b0c23e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations on completing this crucial step in this tutorial series! You've successfully built a Large Language\n",
    "Model (LLM) Inference Service, and you've learned about the role of a transformer in enriching user queries with\n",
    "relevant context from our documents. Together with the Vector Store Inference Service, these components form the\n",
    "backbone of your question-answering application.\n",
    "\n",
    "However, the journey doesn't stop here. The next and final step is to test the LLM Inference Service, ensuring that it's\n",
    "working as expected and delivering accurate responses. This will help you gain confidence in your setup and prepare you\n",
    "for real-world applications. In the next Notebook, you invoke the LLM Inference Service. You see how to construct\n",
    "suitable requests, communicate with the service, and interpret the responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question-answering",
   "language": "python",
   "name": "question-answering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
