{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f891729d-9030-4d9e-a7be-e81386ae820f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating a Large Language Model Inference Service\n",
    "\n",
    "Welcome to the fourth part of the tutorial series on building a question-answering application over a corpus of private\n",
    "documents using Large Language Models (LLMs). In the previous Notebooks, you journeyed through the processes of creating\n",
    "vector embeddings of our documents, setting up a Vector Store Inference Service (ISVC), and testing its performance.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/llm.jpg\" alt=\"llm\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "      Photo by <a href=\"https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Google DeepMind</a> on <a href=\"https://unsplash.com/photos/LaKwLAmcnBc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "Now, you're moving towards the next crucial step: creating an ISVC for the LLM. This ISVC is the centerpiece of the\n",
    "question-answering system, working in tandem with the Vector Store ISVC to deliver comprehensive and accurate answers to\n",
    "user queries.\n",
    "\n",
    "In this Notebook, you set up this LLM ISVC. You learn how to build a Docker image for the custom predictor, the\n",
    "role of the transformer component, define a KServe ISVC YAML file, and deploy the service. By the end of this Notebook,\n",
    "you'll have a fully functioning LLM ISVC that can accept user queries, interact with the Vector Store, and provide\n",
    "insightful responses.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Architecture](#architecture)\n",
    "1. [Creating the Inference Service](#creating-the-inference-service)\n",
    "1. [Conclusion and Next Steps](#conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d427c712-aea0-4251-b0f3-aa9e88b42447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running behind a proxy, set set your proxy URLs below.\n",
    "# DO NOT edit the \"NO_PROXY\" variable unless you know what you're doing!\n",
    "os.environ[\"HTTP_PROXY\"] = \"\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"\"\n",
    "os.environ[\"NO_PROXY\"] = \"10.96.0.0/12, .local\"  # DO NOT edit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0cc08-1c00-49cc-85bd-b792383b26e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Architecture\n",
    "\n",
    "In this setup, an additional component, called a \"transformer\", plays a pivotal role in processing user queries and\n",
    "integrating the Vector Store ISVC with the LLM ISVC. The transformer's role is to intercept the user's request, extract\n",
    "the necessary information, and then communicate with the Vector Store ISVC to retrieve the relevant context. The\n",
    "transformer then takes the response of the Vector Store ISVC (i.e., the context), combines it with the user's query, and\n",
    "forwards the enriched prompt to the LLM predictor.\n",
    "\n",
    "Here's a detailed look at the process:\n",
    "\n",
    "1. Intercepting the User's Request: The transformer acts as a gateway between the user and the LLM ISVC. When a user\n",
    "   sends a query, it first reaches the transformer. The transformer extracts the query from the request.\n",
    "1. Communicating with the Vector Store ISVC: The transformer then takes the user's query and sends a POST request to the\n",
    "   Vector Store ISVC including the user's query in the payload, just like you did in the previous Notebook.\n",
    "1. Receiving and Processing the Context: The Vector Store ISVC responds by sending back the relevant context.\n",
    "1. Combining the Context with the User's Query: The transformer then combines the received context with the user's\n",
    "   original query using a prompt template. This creates an enriched prompt that contains both the user's original\n",
    "   question and the relevant context from our documents.\n",
    "1. Forwarding the Enriched Query to the LLM Predictor: Finally, the transformer forwards this enriched query to the LLM\n",
    "   predictor. The predictor then processes this query and generates a response, which is sent back to the transformer.\n",
    "   Steps 2 through 5 are transparent to the user.\n",
    "1. The transformer returns the response to the user.\n",
    "\n",
    "As such, you should build two custom Docker images at this point: one for the predictor and one for the transformer. The\n",
    "source code and the Dockerfiles are provided in the corresponding folders: `dockerfiles/llm` and\n",
    "`dockerfiles/transformer`. For your convenience, you can use the images we have pre-built for you:\n",
    "\n",
    "- Predictor: `marketplace.us1.greenlake-hpe.com/ezmeral/ezkf/qna-llm:v1.3.0-e658264`\n",
    "- Transformer: `marketplace.us1.greenlake-hpe.com/ezmeral/ezkf/qna-transformer:v1.3.0-f7315d3`\n",
    "\n",
    "Once ready, proceed with the next steps.\n",
    "\n",
    "# Creating the Inference Service\n",
    "\n",
    "As before, you need to provide a few variables:\n",
    "\n",
    "1. The custom predictor image you built.\n",
    "1. The custom transfromer image you built.\n",
    "\n",
    "You can leave any field empty to use the image we provide for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6a66c-ce92-40c5-a70a-d0cb2e413d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>KServe Images</h2>\")\n",
    "display(heading)\n",
    "\n",
    "predictor_image_widget = widgets.Text(\n",
    "    description=\"Predictor Image Name:\",\n",
    "    placeholder=\"marketplace.us1.greenlake-hpe.com/ezmeral/ezkf/qna-llm:v1.3.0-e658264\",\n",
    "    layout=widgets.Layout(width='30%'))\n",
    "transformer_image_widget = widgets.Text(\n",
    "    description=\"Transformer Image Name:\",\n",
    "    placeholder=\"marketplace.us1.greenlake-hpe.com/ezmeral/ezkf/qna-transformer:v1.3.0-f7315d3\",\n",
    "    layout=widgets.Layout(width='30%'))\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "success_message = widgets.Output()\n",
    "\n",
    "predictor_image = None\n",
    "transformer_image = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global predictor_image, transformer_image\n",
    "    predictor_image = predictor_image_widget.value\n",
    "    transformer_image = transformer_image_widget.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        if not predictor_image:\n",
    "            predictor_image = \"marketplace.us1.greenlake-hpe.com/ezmeral/ezkf/qna-llm:v1.3.0-e658264\"\n",
    "        print(f\"The name of the predictor image will be: '{predictor_image}'\")\n",
    "        if not transformer_image:\n",
    "            transformer_image = \"marketplace.us1.greenlake-hpe.com/ezmeral/ezkf/qna-transformer:v1.3.0-f7315d3\"\n",
    "        print(f\"The name of the transformer image will be: '{predictor_image}'\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(predictor_image_widget, transformer_image_widget, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c9d-72e4-491a-ad99-f6f533f0ef94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isvc = \"\"\"\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: llm\n",
    "spec:\n",
    "  predictor:\n",
    "    timeout: 600\n",
    "    containers:\n",
    "      - name: kserve-container\n",
    "        image: {0}\n",
    "        imagePullPolicy: Always\n",
    "        # If you are running behind a proxy, uncomment the following lines and replace the values with your proxy URLs.\n",
    "        env:\n",
    "        - name: HTTP_PROXY\n",
    "          value: {1}\n",
    "        - name: HTTPS_PROXY\n",
    "          value: {2}\n",
    "        - name: NO_PROXY\n",
    "          value: .local\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"8Gi\"\n",
    "            cpu: \"1000m\"\n",
    "          limits:\n",
    "            memory: \"8Gi\"\n",
    "            cpu: \"1000m\"\n",
    "  transformer:\n",
    "    timeout: 600\n",
    "    containers:\n",
    "      - image: {3}\n",
    "        imagePullPolicy: Always\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "        name: kserve-container\n",
    "        args: [\"--use_ssl\"]\n",
    "\"\"\".format(predictor_image,\n",
    "           os.environ[\"HTTP_PROXY\"],\n",
    "           os.environ[\"HTTPS_PROXY\"],\n",
    "           transformer_image)\n",
    "\n",
    "with open(\"llm-isvc.yaml\", \"w\") as f:\n",
    "    f.write(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631d06-ea76-4159-a08b-57e76850ff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"llm-isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20235af7-7b47-4b68-8f6d-f0b01b0c23e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations on completing this crucial step in this tutorial series! You've successfully built an LLM ISVC, and\n",
    "you've learned about the role of a transformer in enriching user queries with relevant context from our documents.\n",
    "Together with the Vector Store ISVC, these components form the backbone of your question-answering application.\n",
    "\n",
    "However, the journey doesn't stop here. The next and final step is to test the LLM ISVC, ensuring that it's working as\n",
    "expected and delivering accurate responses. This will help you gain confidence in your setup and prepare you for\n",
    "real-world applications. In the next Notebook, you invoke the LLM ISVC. You see how to construct suitable requests,\n",
    "communicate with the service, and interpret the responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
