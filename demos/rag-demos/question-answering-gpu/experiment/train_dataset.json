{
    "queries": {
        "e16be2e7-20c0-40b2-8d23-9d38dae4a265": "What is the purpose of the \"--region\" argument in the \"det deploy aws list\" command? What is the default value if no region is specified?",
        "6c96852b-cdd7-406a-9ed6-743be496cbb7": "What does the \"--profile\" argument represent in the \"det deploy aws list\" command? What is the default profile used if no profile is specified?",
        "fab959d6-a0ce-4812-98b7-2995a93dbb4c": "What is the purpose of YARN resource containers in the context of distributed processing? How are they allocated and utilized by YARN?",
        "4fa1c75e-32db-4769-8fb6-d885b3e0f7ff": "Explain the role of ZooKeeper in distributed applications. How does it provide coordination and organization for these applications?",
        "3b58d492-555b-48f4-8bbc-64cc55a2b072": "What is the purpose of the Determined AMIs mentioned in the context? How are they used in the installation process?",
        "3b844b41-5417-4981-9236-3c0f76578a43": "Can you explain the difference between installing Determined for the first time and upgrading an existing installation? What steps should be followed for each scenario?",
        "3e70d613-0cc4-4876-97f6-2f5847c7b79c": "How does the train_batch() method in Determined differ from the traditional training loop in native PyTorch?",
        "5b2b6a36-606a-4db5-81f8-f9b69e7147d0": "What are the PyTorch functions that need to be converted to Determined's equivalents in the train_batch() method?",
        "ecd07a30-3860-42b5-bfc5-6450ab4c9494": "What is the purpose of Feast in HPE Ezmeral Unified Analytics Software? How does it contribute to the ride-sharing driver satisfaction model?",
        "0fc6051c-d5a3-481a-895c-54d71592e3a4": "Describe the steps involved in the Feast Ride Sharing Use Case workflow. How does it generate training data and perform online model inference?",
        "bd8a4e99-73e5-430c-9b8e-c5ea4c03e68e": "What are the fields that can be configured for the determined-container in a pod spec?",
        "faeb67b4-cbfc-467d-bf05-2fd011b1f301": "How can you configure a Pachyderm notebook plugin to run in det notebook using the determined-container?",
        "2a63a233-015f-405f-9d91-f606237809a8": "What are the required connection parameters for connecting to a Snowflake database? Provide a brief description of each parameter and its default value.",
        "4130260e-9a43-49b0-9c68-e21a2306fb7d": "How can you enable caching while querying in Snowflake? Is it an optional or required connection parameter?",
        "1c9835b8-d91d-4883-9944-433542653b76": "How can the shell commands be used to configure Docker in the Determined agent container? Provide an example of how this feature can be utilized.",
        "6383976e-5a63-4268-86be-c20c89f45490": "What is the default value for the shell commands that are run when the Determined agent container is started?",
        "f24e6311-fa54-4097-a645-59905be4193c": "What is the role of a data-fabric gateway in a cluster? How does it facilitate communication between source and destination clusters?",
        "234f82cd-a0bd-4e23-b9fb-310ed7776fcf": "Explain the concept of a global namespace in the context of HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "28ba0fdc-b55d-4839-955b-912a0829406e": "How does the horovod launcher simplify the configuration of workers for a trial in Determined?",
        "73ed4d49-91fc-43ff-bfa5-a206c77c1d56": "Can you provide an example of how to pass arguments directly to horovodrun using the horovod launcher in Determined?",
        "bd8b6919-a525-4f9c-99b5-e66753909a9a": "What are some key features of the HPE Machine Learning Development Environment (HPE MLDE) in HPE Ezmeral Unified Analytics Software?",
        "22924aaf-84ab-4727-bdda-820965c0fce2": "How can HPE MLDE support efficient model training and deployment in machine learning workflows?",
        "7d0f3023-66dd-4dec-9c46-c820004cdc1a": "How can OpenTelemetry traces be enabled in Determined?",
        "0f29695a-c7f3-41a7-9fb3-608cb1682775": "What is the default endpoint for sending OpenTelemetry traces in Determined?",
        "71b3f015-e118-4cb9-9558-b7630bb7dba1": "How can you enable or disable fabric auditing in HPE Ezmeral Data Fabric? Provide the step-by-step procedure.",
        "aa104ddc-bd3a-42d6-85e7-84ea3ec63fa3": "What is the purpose of enabling fabric auditing in Data Fabric? Explain the benefits and outcomes of enabling auditing for fabric administration operations.",
        "ea0f0272-3b62-48c3-96fb-af1e64902fdd": "What is the purpose of running the first cell of the financial_time_series_example.ipynb file in the Financial-Time-Series folder?",
        "1fe80530-e24e-4cdc-9174-9c4ba24d0243": "How can you view the Spark application after triggering the DAG in Airflow?",
        "f175ffc1-9a42-4eeb-99e6-21894fb8e1bd": "What components are included in a checkpoint for a machine learning model?",
        "6ead00b9-8fc3-40a5-9b0b-35e459360644": "How are TensorFlow Keras trials checkpointed and saved?",
        "08e44157-ac3d-4948-83f4-2f04db787b15": "How can you customize a trial in the QA Beam Search model?",
        "15f01977-9ace-4490-ac4d-4187c0c7d4b9": "As a teacher/professor, how would you ensure diversity in the questions for an upcoming quiz/examination?",
        "f034384d-4399-489c-924f-0db331c9e041": "How is the AWS secret key used in the AWS environment?",
        "bd77dc77-b13c-4445-a3b8-c762a1c86cc5": "What is the significance of providing the AWS secret key for authentication and authorization purposes?",
        "2d1a4239-85df-4d2e-8cd9-b8668acff4f6": "What is the purpose of the \"Feast Ride Sharing Use Case\" tutorial in HPE Ezmeral Unified Analytics Software?",
        "ae456eab-d8ce-4db4-9271-0e8c2ad6b2ea": "How can data engineers use HPE Ezmeral Unified Analytics Software to transform and transport data into usable formats for data consumers?",
        "288370cc-09cf-4908-a967-55107882d678": "How does the signing_key configuration setting in webhooks ensure the security of outgoing webhooks?",
        "64aad730-ae3c-4799-b6e2-b44e6c3e58fc": "How does the base_url configuration setting in webhooks help in generating hyperlinks for users accessing Determined?",
        "24df5c47-1c0c-4687-969e-18047f0623f8": "What is the purpose of the Python SDK Reference in the context of this API?",
        "b306ad27-5672-4270-863d-05f201f5e73f": "How can the Python SDK Reference be used to enhance the understanding and usage of the API's checkpoints?",
        "8c1734fe-d81a-49ad-a5a7-ac4c9180bc4f": "What is the role of a data-fabric gateway in a cluster? How does it facilitate communication between source and destination clusters?",
        "30932457-aaa9-4ad5-ab8a-6f1f339c5b50": "Explain the concept of a global namespace in the HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "1c01b111-0548-4110-a985-7ea44b23c55e": "How can a startup-hook.sh file be used in a Docker container startup? What are some possible tasks that can be performed using a startup hook?",
        "6c615213-1eaf-4536-b7d6-1ea350ffc46f": "What is the potential impact of including expensive or long-running operations in a startup hook? How can this affect the performance of the workload?",
        "880e9929-2b91-450c-94ec-03103236c4f0": "What is the role of the data-fabric user in a cluster? How does it differ from other users in terms of privileges and control?",
        "3e560e71-df91-4bc7-bed8-72cecf70ddd9": "Explain the concept of the global namespace in the HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "3e715607-003b-49e3-9850-461bdf284d9e": "What is the purpose of the data-fabric user in a cluster? How does it differ from other users in terms of privileges and control?",
        "3b92a7ca-3038-4747-8b17-f5c1d0137a30": "Explain the concept of a global namespace in the HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "ad8afcdb-6d81-4776-9ff6-150bea48a693": "How does the Determined helm chart allow users to set default pod specs for CPU and GPU tasks in a Kubernetes cluster?",
        "fd7a9701-84b0-468d-b65e-888df67b3c4f": "Where can users define default pod specs for CPU and GPU tasks in the Determined helm chart?",
        "a54459dd-5eae-4a63-b9da-a3bf8502366e": "How can you retrieve a list of models registered in Determined using the Python SDK?",
        "f12e45d1-cb37-48ff-9d00-a0fcda45a65a": "What are the sorting and ordering options available when retrieving models in Determined using the Python SDK?",
        "c23ee1be-6c43-4173-9811-9486f8fdf570": "What is the purpose of subclassing the SearchMethod class in Determined's custom hyperparameter tuning algorithm?",
        "46c78383-4249-4d13-9016-4adf7fb4e939": "How can fault tolerance be achieved in a custom hyperparameter tuning algorithm in Determined?",
        "dc557c6a-5a6e-40c6-a687-21fa975a3aa7": "What is the purpose of this tutorial? How does it relate to the PyTorch MNIST example?",
        "6975b8a4-534e-4faf-a17c-b3f1030833be": "Can you explain the process of porting an existing PyTorch model to Determined? How does it differ from training the model in PyTorch directly?",
        "54d4ead9-2cca-456f-adab-933d72b69366": "What is the purpose of configuring TLS encryption in a Determined cluster? How can it be configured to terminate inside a load-balancer or inside the Determined master itself?",
        "3b9b02f9-7f1f-4416-bd02-e49e8f2420d8": "How can TLS termination be achieved in a Determined cluster? Explain the options of TLS termination in a load-balancer and in the Determined master, and the steps required to configure each option.",
        "3e1ea7da-002c-469e-bd00-1b5f4427e91e": "What is the main requirement for enabling Determined to submit jobs to a Slurm cluster?",
        "c1b67de6-05c8-4ee8-87be-bd848d03eb69": "What is the difference between Determined Enterprise Edition and the regular version in terms of job submission to a Slurm cluster?",
        "15f3c3ff-ba86-4b54-ba08-ee70b5341c40": "How does whylogs integrate with MLflow in HPE Ezmeral Unified Analytics Software to analyze data quality throughout the machine learning lifecycle?",
        "ca675616-3a3c-40f9-8fc6-7561beb0c658": "What are the steps involved in using whylogs with MLflow in HPE Ezmeral Unified Analytics Software, and how can the data quality metrics be analyzed using whylogs output?",
        "0b99a4f9-a8f6-4613-8b12-610a964450ca": "What is the purpose of setting the network interface for the Determined agent instances? How does it affect the functionality of the agents?",
        "c0bea6b7-0db0-49fd-bef5-1d15459d87de": "How can the use of public IP addresses for the Determined agents impact the overall performance and security of the system?",
        "62af850a-e607-4e40-a34c-7fc8b389b59f": "What is the purpose of adding an end of training callback to EstimatorTrial in the given context?",
        "e17166f5-8687-4fb0-a753-baf4ad2c6d77": "Can you explain the significance of the release date mentioned in the context information?",
        "462d7c21-4269-494e-be28-8925e1ac8efd": "How can advanced users customize the master settings in the det deploy gcp tool?",
        "83fd3d84-d567-4654-826b-7d891e0f7910": "What is the purpose of the master.yaml templating feature in det deploy gcp?",
        "2320fa27-c1cc-415f-9bc3-39ca803d71cb": "What is the difference between the standard data parallel model engine and the pipeline parallel engine available in DeepSpeed?",
        "e00429d1-b806-4296-a6f7-8b26e7a7ad9c": "How do the training and evaluation routines for the DeepSpeedTrial differ from those of the PyTorchTrial?",
        "a58b015e-c8ce-4679-927b-0d40e8202b3e": "How does the new job queue feature in the release allow users to modify job parameters without canceling and resubmitting them?",
        "b631d8db-0e07-41fa-8ff4-fb5ca82e2ae5": "What are the breaking changes in the API and how do they affect the endpoints related to experiments and trials?",
        "e4e92c33-7014-4b4d-8143-d141b4257e72": "How does YAML handle unquoted strings? Provide an example from the given context information.",
        "0c52db23-2f05-4de2-84ee-060cd70ff7ef": "In the given YAML block, identify two examples where a small edit to a value changes its type.",
        "53c184cf-e6a5-4d81-92e6-70fcd732d187": "What is the purpose of setting a volume quota in HPE Ezmeral Data Fabric? How does it help in managing data storage?",
        "240cf2a1-b0dc-4f73-8199-7b6a228507e3": "Describe the steps involved in setting a volume quota for a specific volume in HPE Ezmeral Data Fabric. What happens if the data on the volume exceeds the set quota?",
        "6c84bda6-2517-41c8-8425-848161d7a245": "What are the limitations of using Spark OSS images in HPE Ezmeral Unified Analytics Software?",
        "2c8b0cdb-65c0-4538-a5b3-7414579a3e92": "How can you use Spark OSS images to submit Spark applications in HPE Ezmeral Unified Analytics Software?",
        "53da31de-6c15-4ce0-b3e1-20d04540dc84": "What is the purpose of an access control list (ACL) in the MapR Converged Data Platform? How does it determine user permissions for specific actions on an object?",
        "39af0ef9-d661-49db-af64-c1eb6adfdf8f": "Explain the concept of a data container in a data-fabric cluster. What are the different types of data containers and how do they function in terms of replication roles?",
        "01126020-c4b9-4408-945f-0fa8b85ab68b": "What is re-replication in the context of MapR Converged Data Platform? Provide examples of situations that may trigger re-replication.",
        "99f3f504-d033-4bb5-bb23-eaaa3f130e12": "Explain the concept of an access control list (ACL) in the context of HPE Ezmeral Data Fabric. How does an ACL specify user permissions and what objects can it be attached to?",
        "3217dcaf-fbd2-4693-b29f-39df350602a6": "What is the purpose of configuring the IdP to allow users to SSO to Determined?",
        "4337254e-a7a3-4e6d-962c-532bc4e5a79b": "How should the callback URL be specified when configuring the IdP for SSO to Determined?",
        "a88e3301-ca20-4040-8bac-04cff4e97f23": "How is GPU usage metered in HPE Ezmeral Unified Analytics Software?",
        "bd0f8ae3-9709-4675-bd20-37d1e3faec2f": "What is the formula used to calculate the total GPU utilization for billing in HPE Ezmeral Unified Analytics Software?",
        "9696c869-d720-4b05-b5a9-92c2bbb5d226": "What is the purpose of the minimum replication factor in a data-fabric cluster? How does it affect the cluster's operation?",
        "9ae3b7e3-4cec-4c95-bde3-42c56dc90c04": "Explain the concept of an object store in the HPE Ezmeral Data Fabric. How does it leverage the capabilities of the file system for performance and scalability?",
        "7a1e3d61-bf5f-4cfa-8a4d-1cb644a2d66d": "What are the different methods available for installing Determined on a single machine?",
        "5ee66a9d-bcae-444d-be02-7e25c4dc9be5": "Can you explain the steps to install Determined using Homebrew on macOS?",
        "801bf04d-f3c5-4cd9-960c-4ee4f64dbe00": "How does the WebUI visualization help in understanding historical cluster usage? Provide examples of the information that can be obtained from these visualizations.",
        "f6efe481-97fb-4f03-9aec-5114c30a8b00": "Explain the importance of having a quick snapshot of historical cluster usage through WebUI visualizations. How can these visualizations assist in making informed decisions related to cluster management and optimization?",
        "f9898e6e-e56d-4e32-a068-729ad33d33c6": "What is the purpose of defining the resources section in an experiment? How does it contribute to the overall experiment setup?",
        "a541dd02-1b39-4cf4-90cb-7d7d7918f2df": "Can you provide examples of the types of resources that might be included in the resources section of an experiment? How do these resources impact the experimental process?",
        "5647b8bb-3a6d-4238-ba7f-ef0d2e42dba5": "How can you activate and register a new fabric in the HPE Ezmeral Data Fabric to enable automated billing?",
        "43abbdc0-678c-4490-93bb-e32008dc8e30": "What are the important dates associated with the activation code in an air-gapped environment, and how can you view them in the Data Fabric UI?",
        "1c20d7d0-0dab-4d0a-bd59-22de5a536b14": "How can you obtain a consumption-based license for the HPE Ezmeral Data Fabric from the My HPE Software Center?",
        "d1d00485-e1c2-4561-83cd-716c33d3c7c8": "What are the steps involved in activating and registering a new fabric in order to enable automated billing for the HPE Ezmeral Data Fabric?",
        "d16b52cd-e5da-4031-83e5-98e40e1f99a0": "How does the use of a certificate file enhance the security of a TLS server?",
        "63c0d971-2d6b-40ae-8440-04945df7809f": "What are the key components of a TLS certificate file and how do they contribute to the establishment of a secure connection?",
        "4f150953-a469-44fa-8c6a-872a7ecf5d1e": "What is the purpose of a data-access gateway in a MapR Converged Data Platform environment? How does it facilitate communication between client applications and the data-fabric cluster?",
        "5d5dd7a9-afb6-4724-9723-26149aa79cbe": "Explain the concept of an access control list (ACL) in the context of HPE Ezmeral Data Fabric. How does an ACL define permissions for users or system processes to perform specific actions on an object?",
        "1aa84708-3d2e-419e-a630-d75f9526fd3d": "What is the purpose of a validation metric in evaluating the performance of a hyperparameter configuration?",
        "7fcb609b-9898-4213-abc4-1cc16ee1e80e": "Can you provide an example of a validation metric that is commonly used to evaluate the performance of hyperparameter configurations?",
        "7de50878-698c-4607-a35d-a248dc05fe96": "What are some examples of distributed file systems available in cloud environments and on-premise deployments?",
        "2112d612-b7ab-440e-8c66-f6f0a4c91f71": "How can you ensure that a distributed file system is accessible to each trial container in a Determined cluster?",
        "597cf057-ff85-4235-b220-2ea316f74043": "How does the new release of Determined's WebUI enhance the management of experiment state changes?",
        "7f7381bc-6a6a-422f-a432-09a532736d0d": "What new capabilities does the Python SDK offer in terms of reading logs and accessing the first trial created for an experiment?",
        "4fff5728-045f-4d39-bed5-6e67a9d68822": "What is the purpose of the data-fabric user in the context of Hewlett Packard Enterprise Development LP?",
        "05ca946b-f615-4675-b199-d5727f525702": "How can the data-fabric user be utilized to support various aspects of the organization, such as partners, support, dev-hub, community, training, ALA, and privacy policy?",
        "4321fd16-c50f-4483-8a8d-1a36e8bb7c92": "What are the limitations of the default logging backend and when is it recommended to migrate to Elasticsearch?",
        "067a5479-77fb-491b-bceb-f505a924647e": "How can the default logging backend be optimized to increase log ingestion speed and what are the potential bottlenecks in the system?",
        "46f28aac-b7d6-4202-bd31-d9456a1d1c51": "What is the purpose of volume mirroring in a storage system? How does it work?",
        "2f814be1-7be3-4c17-abf1-c0f9273c4957": "Explain the steps involved in scheduling volume mirroring and how it can be beneficial for data management in a storage environment.",
        "bf2845bb-d53c-42ee-a7a6-336cc8be6c0a": "Which libraries or packages in the given context information have the MIT license?",
        "d8ae1d66-cc0f-4917-8e51-1205d0349ed4": "What is the repository URL for the \"grommet\" library/package with version 2.31.0?",
        "32e5cb72-d670-4efc-825a-222c6a7c0c63": "What is the purpose of the replication factor in a data-fabric cluster? How does it affect the write operations in the cluster?",
        "e6e7855c-2d76-45eb-b669-5a5b1b10271e": "Explain the role of the ResourceManager (RM) in a YARN cluster. How does it manage cluster resources and schedule applications?",
        "da0db22e-8806-4e39-89e5-bd0100adb41b": "What is the purpose of an access control list (ACL) in the MapR Converged Data Platform? How does it determine user permissions for specific actions on an object?",
        "f8777fe6-e6c3-4845-9a50-fbe382eaafc6": "Explain the concept of a data container in the MapR Converged Data Platform. What are the different types of data containers and their roles in the replication process?",
        "acda3487-2468-4a99-9572-d0e178dc38e8": "What are the expected file modifications for an RPM-based installation of the launcher package? Provide examples of the modified files and their corresponding status codes.",
        "7d4a6ceb-6057-4826-b775-b03f1996f629": "How can you verify the integrity of the launcher package installation on a Debian distribution? Explain the command to run and provide an example of the expected output.",
        "66867757-5458-433c-a910-51782d0fb5cc": "How does Singularity differ from Docker in terms of resource sharing and conflicts with other programs running on the cluster?",
        "0e00fd6f-a761-4d16-b014-3d236a50f5f7": "What are some potential issues with the default behavior of Singularity in sharing /tmp and /dev/shm from the host compute node?",
        "ef33ccf9-76a6-485f-a10c-ef9edff714b4": "How can you modify the job queue in the CLI using the \"det job update\" command?",
        "6eb31920-30e7-42ba-9605-d7a4fcfe0cff": "Can you provide an example of updating a job's priority using the \"det job update\" command?",
        "f1e58ed5-ebd2-4ac5-a0f3-fd4d02928a74": "How can you update multiple jobs in batch using the \"det job update-batch\" command?",
        "127e7b9e-0e45-44b2-9131-34083dae08a3": "What is the syntax for updating a job's resource pool using the \"det job update\" command?",
        "f06b3a5f-bdea-4561-ae7f-fdad794c5248": "How can you update the order of jobs in the queue using the \"det job update\" command?",
        "c653e2ec-abe3-4398-8e46-446c069ca397": "What information is displayed in the job list after updating a job's order using the \"det job update\" command?",
        "61460b54-5764-43e8-af29-b23dbec6a3b3": "Can you provide an example of updating a job's priority in batch using the \"det job update-batch\" command?",
        "8e0c7f9d-ee21-4680-9c3c-39a8967186aa": "How can you view the job list after updating multiple jobs using the \"det job update-batch\" command?",
        "7a7c3676-bdb5-4720-8672-7865b724acee": "What is the purpose of the \"det job update --help\" command?",
        "02346efd-b581-44b9-87cf-f275bcb36d56": "How can you view the current job queue in the CLI?",
        "b546f2ed-e745-45fa-bad8-5f68adc3a609": "What is the purpose of the connection string in the context of Azure Blob Storage service account?",
        "fb343c3e-c18f-41c5-8eba-50889e998aee": "How does the connection string for the Azure Blob Storage service account contribute to the overall functionality of the storage service?",
        "638146eb-cb70-44b1-bddb-074c49162a92": "How does pausing a trial in an experiment benefit the scheduler and free up resources for other tasks?",
        "207f1e89-371c-4d7f-bf40-efa7da02a41c": "What happens to the training progress and checkpoints when a trial is manually paused and then resumed in an experiment?",
        "0212ba69-36ee-4933-8cab-5c0925ea6d71": "How can an administrator configure, delete, or update imported tools and frameworks in HPE Ezmeral Unified Analytics Software?",
        "b5d9109c-2c99-4c79-86a4-7675ec5f0093": "What are the steps to import applications in HPE Ezmeral Unified Analytics Software?",
        "4ccae256-f934-4ebe-b4cf-13e110b6087d": "What is the purpose of the replication factor in a data-fabric cluster? How does it affect the write operations in the cluster?",
        "d6919cfb-7cec-4cd3-84fe-e1cee994226a": "Explain the role of the ResourceManager (RM) in a YARN cluster. How does it manage cluster resources and schedule applications?",
        "73bcf15b-fd27-43d4-8238-6d9000a66b21": "How does the Determined master and the Determined agents work together in the same project?",
        "9dc1da74-4651-42bd-848c-99efba882470": "What is the significance of having the Determined master and the Determined agents in the same project?",
        "d6ce35b1-6951-49a6-afd8-99f56a8ae03b": "How can dynamic agents be run on GCP? Provide a brief explanation of the process.",
        "914abb4b-3bba-43c8-ac0f-006065ac9629": "As a teacher/professor, how would you ensure that the questions for the quiz/examination are diverse in nature, specifically in the context of running dynamic agents on GCP?",
        "7dccf381-22d8-489e-a60c-60ec2bc9c319": "What is the purpose of implementing a trial class that inherits TFKerasTrial in the Keras API?",
        "da4def65-b9ea-459a-819d-c0fc4b5e0b56": "Can you provide examples of trial definitions that can be used to learn about the Keras API?",
        "7bc0c0d8-9efc-444d-b757-57ac8ed92d16": "How can you optimize DeepSpeed parameters using Determined's DeepSpeed Autotune with the HuggingFace Trainer and DetCallback?",
        "6f092b35-50af-4ce0-ac61-0267ca44cd60": "What additional steps are required when using dsat to perform a search over different batch sizes and HuggingFace expects parameters to be specified as command-line arguments?",
        "cb24ad5c-dec0-482d-85e3-db7b1960fdfa": "How can you specify a specific GPU type when using Slurm's --gpus or --gres option?",
        "072b54c6-19df-49e8-ba51-29beed867b64": "What is the default behavior when selecting GPUs in Slurm without specifying a GPU type?",
        "693b8470-6bcc-4ed4-95ee-7b7677fee3c6": "What is the default backend used by PyTorch Trial?",
        "7463c6eb-2932-4135-9c42-80a81407f4c4": "How can you choose to use torch.distributed and DistributedDataParallel as your distributed backend in PyTorch?",
        "959e6118-9309-4612-9d05-de28016ccc3e": "How does the S3 bucket name impact the storage and retrieval of data?",
        "7c56dcc5-3be1-4dbf-9fa7-00f6a7eeffc0": "What factors should be considered when choosing an appropriate S3 bucket name?",
        "5daad02e-b1ba-489a-af90-9700409088a2": "Which licenses are used by the \"styled-components\" library and the \"grommet-theme-hpe\" library?",
        "54a93722-7e8a-4ed5-8320-13fa443a542b": "What is the repository URL for the \"react-immutable-proptypes\" library?",
        "7634ab24-b917-4143-86ec-76748202f77f": "How can you assign a role for a user globally using the command line interface?",
        "43adb5c1-2135-448d-a829-b50ca28bcdd2": "What is the syntax for unassigning a role for a group on a particular workspace using the det rbac command?",
        "06675714-40e7-415f-9a64-c5f4eb2b1b56": "How does the initialization of weights from the most recent checkpoint of a trial ID contribute to the success of an experiment? Provide an example to support your answer.",
        "5429bf1e-02fa-4c36-ba72-fa3a7417e1f2": "As a teacher, how would you ensure that the questions you set for the quiz/examination are diverse in nature, considering the context information provided? Explain your approach.",
        "282f4043-6217-488a-b7fd-857c83b1806c": "How can you use HPE-curated Spark images when creating Spark interactive sessions?",
        "71b44de1-cbc5-47c8-9bd5-29e629a5d290": "What are the options available for specifying the Spark image when using Spark magic to create Livy sessions?",
        "12cd3fd7-84cc-47ad-bba6-078ea7403403": "How can you retrieve the current user's permission list using the Det RBAC command?",
        "bad87a97-c787-4090-9384-a45c40c7111f": "What command should you use to list all existing roles and their permissions in the Det RBAC system?",
        "ff08c482-0709-40c1-8694-dc89c1b3ba29": "How does the new \"Notes\" tab in the WebUI enhance the functionality of experiment pages in Determined?",
        "944740fb-ff00-4330-b639-37a515aa02cd": "What improvements have been made to the documentation in order to make it more user-friendly and reduce the need for users to jump between different documents?",
        "eec153aa-6058-47e9-84b4-a4a2abad8b54": "What is the purpose of the first kubectl command \"kubectl get pods -l=determined\" mentioned in the context information?",
        "86a54a7a-763d-43aa-bcc0-81bf783cdfd1": "When would a user need to run the second kubectl command \"kubectl get pods --no-headers=true -l=determined | awk '{print $1}' | xargs kubectl delete pod\"?",
        "81b62d9d-76b2-46c0-98cf-0258cfbd5b65": "What is the purpose of the minimum replication factor in a data-fabric cluster? How does it affect the cluster's operation and what happens when the replication factor falls below this minimum?",
        "7bf4c2a6-d83c-4dcf-a2ca-98d11221893b": "Explain the concept of an object store in the HPE Ezmeral Data Fabric. How does it store and manage data, and what advantages does it offer in terms of performance, reliability, and scalability?",
        "4606474e-0e89-47c4-b1fc-b7ff7f9f3f8b": "What is the purpose of the replication factor in a data-fabric cluster? How does it affect the availability of writes in the cluster?",
        "d1cf3889-a01d-4e8d-8a1d-1cfcd2d7b03e": "Explain the role of the ResourceManager (RM) in a YARN cluster. How does it manage cluster resources and schedule applications?",
        "cefe0b1b-28e8-4fca-8e2b-6e402bcdbf5c": "How can system metrics, such as GPU utilization and network throughput, be useful in optimizing software performance for hardware resources?",
        "8e7d07d8-1b15-4244-8f22-285f7224f1ca": "Why is it recommended to have only a single experiment per agent when analyzing system metrics in a distributed training setup?",
        "d7210808-2790-40e3-96f4-683d91ac9c72": "What is the purpose of updating the path for the driver_stats.parquet file in the definitions.py file and the ride-sharing-example.ipynb file?",
        "59c8766e-d1ff-4390-abd5-57b179355d94": "How can you access the Feast web interface to explore the defined feature definitions?",
        "c2b2b54b-565f-4d67-9c6e-fd6210bc14c4": "How can a model be added to the registry in Determined?",
        "7a135a11-deb2-4372-a8f3-aac96efcb9c4": "What are the different methods available to add a model to the registry in Determined?",
        "f3bf5f5a-c553-4049-a4e7-d0616ac97d7c": "How is the behavior of an experiment configured in Determined?",
        "33e7be56-6859-43b2-9d32-a06df06dc186": "What is the purpose of passing a configuration file as a command-line argument when creating an experiment with the Determined CLI?",
        "1c03180c-f4af-4071-bbc3-46e5edb427b1": "How does the Determined master launch Determined agent instances without requiring any installation?",
        "ced7e140-68cb-4b54-9737-1f361a158c01": "What factors does the Determined master consider when dynamically launching Determined agent instances based on the cluster configuration?",
        "b2e6e773-d4f3-4193-a485-f42f4909e3bf": "What was the bug fix related to database migration in Determined version 0.13.0 and why was it important for users to upgrade to version 0.13.1?",
        "017565d7-bca7-4f7e-9f38-bb008f6be195": "How did the bug in TensorBoard affect experiments with old experiment configuration versions and what was the fix for it?",
        "f80af23d-bff1-4230-9ecd-d4b032f1834b": "What is the default port value for the S3 server? Can it be changed?",
        "655ef9f5-19a3-4969-a5c5-2c725ff6ef60": "How can you enable TLS encryption for communication with the S3 object store?",
        "6ba631ee-57a2-4ff9-b857-a6e7d61e8046": "What is the purpose of the Torch Batch Processing API? How does it differ from other APIs?",
        "6b261721-3754-47cb-b38b-f263e3883077": "How can the Torch Batch Processing API be used in a real-world scenario? Provide an example use case.",
        "3b4c92ce-251a-458a-a029-2f2d2eff19f7": "What are the different components of the HPE Ezmeral Data Fabric that require additional license authorizations?",
        "25fa805d-c9c7-4330-9183-ca81244d4b14": "Which storage technologies are supported by HPE Ezmeral Data Fabric for global-namespace support?",
        "65376742-1ec8-4062-957d-11f169975dc0": "How does Data Fabric support IPv6 addresses in terms of hardware compatibility and software enablement?",
        "83473034-6060-4c9d-a5a6-53314b5c3438": "Explain the different terminology related to IPv6 client/server nodes and their implications in Data Fabric communication.",
        "3c08e470-8e32-4a13-b67f-8f506f111b72": "How does Determined handle the interaction with the Kubernetes cluster when running on Kubernetes?",
        "d072cb1e-896d-4d82-93a6-18619b951d56": "What is the difference between priority scheduling in Kubernetes deployments and non-Kubernetes deployments when running Determined?",
        "81f2c6d6-e9fe-42f8-a776-418b3e653108": "How can you identify the number of GPUs available on a node in the PBS workload manager?",
        "5a70d845-a167-4aa3-aadc-c4fc650e0b2c": "What steps should be taken to ensure that GPUs will be available on nodes selected for a job in the HPC workload manager?",
        "cf7fd2f3-b14f-4ef6-a527-f5e94e90e019": "What is the purpose of authentication in API calls to a Determined cluster?",
        "4d8668b6-dcf9-433f-b68c-cc5406e926f7": "How can a Bearer token be obtained for authentication in API calls to a Determined cluster?",
        "cdfb0861-84d4-4a70-bebd-7240bb436c00": "What is the purpose of the replication role balancer in a MapR Converged Data Platform environment? How does it ensure equal distribution of containers among nodes?",
        "66e0712d-eb3e-4874-8be8-ceb705b12753": "Explain the concept of an access control list (ACL) in the context of HPE Ezmeral Data Fabric. How does an ACL specify user permissions for performing actions on an object?",
        "0864413c-f290-4751-9f64-121fa94b3486": "What are the steps to install the Determined master and agent packages on a Debian distribution?",
        "34940a7d-ed9d-48e8-8b23-7c667e8d746f": "How can Docker be installed on each agent machine before running the Determined agent?",
        "eb477fb0-9c57-499f-9caf-306fd0aa1932": "What is the default value for the \"Delta Max Splits Batch Size\" parameter?",
        "058969e5-129f-4b4f-81af-8adff9014a4e": "How does enabling the \"Hive Insert Overwrite Immutable Partitions Enabled\" configuration affect insertion queries?",
        "4ce288cd-5b6c-40cb-8edf-b668cbde5af0": "What are the considerations for importing an as-a-service fabric into a global namespace?",
        "50c2a564-6186-47f6-8b93-6872ee4acd0a": "How can you prepare to import an as-a-service fabric, including steps to stop Keycloak and reset SSO information?",
        "deb55470-c6aa-4af5-ab3b-b36dc5c071d1": "What is the purpose of the download() method in the Checkpoint class?",
        "13e0d35f-5154-43c6-a381-e8bbd13cdedc": "How does the load() method in the Checkpoint class differ depending on whether you are using TensorFlow or PyTorch?",
        "b2db12a0-f3bc-4fdb-b5ee-51d9cf28c7ac": "What is the purpose of an access control expression (ACE) in the HPE Ezmeral Data Fabric platform? How does it define user access to objects stored in the platform?",
        "8c2f3de1-fcae-4691-8a99-9632e79dd4d9": "Explain the concept of an air gap in computer systems and its role in enhancing security. How does it relate to the context of the document?",
        "4882a58d-fdc9-4e6f-b64c-26967346d44c": "What are the potential storage options for storing checkpoints and TensorBoard events in Determined on Kubernetes?",
        "40928ce3-01b2-4245-9958-2c58c9f1e036": "Why is it discouraged to use shared_fs for actual deployments of Determined on Kubernetes?",
        "47d023b2-58cb-4bb8-9aa4-98651ae5d313": "How does DeepSpeedTrial ensure that a total of train_batch_size samples are processed in each training iteration?",
        "bd0128d6-d764-4c45-9d24-d0a130a95509": "What is the purpose of calling disable_auto_grad_accumulation() in the __init__() method of DeepSpeedTrial?",
        "ad8dece1-2399-4935-aea1-76b8f4c58e52": "How is the resource pool represented in the context information for requesting NVIDIA GPUs?",
        "4c9fdfbc-0949-441b-a31d-f2873f69aa5d": "Can the slot type for the resource pool be overridden using partition_overrides?",
        "3131bd54-c273-4978-80f5-ed0e01679b25": "How can Keycloak interface with an external LDAP directory to allow LDAP users access to the Data Fabric UI?",
        "0cbd277f-af73-4d4c-87d5-de4206ab8847": "What is the process for updating the single sign-on (SSO) configuration information using the Data Fabric UI?",
        "d09cb46b-f94e-412d-96a9-28e1a17b8ca6": "What is the purpose of a data node in a MapR Converged Data Platform environment? How does it contribute to the functioning of the cluster?",
        "9acb2529-7f23-42b2-9956-60aa4309a8dc": "Explain the concept of an access control list (ACL) in the context of HPE Ezmeral Data Fabric. How does an ACL specify user permissions and what actions can be performed on an object?",
        "040d392d-8a44-45df-a218-7980b941b1b8": "What is the purpose of having an elastic IP for the master in a production cluster? How does it differ from an ephemeral IP assigned by AWS?",
        "9880fe38-8747-4258-b23d-a245c53aea30": "How can the Determined cluster leverage an existing S3 bucket? What are the necessary permissions required for the bucket to be used by the cluster?",
        "9196e0b8-8898-454b-b440-45b2160c72b1": "What is the definition of \"essential patent claims\" according to the context information? How does it differ from claims that would be infringed only as a consequence of further modification of the contributor version?",
        "83dc43bb-03b1-4c40-afaf-b2ba77c065a5": "Explain the concept of a \"discriminatory\" patent license as mentioned in the context information. Provide an example of a situation where conveying a covered work would be prohibited due to a discriminatory patent license.",
        "36a0fa18-6af3-43f1-90ad-426c268d4f12": "What is the recommended version of the PostgreSQL Docker image to use for this setup?",
        "f3b4eaf3-9dd5-42fb-8270-22e073f8d9d8": "Why might you want to expose port 5432 when starting PostgreSQL via Docker?",
        "de067880-58b3-4091-b85c-d24dba36648c": "What are the different types of hyperparameter data types supported by Determined for automatic hyperparameter tuning?",
        "0fd6ef45-ec89-403a-89f3-08f630bb66ee": "How does the machine learning engineer specify the range of possible values for each hyperparameter in the experiment configuration?",
        "1f276d21-9ce7-489b-bae1-960755e3bb37": "What is the purpose of an access control list (ACL) in the MapR Converged Data Platform? How does it determine user permissions for specific actions on an object?",
        "599b9b62-f31a-4a81-8577-82f93024743b": "Explain the concept of a data container in the MapR Converged Data Platform. What are the different types of data containers and their roles in the replication process?",
        "24ed398e-6cbb-4816-a0f4-8703f1967669": "What are the steps to view table replicas in the HPE Ezmeral Data Fabric UI?",
        "bfad6992-f14a-4095-a1a2-cfea6c014b37": "How can you administer access controls for tables in the HPE Ezmeral Data Fabric?",
        "f2752333-24b2-4073-ad3f-3cca618b8c9b": "What is the purpose of an advisory quota in the MapR Converged Data Platform? How does it work and what happens when the disk usage exceeds the advisory quota?",
        "9723ec76-3487-4fa3-a69d-6ef24ce3f18b": "Explain the concept of an access control list (ACL) in the context of HPE Ezmeral Data Fabric. How does an ACL define user permissions and what actions can be performed on an object?",
        "2131edfc-d7a9-4360-9533-716acff9e3fe": "What are the two options for running the Determined master as a service account with the necessary permissions to manage Compute Engine instances?",
        "d3fe8c4f-dde2-4e86-8a96-08a414dea0b0": "What role does the service account of the Determined master need to have in order for the Determined agent to be associated with a service account?",
        "7dd67a03-9220-40b6-9053-99f3e9b544b0": "What are the steps to expand the cluster in HPE Ezmeral Unified Analytics Software?",
        "14c1121e-f03f-43fb-892b-d240d0793130": "What additional steps are required to configure HPE MLDE for added GPU nodes in the cluster?",
        "25b65994-bed2-4a8a-9a5b-b9bbe00fd1e1": "How does the URL of the IdP relate to the process of sending SAML assertions?",
        "f36e8bc4-57ca-41a9-9e02-3c9e8ea683be": "What is the significance of the IdP sending SAML assertions to a specific URL?",
        "2c179cf6-206c-42d5-9dab-15925f8f65fc": "What are the conditions under which this program is distributed? Does it come with any warranty? Explain.",
        "d8606758-f7fc-41a1-b1c9-a6912c5c7b8f": "What are the differences between the GNU General Public License and the GNU Lesser General Public License? When would it be more appropriate to use the Lesser GPL?",
        "e8bf5f7a-5b60-48fd-a7fa-f7740fb4dee7": "What is the purpose of the evaluate_batch() method in the given context? How does it compute the validation metrics?",
        "bffada8c-2151-4b7a-8c88-b34d3f927c36": "How can the behavior of metric values be customized in the evaluate_batch() method?",
        "6948e2ff-9934-419f-9fdf-7cdcc854106c": "How can you modify the job queue in the Webui?",
        "b0af4d91-951d-4d8f-b6ee-2650823d877d": "What steps should be followed to make changes to a specific job in the job queue?",
        "d2eae39e-2058-4d0a-a26b-0f446a7543e7": "What are the main components and functionalities of the BaseTransformerTrial class in the given context?",
        "582b6fd7-017b-4d6a-afad-469d18840d34": "How does the train_batch method in the BaseTransformerTrial class work?",
        "547f6b26-8b16-4acf-8651-1e4e0fb96887": "What is the role of a data-fabric gateway in a cluster? How does it facilitate communication between source and destination clusters?",
        "9910a987-d3a8-447b-abd6-34ab35363f64": "Explain the concept of a global namespace in the HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "9b9a5eb1-802c-4788-9558-07e40ec73f9f": "What is the purpose of the checkpoint_storage section in the model training process?",
        "7584bd23-c951-4e99-a6d1-dfb5525c5f4d": "How is the name of a checkpoint directory determined in the external storage system?",
        "10478766-f279-4a8f-bf7d-74c6d84e8c17": "What is the purpose of the `train_batch` method in the `MNISTTrial` class? How does it contribute to the overall training process?",
        "c435ca8d-e7b9-4eba-97b1-43879764eff6": "Can you explain the role of the `build_validation_data_loader` method in the `MNISTTrial` class? How does it help in evaluating the model's performance?",
        "b819f1c1-eebc-4518-8532-80c607575e56": "What is the purpose of the Trainer class in the determined.pytorch package?",
        "a3cd2ff9-cdfc-420b-97ae-bad50ea6adac": "How does the fit() method in the Trainer class handle checkpointing and validation steps?",
        "5f0941f2-b8b0-4dfb-a589-0e67611e68ad": "What is the default value for the maximum number of trials that can be worked on simultaneously?",
        "15cfe82e-c4fc-4aa5-a3e2-3aafbe1d746a": "How does setting the value to 0 affect the number of trials that can be worked on simultaneously?",
        "25a2dc05-74db-4852-9877-826c8c9514bf": "What is the purpose of the ModelParallelUnit in DeepSpeedTrial? How does it determine whether a GPU slot should build the data loaders and report metrics?",
        "f85352f4-3a12-4a74-ad53-cd9b17572783": "How can you modify the behavior of the ModelParallelUnit in DeepSpeedTrial to support custom model parallelism? Provide an example of how to pass a custom set_mpu to achieve this.",
        "58e3bb25-6d95-43c0-ac04-5934c44d5e90": "What are the main components of Apache Airflow in HPE Ezmeral Unified Analytics Software and what are their functions?",
        "123350c2-a44b-495f-b140-8a185b247d62": "What are the limitations of Airflow in HPE Ezmeral Unified Analytics Software and how do they impact its usage?",
        "158f2777-4563-49da-a524-b011c8d8cee3": "How does the Azure Blob Storage container name impact the storage and retrieval of data?",
        "d2e18868-c7ef-4bee-91a4-4ac223f9b8dc": "What factors should be considered when choosing a suitable Azure Blob Storage container name?",
        "33357757-946b-444d-8727-12210d44bbc1": "What are the required connection parameters for connecting to a Teradata database using HPE Ezmeral Unified Analytics Software?",
        "4f576b0e-3e70-46be-a4d2-becdaebf9307": "How can you enable caching while querying a Teradata database in HPE Ezmeral Unified Analytics Software?",
        "56874aa4-a45a-4a8a-8bd4-52eec54dc313": "In HPE Ezmeral Unified Analytics Software, what is the purpose of the \"Data Engineering\" feature? How does it help data consumers?",
        "a4e7e28c-158b-4650-8375-385106fbcd00": "How can you submit statements in HPE Ezmeral Unified Analytics Software? Provide step-by-step instructions for submitting statements in Python, R, and Scala.",
        "e48b6bd7-983a-431b-9005-802630a5d2f1": "What are the new features added in the upcoming release of the software? How does the addition of systemd socket activation benefit the master?",
        "008ca114-c7f7-402a-96e8-b3cc22d4fa65": "What improvements have been made in the documentation? How do these improvements address onboarding gaps?",
        "cfd1f96a-d079-490e-9e3e-e20c90d7f0e5": "What is the purpose of a data node in a data-fabric cluster? How does it contribute to the overall functioning of the cluster?",
        "15d7d6f3-c9be-4fbc-b995-eed3aadce732": "Explain the concept of a global namespace in the context of HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "e14e4d57-a674-4414-977d-573fa129bec9": "What is the purpose of the save_experiment_best, save_trial_best, and save_trial_latest parameters in the experiment system?",
        "80309861-89d5-49d1-9024-8adedc85eb50": "How does the system reclaim space after an experiment finishes?",
        "f828eaed-bc10-4547-9dfb-1f8386141c89": "What are the required connection parameters for setting up a Hive Glue Metastore in HPE Ezmeral Unified Analytics Software?",
        "aff9d775-117f-4909-948a-7cb4093ed122": "How can external data sources, such as CSV and Parquet files in S3, be accessed using the Hive connector in HPE Ezmeral Unified Analytics Software?",
        "f3f52cbb-27ee-4cf9-b9bd-b437360d1ada": "How can you load data into PyTorchTrial models? Provide an example of the code.",
        "25ff8a11-73d6-43b6-a7d9-b032acbe39c1": "What are the different formats in which the output of the train_batch() function can be returned? Give examples of each format.",
        "aa6f93d8-3dd4-4d01-98a0-526be6d8d156": "How can configuration templates help reduce redundancy in an organization's Determined configuration files?",
        "62fccb36-85d9-4c08-8639-3c7bc5f7d846": "What is the purpose of merging a configuration file with a template in Determined?",
        "4d25a475-8a9d-4dfb-86a8-52fd8db06786": "What are the default values for the number of cores, core limit, and memory in the driver configuration for a Spark application?",
        "cc34e90d-0e73-411b-8571-26c435644179": "How can you schedule a Spark application to run at a certain time?",
        "bcef19c2-b119-4372-8002-0ebe2a07570a": "What is the default resource pool that an agent will join if no specific resource pool is specified?",
        "75e316de-6ae4-4c0b-b41d-fb30f6245572": "How can an agent determine which resource pool to join if the default resource pool is not suitable for their needs?",
        "a318d046-b67b-4cbc-8ff6-438705bfb886": "What are the valid schemes that can be used in the URL format for the master?",
        "53573885-cafd-4886-a422-dac4999afe80": "How can you dynamically set the host for the master if it is deployed on EC2 or GCP?",
        "6f8b26dc-60d5-4924-af86-55ac13d23156": "How does the Network File System (NFS) protocol enable users on client computers to access files over a network?",
        "b27689be-a7d9-4e39-bd20-1e4f98581045": "In what way does the hierarchical namespace provided by the shared file system resemble a standard file system?",
        "b9234162-c37e-46b6-a12d-079a5f6dc9b3": "What is the purpose of the default_aux_resource_pool in the resource_manager section of the cluster configuration?",
        "ad7ac415-ddf7-4169-abcc-5eea9ae5a56d": "How can the ngpus resource be used to identify the number of GPUs available on a node in PBS?",
        "2cbc67c3-bb4c-4d49-b9c7-11b4785be8b3": "What are the different operations that can be performed related to tables in HPE Ezmeral Data Fabric?",
        "a2daf3dc-6a61-4e8c-9d06-9ca829aa45d9": "How can column family permissions be configured for a table in HPE Ezmeral Data Fabric?",
        "093b259e-20df-4a57-99ae-c75bdeb2e18c": "What is the purpose of the PyTorch MNIST dataset in the context of the given framework?",
        "1c9f58c9-511e-4a04-a232-96590064d38f": "How can custom reducers be utilized in the PyTorch MNIST framework to enhance its functionality?",
        "b98c283d-b4f9-41c7-9108-ce8ce8036bc8": "How does Determined distributed training work and what are some strategies to reduce computation and communication overhead?",
        "749f9afc-a699-4f18-a69a-9dacb219532b": "What are the key considerations and configuration options for implementing distributed training, including connectivity, batch sizes, and scheduler behavior?",
        "6ed79d95-37b9-4805-8851-6d6289faba46": "What are the connection properties required to connect HPE Ezmeral Unified Analytics Software to the Hive data source? Provide an example of these connection properties.",
        "d1423b7d-4a37-4620-9c5f-2999d4a4a589": "What are the JVM configuration settings specified in the jvmConfig section?",
        "484f5c94-f32a-46c5-8cc7-94d65c624810": "What is the purpose of an access control list (ACL) in the MapR Converged Data Platform? How does it define user permissions?",
        "73c6c301-8402-4ba7-b3ca-8f5bc6f60d4f": "Explain the concept of a data container in a data-fabric cluster. What are the different types of data containers and their roles in replication?",
        "d1d0790e-f014-4e12-aa8c-c61ce891b30e": "What are the different licenses used for the libraries mentioned in the context information?",
        "59933338-65bb-408a-9772-bce5bd50f99a": "Can you provide the homepage URLs for the libraries included in the project?",
        "cfa9d316-859d-44d3-ad50-8086c1818560": "What is the purpose of the RemoteSearchRunner class in the determined.searcher module?",
        "fbc20381-0137-4b9b-bdfb-f2d1a6f8b042": "How does the RemoteSearchRunner class execute the search for optimal hyperparameter values?",
        "959a637c-7f19-4d72-bbb1-621821f4fb61": "How does the parameter \"slots_per_node\" help in utilizing multiple GPUs per node when \"gres_supported\" is set to false?",
        "b53a13a5-6165-43ae-9ca8-214fff743b50": "What are some ways in which a user can ensure that the required number of GPUs specified by \"slots_per_node\" will be available on selected nodes for a job?",
        "4e17fe70-e070-457c-828c-7b39c763f2f1": "How does the Determined master determine the hostname or IP address that is required for its setup?",
        "e82e763a-377d-43b5-b9f1-4443d2b153eb": "What are the possible methods to obtain the hostname or IP address of the Determined master?",
        "7d26dbf1-e3f5-47bd-89b6-e1fc10e1ec1a": "How can a user access the built-in help in the Determined CLI?",
        "2b06b379-3527-43e0-8fb7-c9ff651c761b": "What are the top-level commands available in the Determined CLI?",
        "d711cb65-65c2-4f5e-9870-dab1820ac069": "What are the steps to download and extract the code for the tutorial on PyTorch?",
        "3ee449ce-71c6-4305-ad4a-3c4e3ef1322c": "How can you navigate to the mnist_pytorch directory using the terminal?",
        "b8a06e25-1a22-417d-b0ac-da4fcdc82d82": "What is the purpose of the gcr.io/mapr-252711/kubeflow/notebooks/jupyter-tensorflow-cuda-full image? How does it facilitate faster model training and data processing?",
        "c510c00f-d10b-4d89-8ccd-b1a15d67cebe": "How does the gcr.io/mapr-252711/kubeflow/notebooks/codeserver image enable developers to edit and develop code in a remote server setup? What features does it provide for code editing and development?",
        "501d9a60-db33-4249-8275-3597ca378646": "How does the TorchBatchProcessor class utilize the TorchBatchProcessorContext object during initialization? Explain with an example.",
        "8f41fbed-1e61-4a92-9584-b6f28a4db1b6": "Can you explain the compatibility between TorchBatchProcessor and Determined's MetricReducer? How can MetricReducer be passed to TorchBatchProcessor?",
        "b945d7ea-8de5-4e3a-9102-bbd3701fdb10": "What is the purpose of the replication factor in a data-fabric cluster? How does it affect the write operations in the cluster?",
        "950f784f-4849-42c4-871d-9452ce4d338c": "Explain the role of the ResourceManager (RM) in a YARN cluster. How does it manage cluster resources and schedule applications?",
        "e765c8a1-bb94-449e-acfc-68dd1d170449": "What is the purpose of observability in HPE Ezmeral Unified Analytics Software and how does it contribute to managing applications and clusters?",
        "5e3606dd-8c65-4f12-89c7-ce665c06bbba": "How does logging work in HPE Ezmeral Unified Analytics Software and what are the steps to access log files for both the platform and applications?",
        "6b980f00-fd4b-42ea-ba57-d0a5b19dd500": "What is the purpose of a data node in a data-fabric cluster? How does it contribute to the overall functioning of the cluster?",
        "3709b522-efdf-4774-bdcc-ca84dc9827a7": "Explain the concept of a global namespace in the context of HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "672b37eb-bd2b-4da2-a25b-c3e1eda714b3": "How can you configure email notifications in the HPE Ezmeral Data Fabric UI? Provide step-by-step instructions on setting up SMTP for sending email notifications.",
        "a6a238fd-a80a-4e09-a64a-90cc1e763ef2": "What are the parameters that need to be specified when configuring email notifications in the Data Fabric UI? Explain the purpose of each parameter and provide an example for each.",
        "3ed57b96-de1f-4b3c-8181-5b912275f39d": "What are the steps to view fabric service status in the Data Fabric UI?",
        "026ab58f-b79b-47b2-9852-5e166abcc382": "What information is displayed on the Services tab for the fabric in the Data Fabric UI?",
        "c41a2ab8-4286-4a2c-902b-4939c9cc205c": "What are the different types of licenses mentioned in the context information? Provide an example for each type.",
        "a05a6984-e9d9-4429-97d6-fdaf919c4ecd": "Choose two open-source projects from the context information and compare their licenses. What are the similarities and differences between them?",
        "e60285f7-a071-4292-9e1e-2b3d9ce58f46": "How does Transport Layer Security (TLS) ensure secure network communication and protect data in transit?",
        "c3774433-6852-4f64-9367-22e3f5b82384": "Which connections in Determined can be secured by TLS, and which ones do not use TLS?",
        "50ad8095-4f67-4de3-bd6f-1031f601142b": "What is the purpose of setting the search mode to \"standard\" in the adaptive_asha algorithm?",
        "1fc762ca-6dfa-4307-ad60-ae327dafde85": "How can the resource budget settings, such as max_length and max_trials, be adjusted to optimize the performance of the adaptive_asha algorithm?",
        "fcae82c8-a7af-41ca-9f49-b3f5d9fa4c17": "What are the different aspects compared in the Cloud Instance Specifications section of the HPE Ezmeral Data Fabric documentation?",
        "86bf1cb9-ef11-4de2-abe6-56df1e8f6a15": "Which open-source projects are acknowledged and used with HPE software in the Open-Source Software Acknowledgements section of the HPE Ezmeral Data Fabric documentation?",
        "fb8c75a0-bbbc-4a9d-a69e-3fedfb0ea42c": "How does the algorithm for fitting distributed jobs onto agents of different sizes work? Explain the process and the conditions under which it allows heterogeneous fits.",
        "3ebe147e-f1a4-4e61-9577-30348a3aaf94": "Why is it preferred to fit jobs on same sized nodes when distributing jobs onto agents? Discuss the advantages and disadvantages of allowing heterogeneous fits as a fallback option.",
        "a4242024-89e0-4444-aa25-ed0eaac73d54": "What is the purpose of the minimum replication factor in a data-fabric cluster? How does it affect the cluster's operation?",
        "74d46fdc-684d-4baa-a884-abf57d222cbd": "Explain the role of the Object Store in the HPE Ezmeral Data Fabric. How does it leverage the capabilities of the file system for performance and scalability?",
        "454743cc-c7a9-4ee8-bd99-249f660b7814": "What is the purpose of the \"--cluster-id\" argument in the \"det deploy aws down\" command?",
        "5e67b280-06c5-45fa-8a8f-612c2f6b3950": "How can you specify the AWS region for deploying the cluster in the \"det deploy aws down\" command?",
        "e687d470-acd4-451a-98b2-50f8f81bee62": "What are the recommended machine types for installing the Determined master image on GCP Compute Engine?",
        "64976f01-19e2-49ae-bf02-bb43ccf036d0": "How can you start the Determined master after SSHing into the instance?",
        "f1dfad32-4278-44f6-b68c-fdad86680801": "How can you create a local repository for an air-gapped installation on Ubuntu?",
        "1a2e43ab-d425-4385-85e0-d07381878090": "What are the steps involved in setting up a local repository on Ubuntu for the HPE Ezmeral Data Fabric?",
        "f4d70abd-5870-46c4-ad35-a165e40e2f9a": "How does the HPE Ezmeral Data Fabric support single sign-on (SSO)? What are the limitations for non-SSO users?",
        "c3b31295-1cf4-4f2e-87b4-afc87228d077": "What is Keycloak IAM and how does it support SSO in the HPE Ezmeral Data Fabric? How is Keycloak preinstalled and preconfigured in the fabric?",
        "26d8f959-2aac-4423-a104-6f250de97a17": "How does SAML integration benefit Determined Enterprise Edition users?",
        "3c6e651f-50a9-45ed-b0eb-5991532c11d5": "What is the purpose of SAML integration in Determined EE and how does it enhance system administrators' control over resource access?",
        "15d93156-a811-4514-a081-b2efa2222436": "What are the steps to install PostgreSQL 10 or greater on Debian distributions?",
        "128eb153-d5ba-43b2-aaed-681a6350927b": "How can you configure the PostgreSQL yum repository and install version 10 on Red Hat distributions?",
        "79eab579-ca20-4b77-90e4-94724734fc42": "What is the purpose of a domain in the MapR Converged Data Platform? How does it track various metrics within the platform?",
        "a65017bf-7c5f-44a8-b47d-847faf7f665c": "Explain the concept of an access control list (ACL) in the context of the HPE Ezmeral Data Fabric. How does an ACL specify user permissions for performing actions on objects?",
        "860890f6-7cb9-4fd9-b63c-c64663e3c8ef": "What are the steps to install the Data Fabric Client on SLES?",
        "f025156e-f064-4c36-b6ac-0240b0b09972": "How can you remove any previous Data Fabric software before installing the client on SLES?",
        "9e93e93f-0238-4f52-81a6-7ec1f335ba4c": "What is the purpose of a validation metric in evaluating the performance of a hyperparameter configuration?",
        "ba07280f-12b8-4549-9aae-c113e240d587": "Can you provide an example of a validation metric that is commonly used to evaluate the performance of hyperparameter configurations?",
        "a1868ffc-c43c-44c2-b522-a08e578c11a1": "What is the purpose of storage policies in HPE Ezmeral Data Fabric? How do they simplify the lifecycle management of data in a volume?",
        "7e7452d1-fe1f-498b-8d35-71954979eca6": "How can you configure a storage policy in HPE Ezmeral Data Fabric? What are the criteria that can be used to selectively identify files for offloading?",
        "02c348ee-0958-4cbd-8776-c777bea768c1": "How can data engineers use HPE Ezmeral Unified Analytics Software to transform and transport data into usable formats for data consumers?",
        "8bbf6af9-df62-469e-8ac6-5c5770129d6e": "What steps are involved in using whylogs with Spark in HPE Ezmeral Unified Analytics Software?",
        "449b0c37-0c54-48b5-b3cd-2881ff80f13b": "What is the breaking change in the API related to routes with \"/api/v1/models/:id/*\"? How should spaces and special characters in a name be handled?",
        "a9d6a794-b0c8-48ed-ab19-0df9f9268c18": "How can you retrieve a model by its ID using the API?",
        "fa8d3fd1-fa51-4ee4-aab2-e70304cb003c": "How does the choice of authorization system impact the security of a system? Provide examples of different authorization systems and their potential advantages and disadvantages.",
        "2daf030c-446f-478b-a24b-cb5173d9c4d2": "In the context of RBAC (Role-Based Access Control), what are the key factors to consider when selecting an appropriate authorization system? How does RBAC enhance security and access control in comparison to other authorization models?",
        "c5a80d2b-690e-4084-b6bb-1e3c441aeed9": "What is the purpose of compressing gradients during distributed training? How does compression affect gradient values?",
        "cd2c461b-509e-4b74-bb0d-e2e5d783a998": "Explain the concept of gradient compression during distributed training. Discuss its impact on space reduction and its default setting.",
        "1bcc64e7-2510-4444-afe9-96ef00c322bc": "What is the purpose of running an actual Determined training loop with abbreviated workloads in Step 2?",
        "15645701-007c-4d25-a7db-5ccdfbab886e": "What should you do if the local test mode fails during Step 4 of the process?",
        "7a65733e-d267-4770-acba-b76f9436e051": "What are the fabric settings that can be viewed and changed in the HPE Ezmeral Data Fabric UI? Explain each setting briefly.",
        "bb9ad3d0-a8fc-46cb-acfd-53a91b3f5894": "How can you associate an external NFS server with Data Fabric to share data across clusters in the global namespace? Provide a step-by-step procedure.",
        "0c370b8f-62be-404f-a732-bfe963abd084": "What is the role of the data-fabric user in a cluster? How does it differ from other users in terms of privileges and control?",
        "fedabece-9125-4110-ad87-3ccc7e91c724": "Explain the concept of a global namespace in the HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "9f5a6866-a4a9-491d-916b-4c56978d581c": "What is the role of the data-fabric user in a cluster? How does it differ from the data-fabric administrator?",
        "fc2b570f-38ab-4d16-ae34-f55e0d2a62c7": "How does the global namespace technology in HPE Ezmeral Data Fabric enable the management of globally deployed data as a single resource?",
        "7c104137-9025-4458-8915-6beb1914fe4c": "How is the activation key file obtained and used to activate Unified Analytics in connected environments?",
        "e7c7eee6-3d1e-4c08-8664-b7c00c09a7c8": "What is the billing process for connected environments after the activation key is uploaded and how is consumption data calculated?",
        "76fde596-daff-4af0-b4cd-e8a8eaa32714": "What is the purpose of an access control list (ACL) in the MapR Converged Data Platform? How does it determine user permissions for specific actions on an object?",
        "358a3261-d9a6-407d-907f-68d0a82ccd01": "Explain the concept of a data container in a data-fabric cluster. What are the different types of data containers and how do they contribute to data replication and storage in the cluster?",
        "ebd58645-3ed7-4723-a884-caf7f42cc243": "How does OAuth 2.0 authentication differ between Determined Enterprise Edition and other editions?",
        "530e595b-7c0b-4b2e-8c33-fb595f8636f1": "What is the purpose of using the authorization code flow in OAuth 2.0 for Determined EE?",
        "67b60a64-129c-4350-b4f2-a3deaac7dbf1": "How can you replace the placeholder master address in the Prometheus configuration file?",
        "6ffd4ed5-6a75-4c46-a784-17f25424362d": "What is the purpose of the metric_relabel_configs parameter in the Prometheus configuration file?",
        "da2a62fa-79be-4fff-bf72-94615dc20f00": "What is the purpose of the Experimental section in the user settings? How does it allow users to control the features?",
        "c0178c53-4cd3-4ee1-ac65-08afe11a2a23": "Why is it advised not to turn on experimental features if you are unsure about their meaning or potential impact? What makes these features different from regular features in terms of their stability and availability?",
        "2ee93810-34c1-4640-89fd-86e0330cc290": "What is the purpose of the new feature update in the experiment details pages?",
        "e73ac179-c823-40ef-a202-e587787a451f": "How can Elasticsearch be used as an alternative backend for logging in Determined deployment?",
        "a8744cef-5764-4e1b-a59a-e822f15eb117": "What is the purpose of defining a default pod spec for CPU-only tasks in Kubernetes?",
        "0c95fc24-ddbd-4719-a63f-c3957aceb000": "How can a pod spec be customized for CPU-only tasks in Kubernetes?",
        "2d40fb65-befa-4c9c-9b7c-207db7ea3c14": "What is the license for the \"prismjs\" library and where can the full license be found?",
        "b503012d-a601-4729-b244-84589df1403a": "Name two Apache Software Foundation projects mentioned in the context and provide the license information for each.",
        "898bc91e-48f5-4162-9db5-eb71c15a307e": "What are the required parameters for creating a new fabric using Amazon Web Services (AWS)?",
        "44001874-bd06-47d4-a183-63a8cabbfe46": "How does data-at-rest encryption protect sensitive data on a secure fabric?",
        "892e4378-02cf-4c04-a74a-45721203a139": "What is the purpose of log compaction in a data-fabric cluster? How does it ensure the retention of the latest version of messages published to a topic partition?",
        "d17b954d-c414-46cd-b55d-fabadc617afa": "Explain the concept of replication factor in the context of a data-fabric cluster. How does re-replication occur and what are some reasons for it to happen?",
        "31f5a29c-c24a-45b7-80be-1e4cef6f557f": "How does the Successive Halving Algorithm (SHA) work in pruning trials? Explain the process in detail, including the number of rungs, training lengths, and the fraction of trials pruned after each rung.",
        "9191567b-f892-456a-95e9-5f031bf5f1ef": "What is the purpose of the divisor in the SHA algorithm? How does it determine the fraction of trials kept in successive rungs and the training length in each rung? Provide an example to illustrate its usage.",
        "d86f2134-8b7f-41a5-b680-766c5e8468f6": "What are the two options for installing the Determined master and agent on machines running Linux?",
        "f63b930c-1e53-424b-a65e-5112c6a8e8f6": "Which Linux distributions are compatible with the Red Hat 7-based packages for installing Determined?",
        "87120f6e-342c-44d7-8718-c56c329a0f74": "What is the purpose of specifying a trial class as the entry point in the context of deep learning applications?",
        "0920267a-ca66-4933-99e2-2542703cfc2a": "How does the configuration automatically detect distributed training and what framework does it use for distributed training?",
        "1d9ac16b-3f50-43b1-b871-174d074774d0": "What is the purpose of audit logging in HPE Ezmeral Unified Analytics Software? Provide examples of the types of actions that are captured in the audit logs.",
        "2fb74fbe-f461-4592-a8c8-124c4b01c968": "How does auditing in HPE Ezmeral Unified Analytics Software contribute to accountability, tracking, and compliance? Explain the information that is provided in the audit logs for each action.",
        "cf802a33-5bd7-44a9-a268-0b30a83e4381": "What is the default TCP port that the Fluent Bit daemon listens on? Can this port be changed? Explain why it is important for this port to be unique when running multiple agents on the same node.",
        "22b78bb5-9a64-4dda-9002-21dca7f32bf6": "In what scenario would it be necessary to run multiple agents of the Fluent Bit daemon on the same node? How does ensuring a unique TCP port for each agent contribute to the overall functionality and efficiency of the system?",
        "0d2bd6f8-856e-4284-a252-102687a6a24e": "How can a pod be customized in Kubernetes?",
        "68ed16a4-9953-4a6f-9f04-d4af6a3499e9": "What are some examples of customizations that can be applied to a pod in Kubernetes?",
        "7864e08c-1145-462a-8984-4b9384f5e199": "What is the purpose of using mixed precision training with PyTorch during distributed training? How does it differ from O0 configuration?",
        "bd139ccd-ab91-4e97-83aa-b23aea86a6a8": "Why are users advised to call context.configure_apex_amp in the constructor of their trial class instead of using the deprecated configuration setting for mixed precision training?",
        "68356234-5145-4231-ba0e-4e53f5b289a2": "How can you implement learning rate scheduling using framework abstractions instead of directly changing the learning rate?",
        "d188b9fb-7486-4696-b9b4-a2032f6ec95e": "Why is it important to download artifacts to a unique temporary directory when using distributed training in Determined?",
        "bd2510df-6020-4cca-8a97-6e500569273b": "What are the required images for installing and running Spark and Spark based services in HPE Ezmeral Unified Analytics Software 1.3.0? Provide examples of these images.",
        "42628921-8608-4959-a23c-b7ccaecd395e": "What are the different types of Spark images available for supporting Data Fabric Services and not supporting Data Fabric Services? Provide examples of each type of image.",
        "7ba8ee55-35f5-43fc-b3f6-5e37daed3c0f": "What is the purpose of the data-fabric user in a cluster? How does it differ from other users in terms of privileges and control?",
        "b2750dce-eb1c-4746-aea6-4ed4731730b3": "Explain the concept of a global namespace in HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "7501f44c-4e9d-442d-be42-90847d6e393d": "What is the purpose of a data node in a data-fabric cluster? How does it contribute to the overall functioning of the cluster?",
        "50dd1a84-790d-4128-90f0-bf11b7f2deec": "Explain the concept of a global namespace in the context of HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "80fbe5cb-16d2-46e5-90c0-cf3f9880589d": "What are the required connection parameters for establishing a PostgreSQL connection in HPE Ezmeral Unified Analytics Software? Provide a brief description of each parameter and its default value.",
        "5c2601cd-c122-4bec-83ad-c426925d8174": "How can you enable caching while querying in HPE Ezmeral Unified Analytics Software when connecting to a PostgreSQL database? What is the default value for this parameter and what does it signify?",
        "aa693da4-448e-4794-8056-4178e73cfe4c": "What are some additional Slurm options that can be used when launching trials with sbatch? How can these options be accessed and where can more details about them be found?",
        "bcea2274-2dcb-4c85-b005-e4bbd861f9b1": "As a teacher, how would you explain the purpose and significance of environment.slurm in the context of launching trials with sbatch?",
        "39d1504b-d469-44ac-974a-9a7d814fa2a9": "What is the default state of resource pools in a cluster? Can they be bound to a workspace?",
        "b22d9539-55f1-4b4a-8a55-f083d5901ce8": "How can you make a resource pool globally available to all workspaces in a cluster? Can default resource pools be bound to a workspace?",
        "9ab2e647-a3dd-4dd7-8f1c-382ead02343c": "What are some of the third-party storage solutions supported by HPE Ezmeral Data Fabric, and how do they enhance the platform's functionality?",
        "bc936965-478d-4fc1-a9d9-975c468e286f": "Which operating systems are supported for HPE Ezmeral Data Fabric releases, and where can you find the complete list of supported Linux versions?",
        "5cc2e4c9-e508-4369-845a-58f95b3e0afd": "What is the purpose of a data-fabric gateway in a MapR Converged Data Platform environment? How does it facilitate communication between source and destination clusters?",
        "abedf745-bd0e-4929-a77f-7f0153163c3d": "Explain the concept of an access control list (ACL) in the context of HPE Ezmeral Data Fabric. How does an ACL specify user permissions and what actions can be performed on an object?",
        "a8234754-54c4-477e-9a20-182b6a2ab359": "How can you upgrade frameworks sequentially when new versions are available in HPE Ezmeral Unified Analytics Software?",
        "84fee029-9f07-4d53-93e6-af3432f88b0e": "What actions can you perform from the Actions menu when scheduling batch framework updates for later?",
        "7ddd1499-6e84-4376-8ae7-3ebfaa8960cb": "What is the purpose of entering the URL \"http://localhost:8080/\" in the browser for the experiment progress?",
        "6b7cd185-eda1-43e2-a9db-14fcee549125": "How can a user sign in to the local training environment?",
        "3e178ed1-7f26-4cd5-bc9c-6ef07c0a844a": "True or False: When the property \"hive.immutable-partitions\" is set to true, empty files are created for temporary tables when there is no data. Explain your answer.",
        "7701b686-1461-46e7-9e79-25f8d286c8e1": "What is the purpose of the \"Hive Partition Statistics Based Optimization Enabled\" property? How does it affect query optimization in Hive?",
        "d766111b-df30-4dab-b8c1-df0864e8aa8d": "How does enabling the option to average training metrics across GPUs impact the metrics shown in the Determined UI and TensorBoard?",
        "7d1a7f2e-28bb-43d9-b14c-c5fd79b53761": "Which types of trial instances currently support the option to average training metrics across GPUs?",
        "266b3841-f471-49f4-b26c-d8691bcd7215": "What is the purpose of implementing the PyTorchCallback and supplying it to the PyTorchTrial in the context of executing arbitrary Python code?",
        "35bf41c0-031b-4888-bd6c-275413f47641": "How can a PyTorchCallback be implemented to execute arbitrary Python code during the lifecycle of a PyTorchTrial?",
        "b9ab834e-a34d-4ade-bdf8-267af3e71a80": "What is the purpose of the `determined.pytorch.deepspeed.overwrite_deepspeed_configbase_ds_config` function?",
        "7fb0e828-f51a-461d-af47-fd64e31d7e76": "How does the `overwrite_deepspeed_configbase_ds_config` function work to overwrite the `base_ds_config` with values from the `source_ds_dict`?",
        "e9868d42-2191-478a-b4ff-7f2a7356c212": "How does the minimum number of determined agent instances affect the default setting?",
        "3625e480-bfa1-4561-abd0-11a03520467d": "In what scenarios would it be necessary to increase the minimum number of determined agent instances?",
        "f251605a-fa5d-4070-9b50-61eb044c234c": "What are the required parameters for creating a new fabric using Google Cloud Platform (GCP)? How should the name of the fabric be formatted?",
        "e0c4695e-d235-4ad1-9da1-aab08539ff8e": "How does data-at-rest encryption in the HPE Ezmeral Data Fabric protect against unauthorized access and data theft?",
        "dbda5546-feb9-4da7-96cc-901c69261c5a": "How does Airflow in HPE Ezmeral Unified Analytics Software handle role-based access controls (RBACs) for defining access to DAGs?",
        "a65916b0-9482-47ec-85e8-3fe3e880b01f": "Can you explain the process of submitting Spark applications using DAGs in Airflow?",
        "c30f6fd8-569f-43e7-9a84-10c6aa0b9f9d": "What is the significance of the XDG_RUNTIME_DIR environment variable in the context of Enroot and compute jobs? How does its absence affect the execution of Enroot commands?",
        "92679f2d-85c0-436d-8120-ff7cce476095": "Explain the error message \"No such file or directory: /home/users/test/.local/share/enroot/determinedai+environments+cuda-11.1-base-gpu-mpi-0.18.5\" in the context of Enroot containers. What steps should the user take to resolve this error and create the necessary Enroot container?",
        "14ee20c9-3a7f-4399-a576-db98a2e3999a": "How does the adaptive_asha search method differ from the traditional Successive Halving Algorithm (ASHA)?",
        "aac9324a-a144-4818-9d62-a83346a3fdc8": "In what scenarios would the adaptive_asha search method be particularly useful compared to other search methods?",
        "138d2219-d608-4d4a-9446-d0d293755ad7": "How does YARN allocate memory for each map or reduce task?",
        "674c3e17-37ab-4e34-b80d-0c5c9923de93": "What is the purpose of ZooKeeper in distributed applications?",
        "a9101a89-e7d4-4de0-bef0-de11f2bec0b4": "How does systemd socket activation benefit the master in terms of starting automatically and reducing loss of connection state?",
        "d20151e8-fab4-403e-a1a4-01fdd5e5d9a9": "How can the listening port for the master be configured when using socket activation in systemd?",
        "05d3cd8a-1e2a-43c6-9ae0-2f5d86ebf8fd": "How can the Model Registry be accessed and what are some of the available options for accessing it?",
        "b7fb07d6-cbd6-4675-8549-c8a83b972540": "What are the main features of the Model Registry and how can it be used to organize and manage models effectively?",
        "4762c26d-6dcd-4cbd-915c-2d4a52dba143": "How does GPU resource management in HPE Ezmeral Unified Analytics Software optimize analytical workloads and increase efficiency?",
        "903872fe-d298-4256-ad56-81d2af364f39": "What is the purpose of the custom scheduler in HPE Ezmeral Unified Analytics Software and how does it enhance the default Kubernetes scheduler?",
        "32615a14-5208-40aa-b8dc-3757115e278c": "What are the two types of continuations mentioned in the context information? How do they differ in terms of behavior?",
        "58750c26-771d-4aa3-aaaf-5a9e46fa1031": "How can you enable pausing and resuming an experiment according to the context information?",
        "3cc1cdaa-36bd-4ed6-b65e-811e77e4b94e": "How can you select and cache specific data sets in the HPE Ezmeral Unified Analytics Software UI?",
        "ef73d320-18bc-41e4-ae5d-6e6a8733343e": "What is the purpose of the Cache Overview in the Manage Datasets screen and how can it be used to verify changes made to the data sets?",
        "d9b4ae8a-b115-44ae-91b6-a3ba3bb3920e": "What is the license for the \"react-highcharts\" library?",
        "3aaa71fe-64cb-458e-a887-db7ca41ea046": "What is the repository URL for the \"lodash\" library?",
        "7809803c-6524-4959-b737-32194ae9d50b": "What is the purpose of setting the \"frequency\" parameter in system operations for training models?",
        "1132fb93-6a20-4be3-9da3-a28af2314359": "How can setting the \"frequency\" parameter too small or too large impact the training process and resource allocation in a workload?",
        "2b22e832-1611-4601-9268-691e17c1b50b": "What are the prerequisites for creating an on-premises fabric in the HPE Ezmeral Data Fabric platform?",
        "a94540ca-367e-468b-ac17-63a9d81a010e": "How can data-at-rest encryption be enabled in the HPE Ezmeral Data Fabric platform?",
        "dbe32245-4aac-4801-a920-04e008e8d307": "What is the license for the \"react-select\" package?",
        "82f8dba0-c848-485e-9703-908d96ccb728": "Which package has the repository URL \"https://github.com/ghinda/css-toggle-switch\"?",
        "cfbf042c-c64c-432a-8b5f-17911a1385ee": "How can Docker be used by the Determined agent to run workloads?",
        "e8e7b709-edf2-4290-8217-7b5e03fd3db5": "What steps should be taken to configure the storage path for checkpoints in Determined when using Docker for Mac?",
        "d6707ca2-702e-4e2c-bb18-6fe161d39778": "How does the resource pool in this context contribute to the overall efficiency and effectiveness of a project or organization?",
        "2c466fbc-b702-4142-9c95-5371778a1cae": "What are some potential challenges or limitations that may arise when managing a resource pool, and how can they be overcome?",
        "0fcfdec2-7797-4695-8b76-b0e50231eca1": "Which library in the context information has the highest version number?",
        "90342b19-b259-456d-a706-f805051671a2": "What is the license for the \"react-joyride\" library?",
        "d4e6c7bf-47f7-46dc-8d7b-3c5050bec646": "What is the purpose of the replication factor in a data-fabric cluster? How does it affect the availability of writes in the cluster?",
        "1f295612-1535-4512-a87f-c8db5e5f200e": "Explain the role of the ResourceManager (RM) in a YARN cluster. How does it manage cluster resources and schedule applications?",
        "b914be89-37e8-441b-b4cd-ffb722a179e2": "What are the components used in the Fraud Detection Use Case tutorial in HPE Ezmeral Unified Analytics Software?",
        "f906642b-ce1e-447e-8e2c-124ded5b444f": "How does the Question-Answering Use Case tutorial in HPE Ezmeral Unified Analytics Software utilize Large Language Models (LLMs)?",
        "9db13d44-c205-46da-8757-7e129b93b415": "What is the purpose of a data node in a data-fabric cluster? How does it contribute to the overall functioning of the cluster?",
        "cef091d8-ab0d-4eca-b324-0dd5df44fcab": "Explain the concept of a global namespace in the context of HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "76df1522-495c-41c2-8c34-97628102cf17": "What are the possible reasons for a trial to exit early in the determined.searcher class?",
        "beedece7-bfff-4ff6-a334-40c725829f9f": "Can you explain the difference between the \"ERRORED\" and \"USER_CANCELLED\" reasons for a trial to exit early in the determined.searcher class?",
        "7220a2f5-0290-432f-aa7f-58c67cca155d": "What are the recommended options for storing metadata in Determined when running on AWS?",
        "663fd9a2-c034-419f-a9a7-fab41d15784b": "How do users interact with the Determined cluster when running on AWS?",
        "41133952-c3bf-4b34-b9d1-0758485f8c61": "What is the purpose of the `TrainingMetricstotal_batches` class in the `determined.experimental.client` module?",
        "f2f7daf3-ebec-4ad5-b9f1-cbe942d39ff9": "Why is the `TrainingMetricstotal_batches` class marked as deprecated and what should be used instead?",
        "3d0c7d1f-b56f-4aef-a9d5-45eadade157f": "How can Jupyter Notebooks be started according to the WebUI?",
        "39c90a70-86e5-45b2-83df-4a82c0c9f69c": "As a teacher, how would you ensure diversity in the nature of the questions for the upcoming quiz/examination?",
        "44ca536f-54db-4336-b661-b1865165e66c": "How can you set an alias for the fabric manager using MinIO client commands?",
        "538263d6-60c2-4af1-88af-fed019ea3a2e": "What happens if you attempt to generate keys more than twice for the same global namespace?",
        "b8392dd6-2390-4cb4-a012-30a7fd115c31": "What is the purpose of the container location database (CLDB) in a data-fabric cluster? How does it maintain the locations of services, containers, and other cluster information?",
        "e9b0d3db-5a6d-486c-b37c-6ce9877f9462": "Explain the concept of an access control list (ACL) in the context of the data-fabric cluster. How does an ACL specify permissions for users or system processes to perform specific actions on an object?",
        "85a659e6-66db-4091-a72f-ff29f043e973": "What is the purpose of the data-fabric user in a cluster? How does it differ from other users in terms of privileges and control?",
        "d8dab93c-9402-40ec-ab45-c72f95f360b4": "Explain the concept of a global namespace in the HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "d80c88d5-2225-4847-a01c-8fed2081ff7e": "What is the purpose of the first kubectl command \"kubectl get pods -l=determined\" mentioned in the context information?",
        "88da89da-b7ed-432c-876b-f81d2f828a79": "Why is it mentioned that users should never have to run the second kubectl command \"kubectl get pods --no-headers=true -l=determined | awk '{print $1}' | xargs kubectl delete pod\" unless they are removing a deployment of Determined?",
        "0bd852bc-8229-4854-914e-181c60c3723a": "What is the purpose of using the -h or --help argument in the CLI? How does it help users navigate through the command line interface?",
        "c58a670d-f2bb-4121-8e6a-5bc7ff2dfb30": "How can you obtain help for a specific command or subcommand in the CLI? Provide an example of how to get help for the \"deploy aws\" subcommand.",
        "956ec280-e7f7-4e53-9a20-03bf24460616": "What is the purpose of the replication factor in a data-fabric cluster? How does it affect the availability of writes in the cluster?",
        "18a5f683-f4d9-4d31-8ebe-6f133e41a6f0": "Explain the role of the ResourceManager (RM) in a YARN cluster. How does it manage cluster resources and schedule applications?",
        "35e99d72-ec7e-4a68-b78c-b05c9d23158f": "What is the purpose of the replication role balancer in the HPE Ezmeral Data Fabric platform? How does it ensure equal distribution of containers across storage pools?",
        "f48fe1d0-c9b5-4b0f-8dae-cc1359239ad1": "Explain the concept of a snapshot in the context of the data-fabric cluster. How can snapshots be used to restore data to a specific point in time?",
        "0e632782-d822-49e4-8d4c-e0c63515c42d": "How does the experiment configuration tags list affect the passing of values to the workload manager? Explain the process and any conditions that need to be met for a value to be passed.",
        "160a5506-37c7-4d5e-bff5-7d6ad9bb362c": "What is the purpose of using a prefix in the experiment configuration tags list? How does it affect the concatenation of tag values for different workload managers?",
        "5991bc6f-85c5-4991-923e-27e329c9de7a": "How can you mute or dismiss an alarm in the HPE Ezmeral Data Fabric UI?",
        "5719b65e-aa55-43d4-a716-fbdc12040095": "What are the different durations for muting an alarm in the Data Fabric UI?"
    },
    "corpus": {
        "23941117-0b3c-41b1-ac81-b2b745f84243": "det deploy aws list Argument Description Default Value --region AWS region to deploy into. The default region for the AWS user --profile AWS profile used for deploying cluster resources. default",
        "e4e6ff92-8f29-4afb-9945-bd5dd760a101": "YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way\n            communication between a source data-fabric cluster and a destination cluster. The data-fabric gateway also applies updates from JSON\n            tables to their secondary indexes and propagates Change Data Capture (CDC)\n            logs. (Topic last modified: 2023-04-03) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "15260de1-40d3-4706-8c65-b2031ce36cfa": "These instructions describe how to install Determined for the first time. For directions on how to upgrade an existing Determined installation, see the Upgrades section below. Ensure that you are using the most up-to-date Determined AMIs. Keep the AMI IDs handy; you will need them later (e.g., ami-0f4677bfc3161edc8).",
        "b561a67d-165f-4991-8e68-bbdbc6a230c9": "You need to implement the train_batch() method of your PyTorchTrial subclass. Typically when training with native PyTorch, you write a training loop, which iterates through the dataloader to access and train your model one batch at a time. You can usually identify this code by finding the common code snippet: for batch in dataloader. In Determined, train_batch() also works with one batch at a time. Take this script implemented with the native PyTorch as an example. It has the following code for the training loop. for i, (images, target) in enumerate(train_loader): # measure data loading time data_time.update(time.time() - end) # move data to the same device as model images = images.to(device, non_blocking=True) target = target.to(device, non_blocking=True) # compute output output = model(images) loss = criterion(output, target) # measure accuracy and record loss acc1, acc5 = accuracy(output, target, topk=(1, 5)) losses.update(loss.item(), images.size(0)) top1.update(acc1[0], images.size(0)) top5.update(acc5[0], images.size(0)) # compute gradient and do SGD step optimizer.zero_grad() loss.backward() optimizer.step() # measure elapsed time batch_time.update(time.time() - end) end = time.time() if i % args.print_freq == 0: progress.display(i + 1) Notice that this pure-PyTorch loop manages the per-batch metrics. With Determined, metrics returned by train_batch() are automatically averaged and displayed, so we do not need to do this ourselves. Next, we will convert some PyTorch functions to use Determined\u2019s equivalents. We need to change optimizer.zero_grad(), loss.backward(), and optimizer.step(). The self.context object will be used to call loss.backwards and handle zeroing and stepping the optimizer. The final train_batch() will look like: def train_batch(self, batch: TorchData, epoch_idx: int, batch_idx: int): images, target = batch output = self.model(images) loss = self.criterion(output, target) acc1, acc5 = self.accuracy(output, target, topk=(1, 5)) self.context.backward(loss) self.context.step_optimizer(self.optimizer) return {\"loss\": loss.item(), \"top1\": acc1[0], \"top5\": acc5[0]}",
        "f71f2d1d-e40f-46be-93eb-900102ad4a41": "Feast Ride Sharing Use Case Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . About Provides an overview of HPE Ezmeral Unified Analytics Software . Tutorials Provides a set of tutorials that you can use to experience HPE Ezmeral Unified Analytics Software and the included     applications, such as tutorials for data science and data analytics workflows with notebooks and     applications like Spark, MLflow, Feast, Airflow, and EzPresto. Preparing the Tutorial Environment Describes how to prepare the environment for the Financial Time Series and MNIST Digits     Recognition tutorials. Data Source Connectivity and Exploration Provides basic steps for using the Data Engineering space within HPE Ezmeral Unified Analytics Software . BI Reporting (Superset) Basics Provides basic steps for using the BI Reporting (Superset) space within HPE Ezmeral Unified Analytics Software . Candy Sharing Tutorial (Kale) Describes how Kale converts Notebook to pipeline by applying notebook         annotations. Feast Ride Sharing Use Case Provides an end-to-end workflow using Feast in HPE Ezmeral Unified Analytics Software to generate training data and perform online         model inference for the ride-sharing driver satisfaction model. Financial Time Series Workflow Describes how to use HPE Ezmeral Unified Analytics Software to run a Spark application from an Airflow DAG and then run a     Jupyter notebook to analyze and visualize data that the Spark application puts into a shared     directory in the shared volume that the data scientist\u2019s notebook is mounted to. MLflow Bike Sharing Use Case Provides an end-to-end workflow in HPE Ezmeral Unified Analytics Software for an MLflow prediction model to determine bike rentals per hour     based on weather and time. MNIST Digits Recognition Workflow Provides an end-to-end workflow in HPE Ezmeral Unified Analytics Software for an MNIST digits recognition example. Retail Store Analysis Dashboard (Superset) Provides an end-to-end workflow example for a retail store analysis scenario in HPE Ezmeral Unified Analytics Software using EzPresto and Superset. Submitting a Spark Wordcount Application Provides an end-to-end example for creating and submitting a wordcount Spark         Application in HPE Ezmeral Unified Analytics Software . Resources Provides links to additional resources such as product licensing information, on-demand     training, videos, blogs, and HPE Ezmeral Unified Analytics Software community. Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Feast Ride Sharing Use Case Provides an end-to-end workflow using Feast in HPE Ezmeral Unified Analytics Software to generate training data and perform online\n        model inference for the ride-sharing driver satisfaction model. Prerequisites Sign in to HPE Ezmeral Unified Analytics Software . About this task Use Feast to generate training data and perform online model inference for the\n                ride-sharing driver satisfaction model. In this tutorial, you will: Deploy a local feature store with a Parquet file offline store and SQLite online\n                    store. Build a training dataset using time series features from Parquet files. Read the latest features from the offline store for batch scoring. Ingest batch features (\"materialization\") and streaming features into the online\n                    store. Read the latest features from the online store for real-time inference. Explore the Feast web interface to see Data Sources, Entities, Feature Views,\n                    Feature Services, and Datasets which are defined through feature\n                    definitions. Procedure Connect to the notebook server. See Creating and Managing Notebook Servers . Copy the Feast folder from the /shared directory into the /<username> directory. NOTE If the Feast folder is not available in the /shared directory, perform: Go to GitHub repository for tutorials . Clone the repository. Navigate to ezua-tutorials/Data-Science . Navigate back to the /shared directory. Copy the /Feast folder from the ezua-tutorials/Data-Science repository into\n                                    the /shared directory. Copy the /Feast folder from /shared folder to <username> directory. Validate the ride-sharing-example.ipynb file, definitions.py file, and the data folder\n                    are available in the /<username>/Feast directory.",
        "e55be723-bc6b-4c6e-9859-656025f71dec": "As part of your pod spec, you can specify initContainers and containers. Additionally you can configure the determined-container that executes the task (e.g., training), by setting the container name in the pod spec to determined-container. For the determined-container, Determined supports configuring the following fields: Resource requests and limits (except GPU resources). Volume mounts and volumes. All securityContext fields within the pod spec of the determined-container container except for RunAsUser and RunAsGroup. For those fields, use det user link-with-agent-user instead. Example of configuring a Pachyderm notebook plugin to run in det notebook: environment: pod_spec: apiVersion: v1 kind: Pod spec: containers: - name: determined-container securityContext: privileged: true",
        "c5ab7301-4df1-4281-80f0-dd7cf3b97d43": "Snowflake Connection Parameters Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Accessing Data in External S3 Object Stores Describes how to access data in external object stores from clients, such as Spark and     Kubeflow notebooks. EzPresto Describes the EzPresto SQL query engine     and its featues. Connect Data Sources Provides instructions for connecting HPE Ezmeral Unified Analytics Software to external data sources. Delta Connection Parameters List of Delta connection parameters, descriptions, default values, and supported data     types. Delta Thrift Connection Parameters List of  Delta Thrift connection parameters, descriptions, default values, and     supported data types. Hive Connection Parameters List of Hive connection parameters, descriptions, default values, and supported data     types. Hive Glue Metastore Connection Parameters List of Hive Glue Metastore connection parameters, descriptions, default values, and     supported data types. Hive Thrift Metastore Connection Parameters List of Hive Thrift Metastore connection parameters, descriptions, default values, and     supported data types. Hive Discovery Metastore Connection Parameters Lists Hive discovery metastore connection parameters, parameter descriptions, default     values, and supported data types. MySQL Connection Parameters List of MySQL connection parameters, descriptions, default values, and supported data     types. Oracle Connection Parameters List of Oracle connection parameters, descriptions, default values, and supported data     types. Snowflake Connection Parameters List of  Snowflake connection parameters, descriptions, default values, and supported     data types. SQL Server Connection Parameters List of SQL Server connection parameters, descriptions, default values, and supported     data types. Teradata Connection Parameters List of  Teradata connection parameters, descriptions, default values, and supported     data types. PostgreSQL Connection Parameters List of PostgreSQL connection parameters, descriptions, default values, and supported     data types. Iceberg Connection Parameters List of Iceberg connection parameters, descriptions, default values, and supported data     types. Configuring a Hive Data Source with Kerberos Authentication Describes the required prerequisite steps to complete before you connect HPE Ezmeral Unified Analytics Software to a Hive data source     that uses Kerberos authentication. Connect to CSV and Parquet Data in an External S3 Data Source via Hive Connector Describes how to use the Hive connector with Presto in HPE Ezmeral Unified Analytics Software to connect to CSV and     Parquet data in S3-based external data sources. EzPresto/connect-external-s3-data-source.html Connect to External Applications via JDBC Describes how to connect external applications and BI tools, such as Tableau and     PowerBI, to EzPresto through the EzPresto JDBC endpoint. Using Spark to Query EzPresto Describes how to use Spark to query EzPresto . Connect to EzPresto via Python Client Provides information for connecting to EzPresto from a Python client. Cache Data Describes data caching and provides the steps for caching data in HPE Ezmeral Unified Analytics Software . Submitting Presto Queries from Notebook Describes how to submit Presto queries from the notebook. Airflow Provides an overview of Apache Airflow in HPE Ezmeral Unified Analytics Software . Superset Provides a brief overview of  Superset in HPE Ezmeral Unified Analytics Software . Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Snowflake Connection Parameters List of  Snowflake connection parameters, descriptions, default values, and supported\n    data types. The following tables list the required and optional Snowflake connection parameters. Required Connection Parameters Parameter Description Default Value Data Type Connection Url JDBC connection url. null STRING Connection User Specifies the login name of the user for the connection. null STRING Connection Password Specifies the password of the user for the connection. null STRING Snowflake Db Specifies the default database to use once connected. null STRING Enable Local Snapshot Table Enable Caching while querying. true BOOLEAN Optional Connection Parameters Parameter Description Default Value Data Type Case Insensitive Name Matching Match schema and table names case insensitively. false BOOLEAN Case Insensitive Name Matching Cache Ttl Duration for which remote dataset and table names will be cached.",
        "c394b048-173e-4b0e-9228-991570e70e3b": "One or more shell commands that will be run when the Determined agent container is started. These commands are executed inside the agent container but before the Determined agent itself is launched. For example, this feature can be used to configure Docker so that the agent can pull task images from GCR securely (see this example for more details). The default value is the empty string.",
        "6bbdba0a-4d08-40e2-aaa5-167b9515375a": "data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster. The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza.",
        "60d018ec-ad87-44f8-8c84-050a35da5b91": "Format: determined.launch.horovod [[HVD_OVERRIDES...] --] (--trial TRIAL)|(SCRIPT...) The horovod launcher is a wrapper around horovodrun which automatically configures the workers for the trial. You can pass arguments directly to horovodrun, overriding Determined values, as HVD_OVERRIDES, which must end with a -- to separate the overrides from the normal arguments. Example: python3 -m determined.launch.horovod --fusion-threshold-mb 1 --cycle-time-ms 2 -- --trial model_def:MyTrial",
        "792fed08-fdd6-46f4-ba92-60f565025f5b": "HPE Machine Learning Development Environment Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Feast Provides a brief overview of Feast in HPE Ezmeral Unified Analytics Software . Kubeflow Provides a brief overview of Kubeflow in HPE Ezmeral Unified Analytics Software . HPE Machine Learning Development Environment Provides a brief overview of HPE Machine Learning Development Environment ( HPE MLDE ) in HPE Ezmeral Unified Analytics Software . HPE MLDE User Authentication Describes the methods of user authentication in HPE Machine Learning Development Environment . Enabling HPE MLDE in an Air-Gapped Environment Describes how to enable HPE MLDE in an air-gapped     (disconnected) environment. Configuring HPE MLDE for Added GPU Nodes Describes how to configure HPE MLDE for added GPU nodes     in a cluster. MLflow Provides a brief overview of MLflow in HPE Ezmeral Unified Analytics Software . Ray Provides a brief overview of Ray in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. HPE Machine Learning Development Environment Provides a brief overview of HPE Machine Learning Development Environment ( HPE MLDE ) in HPE Ezmeral Unified Analytics Software . HPE Machine Learning Development Environment is a machine learning platform that offers features\n      such as automated hyperparameter tuning, distributed training and scaling of computations\n      across multiple GPUs, ensuring faster model training times. HPE MLDE supports various deep learning frameworks, enabling\n      you to work with tools such as TensorFlow and PyTorch. It enables efficient model training and\n      deployment, optimizing machine learning workflow. To learn more, see HPE\n        MLDE . To access HPE MLDE in HPE Ezmeral Unified Analytics Software , click the Tools &\n        Frameworks icon on the left navigation bar. Navigate to the HPE MLDE tile under the Data Science tab\n      and click Open . HPE MLDE User Authentication Describes the methods of user authentication in HPE Machine Learning Development Environment . Enabling HPE MLDE in an Air-Gapped Environment Describes how to enable HPE MLDE in an air-gapped     (disconnected) environment. Configuring HPE MLDE for Added GPU Nodes Describes how to configure HPE MLDE for added GPU nodes     in a cluster. Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "db75bbb2-1846-4d2f-98ea-b2a1e957135f": "Separate from the telemetry reporting mentioned above, Determined also supports OpenTelemetry to collect traces. This is disabled by default; to enable it, use the master configuration setting telemetry.otel-enabled. When enabled, the master will send OpenTelemetry traces to a collector running at localhost:4317. A different endpoint can be set via the telemetry.otel-endpoint configuration setting.",
        "13742b1d-a1f3-4b6d-9982-b8ccabe2ec10": "Enabling/Disabling Fabric Auditing Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Auditing Fabric and Fabric Data Auditing in Data Fabric Enabling/Disabling Fabric Auditing Enable/disable fabric auditing HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Enabling/Disabling Fabric Auditing Enable/disable fabric auditing Configuring Auditing for Data Access Operation Enable or disable auditing for data access operations on fabric. Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Enabling/Disabling Fabric Auditing Enable/disable fabric auditing About this task Follow the steps given below to enable/disable auditing of fabric administration\n                operations. Procedure Log on to the Data Fabric UI with Admin or\n                    Fabric Manager credentials . Under the default Fabric user experience , click the Table view icon on\n                    the Resources card. Click the link for the fabric in the Fabrics list for\n                    which you wish to enable auditing. Navigate to the Settings tab on the fabrics page. Under Fabric Settings , click the Edit icon. Toggle Cluster Auditing to enable auditing on the fabric Results Auditing of fabric administration operations is enabled on\n            the fabric. After auditing is enabled, audit log entries are generated when fabric\n            administration operations are performed. (Topic last modified: 2023-10-18) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "776e095a-83b8-4cf2-bb74-543b64650795": "Click Connect to connect to your notebook server. Go to the /<username> folder. Copy the template object_store_secret.yaml.tpl file from the shared/ezua-tutorials/Data-Analytics/Spark directory to the <username> folder. In the <username>/Financial-Time-Series folder, open the financial_time_series_example.ipynb file. NOTE If you do not see the Financial-Time-Series folder in the <username> folder, copy the folder from the /shared/ezua-tutorials/Data-Science/Kubeflow directory into the <username> folder. The /shared directory is\n              accessible to all users. Editing or running examples from the /shared directory is not advised. The <username> directory is specific to\n              you and cannot be accessed by other users If the Financial-Time-Series folder is not available in the /shared/ezua-tutorials/Data-Science/Kubeflow directory,\n                perform: Go to GitHub repository for tutorials . Clone the repository. Navigate to ezua-tutorials/Data-Science/Kubeflow . Navigate back to the <username> directory. Copy the Financial-Time-Series folder from the ezua-tutorials/Data-Science/Kubeflow directory into the <username> directory. To generate a secret to read data source files from S3 bucket by Spark application\n            (Airflow DAG), run the first cell of the financial_time_series_example.ipynb file: import kfp\nkfp_client = kfp.Client()\nnamespace = kfp_client.get_user_namespace()\n!sed -e \"s/\\$AUTH_TOKEN/$AUTH_TOKEN/\" /mnt/user/object_store_secret.yaml.tpl > object_store_secret.yaml A - Run a DAG in Airflow In Airflow, run the DAG named spark_read_csv_write_parquet_fts . The DAG\n        runs a Spark application that reads CSV data (financial.csv) from an S3\n        bucket, transforms the data into Parquet format, and writes the transformed Parquet data\n        into the shared volume. Run the DAG Navigate to the Airflow screen using either of the following methods: Click Data Engineering > Airflow Pipelines . Click Tools & Frameworks , select the Data Engineering tab, and click Open in the Airflow tile. In Airflow , verify that you are on the DAGs screen. Click spark_read_csv_write_parquet_fts DAG. NOTE The DAG is pulled from a pre-configured HPE GitHub repository. This DAG is\n                    constructed to submit a Spark application that pulls financial.csv file into Parquet format, and places the\n                    converted files in a shared directory. If you want to use your private GitHub\n                    repository, see Airflow DAGs Git Repository to learn how to configure your\n                    repository. Click Code to view the DAG code. Click Graph to view the graphical representation of the\n                  DAG. Click Run (play button). Upon successful DAG\n                    completion, the data is accessible inside your notebook server in the following\n                    directory for further processing: /<username>/financial-processed\" To view details for the DAG, click Details . Under DAG Details , you can see green, red, and/or yellow\n                  buttons with the number of times the DAG ran successfully or failed. Click the Success button. To find your job, sort by End Date to see the latest jobs that have run,\n                  and then scroll to the right and click the log icon under Log URL for that run.\n                  Note that jobs run with the\n                    configuration: Conf \"username\":\"your_username\" When running Spark applications using Airflow, you can see the\n                    following logs: Reading from s3a://ezaf-demo/data/financial.csv; \nsrc format is csv 22/11/04 11:53:26 WARN \nAmazonHttpClient: SSL Certificate checking for endpoints has been explicitly disabled. \nRead complete Writing to file:///mounts/data/financial-processed; dest format is parquet Write complete IMPORTANT The cluster clears the logs that result from the DAG runs.\n                      The duration after which the cluster clears the logs depends on the Airflow\n                      task, cluster configuration, and policy. B \u2013 View the Spark Application Once you have triggered the DAG, you can view the Spark application in the Spark\n          Applications screen. To view the Spark application, go to Analytics > Spark\n        Applications . Alternatively, you can go to Applications & Frameworks and then\n        click on the Analytics tab. On the Analytics tab, select the Spark tile and click Open . C \u2013 Run the Jupyter\n        Notebook Run the Jupyter notebook file to analyze and visualize the financial time\n        series data. To run the notebook: Connect to the notebook server. See Creating and Managing Notebook Servers . In the Notebooks screen, navigate to the <username>/financial-processed/ folder to validate that the data processed by the\n          Spark application is available.",
        "7eb7018c-7c30-4669-b6c7-5e53f1402c5d": "A checkpoint includes the model definition (Python source code), experiment configuration file, network architecture, and the values of the model\u2019s parameters (i.e., weights) and hyperparameters. When using a stateful optimizer during training, checkpoints will also include the state of the optimizer (i.e., learning rate). You can also embed arbitrary metadata in checkpoints via a Python SDK. TensorFlow Keras trials are checkpointed to a file named determined-keras-model.h5 using tf.keras.models.save_model. You can learn more from the TF Keras docs.",
        "87f6a9f6-f666-47b0-8b7a-2719eb18e0de": "Take a look at qa_beam_search_trial.py for an example of how you can further customize your trial. Dive into the api.",
        "63395af6-8dad-44cb-a6e2-d9220978a71b": "Required. The AWS secret key to use.",
        "29314430-7c0e-46fd-b7da-4bac086feacf": "Preparing the Tutorial Environment Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . About Provides an overview of HPE Ezmeral Unified Analytics Software . Tutorials Provides a set of tutorials that you can use to experience HPE Ezmeral Unified Analytics Software and the included     applications, such as tutorials for data science and data analytics workflows with notebooks and     applications like Spark, MLflow, Feast, Airflow, and EzPresto. Preparing the Tutorial Environment Describes how to prepare the environment for the Financial Time Series and MNIST Digits     Recognition tutorials. Data Source Connectivity and Exploration Provides basic steps for using the Data Engineering space within HPE Ezmeral Unified Analytics Software . BI Reporting (Superset) Basics Provides basic steps for using the BI Reporting (Superset) space within HPE Ezmeral Unified Analytics Software . Candy Sharing Tutorial (Kale) Describes how Kale converts Notebook to pipeline by applying notebook         annotations. Feast Ride Sharing Use Case Provides an end-to-end workflow using Feast in HPE Ezmeral Unified Analytics Software to generate training data and perform online         model inference for the ride-sharing driver satisfaction model. Financial Time Series Workflow Describes how to use HPE Ezmeral Unified Analytics Software to run a Spark application from an Airflow DAG and then run a     Jupyter notebook to analyze and visualize data that the Spark application puts into a shared     directory in the shared volume that the data scientist\u2019s notebook is mounted to. MLflow Bike Sharing Use Case Provides an end-to-end workflow in HPE Ezmeral Unified Analytics Software for an MLflow prediction model to determine bike rentals per hour     based on weather and time. MNIST Digits Recognition Workflow Provides an end-to-end workflow in HPE Ezmeral Unified Analytics Software for an MNIST digits recognition example. Retail Store Analysis Dashboard (Superset) Provides an end-to-end workflow example for a retail store analysis scenario in HPE Ezmeral Unified Analytics Software using EzPresto and Superset. Submitting a Spark Wordcount Application Provides an end-to-end example for creating and submitting a wordcount Spark         Application in HPE Ezmeral Unified Analytics Software . Resources Provides links to additional resources such as product licensing information, on-demand     training, videos, blogs, and HPE Ezmeral Unified Analytics Software community. Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Preparing the Tutorial Environment Describes how to prepare the environment for the Financial Time Series and MNIST Digits\n    Recognition tutorials. Prerequisites: Sign in as an administrator to prepare the environment for the\n      Financial Time Series and MNIST Digits Recognition tutorials. Create an S3 Object Store Bucket and Load Data The Spark application reads raw data from the S3 Object Store. Use MinIO to create an S3 bucket named ezaf-demo and put the following\n        files in the ezaf-demo bucket, as described: Create data/mnist directory in the ezaf-demo bucket,\n            and upload the following dataset to the mnist folder: https://github.com/HPEEzmeral/ezua-tutorials/tree/main/Data-Science/Kubeflow/MNIST-Digits-Recognition/dataset Create a data folder in the ezaf-demo bucket, and\n            the following data set to the data folder: https://github.com/HPEEzmeral/ezua-tutorials/tree/main/Data-Science/Kubeflow/Financial-Time-Series/dataset Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "3de39877-c8dc-475e-b616-4bb488095cfc": "Specifies configuration settings related to webhooks. signing_key: The key used to sign outgoing webhooks. base_url: The URL users use to access Determined, for generating hyperlinks.",
        "7eeddea0-a5cf-4552-8c15-45c6d4de9352": "Python SDK Reference: The reference documentation for this API. Checkpoints",
        "8cfe60b3-adc2-4585-83ed-e1696065a5d2": "data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster. The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza.",
        "15011b9e-a5c7-438f-9e96-f8b1a5432c07": "If a startup-hook.sh file exists in the top level of your model definition directory, this file is automatically run with every Docker container startup. This occurs before any Python interpreters are launched or deep learning operations are performed. The startup hook can be used to customize the container environment, install additional dependencies, and download data sets among other shell script commands. Startup hooks are not cached and run before the start of every workload, so expensive or long-running operations in a startup hook can result in poor performance. This example startup hook installs the wget utility and the pandas Python package: apt-get update && apt-get install -y wget python3 -m pip install pandas This Iris example contains a TensorFlow Keras model that uses a startup hook to install an additional Python dependency.",
        "10b4f892-64b8-45f0-8c5f-1f08c7eb1f05": "The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage.",
        "29bff331-071c-4290-8376-29a84877f793": "data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage. minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation.",
        "0b23b381-e119-4ff1-a446-3e362d34a89b": "As described in the Deploy on Kubernetes guide, when tasks (e.g., experiments, notebooks) are started in a Determined cluster running on Kubernetes, the Determined master launches pods to execute these tasks. The Determined helm chart makes it possible to set default pod specs for all CPU and GPU tasks. The defaults can be defined in values.yaml under taskContainerDefaults.cpuPodSpec and taskContainerDefaults.gpuPodSpec. For examples of how to do this and a description of permissible fields, see the specifying custom pod specs guide.",
        "aeb9e64f-b315-4549-849f-1eb7c268de75": "The following example returns models registered in Determined as a list of Model objects. Models can be sorted by name, description, creation time, and last updated time. Additionally, models can be filtered by name or description via the Python SDK. For sorting and ordering options, see ModelSortBy and ModelOrderBy respectively. from determined.experimental import Determined, ModelOrderBy d = Determined() all_models = d.get_models() chronological_sort = d.get_models(sort_by=ModelSortBy.CREATION_TIME) # Find all models with \"mnist\" in their name. Some possible model names # are \"mnist_pytorch\", \"mnist_cnn\", \"mnist\", etc. mnist_models = d.get_models(name=\"mnist\") # Find all models whose description contains \"ocr\". ocr_models = d.get_models(description=\"ocr\") Similarly, you can list models from the CLI using the following command. det model list --sort-by={name,description,creation_time,last_updated_time} --order-by={asc,desc} The following snippet queries for a single model by name. from determined.experimental import Determined model = Determined().get_model(\"model_name\") The CLI equivalent is below. The describe command will print information about the latest version of the model by default as well. det model describe <model_name>",
        "4b7ea1c2-367c-45e2-a766-47d6dd44de93": "API reference Custom Searcher Reference Determined supports defining your own hyperparameter search algorithms and provides search runner utilities for executing them. Remember that a Determined experiment is a set of trials, each corresponding to a point in the hyperparameter space. To implement a custom hyperparameter tuning algorithm, subclass SearchMethod, overriding its event handler methods. If you want to achieve fault tolerance and your search method carries any state in addition to the SearcherState passed into the event handlers, also override save_method_state() and load_method_state(). To run the custom hyperparameter tuning algorithm, you can use: LocalSearchRunner to run on your machine, RemoteSearchRunner to run on a Determined cluster. Using RemoteSearchRunner will create two experiments, with one orchestrating the hyperparameter search of the other. Both search runners execute the custom hyperparameter tuning algorithm and start a multi-trial experiment on a Determined cluster. The following sections explain the steps to take in order to implement and use a custom hyperparameter search algorithm. A detailed example can be found in asha_search_method.tgz.",
        "b92687d2-ee8f-4253-b754-7b9c432c59a4": "In this tutorial, you\u2019ll learn how to port an existing PyTorch model to Determined. We will port a simple image classification model for the MNIST dataset. This tutorial is based on the official PyTorch MNIST example.",
        "5c86d74e-4ad6-4c9a-bf4e-598a3e68ac20": "By default, the Helm chart will deploy a load-balancer which makes the Determined master accessible over HTTP. To secure your cluster, Determined supports configuring TLS encryption which can be configured to terminate inside a load-balancer or inside the Determined master itself. To configure TLS, set useNodePortForMaster to true. This will instruct Determined to deploy a NodePort service for the master. You can then configure an Ingress that performs TLS termination in the load balancer and forwards plain text to the NodePort service, or forwards TLS encrypted data. Please note when configuring an Ingress that you need to have an Ingress controller running your cluster. TLS termination in a load-balancer (e.g., nginx). This option will provide TLS encryption between the client and the load-balancer, with all communication inside the cluster performed via HTTP. To configure this option set useNodePortForMaster to true and then configure an Ingress service to perform TLS termination and forward the plain text traffic to the Determined master. TLS termination in the Determined master. This option will provide TLS encryption inside the Kubernetes cluster. All communication with the master will be encrypted. Communication between task containers (distributed training) will not be encrypted. To configure this option create a Kubernetes TLS secret within the namespace where Determined is being installed and set tlsSecret to be the name of this secret. You also need to set useNodePortForMaster to true. After the NodePort service is created, you can configure an Ingress to forward TLS encrypted data to the NodePort service. An example of how to configure an Ingress, which will perform TLS termination in the load-balancer by default: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: determined-ingress annotations: kubernetes.io/ingress.class: \"nginx\" # Uncommenting this option instructs the created load-balancer # to forward TLS encrypted data to the NodePort service and # perform TLS termination in the Determined master. In order # to configure ssl-passthrough, your nginx ingress controller # must be running with the --enable-ssl-passthrough option enabled. # # nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" spec: tls: - hosts: - your-hostname-for-determined.ai secretName: your-tls-secret-name rules: - host: your-hostname-for-determined.ai http: paths: - path: / backend: serviceName: determined-master-service-<name for your deployment> servicePort: masterPort configured in values.yaml To see information about using AWS Load Balancer instead of nginx visit Using AWS Load Balancer.",
        "e4961f0f-26de-43a8-b46f-3a2c8ee853c8": "Enable Determined to submit jobs to a Slurm cluster. This method is only available on Determined Enterprise Edition. Deploy on Slurm/PBS",
        "6a1a91cf-ca0c-4a74-a3fe-3b8da3134888": "Using whylogs with MLflow Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Feast Provides a brief overview of Feast in HPE Ezmeral Unified Analytics Software . Kubeflow Provides a brief overview of Kubeflow in HPE Ezmeral Unified Analytics Software . HPE Machine Learning Development Environment Provides a brief overview of HPE Machine Learning Development Environment ( HPE MLDE ) in HPE Ezmeral Unified Analytics Software . MLflow Provides a brief overview of MLflow in HPE Ezmeral Unified Analytics Software . Defining RBACs on MLflow Experiments Describes role-based access controls (RBACs) with respect to MLflow in HPE Ezmeral Unified Analytics Software and how to define RBACs     to permit access to experiments in MLflow. Using whylogs with MLflow Describes how to use whylogs with MLflow. Ray Provides a brief overview of Ray in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Using whylogs with MLflow Describes how to use whylogs with MLflow. Prerequisites Sign in to HPE Ezmeral Unified Analytics Software as a\n      member. About this task In HPE Ezmeral Unified Analytics Software , whylogs is integrated with MLflow to log and\n        analyze the data quality. You can use whylogs to analyze the data quality throughout the\n        machine learning lifecycle. To use whylogs with MLflow, refer to MLflow logging example in the GitHub. The basic steps are outlined as\n          follows: Create a notebook or import the notebook into HPE Ezmeral Unified Analytics Software .\n            See Creating and Managing Notebook Servers . Import the required libraries and modules from whylogs. Train a model and create data frames to profile the data, and then run the\n            notebook. Once you finish running your notebook, navigate back to the HPE Ezmeral Unified Analytics Software home screen. Click the Tools & Frameworks icon on the left navigation\n            bar. Navigate to the MLflow tile under the Data\n              Science tab and click Open . View the whylogs output in the whylogs directory within that run\u2019s\n            artifacts in the MLflow UI. Results You can analyze the data quality metrics and ensure the data quality by using whylogs\n        ouput. Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "cd73e8c0-42fb-4fb9-8963-9e7839eddbb3": "Network interface to set for the Determined agent instances. public_ip: Whether to use public IP addresses for the Determined agents. See Set up Internet Access for instructions on whether a public IP should be used. Defaults to true. security_group_id: The ID of the security group that will be used to run the Determined agents. This should be the security group you identified or created in Set up Internet Access. Defaults to the default security group of the specified VPC. subnet_id: The ID of the subnet to run the Determined agents in. Defaults to the default subnet of the default VPC.",
        "f97b2b6d-5359-4950-8639-fa110d5b22c1": "Release Date: June 5, 2020 Add end of training callback to EstimatorTrial.",
        "2bd57364-512d-4dab-bb3d-e7a67f9ff709": "Similarly to a corresponding AWS feature, advanced users who require a deep customization of master settings (i.e., the master.yaml config file) can use the master.yaml templating feature. Since det deploy gcp fills in plenty of infrastructure-related values such as subnetwork ids or boot disk images, we provide a simplified templating solution, similar to helm charts in kubernetes. Template language is based on golang templates, and includes sprig helper library and toYaml serialization helper. Example workflow: Get the default template using det deploy gcp dump-master-config-template > /path/to/master.yaml.tmpl Customize the template as you see fit by editing it in any text editor. For example, let\u2019s say a user wants to utilize (default) 4-GPU instances for the default compute pool, but they also often run single-GPU notebook jobs, for which a single-GPU instance would be perfect. So, you want to add a third pool compute-pool-solo with a customized instance type. Start with the default template, and find the resource_pools section: resource_pools: - pool_name: aux-pool max_aux_containers_per_agent: {{ .resource_pools.pools.aux_pool.max_aux_containers_per_agent }} provider: instance_type: {{- toYaml .resource_pools.pools.aux_pool.instance_type | nindent 8 }} {{- toYaml .resource_pools.gcp | nindent 6}} - pool_name: compute-pool max_aux_containers_per_agent: 0 provider: instance_type: {{- toYaml .resource_pools.pools.compute_pool.instance_type | nindent 8 }} cpu_slots_allowed: true {{- toYaml .resource_pools.gcp | nindent 6}}: Then, append a new section: - pool_name: compute-pool-solo max_aux_containers_per_agent: 0 provider: instance_type: machine_type: n1-standard-4 gpu_type: nvidia-tesla-t4 gpu_num: 1 preemptible: false {{- toYaml .resource_pools.gcp | nindent 6}} Use the new template: det deploy gcp <ALL PREVIOUSLY USED FLAGS> --master-config-template-path /path/to/edited/master.yaml.tmpl All set! Check the Cluster page in WebUI to ensure your cluster has 3 resource pools. In case of errors, ssh to the master instance as instructed by det deploy gcp output, and check sudo journalctl -u google-startup-scripts.service, /var/log/cloud-init-output.log, or sudo docker logs determined-master.",
        "0ba7827e-24db-465e-8986-be1596699b49": "This step covers the training and evaluation routine for the standard data parallel model engine and the pipeline parallel engine available in DeepSpeed. After you create the DeepSpeed model engine and data loaders, define the training and evaluation routines for the DeepSpeedTrial. Differing from PyTorchTrial, train_batch() and evaluate_batch() take an iterator over the corresponding data loader built from build_training_data_loader() and build_validation_dataloader() instead of a batch.",
        "12ecce6d-ee38-40af-bc98-be13befca34b": "Release Date: March 28, 2022 New Features Job queue: Add support for dynamic job modification using the job queue. Users can use the WebUI or CLI to change the priority, weight, resource pool, and queue position of jobs without having to cancel and resubmit them. This feature is currently available for the fair share and priority schedulers. To update jobs through the WebUI, go to the Job Queue section and find the Manage Job option for a job. To update jobs using the CLI, use the det job update command. Run det job update --help for more information. Breaking Changes API: Remove these legacy endpoints: /:experiment_id /:experiment_id/checkpoints /:experiment_id/config /:experiment_id/summary /:experiment_id/metrics/summary /:trial_id/details /:trial_id/metrics The data from those endpoints are still available through the new REST API endpoints under the /api/v1/experiments/:experiment_id and /api/v1/trials/:trial\u1d62d prefixes. Improvements Images: Update default environment images to PyTorch 1.10.2, TensorFlow 2.8, and Horovod 0.24.2. Bug Fixes Database migrations: Ensure that migrations run in transactions. The lack of transactional migrations surfaced as a bug where, if the master was restarted during a migration, it would attempt to rerun the migration when it was already partially or wholly applied (but not marked as complete), resulting in various SQL errors on non-idempotent DDL statements. Distributed training: Allow multiple ranks within a distributed training job to report invalid hyperparameter exits. Previously, if more than one report was received, the experiment would fail.",
        "648c6097-c160-4c15-8ffb-bfa7c2d80836": "Scalars generally behave naturally: null, true, 2.718, and \"foo\" all have the same meanings that they would in JSON (and many programming languages). However, YAML allows strings to be unquoted: foo is the same as \"foo\". This behavior is often convenient, but it can lead to unexpected behavior when small edits to a value change its type. For example, the following YAML block represents a list containing several values whose types are listed in the comments: - true # Boolean - grue # string - 0.0 # number - 0.0. # string - foo: bar # map - foo:bar # string - foo bar # string",
        "9427cd1d-3bbe-4b83-b9b4-be2d19b6f8df": "Setting a Volume Quota Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Editing a Volume Edit accountable entity, volume access for accountable entity and volume hard         quota. Setting a Volume Quota Set space quota for volume. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Creating a Standard Volume Procedure to create standard volume. Creating a Mirror Volume Procedure to create mirror volume. Converting Standard Volume to Mirror Volume Editing a Volume Edit accountable entity, volume access for accountable entity and volume hard         quota. Setting a Volume Quota Set space quota for volume. Configuring Data Access Control for Volume Configuring Volume Administration Settings Configure volume access control for various user types. Renaming a Volume Rename a volume. Viewing Volume Endpoint Info View volume endpoint information. Viewing Object Endpoint Info to Remotely Access Files as Objects View endpoint information for files in a volume to be able to access the files as         objects when accessed by S3 client. Downloading Volume Endpoint Information Download JSON file containing endpoint information for the selected volume endpoint         information. Deleting a Volume Delete a single volume. Administering Volume Snapshots Snapshot overview and administering snapshots. Data Tiering Conceptual information about data tiering. Mirroring Synopsis of mirrors and mirroring process. Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Setting a Volume Quota Set space quota for volume. Prerequisites You must have be a fabric user to configure or set volume quota. About this task You can set hard quota for volume via the Data Fabric UI. NOTE: It is recommended to set advisory quota for a volume. See Setting advisory quota via CLI to set\n                    advisory quota. An alarm is set off when advisory quota is reached or exceeded. Use the following steps to set the volume quota. Procedure Log on to the Data Fabric UI . Select the Fabric user on the Home\n                    page. Click the Table View icon on the Resources card. In the tabular list of\n                    fabrics, click the down arrow for the fabric that contains the volume to set the\n                    quota for. Click the volume name seen under Resource Name . Navigate to the Settings tab. Click the Edit icon seen next to Quota . Enter the value and select the unit of measurement for the quota. Click Save . Results The hard quota for the volume is set. Data on the volume\n                cannot occupy space higher than the quota. If the data on the volume exceeds the\n                hard quota, an alarm is raised. (Topic last modified: 2023-11-02) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "6fc19e12-19ef-451a-b84e-4703a2727bb6": "Using Spark OSS Images Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Spark Provides a brief overview of Apache Spark in HPE Ezmeral Unified Analytics Software . Using Spark Images Describes different types of Spark images supported by HPE Ezmeral Unified Analytics Software . Using HPE-Curated Spark Images Describes how to use HPE-curated Spark images to submit     Spark applications. Using Spark OSS Images Describes how to use Spark Open-Source Software (OSS) images to submit Spark     applications. Using Your Own Open-Source Spark Images Describes how to use your own open-source Spark images to submit Spark     applications. Setting the User Context Describes how to set the user context when using the     Spark OSS     images. List of Spark Images Lists the Spark images distributed by HPE Ezmeral Unified Analytics Software . These         images enables you to run the Spark applications in an air-gapped environment. Creating Spark Applications Describes how to create Spark applications using HPE Ezmeral Unified Analytics Software . Managing Spark Applications Describes how to view and manage Spark applications using HPE Ezmeral Unified Analytics Software . Configuring Memory for Spark Applications Describes how to set memory options for Spark applications. Creating Interactive Sessions Describes how to create interactive sessions in HPE Ezmeral Unified Analytics Software . Submitting Statements Describes how to submit statements in HPE Ezmeral Unified Analytics Software . Managing Interactive Sessions Describes how to view and manage Spark interactive sessions in HPE Ezmeral Unified Analytics Software . Spark History Server Provides an overview of Spark History Server. Using Spark SQL API Describes how to use Spark SQL API in HPE Ezmeral Unified Analytics Software . Enabling GPU Support for Spark Describes NVIDIA spark-rapids accelerator support for Spark, and how     to enable and allocate the GPU resources on Spark. Securely Passing Spark Configuration Values Describes how to pass the sensitive data to Spark configuration using the Kubernetes     Secret. Running Spark Applications in Namespaces Describes how namespaces work with regard to Spark applications in HPE Ezmeral Unified Analytics Software . Using whylogs with Spark Describes how to use whylogs with Spark. Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Using Spark OSS Images Describes how to use Spark Open-Source Software (OSS) images to submit Spark\n    applications. Spark OSS are Apache Spark images that do not\n      support Data Fabric filesystem, Data Fabric Streams, and any other Data Fabric sources and\n      sinks that require a Data Fabric client. These Spark images also do not support Data\n      Fabric-specific security features (data-fabric SASL ( maprsasl )). You can use Spark OSS images with two different\n      workflows as follows: Spark Operator workflow using the Create Spark Application GUI.\n          See Using the Create Spark Application GUI . Spark Operator workflow using Airflow. See Using Airflow . Using the Create Spark Application GUI To use Spark OSS images, choose one of the\n        following option in the GUI: Using Upload YAML in GUI Select the Spark OSS image from the List of Spark Images . Configure your Spark YAML file with the Spark OSS image. image: gcr.io/mapr-252711/apache-spark:<image-tag> To set the logged-in user\u2019s context, add the following configuration in the sparkConf section. spark.hpe.webhook.security.context.autoconfigure: \"true\" To\n                    learn more about user context, see Setting the User Context . Perform the instructions to create a Spark application as described in Creating Spark Applications until you reach the Application\n                      Details step. In the Application Details step, choose the Upload YAML option. Click Select File and, browse and upload the YAML\n                    file. To specify the details for other boxes or options in the Application Details step and to complete creating the\n                    Spark application, see Creating Spark Applications . Using New application in GUI Perform the instructions to create a Spark application as described in Creating Spark Applications until you reach the Review step. To open an editor to change the application configuration using YAML in the\n                    GUI, click Edit YAML .",
        "e1c2d986-16ca-49ac-a1b2-7757c8d58ab1": "data-fabric user Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication.",
        "609ca34d-5390-4890-9c9a-9871c8905db0": "re-replication Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication.",
        "beecc191-7bf2-40b9-a5e9-275921b94cdc": "When configuring your IdP to allow users to SSO to Determined, you will need to specify the location of Determined\u2019s callback URL. This is the URL to which users will be redirected after authentication. The callback URL should be set to the Determined master\u2019s base URL with a path of /oidc/callback.",
        "906267a0-1d64-4072-b673-95d09b639581": "The right side, displays the usage data. In HPE Ezmeral Unified Analytics Software , GPU usage is metered per\n                application and each vGPU counts as an individual GPU for metering. GPU metrics used for metering is DCGM_FI_PROF_GR_ENGINE_ACTIVE\u200b . GPU metrics sampling interval is every five seconds\u200b. The hourly usage is the average GPU utilization over a one hour\n                period\u200b. For example: With 7 small vGPUs and 4 applications using 6 vGPUs as\n                  follows: Applications Pods -vGPU Avg 1hr per vGPU Avg 1hr utilization per application App1 Pods11 - vGPU0 0.8 1.3 Pods12 - vGPU1 0.5 App2 Pods21 - vGPU2 0.8 0.8 App3 Pods31 - vGPU3 0.5 0.5 App4 Pods41 - vGPU4 0 0.5 Pods42 - vGPU5 0.5 Total GPU utilization for billing record is \u200b 1.3 + 0.8 + 0.5 + 0.5 = 3.1\n                vGPU-hour\u200b Top Frameworks provides the list of monthly aggregated usage\n                charges data for all applications in HPE Ezmeral Unified Analytics Software . The total usage for billing is aggregated for all included and imported\n                applications. Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "c95c30da-bc56-4a5b-b2e6-da6db3292cbb": "minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation. When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system.",
        "35178e46-ad37-42ba-ab4b-aa8b2cc8d9a4": "Install Determined on a single machine, for your own use. Compatible with Windows, Mac, and Linux. Ideal for getting started with Determined. Quick Installation Install Determined Using det deploy Install Determined Using Homebrew (macOS) Install Determined Using Windows Subsystem for Linux (Windows)",
        "c6b4d4f8-cf4e-418e-a225-d351680e4dbc": "We build WebUI visualizations for a quick snapshot of the historical cluster usage:",
        "71359944-ae21-46ab-8779-ebe706609e31": "The resources section defines the resources that an experiment is allowed to use.",
        "57494e20-97b3-4653-b3e8-852c42cf85d5": "1. Add an Activation Key\n2. Register a New Air-Gapped Fabric\n3. Collect Usage Records\n4. Send the Usage Record File to HPE\n5. Pay Your Monthly Invoice\n6. Renew Your Activation\nBilling in Air-Gapped Environments Jump to main content Get Started Platform Administration Reference Home Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. Billing in Air-Gapped Environments Describes how billing is enabled in an air-gapped environment. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Release Notes These notes contain information about release 7.6.0 of the HPE Ezmeral Data Fabric as-a-service platform. Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. Adding an Activation Key You must add an activation key after installing the HPE Ezmeral Data Fabric or adding a new fabric. Registering a Fabric You register the HPE Ezmeral Data Fabric after installing the     fabric or adding a new fabric. Registration provides HPE with information about your internet     connection and determines the billing process that you will use. Setting the Billing Model Setting the billing model enables the Data Fabric UI to display estimated billing charges for each fabric. Viewing Activation Information Use the Data Fabric UI to view important activation     information, such as the status of your activation key and activation code (for air-gapped     installations). Displaying the Fabric ID Describes how to display the fabric ID. Obtaining a License Describes the process of obtaining a consumption-based license from the My HPE Software     Center. Billing in Connected Environments Describes how billing is enabled in a connected environment. Billing in Air-Gapped Environments Describes how billing is enabled in an air-gapped environment. Restoring a Disabled Fabric Describes how to obtain an activation key to restore a disabled fabric. Displaying a maprcli Prompt You can use maprcli commands to register the fabric and perform     certain configuration tasks. The steps for displaying a maprcli prompt are the     same for all cloud-based deployments but are different for on-premises deployments. SSO Using Keycloak Describes how single sign-on (SSO) is implemented by using       Keycloak. Setting Up Clients Summarizes the steps for enabling client communication with the HPE Ezmeral Data Fabric . Upgrade This section contains information that describes how to upgrade the HPE Ezmeral Data Fabric as-a-service platform. User Assistance Describes how to access different resources that can help you learn how to use the HPE Ezmeral Data Fabric . Billing in Air-Gapped Environments Describes how billing is enabled in an air-gapped environment. In an air-gapped environment, manual steps are needed to support billing and activation for a\n      consumption-based fabric. This section describes how to activate an air-gapped fabric and keep\n      the fabric operational. Using maprcli Commands in an Air-Gapped environment Some tasks for keeping an air-gapped fabric operational require you to use maprcli commands. This is because certain operations are not currently\n        available in the Data Fabric UI . The maprcli commands you need are provided on this page. To run maprcli commands, use an ssh connection to any\n        node in the fabric. Understanding the Activation Code and Billing Cycle When you place an order for the HPE Ezmeral Data Fabric and specify an\n        air-gapped environment, HPE provides you with an activation code. The activation code allows\n        you to register the product and sign usage records for one billing cycle. The billing cycle\n        is one month with a 15-day grace period. The activation code has two important dates: Start Date \u2013 The first day of the one-month billing cycle. End Date \u2013 The end of the month-long billing cycle and the start of the grace\n            period. This date is usually 30 days after the Start Date. You can view these dates by using the Data Fabric UI : Sign in to the Data Fabric UI , and switch to the Fabric manager view. Click Fabric administration . Locate the Activation card. The Activation\n              code section of the card displays the start date, end date, and current\n            month charges. Once the activation code is applied, the code is valid (and the fabric is operational)\n        until the End Date .",
        "1f1cc489-ea48-45fb-a24e-fa0be09161aa": "Obtaining a License Jump to main content Get Started Platform Administration Reference Home Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. Obtaining a License Describes the process of obtaining a consumption-based license from the My HPE Software     Center. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Release Notes These notes contain information about release 7.6.0 of the HPE Ezmeral Data Fabric as-a-service platform. Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. Adding an Activation Key You must add an activation key after installing the HPE Ezmeral Data Fabric or adding a new fabric. Registering a Fabric You register the HPE Ezmeral Data Fabric after installing the     fabric or adding a new fabric. Registration provides HPE with information about your internet     connection and determines the billing process that you will use. Setting the Billing Model Setting the billing model enables the Data Fabric UI to display estimated billing charges for each fabric. Viewing Activation Information Use the Data Fabric UI to view important activation     information, such as the status of your activation key and activation code (for air-gapped     installations). Displaying the Fabric ID Describes how to display the fabric ID. Obtaining a License Describes the process of obtaining a consumption-based license from the My HPE Software     Center. Billing in Connected Environments Describes how billing is enabled in a connected environment. Billing in Air-Gapped Environments Describes how billing is enabled in an air-gapped environment. Restoring a Disabled Fabric Describes how to obtain an activation key to restore a disabled fabric. Displaying a maprcli Prompt You can use maprcli commands to register the fabric and perform     certain configuration tasks. The steps for displaying a maprcli prompt are the     same for all cloud-based deployments but are different for on-premises deployments. SSO Using Keycloak Describes how single sign-on (SSO) is implemented by using       Keycloak. Setting Up Clients Summarizes the steps for enabling client communication with the HPE Ezmeral Data Fabric . Upgrade This section contains information that describes how to upgrade the HPE Ezmeral Data Fabric as-a-service platform. User Assistance Describes how to access different resources that can help you learn how to use the HPE Ezmeral Data Fabric . Obtaining a License Describes the process of obtaining a consumption-based license from the My HPE Software\n    Center. To obtain a license: For new deployments, install the fabric as described in Installation . Or, for existing deployments, create a new fabric as described in Creating a Fabric , or import a fabric as described in Importing a Fabric . Note the ID of the new fabric. To obtain a license, you must supply the fabric ID. See Displaying the Fabric ID . After purchasing HPE Ezmeral Data Fabric software, a license key is made available to\n          you through the Access Your Products button in the HPE\n            Subscription Electronic Receipt email that you receive from HPE. This\n          receipt will direct you to MY HPE SOFTWARE CENTER where you can activate your product Log in to the MY HPE SOFTWARE CENTER with your HPE Passport user ID and\n          password. You should see an Activate EON: page. In the Qty to Activate field, specify 1. Click Confirm Selection . In Step 2: Designate Activatee, click Next . In Step 3, enter your cluster ID (fabric ID). Click Activate . The activation process can take several minutes\n          to complete. Eventually, the HPE Ezmeral Data Fab SW Base SaaS page is displayed. Click the box to accept the license terms and authorizations. Click the box for Licenses Keys (3) . Click Download . The licenses are downloaded as .DAT files. You can now add an activation key to your fabric. See Adding an Activation Key . (Topic last modified: 2023-07-23) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "8ad7f83e-02ba-43b2-9cbe-0168908f6a2b": "Certificate file to use for serving TLS.",
        "f884b9a7-16e7-422d-a576-a5ffff897fd3": "data-access gateway Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster. The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs.",
        "63e0adfa-d702-472e-8a4a-7296875a0339": "Required. The name of the validation metric used to evaluate the performance of a hyperparameter configuration.",
        "9cb68f5b-5b44-4ee5-8d80-b4f016b8a393": "Another way to store data is to use a distributed file system, which enables a cluster of machines to access a shared data set via the familiar POSIX file system interface. Amazon\u2019s Elastic File System and Google\u2019s Cloud Filestore are examples of distributed file systems that are available in cloud environments. For on-premise deployments, popular distributed file systems include Ceph, GlusterFS, and NFS. To access data on a distributed file system, you should first ensure that the file system is mounted at the same mount point on every Determined agent. For cloud deployments, this can be done by configuring provisioner.startup_script in master.yaml to point to a script that mounts the distributed file system. An example of how to do this on GCP can be found here. Next, you will need to ensure the file system is accessible to each trial container. This can be done by configuring a bind mount in the experiment configuration file. Each bind mount consists of a host_path and a container_path; the host path specifies the absolute path where the distributed file system has been mounted on the agent, while the container path specifies the path within the container\u2019s file system where the distributed file system will be accessible. To avoid confusion, you may wish to set the container_path to be equal to the host_path. You may also want to set read_only to true for each bind mount, to ensure that data sets are not modified by training code. The following example assumes a Determined cluster is configured with a distributed file system mounted at /mnt/data on each agent. To access data on this file system, we use an experiment configuration file as follows: bind_mounts: - host_path: /mnt/data container_path: /mnt/data read_only: true Our model definition code can then access data in the /mnt/data directory as follows: def build_training_data_loader(self): return make_data_loader(data_path=\"/mnt/data/training\", ...) def build_validation_data_loader(self): return make_data_loader(data_path=\"/mnt/data/validation\", ...)",
        "df2f9d55-1c8c-4e96-88cb-8d1462cb650a": "Release Date: November 14, 2022 New Features WebUI: Adds support for creating and managing webhooks to enable receiving updates regarding experiment state changes. Checkpoint storage can now be configured at a workspace level. Experiments created in projects will now inherit checkpoint storage configuration from the project\u2019s workspace if set. Experiment configuration can override the workspace level checkpoint storage configuration. Example: Textual Inversion training and generation using Stable Diffusion with Core API and Hugging Face\u2019s Diffusers. Python SDK now supports reading logs from trials, via the new logs() method. Additionally, the Python SDK also supports a new blocking call on an experiment to get the first trial created for an experiment via the await_first_trial() method. Users who have been writing automation around the det e create --follow-first-trial CLI command may now use the Python SDK instead, by combining .await_first_trial() and .logs(). RBAC: the enterprise edition of Determined (HPE Machine Learning Development Environment) has added preliminary support for Role-Based Access Control. Administrators can now configure which users or user groups can administer users, create or configure workspaces, run or view experiments in particular workspaces, or perform other actions. See RBAC for more information. Bug Fixes Master: Correctly handle pending allocations in historical resource allocation aggregation.",
        "778aa07b-7328-4679-9caa-ef8b103fa22c": "For more information, see data-fabric user . (Topic last modified: 2020-07-21) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "3f453cfa-7872-4a24-93fa-a03411ee7779": "This guide covers the limitations of the default logging backend, as a guideline on when to migrate to Elasticsearch, and some tips for how to tune Elasticsearch to work best with Determined. Elasticsearch is a search engine commonly used for storing application logs for search and analytics. Determined supports using Elasticsearch as the storage backend for task logs. Configuring Determined to use Elasticsearch is simple; however, managing an Elasticsearch cluster at scale is an involved task, so this guide is recommended for users who have hit the limitations of the default logging backend. Using the default logging backend, with a standard deployment using det deploy, the cluster can ingest logs about as fast as Postgres can persist them. For example, with det deploy aws using Aurora Serverless with 2 capacity units, ingestion speed maxes out around 10-15 MB/s (where the database\u2019s CPU hits ~90%). To get a little more mileage from the default, we recommend increasing the capacity of the database. At a certain point, the master instance itself will become the bottleneck, since it has limited incoming network bandwidth for HTTP requests delivering logs and limited resources to process them. The master instance size can be increased, but vertical scaling is likely to be limited to a log throughput of around hundreds of megabytes per second; we recommend moving to Elasticsearch to get past that limit. Determined offers some additional recommendations for the Elasticsearch cluster configuration based on how the cluster will be used: Tune the default shards per index to your expected throughput (or use index templates). Determined ships logs in Logstash format rolling over to a new index each day. Depending on your log volume, the default number of shards could be too high or too low. The general rule of thumb is not to exceed 50 GB per shard while minimizing the number of shards per index. For high-utilization clusters, this may entail increasing the shards per index and rotating indices older than a few months out of the cluster periodically, to avoid the overhead accumulated from having too many shards. A more in-depth guide can be found here. Though it may increase latency for end users, increasing the refresh interval may help increase total throughput. Apply the following index template to optimize the mappings in Determined log indices for ingest speed. This turns off analysis and in some cases indexing on properties for which Determined does not use these features. { \"index_patterns\": [\"determined-tasklogs-*\"], \"mappings\": { \"properties\": { \"task_id\": {\"type\": \"keyword\", \"index\": true}, \"allocation_id\": {\"type\": \"keyword\": \"index\": true}, \"agent_id\": {\"type\": \"keyword\", \"index\": true}, \"container_id\": {\"type\": \"keyword\", \"index\": true}, \"level\": {\"type\": \"keyword\", \"index\": true}, \"log\": {\"type\": \"text\", \"index\": false}, \"message\": {\"type\": \"text\", \"index\": false}, \"source\": {\"type\": \"keyword\", \"index\": true}, \"stdtype\": {\"type\": \"keyword\", \"index\": true} } } } The configuration settings to enable Elasticsearch as the task log backend are described in the cluster configuration reference.",
        "24d9b223-83a5-4fa2-b525-2f33364ab7f7": "Local Mirroring Remote Mirroring Starting Volume Mirroring Start mirroring of data on a mirror volume. Stopping Volume Mirroring Stop mirroring of data that is in progress on a mirror volume. Scheduling Volume Mirroring (Topic last modified: 2023-07-28) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "5b07b4d0-8090-42c9-855e-7ee0d14d12f6": "4\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/reduxjs/redux-devtools\",\n    \"licenseUrl\": \"https://github.com/reduxjs/redux-devtools/raw/main/LICENSE.md\",\n\n-----------------------------------------------------------\n\n\"css-toggle-switch@4.1.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/ghinda/css-toggle-switch\",\n    \"licenseUrl\": \"https://github.com/ghinda/css-toggle-switch/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"dompurify@2.3.8\"\n\n    \"licenses\": \"MPL-2.0 OR Apache-2.0\",\n    \"repository\": \"https://github.com/cure53/DOMPurify\",\n    \"licenseUrl\": \"https://github.com/cure53/DOMPurify/raw/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-copy-to-clipboard@5.0.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/nkbt/react-copy-to-clipboard\",\n    \"licenseUrl\": \"https://github.com/nkbt/react-copy-to-clipboard/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-duallist@1.1.6\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/jyotirmaybanerjee/react-duallist\",\n    \"licenseUrl\": \"https://github.com/jyotirmaybanerjee/react-duallist/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"redux-devtools-extension@2.13.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/zalmoxisus/redux-devtools-extension\",\n    \"licenseUrl\": \"https://github.com/zalmoxisus/redux-devtools-extension/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"axios-mock-adapter@1.19.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/ctimmerm/axios-mock-adapter\",\n    \"licenseUrl\": \"https://github.com/ctimmerm/axios-mock-adapter/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"axios@0.21.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/axios/axios\",\n    \"licenseUrl\": \"https://github.com/axios/axios/raw/v0.x/LICENSE\",\n\n-----------------------------------------------------------\n\n\"axios@0.27.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/axios/axios\",\n    \"licenseUrl\": \"https://github.com/axios/axios/raw/v0.x/LICENSE\",\n\n-----------------------------------------------------------\n\n\"codemirror@5.62.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/codemirror/basic-setup\",\n    \"licenseUrl\": \"https://github.com/codemirror/basic-setup/raw/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"codemirror@5.65.12\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/codemirror/basic-setup\",\n    \"licenseUrl\": \"https://github.com/codemirror/basic-setup/raw/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"grommet-icons@4.9.0\"\n\n    \"licenses\": \"Apache-2.0\",\n    \"repository\": \"https://github.com/grommet/grommet-icons\",\n    \"licenseUrl\": \"https://github.com/grommet/grommet-icons/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"grommet-icons@4.10.0\"\n\n    \"licenses\": \"Apache-2.0\",\n    \"repository\": \"https://github.com/grommet/grommet-icons\",\n    \"licenseUrl\": \"https://github.com/grommet/grommet-icons/raw/master/LICENSE\",\n-----------------------------------------------------------\n\n\"grommet@2.25.1\"\n\n    \"licenses\": \"Apache-2.0\",\n    \"repository\": \"https://github.com/grommet/grommet\",\n    \"licenseUrl\": \"https://github.com/grommet/grommet/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"grommet@2.31.0\"\n\n    \"licenses\": \"Apache-2.0\",\n    \"repository\": \"https://github.com/grommet/grommet\",\n    \"licenseUrl\": \"https://github.com/grommet/grommet/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"highcharts-react-official@3.0.0\"\n\n    \"licenses\": \"https://github.com/highcharts/highcharts-react/raw/master/LICENSE\",\n    \"repository\": \"https://github.com/highcharts/highcharts-react\",",
        "a0e06fd1-cbec-4dfc-8b52-c566d9e81b44": "When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. gateway node A node on which a mapr-gateway is installed. A gateway node is by\n            definition a data-fabric cluster node.",
        "f1035dc0-6d06-4a53-91a5-f764c8f5cda0": "quota Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster.",
        "35938174-d478-4cd0-bd9a-5c47894776ce": "The launcher installation package supports the verification of both RPM and DEB packages. There will be several configuration files that the package manager will identify as modified, and with RPM-based installs, some files will show user/group modifications. For an RPM-based installation, run sudo rpm -V hpe-hpc-launcher which should produce output similar to that shown below: S.5....T. c /etc/launcher/launcher.conf S.5....T. /etc/launcher/suid.conf S.5....T. /etc/sudoers.d/zz_launcher .....U... /opt/launcher/bin/capsules-dev-keytool.jar .....U... /opt/launcher/bin/dev-keytool .....U... /opt/launcher/bin/user-keytool .....U... /opt/launcher/jetty/base/etc/keystore S.5....T. /opt/launcher/jetty/base/resources/dispatcher.properties .....U... /opt/launcher/sbin ......G.. /opt/launcher/sbin/suid INFO: The following file modifications are expected: /etc/launcher/launcher.conf /etc/launcher/suid.conf /etc/sudoers.d/zz_launcher /opt/launcher/jetty/base/resources/dispatcher.properties INFO: The following file owner/group changes are expected: /opt/launcher/bin/capsules-dev-keytool.jar /opt/launcher/bin/dev-keytool /opt/launcher/bin/user-keytool /opt/launcher/sbin /opt/launcher/sbin/suid On Debian distributions, run sudo dpkg -V hpe-hpc-launcher which should produce output similar to that shown below: ??5?????? c /etc/launcher/launcher.conf ??5?????? c /etc/launcher/suid.conf ??5?????? c /etc/sudoers.d/zz_launcher ??5?????? /opt/launcher/jetty/base/resources/dispatcher.properties",
        "dfe6ff9e-79a0-4be3-8575-8be9725fe20d": "Some constraints are due to differences in behavior between Docker and Singularity, summarized here: Singularity tends to explicitly share resources/devices from the host compute node on which it is running which results in more opportunities for conflicts with other programs running on the cluster, or between multiple determined experiments that are launched concurrently on the same compute node. By default /tmp and /dev/shm are mounted from the compute node instead of private to the container. If multiple containers are running on the same node there can be more sharing than they expect. The contents of /tmp persist beyond the container lifetime and are visible to other trials. The experiment configuration might need to be updated to accommodate these issues. Determined mitigates potential file name and disk space conflicts on /tmp content by automatically using space in job_storage_root for a per-job /tmp directory. You can override this behavior by providing an explicit bind mount of the container_path /tmp folder in the Singularity container. You can restore the default Singularity behavior of sharing /tmp on the compute node by including the following bind mount in your experiment configuration or globally by using the task_container_defaults section in your master configuration: bind_mounts: - host_path: /tmp container_path: /tmp The singularity.conf options can also be used to change this behavior, or by using individual environment variables added to your experiment. Here are some configuration options that might be useful to tune sharing available in the singularity.conf file: Option Description sessiondir max size Controls the disk space, in MB, allocated to support directories not shared from the host compute node, such as /tmp and /usr/tmp, depending upon your configuration. mount tmp Isolates /tmp from the host compute node. The size of this area is configured by sessiondir max size. Singularity attempts to automatically download and convert Docker images, however, the behavior is somewhat different than with Docker. By default converted Singularity images are stored per user in ~/.singularity. Determined environment images are relatively large and this can result in excessive duplication. You likely want to predownload images under singularity_image_root as described in Provide a Container Image Cache or configure SINGULARITY_CACHEDIR to point to a shared directory. Some Docker features do not have an exact replacement in Singularity, and therefore the associated Determined features are not supported. Feature Description resources.devices By default /dev is mounted from the compute host, so all devices are available. This can be overridden by the singularity.conf mount dev option. resources.shm_size By default /dev/shm is mounted from the compute host. This can be overridden by the singularity.conf mount tmp option. When enabled, the size can be increased using compute node /etc/fstab settings. environment.registry_auth.server No equivalent setting in Singularity. environment.registry_auth.email No equivalent setting in Singularity.",
        "30929f92-553a-4d77-add0-18d0147b8356": "To modify the job queue in the CLI, use the det job update command. Run det job update --help for more information. Example operations: $ det job update jobID --priority 10 $ det job update jobID --resource-pool a100 $ det job update jobID --ahead-of jobID-2 To update a job in batch, provide updates as shown: $ det job update-batch job1.priority=1 job2.resource-pool=\"compute\" job3.ahead-of=job1 Example workflow: $ det job list # | ID | Type | Job Name | Priority | Submitted | Slots (acquired/needed) | Status | User -----+--------------------------------------+-----------------+--------------------------+------------+---------------------------+--------- 0 | 0d714127 | TYPE_EXPERIMENT | first_job | 42 | 2022-01-01 00:01:00 | 1/1 | STATE_SCHEDULED | user1 1 | 73853c5c | TYPE_EXPERIMENT | second_job | 42 | 2022-01-01 00:01:01 | 0/1 | STATE_QUEUED | user1 $ det job update 73853c5c --ahead-of 0d714127 $ det job list # | ID | Type | Job Name | Priority | Submitted | Slots (acquired/needed) | Status | User -----+--------------------------------------+-----------------+--------------------------+------------+---------------------------+--------- 0 | 73853c5c | TYPE_EXPERIMENT | second_job | 42 | 2022-01-01 00:01:01 | 1/1 | STATE_SCHEDULED | user1 1 | 0d714127 | TYPE_EXPERIMENT | first_job | 42 | 2022-01-01 00:01:00 | 0/1 | STATE_QUEUED | user1 $ det job update-batch 73853c5c.priority=1 0d714127.priority=1 $ det job list # | ID | Type | Job Name | Priority | Submitted | Slots (acquired/needed) | Status | User -----+--------------------------------------+-----------------+--------------------------+------------+---------------------------+--------- 0 | 73853c5c | TYPE_EXPERIMENT | second_job | 1 | 2022-01-01 00:01:01 | 1/1 | STATE_SCHEDULED | user1 1 | 0d714127 | TYPE_EXPERIMENT | first_job | 1 | 2022-01-01 00:01:00 | 0/1 | STATE_QUEUED | user1",
        "30734e2a-ef15-4b5f-b333-8f63905fcad3": "Required. The connection string for the Azure Blob Storage service account to use.",
        "97299f71-7aa4-4d5c-9773-f102b7097ddc": "A trial can be paused and reactivated without losing training progress. Pausing a trial preserves its progress by saving a checkpoint before exiting the cluster. The scheduler can pause a trial to free its resources for another task. Also, you can manually pause an experiment, which pauses all trials in the experiment. This frees the slots used by the trial. When the trial resumes, because more slots become available or because you activate an experiment, the saved checkpoint is loaded and training continues from the saved state.",
        "5f486e0b-a6ca-4e9b-ac4d-921e2fd1c97e": "Managing Imported Tools and Frameworks Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Installation Provides links to HPE Ezmeral Unified Analytics Software installation and service activation topics. Identity and Access Management Describes identity and access management in HPE Ezmeral Unified Analytics Software . Expanding the Cluster Describes how to add additional user-provided hosts to the management cluster to increase resource capacity and how     to expand the cluster to include the additional user-provided hosts. Shutting Down an HPE Ezmeral Unified Analytics Software Cluster Describes how to gracefully shut down an HPE Ezmeral Unified Analytics Software cluster when you want to perform maintenance or upgrade     tasks. Importing Applications and Managing the Application Lifecycle Describes how to import, manage, and secure applications and frameworks in HPE Ezmeral Unified Analytics Software . Importing Applications Describes how to import applications in HPE Ezmeral Unified Analytics Software . SSO Support for Imported Applications Describes SSO support for imported applications integrated with native authentication     and applications configured with authentication proxy. Managing Imported Tools and Frameworks Describes how to configure, delete, and update imported tools and frameworks in HPE Ezmeral Unified Analytics Software . Configuring Included Applications Describes how to configure tools and frameworks included with the HPE Ezmeral Unified Analytics Software installation. Upgrading Included Frameworks Describes how to upgrade tools and frameworks included with the HPE Ezmeral Unified Analytics Software installation. Connecting to External S3 Object Stores Describes how to connect HPE Ezmeral Unified Analytics Software to external S3 object storage in AWS, MinIO, and HPE Ezmeral Data Fabric Object Store . Connecting to External HPE Ezmeral Data Fabric Clusters Describes how to connect HPE Ezmeral Unified Analytics Software to an external HPE Ezmeral Data Fabric cluster. Configuring Endpoints Describes the endpoints in HPE Ezmeral Unified Analytics Software and how to configure them. GPU Support Provides information about support for NVIDIA GPU, MIG partitioning, preparing hosts     for GPU-enabled environment, adding hosts and enabling GPU in HPE Ezmeral Unified Analytics Software . Troubleshooting Describes how to identify and debug issues in HPE Ezmeral Unified Analytics Software . Support Matrix The tables on this page show the tools and frameworks, HPE Ezmeral Data Fabric ,operating system       versions , and GPU models that are supported for HPE Ezmeral Unified Analytics Software releases. Release Notes This document provides a comprehensive overview of the latest updates and enhancements     in HPE Ezmeral Unified Analytics Software (version     1.3.0), including new features, improvements, bug fixes, and known issues. Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Managing Imported Tools and\n    Frameworks Describes how to configure, delete, and update imported tools and frameworks in HPE Ezmeral Unified Analytics Software . Prerequisites An administrator should sign in to HPE Ezmeral Unified Analytics Software to manage applications. About this task You can configure, delete, or update imported applications and frameworks. Tiles for\n        imported tools and frameworks\n        display a yellow Imported label. Procedure In the left navigation bar, click Tools & Frameworks . Click the three-dots on the tile of the application you want to\n          manage. Perform one of the following tasks: Configure Select Configure . In the editor that opens, modify the application values.yaml file. Click Configure to apply the changes or Cancel to discard the changes. Delete To delete the application, select Delete . You can delete imported\n                    applications only. You cannot delete the applications that were installed with HPE Ezmeral Unified Analytics Software . Update ATTENTION You cannot undo the update action. Select Update . This Update option is only\n                      available for imported applications. Browse to the location where the Helm chart is stored and select the Helm\n                      chart. Click Upload . Clicking Upload enables the Upgrade button in the application tile. To upgrade the application, click Upgrade . More information Configuring Included Applications Upgrading Included Frameworks Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "9d354c46-4c15-4606-8655-7754ced8af93": "When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. node An individual server (physical or virtual machine) in a cluster.",
        "71fd215b-1c30-42de-bc7e-e37a78f0f215": "The Determined master and the Determined agents are intended to run in the same project.",
        "66f331c0-24a9-4763-bb41-959e836cdd98": "Required. Specifies running dynamic agents on GCP.",
        "7e6e5300-c746-4d0f-b82c-0965165c053c": "In this guide, you\u2019ll learn how to use the Keras API. Visit the API reference det.keras API Reference This document guides you through training a Keras model in Determined. You need to implement a trial class that inherits TFKerasTrial and specify it as the entrypoint in the experiment configuration. To learn about this API, you can start by reading the trial definitions from the following examples: Fashion MNIST example CIFAR-10 example",
        "36555817-f139-4cd5-b6aa-828fdedcdd7b": "You can also use Determined\u2019s DeepSpeed Autotune with the HuggingFace (HF) Trainer and Determined\u2019s DetCallback callback object to optimize your DeepSpeed parameters. Similar to the previous case (Core API), you need to add a deepspeed_config field to the hyperparameters section of your experiment configuration file, specifying the relative path to the DS json config file. Reporting results back to the Determined master requires both the dsat.dsat_reporting_context context manager and DetCallback. Furthermore, since dsat performs a search over different batch sizes and HuggingFace expects parameters to be specified as command-line arguments, an additional helper function, get_hf_args_with_overwrites(), is needed to create consistent HuggingFace arguments. Here is an example code snippet from a HuggingFace Trainer script that contains key pieces of relevant code: from determined.transformers import DetCallback from determined.pytorch import dsat from transformers import HfArgumentParser,Trainer, TrainingArguments, hparams = self.context.get_hparams() parser = HfArgumentParser(TrainingArguments) args = sys.argv[1:] args = dsat.get_hf_args_with_overwrites(args, hparams) training_args = parser.parse_args_into_dataclasses(args, look_for_args_file=False) det_callback = DetCallback(core_context, ...) trainer = Trainer(args=training_args, ...) with dsat.dsat_reporting_context(core_context, op=det_callback.current_op): train_result = trainer.train(resume_from_checkpoint=checkpoint) The dsat_reporting_context context manager shares the same initial SearcherOperation as the DetCallback instance through its op=det_callback.current_op argument. The entire train method of the HuggingFace trainer is wrapped in the dsat_reporting_context context manager. To find examples that use DeepSpeed Autotune with HuggingFace Trainer, visit the Determined GitHub Repo and navigate to examples/hf_trainer_api.",
        "5e785c55-110b-4337-b9d1-0e94906a04f5": "Optional. An optional GPU type name to be included in the generated Slurm --gpus or --gres option if you have configured GPU types within your Slurm gres configuration. Specify this option to select that specific GPU type when there are multiple GPU types within the Slurm partition. The default is to select GPUs without regard to their type. For example, you can request the tesla GPU type with: slurm: gpu_type: tesla",
        "1bfacc2c-4baa-46f8-8d97-3ac0bb0de9c9": "By default, PyTorch Trial uses Horovod as the backend. You can choose to use torch.distributed and DistributedDataParallel as your distributed backend, by following PyTorch Distributed Launcher.",
        "d20125a1-5577-423d-9ac4-76edb6a1e5de": "The S3 bucket name to use.",
        "04c38740-b39c-4d98-85b4-beb7e3002e68": "\"repository\": \"https://github.com/highcharts/highcharts-react\",\n\n-----------------------------------------------------------\n\n\"react-codemirror2@7.2.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/scniro/react-codemirror2\",\n    \"licenseUrl\": \"https://github.com/scniro/react-codemirror2/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"styled-components@5.3.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/styled-components/styled-components\",\n    \"licenseUrl\": \"https://github.com/styled-components/styled-components/raw/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"styled-components@5.3.9\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/styled-components/styled-components\",\n    \"licenseUrl\": \"https://github.com/styled-components/styled-components/raw/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"grommet-theme-hpe@3.2.1\"\n\n    \"licenses\": \"Apache-2.0\",\n    \"repository\": \"https://github.com/grommet/grommet-theme-hpe\",\n    \"licenseUrl\": \"https://github.com/grommet/grommet-theme-hpe/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"uuid@8.3.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/uuidjs/uuid\",\n    \"licenseUrl\": \"https://github.com/uuidjs/uuid/raw/main/LICENSE.md\",\n\n-----------------------------------------------------------\n\n\"use-debounce@7.0.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/xnimorz/use-debounce\",\n    \"licenseUrl\": \"https://github.com/xnimorz/use-debounce/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"deep-equal@1.0.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/inspect-js/node-deep-equal\",\n    \"licenseUrl\": \"https://github.com/inspect-js/node-deep-equal/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-d3@0.4.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/esbullington/react-d3\",\n    \"licenseUrl\": \"https://github.com/esbullington/react-d3/raw/master/LICENSE.md\",\n\n-----------------------------------------------------------\n\n\"react-immutable-proptypes@2.1.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/HurricaneJames/react-immutable-proptypes\",\n    \"licenseUrl\": \"https://github.com/HurricaneJames/react-immutable-proptypes/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"swagger-ui-dist@3.23.11\"\n\n    \"licenses\": \"Apache-2.0\",\n    \"repository\": \"https://github.com/swagger-api/swagger-ui\",\n    \"licenseUrl\": \"https://github.com/swagger-api/swagger-ui/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"swagger-ui-themes@3.0.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/ostranme/swagger-ui-themes\",\n\n-----------------------------------------------------------\n\n\"deepmerge@4.3.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/TehShrike/deepmerge\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/TehShrike/deepmerge/master/license.txt\",\n\n-----------------------------------------------------------\n\n\"exenv@1.2.2\"\n\n    \"licenses\": \"BSD\",\n    \"repository\": \"https://github.com/JedWatson/exenv\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/JedWatson/exenv/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"grommet-styles@0.2.0\"\n\n    \"licenses\": \"Apache-2.0\",\n    \"repository\": \"https://github.com/grommet/grommet-styles\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/grommet/grommet-styles/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"hoist-non-react-statics@3.3.2\"\n\n    \"licenses\": \"BSD\",\n    \"repository\": \"https://github.com/mridgway/hoist-non-react-statics\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/mridgway/hoist-non-react-statics/master/LICENSE.md\",\n\n-----------------------------------------------------------\n\n\"object-assign@4.1.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/sindresorhus/object-assign\",",
        "0eb13f88-8153-4dfa-84b2-5a405e53a51e": "To assign or unassign a role for a user or a group globally: det rbac assign-role -u USER_NAME ROLE_NAME det rbac unassign-role -u USER_NAME ROLE_NAME det rbac assign-role -g GROUP_NAME ROLE_NAME det rbac unassign-role -g GROUP_NAME ROLE_NAME To assign or unassign a role for a user or a group on a particular workspace, use -w WORKSPACE_NAME switch: det rbac assign-role -u USER_NAME ROLE_NAME -w WORKSPACE_NAME det rbac unassign-role -u USER_NAME ROLE_NAME -w WORKSPACE_NAME det rbac assign-role -g GROUP_NAME ROLE_NAME -w WORKSPACE_NAME det rbac unassign-role -g GROUP_NAME ROLE_NAME -w WORKSPACE_NAME",
        "8c272b39-04c2-499f-ac82-78048e56c012": "Optional. If specified, the weights of this trial will be initialized to the most recent checkpoint of the given trial ID. This will fail if the source trial\u2019s model architecture is inconsistent with the model architecture of this experiment.",
        "a45bdff9-ef6e-474f-b765-8e94bf2e79af": "Using the Spark Interactive Sessions GUI To use HPE-curated Spark images when using the Spark\n        Interactive Sessions, follow these steps: Perform the creating interactive sessions instructions until you reach the Spark Configurations box in the Session\n              Configurations and Dependencies step. See Creating Interactive Sessions . In the Spark Configurations box, you have two options: If you leave the Key and Value boxes empty, the Spark interactive sessions will be\n                created with the HPE-curated Spark image. The List of Spark Images page also lists the default HPE-curated Spark images used for GUI experience. If you set the Key and Value boxes for the Spark image of your choice by adding\n                the following key-value pairs, your Spark interactive session will be created with\n                the Spark image of your choice. Key: spark.kubernetes.container.image\nValue: <spark-image-of-your-choice> To specify the details for other boxes or options in the Session\n                Configurations and Dependencies step and to complete creating\n              interactive sessions, see Creating Interactive Sessions . Using Notebooks To use HPE-curated Spark images when using Spark magic\n          ( %manage_spark ) to create Livy sessions, follow these steps: Run %manage_spark to connect to the Livy server and start a new\n            session. See %manage_spark for details. Once you run %manage_spark , you have two options: Creating sessions with the default Spark configurations. This will use the HPE-curated Spark image to create an interactive\n                session. The List of Spark Images page also lists the default HPE-curated Spark images used for GUI experience. Running %config_spark and updating the value of spark.kubernetes.container.image to the Spark image of your\n                choice. This will use the Spark image of your choice to create an interactive\n                session. To specify the details for the other boxes or options in the Create\n              Session step and to complete creating Livy session, see %manage_spark . Using Airflow When you submit the Spark application by using Airflow, your Spark application will be\n        configured with your chosen Spark image on your YAML file. This YAML file is set in the\n        Airflow DAG. For example: submit = SparkKubernetesOperator(\n    task_id='submit',\n    namespace=\"example\", application_file=\"example.yaml\", dag=dag,\n    api_group=\"sparkoperator.hpe.com\",\n    enable_impersonation_from_ldap_user=True\n) To learn about how to submit Spark applications by using Airflow DAG, see Submitting Spark Applications by Using DAGs . On this page Using the Create Spark Application GUI Using the Spark Interactive Sessions GUI Using Notebooks Using Airflow Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "7db097fe-b93d-44f1-b3e4-48c5df72d8cb": "To get the current user\u2019s permission list: det rbac my-permissions To list all permissions on the role as well as all users and groups who bear it, whether globally or at a workspace level: det rbac describe-role ROLE To list all existing roles and their permissions: det rbac list-roles To list existing users, group and their membership: det user list det user-group list det user-group describe GROUP_NAME To list the role assignments for a user or a group: det rbac list-groups-roles GROUP_NAME det rbac list-users-roles USER_NAME",
        "2baeb6a2-ada1-4280-9157-be395ef45a8d": "Release Date: October 18, 2021 New Features WebUI: Add a \u201cNotes\u201d tab allowing for the input and viewing of free-form Markdown text on experiment pages. This works for both single-trial experiments and trials within a multi-trial experiment. Improvements Docs: reorganize documents to be more user-friendly. Merge some how-to guides, topic guides, and reference guides. Users should now need to read very few documents to understand what they need to do in Determined rather than having to jump around between documents. Merge most information on best practices into how-to guides so that users find out about best practices as soon as they learn how to use something. Decompose the top-level FAQ document and move different parts of it to relevant pages so that users can develop a better expectation of what common issues they might hit. Profiler: samples_per_second in PyTorch now reflects samples across all workers. Database migrations: Run upgrades in transactions to improve stability. Bug Fixes Deploy: Fix an issue where the default checkpoint storage directory was not created for some users.",
        "cbabce88-74a1-444c-91c6-9ac95ad96a32": "These kubectl commands list and delete pods which are running Determined tasks: # Get all pods that are running Determined tasks. kubectl get pods -l=determined # Delete all Determined task pods. Users should never have to run this, # unless they are removing a deployment of Determined. kubectl get pods --no-headers=true -l=determined | awk '{print $1}' | xargs kubectl delete pod",
        "745f7248-1c3b-4f80-be54-32f50b7f79bb": "minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation. When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system.",
        "68688c66-3da9-4a87-9d92-86aacc1de7b1": "When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data\n            container.",
        "e758bcc0-4617-428e-a252-f73844385664": "System Metrics are statistics around hardware usage, such as GPU utilization and network throughput. These metrics are useful for seeing whether training is using the hardware effectively. When the System Metrics reported for an experiment are below what is expected from the hardware, that is a sign that the software may be able to be optimized to make better use of the hardware resources. Specifically, Determined tracks: GPU utilization GPU free memory Network throughput (sent) Network throughput (received) Disk IOPS Disk throughput (read) Disk throughput (write) Host available memory CPU utilization averaged across cores For distributed training, these metrics are collected for every agent. The data are broken down by agent, and GPU metrics can be further broken down by GPU. System Metrics record agent-level metrics, so when there are multiple experiments on the same agent, it is difficult to analyze. We suggest that profiling is done with only a single experiment per agent.",
        "3d832b59-8fe7-4da4-810d-c7057203ae35": "Validate the driver_stats.parquet file is available in the /<username>/Feast directory. Open the definitions.py file and update the path for the driver_stats.parquet file for your username. For\n                        eg: /home/<username>/<username>/feast/data/driver_stats.parquet Open the ride-sharing-example.ipynb file and update the path\n                    for the driver_stats.parquet file for your username. For\n                        eg: /home/<username>/<username>/feast/data/driver_stats.parquet Select the first cell of the notebook and click Run the selected cells and\n                        advance (play icon). Results Click the Tools & Frameworks icon on the left\n                    navigation bar. Navigate to the Feast tile under the Data Science tab and click Open . Explore the Feast web interface to see Data Sources, Entities, Feature Views,\n                    Feature Services, and Datasets that are defined through feature definitions. Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "b231217d-9b15-4f74-97f3-3099dfdb9dc5": "A model can be added to the registry via the WebUI, Python SDK, REST API, or CLI. This guide will cover the Python and CLI methods. For information on the REST API, see the REST API documentation. The following example demonstrates how to add a new model to the registry; create_model() returns an instance of the Model class. The new model will not have any versions (model checkpoints) associated with it; adding versions to a model is described below. from determined.experimental import Determined model = Determined().create_model( \"model_name\", description=\"optional description\", metadata={\"optional\": \"JSON serializable dictionary\"}, ) Similarly, you can create a model from the CLI using the following command. det model create <model_name>",
        "1a00904f-c392-47bf-8ad2-be104dd150e8": "The behavior of an experiment is configured via a YAML file. A configuration file is typically passed as a command-line argument when an experiment is created with the Determined CLI. For example: det experiment create config-file.yaml model-directory",
        "969be782-3f0f-4dbe-bdb0-a286dbffdb99": "There is no installation needed for the agent. The Determined master will dynamically launch Determined agent instances based on the Configuring the Cluster.",
        "18a13e96-ac1f-4352-b537-2dddb2048c48": "Release Date: August 31, 2020 Bug Fixes Database migration: Fix a bug with a database migration in Determined version 0.13.0 which caused it to run slow and backfill incorrect values. Users on Determined versions 0.12.13 or earlier are recommended to upgrade to version 0.13.1. Users already on version 0.13.0 should upgrade to version 0.13.1 as usual. TensorBoard: Fix a bug that prevents TensorBoards from experiments with old experiment configuration versions from being loaded. WebUI: Fix an API response decoding issue on React where a null checkpoint resource was unhandled and could prevent trial detail page from rendering. WebUI: Fix an issue where terminated TensorBoard and notebook tasks were rendered as openable.",
        "971458a1-70f3-46b6-bcbe-cdf5e79cbb17": "Enter the S3 server port . The default value is 9000. Select the Use TLS Encryption check box, if you wish to\n                            communicate over a secure connection. By default, TLS encryption is\n                            enabled. Drag and drop the S3 server certificate for secure\n                            communication, if the generic S3 object store is not CA certified. Click Import . Results The S3 object store is imported into Data Fabric. The S3 object store is visible under the list\n                of resources in the global\n                namespace. Related maprcli Commands To implement the\n                    features described on this page, the Data Fabric UI relies on the following maprcli command. The command is provided for general\n                    reference. For more information, see maprcli Commands in This Guide . clustergroup\n                                addexternal (Topic last modified: 2024-02-01) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "2c34534a-bca9-4cf3-92d3-9edb42f84ad7": "User Guide Torch Batch Processing API This is an experimental API and may change at any time. The main arguments to torch_batch_process are batch_processor_cls, a subclass of TorchBatchProcessor and dataset. torch_batch_process( batch_processor_cls=MyProcessor dataset=dataset )",
        "b88f9f33-c3bf-413f-8050-84210fef69e4": "Additional License Authorizations (ALA) Jump to main content Get Started Platform Administration Reference Home Reference Provides reference information for the HPE Ezmeral Data Fabric . Product Licensing Provides information related to product licensing. Additional License Authorizations (ALA) Provides Additional License Authorizations for HPE Ezmeral Software, including HPE     Ezmeral Runtime Enterprise, HPE Ezmeral ML Ops, HPE Ezmeral Data Fabric, and Open Source     Software. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Reference Provides reference information for the HPE Ezmeral Data Fabric . Release History Describes the currently released versions of the HPE Ezmeral Data Fabric as-a-service platform. Cloud Instance Specifications Compares different aspects of the supported cloud instances of the HPE Ezmeral Data Fabric . Third-Party Storage Solutions Describes global-namespace support for HPE partner storage technologies, including     Scality, WEKA, and VAST. Port Information Describes the ports used by HPE Ezmeral Data Fabric services. maprcli Commands in This Guide Describes how to use maprcli commands provided as reference links in     this guide. Operating System Support Matrix The tables on this page show the Linux operating-system versions that are supported for HPE Ezmeral Data Fabric releases. Doc Site Available as a PDF Provides a link to the downloadable PDF file containing all the information for the     current release. Product Licensing Provides information related to product licensing. Additional License Authorizations (ALA) Provides Additional License Authorizations for HPE Ezmeral Software, including HPE     Ezmeral Runtime Enterprise, HPE Ezmeral ML Ops, HPE Ezmeral Data Fabric, and Open Source     Software. Open-Source Software Acknowledgements (Release 7.6.0) Provides licensing information and acknowledges the use of open-source projects with     HPE software. Other Resources Provides links to additional resources such as on-demand training, videos, blogs, and     the HPE Ezmeral Data Fabric community. Contact HPE Provides a link to contact HPE Sales or Support. Additional License Authorizations (ALA) Provides Additional License Authorizations for HPE Ezmeral Software, including HPE\n    Ezmeral Runtime Enterprise, HPE Ezmeral ML Ops, HPE Ezmeral Data Fabric, and Open Source\n    Software. Additional License Authorizations for HPE Ezmeral Software (Topic last modified: 2023-05-02) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "194aca6a-7b04-4d36-b5d7-22099a6a4a05": "IPv6 Support in Data Fabric Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Enabling IPv6 on a fabric Describes the procedure to enable IPv6 communication on a fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Data Fabric can be installed on hosts with IPv6 addresses. In other words, external\n                endpoints for Data Fabric can have IPv6 addresses. Data Fabric can communicate with\n                clients over IPv6 addressing. Inter-cluster traffic and intra-cluster traffic over\n                IPv6 connections is supported with IPv4 compatibility. Data Fabric deployment over IPv6 addresses is possible when both the hardware hosting\n                Data Fabric and the Data Fabric software are able to detect and support IPv6\n                addresses. The underlying hardware that hosts Data Fabric must have a network interface card\n                (NIC) that supports IPv6 addressing. An application that wishes to communicate with Data Fabric over IPv6 can do so, when\n                Data Fabric is installed on IPv6-compatible hardware and IPv6 support is enabled on\n                Data Fabric. The following table describes the terminology related to IPv6 client/server\n                    nodes. Term Description IPv6-aware The term denotes readiness of the underlying hardware. It\n                                    indicates that the NIC associated with a node that hosts Data\n                                    Fabric is IPv6 compatible, and can communicate with other nodes\n                                    with IPv6 and IPv4 addresses. IPv6-unaware The term denotes readiness of the underlying hardware. It\n                                    indicates that the NIC associated with a node that hosts Data\n                                    Fabric is incompatible to handle IPv6 traffic, and can handle\n                                    IPv4 traffic only. IPv6-enabled The term denotes that IPv6 is enabled on Data Fabric\n                                    software. The Data Fabric node on which IPv6 is enabled is able\n                                    to communicate with IPv6 addresses. The node is able to\n                                    communicate with IPv4 addresses. IPv6-only The term denotes that IPv6 is enabled on Data Fabric\n                                    software. The Data Fabric node on which IPv6 is enabled is able\n                                    to communicate exclusively with IPv6 addresses only.\n                                    Communication with IPv4 addresses is not supported on this\n                                    node. The following matrix explains in detail the communication between a client node and a\n                Data Fabric node for various IP address type combinations.",
        "aa572938-32ce-4e52-8283-7bc73f152c3d": "Installing Determined on Kubernetes deploys an instance of the Determined master and a Postgres database in the Kubernetes cluster. Once the master is up and running, you can launch experiments, notebooks, TensorBoards, commands, and shells. When new workloads are submitted to the Determined master, the master launches pods and configMaps on the Kubernetes cluster to execute those workloads. Users of Determined shouldn\u2019t need to interact with Kubernetes directly after installation, as Determined handles all the necessary interaction with the Kubernetes cluster. It is also important to note that when running Determined on Kubernetes, a higher priority value means a higher priority (e.g. a priority 50 task will run before a priority 40 task). This is different from priority scheduling in non-Kubernetes deployments, where lower priority values mean a higher priority (e.g. a priority 40 task will run before a priority 50 task).",
        "d096c550-69b5-4c0b-998b-8eaefe8bb83b": "Indicates if GPU resources are properly configured in the HPC workload manager. For PBS, the ngpus option can be used to identify the number of GPUs available on a node. For Slurm, GresTypes=gpu is set in the Slurm configuration, and nodes with GPUs have properly configured GRES to indicate the presence of any GPUs. The default is true. When false, Determined will request slots_per_trial nodes and utilize only GPU 0 on each node. It is the user\u2019s responsibility to ensure that GPUs will be available on nodes selected for the job using other configurations, such as targeting a specific resource pool with only GPU nodes or specifying a Slurm constraint in the experiment configuration.",
        "8b4545c1-3b8b-4756-8c32-fcbaacfbe031": "Most of the API calls to a Determined cluster require authentication. On each API call, the server expects a Bearer token. To receive a token, POST a valid username and password combination to the login endpoint, /api/v1/auth/login using the following format: { \"username\": \"string\", \"password\": \"string\" } Example request: curl -s \"${DET_MASTER}/api/v1/auth/login\" \\ -H 'Content-Type: application/json' \\ --data-binary '{\"username\":\"determined\",\"password\":\"\"}' Example response: { \"token\": \"string\", \"user\": { \"username\": \"string\", \"admin\": true, \"active\": true, \"agent_user_group\": { \"agent_uid\": 0, \"agent_gid\": 0 } } } When you receive the token, store it and attach it to future API calls under the Authorization header in the Bearer $TOKEN format.",
        "21cddac2-a36d-4f5a-a31c-d3177d57ee92": "replication role balancer Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication.",
        "d05e4834-d9d4-4e34-9336-50ebf734aabe": "To find the latest release of Determined, visit the Determined repo. Download the appropriate Debian or RPM package file, which will have the name determined-master_VERSION_linux_amd64.[deb|rpm] (where VERSION is the actual version, e.g., 0.26.1-dev0). Similarly, the agent package is named determined-agent_VERSION_linux_amd64.[deb|rpm]. Install the master and agent package on one machine. Debian Distributions On Debian distributions, use the following command: sudo apt install <path to downloaded package> Red Hat Distributions On Red Hat distributions, use the following command: sudo rpm -i <path to downloaded package> Before running the Determined agent, install Docker on each agent machine. If you are not using Docker Desktop, you may disregard the prompt to use Docker Desktop and allow Docker to be installed within the WSL distribution.",
        "f974f5a1-4d31-4924-a8e1-bf41ad68e5e7": "See https://prestodb.io/docs/current/connector/deltalake.html . Parameter Description Default Value Data Type Hive Metastore The type of Hive metastore to use thrift STRING Enable Local Snapshot Table Enable Caching while querying true BOOLEAN Optional Connection Parameters Parameter Description Default Value Data Type Delta Parquet Dereference Pushdown Enabled Enable pushing nested column dereferences into table scan so that only\n                      the required fields selected in a struct data type column are selected true BOOLEAN Delta Max Splits Batch Size Delta : Max split batch size 200 INTEGER Delta Max Partitions Per Writer Delta : Maximum number of partitions per writer 100 INTEGER Hive Metastore The type of Hive metastore to use thrift STRING Hive Insert Overwrite Immutable Partitions Enabled When enabled, insertion query will overwrite existing partitions when\n                      partitions are immutable. This config only takes effect with\n                      hive.immutable-partitions set to true false BOOLEAN Hive Create Empty Bucket Files For Temporary Table Create empty files when there is no data for temporary table buckets false BOOLEAN Hive Enable Parquet Batch Reader Verification Enable optimized parquet reader false BOOLEAN Hive Create Empty Bucket Files For Temporary Table Create empty files when there is no data for temporary table buckets false BOOLEAN Hive Min Bucket Count To Not Ignore Table Bucketing Ignore table bucketing when table bucket count is less than the value\n                      specified, otherwise, it is controlled by property hive.ignore-table-bucketing 0 INTEGER Hive Partition Statistics Based Optimization Enabled Enables partition statistics based optimization, including partition\n                      pruning and predicate stripping false BOOLEAN Hive Experimental Optimized Partition Update Serialization Enabled Serialize PartitionUpdate objects using binary SMILE encoding and\n                      compress with the ZSTD compression false BOOLEAN Hive Materialized View Missing Partitions Threshold Materialized views with missing partitions more than this threshold\n                      falls back to the base tables at read time 100 INTEGER Hive S3select Pushdown Max Connections The maximum number of client connections allowed for those operations\n                      from worker nodes 500 INTEGER Hive Temporary Staging Directory Enabled Should use (if possible) temporary staging directory for write\n                      operations true BOOLEAN Hive Temporary Staging Directory Path Location of temporary staging directory for write operations. Use\n                      ${USER} placeholder to use different location for each user. /tmp/presto-${USER} STRING Hive Temporary Table Storage Format The default file format used when creating new tables. ORC STRING Hive Temporary Table Compression Codec The compression codec to use when writing files for temporary tables SNAPPY STRING Hive Use Pagefile For Hive Unsupported Type Automatically switch to PAGEFILE format for materialized exchange when\n                      encountering unsupported types true BOOLEAN Hive Parquet Pushdown Filter Enabled Enable complex filter pushdown for Parquet false BOOLEAN Hive Range Filters On Subscripts Enabled Enable pushdown of range filters on subscripts (a[2] = 5) into ORC\n                      column readers false BOOLEAN Hive Adaptive Filter Reordering Enabled Enable adaptive filter reordering true BOOLEAN Hive Parquet Batch Read Optimization Enabled Is Parquet batch read optimization enabled false BOOLEAN Hive Enable Parquet Dereference Pushdown Is dereference pushdown expression pushdown into Parquet reader enabled false BOOLEAN Hive Max Metadata Updater Threads Maximum number of metadata updated threads 100 INTEGER Hive Partial_aggregation_pushdown_enabled Enable partial aggregation pushdown false BOOLEAN Hive Manifest Verification Enabled Enable verification of file names and sizes in manifest / partition\n                      parameters false BOOLEAN Hive Undo Metastore Operations Enabled Enable undo metastore operations true BOOLEAN Hive Verbose Runtime Stats Enabled Enable tracking all runtime stats. Note that this may affect query\n                      performance false BOOLEAN Hive Prefer Manifests To List Files Prefer to fetch the list of file names and sizes from manifests rather\n                      than storage false BOOLEAN Hive Partition Lease Duration Partition lease duration 0.00s DURATION Hive Size Based Split Weights Enabled Enable estimating split weights based on size in bytes true BOOLEAN Hive Minimum Assigned Split Weight Minimum weight that a split can be assigned when size based split\n                      weights are enabled 0.05 DOUBLE Hive Use Record Page Source For Custom Split Use record page source for custom split. By default, true. Used to query\n                      MOR tables in Hudi. true BOOLEAN Hive Split Loader Concurrency Number of maximum concurrent threads per split source 4 INTEGER Hive Domain Compaction Threshold Maximum ranges to allow in a tuple domain without compacting it 100 INTEGER Hive Max Concurrent File Renames Maximum concurrent file renames 20 INTEGER Hive Max Concurrent Zero Row File Creations Maximum number of zero row file creations 20 INTEGER Hive Recursive Directories Enable reading data from subdirectories of table or partition locations.\n                      If disabled, subdirectories are ignored.",
        "a389b6d0-7dc6-40e9-aeb2-380aed7dfd90": "Considerations for Importing an as-a-Service Fabric An as-a-service fabric is a fabric that exists as part of a global namespace and was\n        created using the Create fabric functionality of the Data Fabric UI . From any as-a-service fabric, you can import\n        another as-a-service fabric into the global namespace by using the Import\n          fabric command. A fabric can belong to only one global namespace at a time. Thus, the act of importing an\n        as-a-service fabric necessarily removes the fabric from the global namespace to which it\n        currently belongs. To view the current list of fabrics in your global namespace, display the Table\n          view or Graph view on the Resources card of the Home page. Note these considerations: Only fabrics configured for SSO can be imported. To import a fabric, you must be an SSO user and have Fabric Manager or Fabric User credentials . You can only import one fabric at a time. You must have a consumption license for each new fabric that you import. Preparing to Import an as-a-Service Fabric On the fabric that you plan to import, stop Keycloak: Use the following command to identify the host running the mapr-keycloak service: maprcli node list -columns svc In the command output,\n                look for a host that shows keycloak in the service column. If no host shows the mapr-keycloak service in the service column, go to step 2. Stop the mapr-keycloak service: maprcli node services -name keycloak -action stop -nodes -json On the fabric that you plan to import, reset the SSO information: Reset the SSO\n                configuration: maprcli cluster resetssoconf -json Restart the mapr-apiserver services on the fabric\n                hosts: maprcli node services -name apiserver -action restart -nodes host1,host2 -json Use the following command to disable the pbs.master role for the\n            fabric to be imported: maprcli config save -values {cldb.pbs.global.master:0} -json If\n            any security policies have been created on the fabric to be imported, they must be\n            manually re-created on the importing fabric after the import operation is completed. To\n            re-create the policies, refer to Administering Security Policies . On the cluster to be imported, create a tar ball of the fabric\n            directory: /opt/mapr/installer/ezdfaas/deployments/<cluster-name> Copy the contents of the tar ball to the importing cluster's /opt/mapr/installer/ezdfaas/deployments directory. Extract the\n            contents, and be sure to delete the .tar file: Obtain the SSO configuration from the importing fabric, and configure it on the fabric\n            to be imported: Use the following command to fetch the SSO parameters from the importing\n                fabric: maprcli cluster getssoconf -json For\n                example: maprcli cluster getssoconf -json\n{\n\"timestamp\":1699432649586,\n\"timeofday\":\"2023-11-08 12:37:29.586 GMT-0800 AM\",\n\"status\":\"OK\",\n\"total\":1,\n\"data\":[\n{\n\"issuerendpoint\":\"https://<hostname>:6443/realms/master\",\n\"providername\":\"keycloak\",\n\"clientid\":\"edf-client\",\n\"clientsecret\":\" <secret> \"\n}\n]\n} Obtain the SSO certificate from the importing fabric's /opt/mapr/keycloak/conf/.crt , and use it to set\n                the SSO configuration information for the fabric to be imported. Use the following\n                command: maprcli cluster setssoconf -issuerendpoint \"https://:8443/realms/TestReallm\" -providername keycloak -clientid edf-client -clientsecret <secret> -certfile -json Restart the mapr-apiserver services. maprcli node services -name apiserver -action restart -nodes host1,host2 -json Wait for a minute to ensure that the SSO configuration is active, then try signing\n                in to the UI: https://<apiserver>:8443/app/dfui You should\n                be redirected to the Keycloak sign-in screen. Use the Data Fabric UI to complete the Import operation as described in the next section. Completing the Import Operation by Using the Data Fabric UI Use the following steps to complete the import operation: Log on to the Data Fabric UI as a Fabric\n            Manager. Click Import fabric . The Import fabric menu appears. Specify the current Name of the fabric to be imported. Do not\n            change the name of the fabric to be imported. Specify the Public IP address of the APIserver of the fabric to\n            be imported. Specify the port of the APIserver for the fabric to be imported. Click Import .",
        "ac128152-2281-4815-9bd6-620c3e69d984": "The Checkpoint class can both download the checkpoint from persistent storage and load it into memory in a Python process. The download() method downloads a checkpoint from persistent storage to a directory on the local file system. By default, checkpoints are downloaded to checkpoints/<checkpoint-uuid>/ (relative to the current working directory). The download() method accepts path as an optional parameter, which changes the checkpoint download location. from determined.experimental import client checkpoint = client.get_experiment(id).top_checkpoint() checkpoint_path = checkpoint.download() specific_path = checkpoint.download(path=\"specific-checkpoint-path\") The load() method downloads the checkpoint, if it does not already exist locally, and loads it into memory. The return type and behavior is different depending on whether you are using TensorFlow or PyTorch.",
        "49588d3b-b942-4627-b4c7-106c07ddd7d4": "access control expression (ACE) Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster.",
        "18b7ac47-e516-4594-9fd2-94d3c621cfc4": "Checkpoints and TensorBoard events can be configured to be stored in shared_fs, AWS S3, Microsoft Azure Blob Storage, or GCS. By default, checkpoints and TensorBoard events are stored using shared_fs, which creates a hostPath Volume and saves to the host file system. This configuration is intended for initial testing only; you are strongly discouraged from using shared_fs for actual deployments of Determined on Kubernetes, because most Kubernetes cluster nodes do not have a shared file system. Instead of using shared_fs, configure either AWS S3, Microsoft Azure Blob Storage, or GCS: AWS S3: To configure Determined to use AWS S3 for checkpoint and TensorBoard storage, you need to set checkpointStorage.type in values.yaml to s3 and set checkpointStorage.bucket to the name of the bucket. The pods launched by the Determined master must have read, write, and delete access to the bucket. To enable this you can optionally configure checkpointStorage.accessKey and checkpointStorage.secretKey. You can optionally configure checkpointStorage.endpointUrl which specifies the endpoint to use for S3 clones (e.g., http://<minio-endpoint>:<minio-port|default=9000>). Microsoft Azure Blob Storage: To configure Determined to use Microsoft Azure Blob Storage for checkpoint and TensorBoard storage, you need to set checkpointStorage.type in values.yaml to azure and set checkpointStorage.container to the name of the container to store it in. You must also specify one of connection_string - the connection string associated with the Azure Blob Storage service account to use, or the tuple account_url and credential - where account_url is the URL for the service account to use, and credential is an optional credential. GCS: To configure Determined to use Google Cloud Storage for checkpoints and TensorBoard data, set checkpointStorage.type in values.yaml to gcs and set checkpointStorage.bucket to the name of the bucket. The pods launched by the Determined master must have read, write, and delete access to the bucket. For example, when launching GKE nodes you need to specify --scopes=storage-full to configure proper GCS access.",
        "4d3fb314-91d1-4522-b1a5-e79700c22972": "DeepSpeedTrial automatically ensures a total of train_batch_size samples are processed in each training iteration. With the assumption that train_batch() calls the model engine\u2019s forward, backward, and optimizer step methods once, DeepSpeedTrial calls train_batch(): gradient_accumulation_steps times when not using pipeline parallelism once when using pipeline parallelism to reach model_engine.train_batch_size() for the first wrapped model engine. To disable this behavior, call disable_auto_grad_accumulation() in the __init__() method of DeepSpeedTrial. In this case, make sure the first model engine processes train_batch_size samples in each call to train_batch().",
        "68503127-1353-46e3-a61f-4d84850973dc": "One NVIDIA GPU will be requested per compute slot. Partitions will be represented as a resource pool with slot type cuda which can be overridden using partition_overrides.",
        "ab4be9e8-eee0-4c90-a10f-32a1a1b21663": "Adding a Group to Keycloak Describes how to add a Keycloak user group. Integrating Your LDAP Directory with Keycloak Keycloak can interface with an external LDAP directory so that LDAP users can access       the Data Fabric UI . Completing SSO Setup Using the Data Fabric UI Describes how to configure the HPE Ezmeral Data Fabric to work       with your SSO server. Resetting the SSO Configuration Describes how to update your single sign-on (SSO) configuration information using the Data Fabric UI . Identifying All CLDB Nodes Explains how you can identify all the CLDB nodes in an HPE Ezmeral Data Fabric . (Topic last modified: 2023-10-30) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "405a7072-7acf-4f05-b904-0ee00f4a2e50": "data node Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster.",
        "1b176057-6174-408e-a20d-cd9097793b6b": "Network/Subnetwork: The Determined cluster runs in an existing or newly created VPC. Elastic IP: For production clusters, the master should have an associated elastic IP; otherwise, AWS automatically assigns an ephemeral IP. Amazon Simple Storage Service (S3) Bucket: The Determined cluster can leverage an existing S3 bucket (assuming it has the correct associated permissions), or the CloudFormation script can create a bucket with the cluster.",
        "cabb9676-fd13-4a3e-9053-c8f8970e6a17": "A contributor's \"essential patent claims\" are all patent claims\nowned or controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version.  For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\n\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\n\n  In the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement).  To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\n\n  If you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients.  \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\n\n  If, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\n\n  A patent license is \"discriminatory\" if it does not include within\nthe scope of its coverage, prohibits the exercise of, or is\nconditioned on the non-exercise of one or more of the rights that are\nspecifically granted under this License.  You may not convey a covered\nwork if you are a party to an arrangement with a third party that is\nin the business of distributing software, under which you make payment\nto the third party based on the extent of your activity of conveying\nthe work, and under which the third party grants, to any of the\nparties who would receive the covered work from you, a discriminatory\npatent license (a) in connection with copies of the covered work\nconveyed by you (or copies made from those copies), or (b) primarily\nfor and in connection with specific products or compilations that\ncontain the covered work, unless you entered into that arrangement,\nor that patent license was granted, prior to 28 March 2007.\n\n\n  Nothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n\n  12. No Surrender of Others' Freedom.\n\n\n  If conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you may\nnot convey it at all.  For example, if you agree to terms that obligate you\nto collect a royalty for further conveying from those to whom you convey\nthe Program, the only way you could satisfy both those terms and this\nLicense would be to refrain entirely from conveying the Program.\n\n\n  13. Use with the GNU Affero General Public License.",
        "9de63f02-6181-430f-a9ca-66a738169de0": "Pull the official Docker image for PostgreSQL. We recommend using version 10 or greater. docker pull postgres:10 This image is not provided by Determined AI; please see its Docker Hub page for more information. Start PostgreSQL as follows: docker run \\ -d \\ --restart unless-stopped \\ --name determined-db \\ -p 5432:5432 \\ -v determined_db:/var/lib/postgresql/data \\ -e POSTGRES_DB=determined \\ -e POSTGRES_PASSWORD=<Database password> \\ postgres:10 If the master will connect to PostgreSQL via Docker networking, exposing port 5432 via the -p argument isn\u2019t necessary; however, you may still want to expose it for administrative or debugging purposes. In order to expose the port only on the master machine\u2019s loopback network interface, pass -p 127.0.0.1:5432:5432 instead of -p 5432:5432.",
        "5e0567bd-6f09-4b3f-bb15-547eca5a1d03": "The first step toward automatic hyperparameter tuning is to define the hyperparameter space, e.g., by listing the decisions that may impact model performance. For each hyperparameter in the search space, the machine learning engineer specifies a range of possible values in the experiment configuration: hyperparameters: ... dropout_probability: type: double minval: 0.2 maxval: 0.5 ... Determined supports the following searchable hyperparameter data types: int: an integer within a range double: a floating point number within a range log: a logarithmically scaled floating point number. Users specify a base, and Determined searches the space of exponents within a range. categorical: a variable that can take on a value within a specified set of discrete values. The values themselves can be of any type. The experiment configuration reference details these data types and their associated options.",
        "916dc445-3b02-4209-a916-84c393f10a5a": "mirror Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. mirror A replica of a volume. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster. The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs.",
        "f1e1e70e-9fa8-4b15-9c04-396fa08c46ab": "Viewing Table Replicas Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Managing Table Replication The topics in this section describe managing table replication. Viewing Table Replicas View the list of created table replicas. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Managing Tables The topics in this section describe managing tables. Managing Column Families and Columns The topics in this section describe managing column families and columns. Viewing Table Information The topics in this section describe viewing table information. Managing Table Replication The topics in this section describe managing table replication. Adding a Table Replica This topic describes adding a table replica. Viewing Table Replicas View the list of created table replicas. Administering Access Controls for Tables This topic describes administering access controls for tables. Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Viewing Table Replicas View the list of created table replicas. Prerequisites You must have permission to view table replicas. About this task You can view table replicas from the Data Fabric UI . Use the following steps to view table\n                replicas. Procedure Log on to the Data Fabric UI . Under the default Fabric user experience , click the Table View icon on the Resources card. In the tabular list of fabrics, click\n                    the down arrow for the fabric that contains the table replica. Click the name of the table for which you want to view replicas The table Overview screen opens. Click the Replication tab. Results You can now view the list of table replicas. (Topic last modified: 2024-01-09) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "ffe4a32d-41df-4a59-a408-0017ae3dc174": "advisory quota Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster.",
        "9dd2dc04-53e1-4917-8cb3-7eba87c5db6f": "The Determined master needs to run as a service account that has the permissions to manage Compute Engine instances. There are two options: Create a particular service account with the Compute Admin role. Then set the Determined master to use this account. See Compute Engine IAM roles for more details on how to configure the service account. In order for the Determined agent to be associated with a service account, the Determined master needs to have access to service accounts. Please ensure the service account of the Determined master has the Service Account User role. In order for the Determined agent to use a shared VPC, the service account that the master runs with needs to have the Compute Network User role. Use the default service account and add the Compute Engine: Read Write scope. Optionally, the Determined agent may be associated with a service account. Access scopes are the legacy method of specifying permissions for your instance. A best practice is to set the full cloud-platform access scope on the instance, then securely limit the service account\u2019s API access with Cloud IAM roles. See Access Scopes for details.",
        "1e84fdc2-66f1-4b42-b2c6-4ba2019806c1": "To\n            identify the EzkfOpsExpand custom resources, run the following\n            command: kubectl get ezkfopsexpand -A \n# (lists the Expand CR names and namespaces) For each of the EzkfOpsExpand\n            custom resources listed in the output, run the following\n            command: kubectl delete ezkfopsexpand -n <expand_CR_namespace> <expand_CR_name> To expand the cluster, complete the following steps: In the left navigation bar, select Administration > Settings . On the Cluster tab, select Expand Cluster . In the Expand Cluster drawer that opens, enter the following information: Number of additional vCPU to allocate. For example, if the current vCPU is 96 and\n              you add 4 vCPU, the vCPU increases to a total of 100 vCPU. Select Use GPU if you want to use GPU and it is not already selected. If Use GPU was selected during installation of HPE Ezmeral Unified Analytics Software , this option\n              cannot be disabled and stays selected by default. Indicate the additional number of vGPU to allocate. For GPU configuration, if a size was selected during HPE Ezmeral Unified Analytics Software installation,\n              you cannot change the size. However, if no vGPU size was selected during installation,\n              you can select a size now. For additional information, see GPU Support . If HA was selected during HPE Ezmeral Unified Analytics Software installation, you cannot disable it. If it was not selected\n              during installation, you can select it now. Currently HA is available for the workload\n              cluster only. You cannot set HA for the management cluster. Click Expand . Configuring HPE MLDE for Added GPU Nodes If you add GPU nodes to the cluster after installing HPE MLDE , you must perform additional steps to ensure HPE MLDE works on these nodes. For details, see Configuring HPE MLDE for Added GPU Nodes . On this page Adding User-Provided Hosts\n        to the Cluster Expanding the Cluster Configuring HPE MLDE for Added GPU Nodes Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "e6ad817d-1e98-4ec1-aed8-998f8b4e537c": "The URL your IdP will send SAML assertions to.",
        "b38f4982-10a9-4cb7-8bfd-106e38a35526": "This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n\nAlso add information on how to contact you by electronic and paper mail.\n\n\n  If the program does terminal interaction, make it output a short\nnotice like this when it starts in an interactive mode:\n\n\n    <program>  Copyright (C) <year>  <name of author>\n    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n\n\nThe hypothetical commands `show w' and `show c' should show the appropriate\nparts of the General Public License.  Of course, your program's commands\nmight be different; for a GUI interface, you would use an \"about box\".\n\n\n  You should also get your employer (if you work as a programmer) or school,\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\nFor more information on this, and how to apply and follow the GNU GPL, see\n<http://www.gnu.org/licenses/>.\n\n\n  The GNU General Public License does not permit incorporating your program\ninto proprietary programs.  If your program is a subroutine library, you\nmay consider it more useful to permit linking proprietary applications with\nthe library.  If this is what you want to do, use the GNU Lesser General\nPublic License instead of this License.  But first, please read\n<http://www.gnu.org/philosophy/why-not-lgpl.html>. (Topic last modified: 2024-01-07) On this page About the NOTICE.txt File Open Source Notice Project-Specific Copyright, Source Code, and License Information Apache License MIT License License for uuid License for JavaMail ZLIB License D3.js license (New BSD License) Lesser GNU Public License (LGPL) Lesser GNU Public License (LGPL) v2.1 Boost Software License - Version 1.0 - August 17th, 2003 GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "a6a60919-4ebd-4670-ba37-46352f81441a": "The evaluate_batch() method is passed a single batch of data from the validation data set; it should compute the user-defined validation metrics on that data, and return them as a dictionary that maps metric names to values. The metric values for each batch are reduced (aggregated) to produce a single value of each metric for the entire validation set. By default, metric values are averaged but this behavior can be customized by overridding evaluation_reducer(). def evaluate_batch(self, batch: TorchData): batch = cast(Tuple[torch.Tensor, torch.Tensor], batch) data, labels = batch output = self.model(data) validation_loss = torch.nn.functional.nll_loss(output, labels).item() pred = output.argmax(dim=1, keepdim=True) accuracy = pred.eq(labels.view_as(pred)).sum().item() / len(data) return {\"validation_loss\": validation_loss, \"accuracy\": accuracy}",
        "1f9fa402-ca18-4d0f-b766-2ec538973f0a": "To modify the job queue in the Webui, Go to the Job Queue section. Find the job to modify. Click the three dots in the right-most column of the job. Find and click the Manage Job option. Make the change you want on the pop-up page, and click OK.",
        "8a55d086-e003-4699-9531-c3e67b5bdb7c": "class model_hub.huggingface.BaseTransformerTrialcontext: determined.pytorch._pytorch_context.PyTorchTrialContext This is the base PyTorchTrial for transformers that implements the __init__ and train_batch methods. You can subclass BaseTransformerTrial to customize a trial for your own usage by filing in the expected methods for data loading and evaluation. The __init__ method replicated below makes heavy use of the helper functions in the next section. def __init__(self, context: det_torch.PyTorchTrialContext) -> None: self.context = context # A subclass of BaseTransformerTrial may have already set hparams and data_config # attributes so we only reset them if they do not exist. if not hasattr(self, \"hparams\"): self.hparams = attrdict.AttrDict(context.get_hparams()) if not hasattr(self, \"data_config\"): self.data_config = attrdict.AttrDict(context.get_data_config()) if not hasattr(self, \"exp_config\"): self.exp_config = attrdict.AttrDict(context.get_experiment_config()) # Check to make sure all expected hyperparameters are set. self.check_hparams() # Parse hparams and data_config. ( self.config_kwargs, self.tokenizer_kwargs, self.model_kwargs, ) = hf_parse.default_parse_config_tokenizer_model_kwargs(self.hparams) optimizer_kwargs, scheduler_kwargs = hf_parse.default_parse_optimizer_lr_scheduler_kwargs( self.hparams ) self.config, self.tokenizer, self.model = build_using_auto( self.config_kwargs, self.tokenizer_kwargs, self.hparams.model_mode, self.model_kwargs, use_pretrained_weights=self.hparams.use_pretrained_weights, ) self.model = self.context.wrap_model(self.model) self.optimizer = self.context.wrap_optimizer( build_default_optimizer(self.model, optimizer_kwargs) ) if self.hparams.use_apex_amp: self.model, self.optimizer = self.context.configure_apex_amp( models=self.model, optimizers=self.optimizer, ) self.lr_scheduler = self.context.wrap_lr_scheduler( build_default_lr_scheduler(self.optimizer, scheduler_kwargs), det_torch.LRScheduler.StepMode.STEP_EVERY_BATCH, ) self.grad_clip_fn = None if optimizer_kwargs.max_grad_norm > 0: # type: ignore self.grad_clip_fn = lambda x: torch.nn.utils.clip_grad_norm_( x, optimizer_kwargs.max_grad_norm ) The evaluate_batch method replicated below should work for most models and tasks but can be overwritten for more custom behavior in a subclass. def train_batch(self, batch: Any, epoch_idx: int, batch_idx: int) -> Any: # By default, all HF models return the loss in the first element. # We do not automatically apply a label smoother for the user. # If this is something you want to use, please see how it's # applied by transformers.Trainer: # https://github.com/huggingface/transformers/blob/v4.3.3/src/transformers/trainer.py#L1324 outputs = self.model(**batch) loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0] self.context.backward(loss) self.context.step_optimizer(self.optimizer, self.grad_clip_fn) return loss",
        "3f7d8c75-4ed4-4749-b09c-4f3c91f7e5de": "data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster. The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza.",
        "b5cd4494-8f31-4620-adaf-7966d59a6ed3": "The checkpoint_storage section defines how model checkpoints will be stored. A checkpoint contains the architecture and weights of the model being trained. Each checkpoint has a UUID, which is used as the name of the checkpoint directory on the external storage system. If this field is not specified, the experiment will default to the checkpoint storage configured in the master configuration.",
        "4e55c5b0-479d-43de-94b7-00dbc4416601": "Outlined below is a basic structure for our trial class: import torch.nn as nn from determined.pytorch import DataLoader, PyTorchTrial, PyTorchTrialContext class MNISTTrial(PyTorchTrial): def __init__(self, context: PyTorchTrialContext): # Initialize the trial class and wrap the models, optimizers, and LR schedulers. pass def train_batch(self, batch: TorchData, epoch_idx: int, batch_idx: int): # Run forward passes on the models and backward passes on the optimizers. pass def evaluate_batch(self, batch: TorchData): # Define how to evaluate the model by calculating loss and other metrics # for a batch of validation data. pass def build_training_data_loader(self): # Create the training data loader. # This should return a determined.pytorch.Dataset. pass def build_validation_data_loader(self): # Create the validation data loader. # This should return a determined.pytorch.Dataset. pass Let\u2019s dive deeper into the implementation of each of these methods.",
        "2c6d9c3d-3737-4358-9507-ff69747df419": "class determined.pytorch.Trainertrial: determined.pytorch._pytorch_trial.PyTorchTrialcontext: determined.pytorch._pytorch_context.PyTorchTrialContext pytorch.Trainer is an abstraction on top of a vanilla PyTorch training loop that handles many training details under-the-hood, and exposes APIs for configuring training-related features such as automatic checkpointing, validation, profiling, metrics reporting, etc. Trainer must be initialized and called from within a pytorch.PyTorchTrialContext. configure_profilersync_timings: boolenabled: boolbegin_on_batch: intend_after_batch: intNone Configures the Determined profiler. This method should only be called before .fit(), and only once within the scope of init(). If called multiple times, the last call\u2019s configuration will be used. Parameters sync_timings \u2013 Specifies whether Determined should wait for all GPU kernel streams before considering a timing as ended. Defaults to \u2018true\u2019. Applies only for frameworks that collect timing metrics (currently just PyTorch). enabled \u2013 Defines whether profiles should be collected or not. Defaults to false. begin_on_batch \u2013 Specifies the batch on which profiling should begin. end_after_batch \u2013 Specifies the batch after which profiling should end. fitcheckpoint_period: Optional[determined.pytorch._pytorch_trial.TrainUnit] = Nonevalidation_period: Optional[determined.pytorch._pytorch_trial.TrainUnit] = Nonemax_length: Optional[determined.pytorch._pytorch_trial.TrainUnit] = Nonereporting_period: determined.pytorch._pytorch_trial.TrainUnit = <determined.pytorch._pytorch_trial.Batch object>checkpoint_policy: str = 'best'latest_checkpoint: Optional[str] = Nonestep_zero_validation: bool = Falsetest_mode: bool = FalseNone fit() trains a PyTorchTrial configured from the Trainer and handles checkpointing and validation steps, and metrics reporting. Parameters checkpoint_period \u2013 The number of steps to train for before checkpointing. This is a TrainUnit type (Batch or Epoch) which can take an int or instance of collections.abc.Container (list, tuple, etc.). For example, Batch(100) would checkpoint every 100 batches, while Batch([5, 30, 45]) would checkpoint after every 5th, 30th, and 45th batch. validation_period \u2013 The number of steps to train for before validating. This is a TrainUnit type (Batch or Epoch) which can take an int or instance of collections.abc.Container (list, tuple, etc.). For example, Batch(100) would validate every 100 batches, while Batch([5, 30, 45]) would validate after every 5th, 30th, and 45th batch. max_length \u2013 The maximum number of steps to train for. This value is required and only applicable in local training mode. For on-cluster training, this value will be ignored; the searcher\u2019s max_length must be configured from the experiment configuration. This is a TrainUnit type (Batch or Epoch) which takes an int. For example, Epoch(1) would train for a maximum length of one epoch. reporting_period: checkpoint_policy \u2013 Controls how Determined performs checkpoints after validation operations \u2013 best (default): A checkpoint will be taken after every validation operation that performs better than all previous validations for this experiment. Validation metrics are compared according to the metric and smaller_is_better options in the searcher configuration. This option is only supported for on-cluster training. all: A checkpoint will be taken after every validation, no matter the validation performance. none: A checkpoint will never be taken due to a validation. However, even with this policy selected, checkpoints are still expected to be taken after the trial is finished training, due to cluster scheduling decisions, before search method decisions, or due to min_checkpoint_period. values (if at all. Should be set to one of the following) \u2013 best (default): A checkpoint will be taken after every validation operation that performs better than all previous validations for this experiment. Validation metrics are compared according to the metric and smaller_is_better options in the searcher configuration. This option is only supported for on-cluster training. all: A checkpoint will be taken after every validation, no matter the validation performance. none: A checkpoint will never be taken due to a validation. However, even with this policy selected, checkpoints are still expected to be taken after the trial is finished training, due to cluster scheduling decisions, before search method decisions, or due to min_checkpoint_period. latest_checkpoint \u2013 Configures the checkpoint used to start or continue training. This value should be set to det.get_cluster_info().latest_checkpoint for standard continue training functionality. step_zero_validation \u2013 Configures whether or not to perform an initial validation before training. test_mode \u2013 Runs a minimal loop of training for testing and debugging purposes. Will train and validate one batch. Defaults to false.",
        "e6b81ec5-b07e-4492-a220-944e04f04d8a": "Optional. The maximum number of trials that can be worked on simultaneously. The default value is 16. When the value is 0 we will work on as many trials as possible.",
        "211ca6ad-a540-48d8-b5b2-9dd1b5ca2a1b": "DeepSpeedTrial relies on a ModelParallelUnit to provide data parallel world size and to determine whether a GPU slot should build the data loaders and report metrics. For data parallel training with DeepSpeed, the data parallel world size is equal to the number of GPU slots and all GPU slots build the data loaders and report metrics. If the model engine passed to wrap_model_engine() is a PipelineEngine, the ModelParallelUnit is built using the MPU associated with the model engine. To change this behavior to support custom model parallelism, pass a custom set_mpu as shown in the following example: context.set_mpu( ModelParallelUnit( data_parallel_rank=[fill in], data_parallel_world_size=[fill in], should_report_metrics=[fill in], should_build_dataloader=[fill in] ) )",
        "0b7a2024-988e-4eaa-8d0e-835f6fa947f9": "Airflow Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Accessing Data in External S3 Object Stores Describes how to access data in external object stores from clients, such as Spark and     Kubeflow notebooks. EzPresto Describes the EzPresto SQL query engine     and its featues. Airflow Provides an overview of Apache Airflow in HPE Ezmeral Unified Analytics Software . Airflow DAGs Git Repository Describes how HPE Ezmeral Unified Analytics Software reads DAGs and how to configure a GitHub repository in     Airflow. Configuring Airflow Describes how to configure Airflow in HPE Ezmeral Unified Analytics Software . Submitting Spark Applications by Using DAGs Describes how to submit the Spark applications by using DAGs in Airflow. Defining RBACs on DAGs Describes role-based access controls (RBACs) with respect to Airflow in HPE Ezmeral Unified Analytics Software and how to define RBACs     to permit access to DAGs. Using whylogs with Airflow Describes how to use whylogs with Airflow DAGs. Superset Provides a brief overview of  Superset in HPE Ezmeral Unified Analytics Software . Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Airflow Provides an overview of Apache Airflow in HPE Ezmeral Unified Analytics Software . You can use Airflow to author, schedule, or monitor workflows and data pipelines. A workflow is a Directed Acyclic Graph (DAG) of tasks used to handle big data processing\n      pipelines. The workflows are started on a schedule or triggered by an event. DAGs define the\n      order to run tasks or rerun tasks in case of failures. The tasks define the actions to be\n      performed, such as ingest, monitor, report, and others. To learn more, see Airflow documentation . Airflow Functionality Airflow in HPE Ezmeral Unified Analytics Software supports the following functionality: Extracting data from multiple data sources and running Spark jobs or other data\n            transformations.\u200b Training machine learning models.\u200b Automated generation of reports.\u200b Backups and other DevOps tasks.\u200b Airflow Architecture In HPE Ezmeral Unified Analytics Software , Airflow\n        consists of the following parts: Airflow Operator Manages and maintains Airflow Base and Airflow Cluster Kubernetes Custom Resources\n                by creating and updating Kubernetes objects. Airflow Base Manages the PostgreSQL database that stores Airflow metadata. Airflow Cluster Deploys the UI and scheduler components of Airflow. In HPE Ezmeral Unified Analytics Software , there is\n        only one instance of Airflow per cluster and Airflow DAGs are accessed by all authenticated\n        users. Airflow Components Airflow consists of the following components: Scheduler Triggers the scheduled workflows and submits the tasks to an executor to run. Executor Executes the tasks or delegates the tasks to workers for execution. Worker Executes the tasks. Web Server Provides a user interface to analyze, schedule, monitor, and visualize the tasks and\n              DAG. The Web Server enables you to manage users, roles, and set configuration\n              options. DAG Directory Contains DAG files read by Scheduler, Executor, and Web Server. Metadata Database Stores the metadata about DAGs\u2019 state, runs, and Airflow configuration options. Airflow Limitations Airflow in HPE Ezmeral Unified Analytics Software has the following limitations: The CPU and memory resource limits for executors cannot be modified (CPU: 1, memory:\n            2Gi). To use the Spark Operator, you must provide the username by specifying it under the\n            \"username\" key in the DAG Run Configuration. The logs of successfully run DAGs are available until the corresponding pods are\n            deleted. To learn more about Airflow, see Airflow Concepts . Airflow DAGs Git Repository Describes how HPE Ezmeral Unified Analytics Software reads DAGs and how to configure a GitHub repository in     Airflow. Configuring Airflow Describes how to configure Airflow in HPE Ezmeral Unified Analytics Software .",
        "6b8688fb-9b5e-40e9-be63-f08fcbfe0a56": "Required. The Azure Blob Storage container name to use.",
        "0293c79d-1be4-44fa-8d20-fb6286c53ef3": "Teradata Connection Parameters Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Accessing Data in External S3 Object Stores Describes how to access data in external object stores from clients, such as Spark and     Kubeflow notebooks. EzPresto Describes the EzPresto SQL query engine     and its featues. Connect Data Sources Provides instructions for connecting HPE Ezmeral Unified Analytics Software to external data sources. Delta Connection Parameters List of Delta connection parameters, descriptions, default values, and supported data     types. Delta Thrift Connection Parameters List of  Delta Thrift connection parameters, descriptions, default values, and     supported data types. Hive Connection Parameters List of Hive connection parameters, descriptions, default values, and supported data     types. Hive Glue Metastore Connection Parameters List of Hive Glue Metastore connection parameters, descriptions, default values, and     supported data types. Hive Thrift Metastore Connection Parameters List of Hive Thrift Metastore connection parameters, descriptions, default values, and     supported data types. Hive Discovery Metastore Connection Parameters Lists Hive discovery metastore connection parameters, parameter descriptions, default     values, and supported data types. MySQL Connection Parameters List of MySQL connection parameters, descriptions, default values, and supported data     types. Oracle Connection Parameters List of Oracle connection parameters, descriptions, default values, and supported data     types. Snowflake Connection Parameters List of  Snowflake connection parameters, descriptions, default values, and supported     data types. SQL Server Connection Parameters List of SQL Server connection parameters, descriptions, default values, and supported     data types. Teradata Connection Parameters List of  Teradata connection parameters, descriptions, default values, and supported     data types. PostgreSQL Connection Parameters List of PostgreSQL connection parameters, descriptions, default values, and supported     data types. Iceberg Connection Parameters List of Iceberg connection parameters, descriptions, default values, and supported data     types. Configuring a Hive Data Source with Kerberos Authentication Describes the required prerequisite steps to complete before you connect HPE Ezmeral Unified Analytics Software to a Hive data source     that uses Kerberos authentication. Connect to CSV and Parquet Data in an External S3 Data Source via Hive Connector Describes how to use the Hive connector with Presto in HPE Ezmeral Unified Analytics Software to connect to CSV and     Parquet data in S3-based external data sources. EzPresto/connect-external-s3-data-source.html Connect to External Applications via JDBC Describes how to connect external applications and BI tools, such as Tableau and     PowerBI, to EzPresto through the EzPresto JDBC endpoint. Using Spark to Query EzPresto Describes how to use Spark to query EzPresto . Connect to EzPresto via Python Client Provides information for connecting to EzPresto from a Python client. Cache Data Describes data caching and provides the steps for caching data in HPE Ezmeral Unified Analytics Software . Submitting Presto Queries from Notebook Describes how to submit Presto queries from the notebook. Airflow Provides an overview of Apache Airflow in HPE Ezmeral Unified Analytics Software . Superset Provides a brief overview of  Superset in HPE Ezmeral Unified Analytics Software . Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Teradata Connection Parameters List of  Teradata connection parameters, descriptions, default values, and supported\n    data types. The following tables list the required and optional Teradata connection parameters. Required Connection Parameters Parameter Description Default Value Data Type Connection Url JDBC connection url null STRING Connection User Specifies the login name of the user for the connection null STRING Connection Password Specifies the password of the user for the connection null STRING Enable Local Snapshot Table Enable Caching while querying true BOOLEAN Optional Connection Parameters Parameter Description Default Value Data Type Case Insensitive Name Matching Match schema and table names case insensitively false BOOLEAN Case Insensitive Name Matching Cache Ttl Duration for which remote dataset and table names will be cached. Set to\n                      0ms to disable the cache 1m DURATION Allow Drop Table Allow connector to drop tables false BOOLEAN Generic Cache Table Ttl TTL for cache table expiry in minutes 1440 INTEGER Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "b96dd3de-8cf6-4a5b-bd1f-608df312fba9": "Submitting Statements Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Spark Provides a brief overview of Apache Spark in HPE Ezmeral Unified Analytics Software . Using Spark Images Describes different types of Spark images supported by HPE Ezmeral Unified Analytics Software . List of Spark Images Lists the Spark images distributed by HPE Ezmeral Unified Analytics Software . These         images enables you to run the Spark applications in an air-gapped environment. Creating Spark Applications Describes how to create Spark applications using HPE Ezmeral Unified Analytics Software . Managing Spark Applications Describes how to view and manage Spark applications using HPE Ezmeral Unified Analytics Software . Configuring Memory for Spark Applications Describes how to set memory options for Spark applications. Creating Interactive Sessions Describes how to create interactive sessions in HPE Ezmeral Unified Analytics Software . Submitting Statements Describes how to submit statements in HPE Ezmeral Unified Analytics Software . Managing Interactive Sessions Describes how to view and manage Spark interactive sessions in HPE Ezmeral Unified Analytics Software . Spark History Server Provides an overview of Spark History Server. Using Spark SQL API Describes how to use Spark SQL API in HPE Ezmeral Unified Analytics Software . Enabling GPU Support for Spark Describes NVIDIA spark-rapids accelerator support for Spark, and how     to enable and allocate the GPU resources on Spark. Securely Passing Spark Configuration Values Describes how to pass the sensitive data to Spark configuration using the Kubernetes     Secret. Running Spark Applications in Namespaces Describes how namespaces work with regard to Spark applications in HPE Ezmeral Unified Analytics Software . Using whylogs with Spark Describes how to use whylogs with Spark. Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Submitting Statements Describes how to submit statements in HPE Ezmeral Unified Analytics Software . Prerequisites Create an interactive session. See Creating Interactive Sessions . About this task Run statements in Python, R, or Scala. Procedure To submit statements, you can choose one of the following options: Click Session ID of your Spark interactive\n                            ession. Click the menu icon in the Actions column and click Open . Select either Python, R, or Scala as the statements' programming\n                    language. Enter statements in Python, R, or Scala. For example: Select Scala as programming language and calculate the value of\n                    Pi by running the following statement. val NUM_SAMPLES = 10000;\nval res = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random();\nval y = Math.random();\nif (x*x + y*y < 1) 1 else 0 }.reduce(_ + _);\nprintln(\"Pi is roughly \" + 4.0 * res / NUM_SAMPLES); Click the Run icon on the top right of the Statements pane. For example: Running the previous statement returns the following statement\n                        result: NOTE Each Spark interactive session\n                        expires in 60 minutes. Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "d4ab7704-e8dd-4e0f-b7dd-4faedd328453": "Release Date: January 20, 2022 New Features Master: Add support for systemd socket activation to the master. Scheduling/CLI: Add support for adjusting job priority and weight through the WebUI and CLI. Add experimental ROCm support. In the environment config for images and environment variables, the rocm key configures ROCm support. The gpu key has been renamed to cuda; gpu is still supported for backward compatibility, but its use is discouraged. Improvement Docs: Improve many pages to address onboarding gaps. Bug Fixes Master: Fix an issue where an update to an experiment\u2019s name wouldn\u2019t be reflected in its job representation until a master restart. Agent: Fix displayed CPU core count for CPU slots. WebUI: Fix an issue where the JupyterLab modal didn\u2019t pass the full config. WebUI: Fix the issue of the profiler filter UI not triggering updates. Improvements Logging: Decrease the volume of Docker image pull logs that are rendered into trial logs, and make the overall image pull progress more understandable by combining all layers\u2019 progress into a single progress bar. Deprecated Features Searcher: The Population Based Training searcher (pbt in the searcher config) will be removed in the next release. Model Registry: The API and Python interface will be returning to primarily identifying models based on their names, rather than their numeric IDs, in the next release. Removed Features Remove support for Python 3.6, which has reached end-of-life.",
        "0d5db34f-1ec9-44bd-9c40-874d9e75c733": "The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage.",
        "07a09ee8-a7da-443c-a3f4-3fc1e46fa3f1": "When an experiment finishes, the system will optionally delete some checkpoints to reclaim space. The save_experiment_best, save_trial_best and save_trial_latest parameters specify which checkpoints to save. If multiple save_* parameters are specified, the union of the specified checkpoints are saved.",
        "597209cf-c3a5-49e7-925f-d524e2dadea5": "Hive Glue Metastore Connection Parameters Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Accessing Data in External S3 Object Stores Describes how to access data in external object stores from clients, such as Spark and     Kubeflow notebooks. EzPresto Describes the EzPresto SQL query engine     and its featues. Connect Data Sources Provides instructions for connecting HPE Ezmeral Unified Analytics Software to external data sources. Delta Connection Parameters List of Delta connection parameters, descriptions, default values, and supported data     types. Delta Thrift Connection Parameters List of  Delta Thrift connection parameters, descriptions, default values, and     supported data types. Hive Connection Parameters List of Hive connection parameters, descriptions, default values, and supported data     types. Hive Glue Metastore Connection Parameters List of Hive Glue Metastore connection parameters, descriptions, default values, and     supported data types. Hive Thrift Metastore Connection Parameters List of Hive Thrift Metastore connection parameters, descriptions, default values, and     supported data types. Hive Discovery Metastore Connection Parameters Lists Hive discovery metastore connection parameters, parameter descriptions, default     values, and supported data types. MySQL Connection Parameters List of MySQL connection parameters, descriptions, default values, and supported data     types. Oracle Connection Parameters List of Oracle connection parameters, descriptions, default values, and supported data     types. Snowflake Connection Parameters List of  Snowflake connection parameters, descriptions, default values, and supported     data types. SQL Server Connection Parameters List of SQL Server connection parameters, descriptions, default values, and supported     data types. Teradata Connection Parameters List of  Teradata connection parameters, descriptions, default values, and supported     data types. PostgreSQL Connection Parameters List of PostgreSQL connection parameters, descriptions, default values, and supported     data types. Iceberg Connection Parameters List of Iceberg connection parameters, descriptions, default values, and supported data     types. Configuring a Hive Data Source with Kerberos Authentication Describes the required prerequisite steps to complete before you connect HPE Ezmeral Unified Analytics Software to a Hive data source     that uses Kerberos authentication. Connect to CSV and Parquet Data in an External S3 Data Source via Hive Connector Describes how to use the Hive connector with Presto in HPE Ezmeral Unified Analytics Software to connect to CSV and     Parquet data in S3-based external data sources. EzPresto/connect-external-s3-data-source.html Connect to External Applications via JDBC Describes how to connect external applications and BI tools, such as Tableau and     PowerBI, to EzPresto through the EzPresto JDBC endpoint. Using Spark to Query EzPresto Describes how to use Spark to query EzPresto . Connect to EzPresto via Python Client Provides information for connecting to EzPresto from a Python client. Cache Data Describes data caching and provides the steps for caching data in HPE Ezmeral Unified Analytics Software . Submitting Presto Queries from Notebook Describes how to submit Presto queries from the notebook. Airflow Provides an overview of Apache Airflow in HPE Ezmeral Unified Analytics Software . Superset Provides a brief overview of  Superset in HPE Ezmeral Unified Analytics Software . Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Hive Glue Metastore Connection Parameters List of Hive Glue Metastore connection parameters, descriptions, default values, and\n    supported data types. The following tables list the required and optional Hive  Glue Glue connection parameters. Required Connection Parameters NOTE Hive connector values vary based on the type of metastore. See https://prestodb.io/docs/current/connector/hive.html . Parameter Description Default Value Data Type Hive Metastore The type of Hive metastore to use thrift STRING Hive Metastore Glue Region AWS region of the Glue Catalog null STRING Hive Metastore Glue Aws Access Key AWS access key to use to connect to the Glue Catalog.",
        "6443ef6d-9d25-4b19-84d3-a1cb86b5602e": "Loading data into PyTorchTrial models is done by defining two functions, build_training_data_loader() and build_validation_data_loader(). Each function should return an instance of determined.pytorch.DataLoader. The determined.pytorch.DataLoader class behaves the same as torch.utils.data.DataLoader and is a drop-in replacement in most cases. It handles distributed training with PyTorchTrial. Each determined.pytorch.DataLoader will return batches of data, which will be fed directly to the train_batch() and evaluate_batch() functions. The batch size of the data loader will be set to the per-slot batch size, which is calculated based on global_batch_size and slots_per_trial as defined in the experiment configuration. See the following code as an example: def build_training_data_loader(self): traindir = os.path.join(self.download_directory, 'train') self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) train_dataset = datasets.ImageFolder( traindir, transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), self.normalize, ])) train_loader = determined.pytorch.DataLoader( train_dataset, batch_size=self.context.get_per_slot_batch_size(), shuffle=True, num_workers=self.context.get_hparam(\"workers\", pin_memory=True), ) return train_loader The output train_batch() returns a batch of data in one of the following formats: # A numpy array batch: np.ndarray = np.array([0, 0], [0, 0]]) # A PyTorch tensor batch: torch.Tensor = torch.Tensor([[0, 0], [0, 0]]) # A tuple of arrays or tensors batch: Tuple[np.ndarray] = (np.array([0, 0]), np.array([0, 0])) batch: Tuple[torch.Tensor] = (torch.Tensor([0, 0]), torch.Tensor([0, 0])) # A list of arrays or tensors batch: List[np.ndarray] = [np.array([0, 0]), np.array([0, 0])] batch: List[torch.Tensor] = [torch.Tensor([0, 0]), torch.Tensor([0, 0])] # A dictionary mapping strings to arrays or tensors batch: Dict[str, np.ndarray] = {\"data\": np.array([0, 0]), \"label\": np.array([0, 0])} batch: Dict[str, torch.Tensor] = {\"data\": torch.Tensor([0, 0]), \"label\": torch.Tensor([0, 0])} # A combination of the above batch = { \"data\": [ {\"sub_data1\": torch.Tensor([[0, 0], [0, 0]])}, {\"sub_data2\": torch.Tensor([0, 0])}, ], \"label\": (torch.Tensor([0, 0]), torch.Tensor([[0, 0], [0, 0]])), }",
        "621d0f98-bba8-41da-8d64-ecdcef988fd0": "At a typical organization, many Determined configuration files will contain similar settings. For example, all of the training workloads run at a given organization might use the same checkpoint storage configuration. One way to reduce this redundancy is to use configuration templates. With this feature, you can move settings that are shared by many experiments into a single YAML file that can then be referenced by configurations that require those settings. Each configuration template has a unique name and is stored by the Determined master. If a configuration specifies a template, the effective configuration of the task will be the result of merging the two YAML files (configuration file and template). The semantics of this merge operation is described under Configuration Templates: Merge Behavior. Determined stores this expanded configuration so that future changes to a template will not affect the reproducibility of experiments that used a previous version of the configuration template. A single configuration file can use at most one configuration template. A configuration template cannot use another configuration template.",
        "5ad1a5c2-72de-437d-b91b-71958cd48683": "Class Name: Enter main class of the application for Java or Scala\n                                        applications. Arguments: Click + Add Argument to add input\n                                        parameters as required by the application. NOTE To refer to data in mounted folders from application source\n                                        code, use file:// schema. If a Spark\n                                            application is reading a file from the shared or user volume and is taking a path to the file as an\n                                            application argument, the argument will be file://[mount-path]/path/to/input/file .\n                                            For\n                                            example: User Directory: file:///mounts/<user-name>-volume/\nShared Directory: file:///mounts/shared-volume/ Dependencies: Add dependencies in\n                            the Dependencies step. To add dependencies required to run your applications, select a\n                                dependency type from excludePackages, files, jars, packages,\n                                pyfiles, or repositories, and enter the value of the dependency. To\n                                add more than one dependency, click Add\n                                    Dependency . For example: Enter the package names as the values for the\n                                        excludePackages dependency type. Enter the locations of file, for example, s3://<path-to\n                                        file>, local://<path-to-file> as the values for files,\n                                        jars, pyfiles, or repositories. Driver Configuration: Configure the number of cores, core limits, and\n                                memory. The number of cores\n                            must be less than or equal to the core limit. See Configuring Memory for Spark Applications . When boxes in this wizard are left blank, the default values are set.\n                                The default values are as follows: Number of Cores: 1 Core Limit: unlimited Memory: 1g Executor Configuration: Configure the number of executors, number of cores,\n                                core limits, and memory. The number of\n                            cores must be less than or equal to the core limit. See Configuring Memory for Spark Applications . When boxes in this wizard are left blank, the default values are set.\n                                The default values are as follows: Number of Executors: 1 Number of Cores per Executor: 1 Core Limit per Executor: unlimited Memory per Executor: 1g Schedule Application: To schedule a Spark\n                            application to run at a certain time, toggle Schedule to\n                                Run . You can configure the frequency intervals and set\n                            the concurrency policy, successful run history limit, and failed run\n                            history limit. Set the Frequency Interval in two ways: To choose from predefined intervals, select Predefined Frequency Interval and\n                                    click Update to open a dialog with\n                                    predefined intervals. To set the frequency interval, select Custom\n                                        Frequency Interval . The Frequency\n                                        Interval accepts any of the following values: CRON expression with Field 1: minute (0\u201359) Field 2: hour (0\u201323) Field 3: day of the month (1\u201331) Field 4: month (1\u201312, JAN - DEC) Field 5: day of the week (0\u20136, SUN - SAT) Example: 0 1 1 * * , 02\n                                                  02 ? * WED, THU Predefined macro @yearly @monthly @weekly @daily @hourly Interval using @every <duration> Units: nanosecond (ns), microsecond (us, \u00b5s),\n                                                  millisecond (ms), second (s), minute (m), and hour\n                                                  (h). Example: @every 1h , @every 1h30m10s Review: Review the application details. Click\n                            the pencil icon in each section to navigate to\n                            the specific step to change the application configuration. To open an editor to change the application configuration using YAML\n                            in the GUI, click Edit YAML . You can use the\n                            editor to add the extra configuration options not available through the\n                            application wizard. To apply the changes, click Save\n                                Changes . To cancel the changes, click Discard\n                                Changes . To submit the application, click Create Spark\n                        Application on the bottom right of the Review step. Results The Spark application is created and will immediately run,\n            or will wait to run at its scheduled time. You can view it on the Spark\n                Applications screen. More information Submitting a Spark Wordcount Application Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "41221b7b-92a8-44ee-a15e-32a5ccda10fa": "Which resource pool the agent should join. Defaults to the value of default, which will work if and only if there is a resource pool named default. For more information please see Resource Pools.",
        "7073ad07-8b70-4718-a58e-4b64a51332b5": "The full URL of the master. A valid URL is in the format of scheme://host:port. The scheme must be either http or https. If the master is deployed on EC2, rather than hardcoding the IP address, you should use one of the following to set the host as an alias: local-ipv4, public-ipv4, local-hostname, or public-hostname. If the master is deployed on GCP, rather than hardcoding the IP address, you should use one of the following to set the host as an alias: internal-ip or external-ip. Which one you should select is based on your network configuration. On master startup, we will replace the above alias host with its real value. Defaults to http as scheme, local IP address as host, and 8080 as port.",
        "cff143a9-feca-4571-8946-b33139e7d4c0": "It provides a shared hierarchical             namespace that is organized like a standard file system. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "dbafc8ad-1bfa-43f0-8e30-2f6407fbfb6e": "The resource_manager section of the cluster configuration contributes the following resource scheduling configuration. See the slurm/pbs section of the cluster configuration reference for the full list of configuration options. slot_type: The default slot type (cuda, rocm, cpu) when users request resources from Determined in terms of slots_per_trial. May be overridden per partition/resource pool. tres_supported: Indicates if SelectType=select/cons_tres is set in the Slurm configuration. gres_supported: For Slurm, it indicates that GresTypes=gpu is set in the Slurm configuration, and nodes with GPUs have properly configured GRES indicating the presence of any GPUs. T For PBS, the ngpus resource can be used to identify the number of GPUs available on a node. default_aux_resource_pool: The default resource pool to use for tasks that do not need dedicated compute resources, auxiliary, or systems tasks. Defaults to the Slurm/PBS default partition if no resource pool is specified. default_compute_resource_pool: The default resource pool to use for tasks that require compute resources, e.g. GPUs or dedicated CPUs. Defaults to the Slurm/PBS default partition if it has GPU resources and if no resource pool is specified. job_project_source: Identifies the source to be used when generating a Slurm Workload Characterization Key (WCKey), or PBS project name.",
        "544c4fac-79ec-400f-ac7f-96ff44575348": "Managing Column Families and Columns Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Managing Column Families and Columns The topics in this section describe managing column families and columns. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Managing Tables The topics in this section describe managing tables. Managing Column Families and Columns The topics in this section describe managing column families and columns. Creating a Column Family This topic describes creating a column family. Configuring Column Family Permissions This topic describes configuring column family permissions for a table. Deleting a Column Family This topic describes deleting a column family. Viewing Table Information The topics in this section describe viewing table information. Managing Table Replication The topics in this section describe managing table replication. Administering Access Controls for Tables This topic describes administering access controls for tables. Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Managing Column Families and Columns The topics in this section describe managing column families and columns. Creating a Column Family This topic describes creating a column family. Configuring Column Family Permissions This topic describes configuring column family permissions for a table. Deleting a Column Family This topic describes deleting a column family. (Topic last modified: 2023-10-23) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "d9bce74f-cad1-496e-817f-acb36b4cc686": "Framework Dataset Filename PyTorch MNIST custom_reducers_mnist_pytorch.tgz",
        "0d708795-94d6-44d4-b46d-a6d2ef4d2c93": "Learn how to perform optimized distributed training with Determined to speed up the training of a single trial. In Concepts of Distributed Training, you\u2019ll learn about the following topics: How Determined distributed training works Reducing computation and communication overhead Training effectively with large batch sizes Model characteristics that affect performance Debugging performance bottlenecks Optimizing training Visit Implementing Distributed Training to discover how to implement distributed training, including the following: Connectivity considerations for multi-machine training Configuration including slots per trial and global batch size Considerations for concurrent data downloads Details to be aware regarding scheduler behavior Accelerating inference workloads Additional Resources: Learn how Configuration Templates can help reduce redundancy. Discover how Determined aims to support reproducible machine learning experiments in Reproducibility. In Optimizing Training, you\u2019ll learn about out-of-the box tools you can use for instrumenting training.",
        "c0ab2f5e-db21-4d16-ae68-7a801a032a5f": "| floor }}MB\n        query.max-total-memory-per-node={{ mulf 0.6 ( tpl .Values.ezsqlPresto.configMapProp.wrk.jvmProp.maxHeapSize . ) | floor }}MB\n        memory.heap-headroom-per-node={{ mulf 0.2 ( tpl .Values.ezsqlPresto.configMapProp.wrk.jvmProp.maxHeapSize . ) | floor }}MB\n        experimental.spill-enabled=false\n        experimental.spiller-spill-path=/tmp\n        orm-database-url=jdbc:sqlite:/data/cache/metadata.db\n        plugin.disabled-connectors=accumulo,atop,cassandra,example-http,kafka,kudu,localfile,memory,mongodb,pinot,presto-bigquery,prestodb,presto-druid,presto-elasticsearch,prometheus,raptor,redis,redshift\n        log.max-size=100MB\n        log.max-history=10\n        discovery.http-client.max-requests-queued-per-destination=10000\n        event.http-client.max-requests-queued-per-destination=10000\n        exchange.http-client.max-requests-queued-per-destination=10000\n        node-manager.http-client.max-requests-queued-per-destination=10000\n        workerInfo.http-client.max-requests-queued-per-destination=10000\n    jvmConfig:\n      jvm.config: |\n        -server\n        -Xms{{ tpl .Values.ezsqlPresto.configMapProp.wrk.jvmProp.minHeapSize . | floor }}M\n        -Xmx{{ tpl .Values.ezsqlPresto.configMapProp.wrk.jvmProp.maxHeapSize . | floor }}M\n        -XX:-UseBiasedLocking\n        -XX:+UseG1GC\n        -XX:G1HeapRegionSize={{ .Values.ezsqlPresto.configMapProp.wrk.jvmProp.G1HeapRegionSize }}\n        -XX:+ExplicitGCInvokesConcurrent\n        -XX:+HeapDumpOnOutOfMemoryError\n        -XX:+UseGCOverheadLimit\n        -XX:+ExitOnOutOfMemoryError\n        -XX:ReservedCodeCacheSize={{ .Values.ezsqlPresto.configMapProp.wrk.jvmProp.ReservedCodeCacheSize }}\n        -XX:PerMethodRecompilationCutoff=10000\n        -XX:PerBytecodeRecompilationCutoff=10000\n        -Djdk.attach.allowAttachSelf=true\n        -Djdk.nio.maxCachedBufferSize={{ .Values.ezsqlPresto.configMapProp.jvmProp.maxCachedBufferSize }}\n        -Dcom.amazonaws.sdk.disableCertChecking=true\n        -Djava.security.krb5.conf=/data/shared/krb5.conf\n### values_cmn_configmap.yaml contents END Click Configure . This updates the configuration on each of the presto pods and\n            restarts the pods. This operation can take a few minutes. Step 3 - Connect HPE Ezmeral Unified Analytics Software to the Hive data source In the left navigation bar, go to Data Engineering > Data\n              Sources . Click Add New Data Source . In the Hive tile, click Create Connection . Using the following connection properties as an example, add the connection properties\n            for your environment and then Connect. Name = kdchive\nHive Metastore = Thrift\nHive Metastore Uri = thrift://m2-dev.mip.storage.mycorp.net:9083\nHive Metastore Authentication Type=KERBEROS\nHive Metastore Service Principal=hive/_HOST@MYCORP.NET\nHive Metastore Client Principal=supergroup@MYCORP.NET\nHive Metastore Client Keytab=<Uploaded the keytab file for supergroup user>\nHive Hdfs Authentication Type=KERBEROS\nHive Hdfs Presto Principal=supergroup@MYCORP.NET\nHive Hdfs Presto Keytab=<Uploaded the keytab file for supergroup user> On this page Step 1 - Upload a krb5 configuration file to the shared location Step 2 - Configure EzPresto to use the krb5.conf file Step 3 - Connect HPE Ezmeral Unified Analytics Software to the Hive data source Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "323ab9ce-1be1-4e8d-b721-3156e45b9f6a": "Docker containers Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster.",
        "32b37005-53a7-4d77-88d6-46567fdcd253": "The following libraries are included in packaged versions of this project:\n\n* ClassMate\n * COPYRIGHT: Copyright 2010 The Apache Software Foundation\n * LICENSE: licenses/LICENSE.apache2.txt\n * HOMEPAGE: https://github.com/cowtowncoder/java-classmate\n\n* Confluent Common\n * COPYRIGHT: Confluent Inc.\n * LICENSE: licenses/LICENSE.apache2.txt\n * HOMEPAGE: https://github.com/confluentinc/common\n\n* Hamcrest\n * COPYRIGHT: Copyright (c) 2000-2006, www.hamcrest.org\n * LICENSE: licenses/LICENSE.bsd.txt\n * HOMEPAGE: http://hamcrest.org/\n\n* Hibernate\n * COPYRIGHT: licenses/COPYRIGHT.hibernate.txt\n * LICENSE: licenses/LICENSE.apache2.txt\n * HOMEPAGE: http://hibernate.org/validator/\n\n* HK2\n * COPYRIGHT: Copyright (c) 2010-2014 Oracle and/or its affiliates. All rights reserved.\n * LICENSE: licenses/LICENSE.cddl+gpl2.html\n * HOMEPAGE: https://hk2.java.net\n\n* Jackson annotations\n * LICENSE: licenses/LICENSE.jackson-annotations.txt (Apache 2)\n * HOMEPAGE: http://github.com/FasterXML/jackson\n\n* Jackson core\n * LICENSE: licenses/LICENSE.jackson-core.txt (Apache 2)\n * NOTICE: licenses/NOTICE.jackson-core.txt\n * HOMEPAGE: http://github.com/FasterXML/jackson\n\n* Jackson databind\n * LICENSE: licenses/LICENSE.jackson-databind.txt (Apache 2)\n * NOTICE: licenses/NOTICE.jackson-databind.txt\n * HOMEPAGE: http://github.com/FasterXML/jackson\n\n* Jackson jaxrs-json-provider\n * LICENSE: licenses/LICENSE.jackson-core.txt (Apache 2)\n * NOTICE: licenses/NOTICE.jackson-core.txt\n * HOMEPAGE: http://github.com/FasterXML/jackson\n\n* Javassist\n * COPYRIGHT: Copyright (C) 1999- by Shigeru Chiba, All rights reserved.\n * LICENSE: licenses/LICENSE.javassist.txt (MPL, LGPL, Apache 2)\n * HOMEPAGE: http://www.javassist.org\n\n* javax.annotation-api, javax.el, javax.el-api, javax.inject, javax.servlet, javax.ws.rs-api, javax.validation\n * COPYRIGHT: Coypright Oracle\n * LICENSE: licenses/LICENSE.cddl+gpl2.html\n\n* JBoss Logging\n * COPYRIGHT: Copyright 2014 Red Hat, Inc.\n * LICENSE: licenses/LICENSE.apache2.txt\n * HOMEPAGE: http://www.jboss.org\n\n* Jersey\n * LICENSE: licenses/LICENSE.cddl+gpl2.html\n * HOMEPAGE: http://jersey.java.net\n\n* Jetty\n * COPYRIGHT: Copyright Mort Bay Consulting Pty Ltd unless otherwise noted\n * LICENSE: licenses/LICENSE.apache2.txt, licenses/LICENSE.epl.html\n * NOTICE: licenses/NOTICE.jetty.txt\n * HOMEPAGE: http://eclipse.org/jetty/\n\n* JUnit\n * LICENSE: licenses/LICENSE.epl.txt\n * NOTICE: licenses/NOTICE.junit.txt\n * HOMEPAGE: http://junit.org/\n\n\n\n-----------------------------------------------------------\n\n\n\nKStreams\n\nCopyright (c) 2004, The Apache Software Foundation\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nHttpComponents\n\nCopyright (c) 2004, The Apache Software Foundation\n\nSource code: http://hc.apache.org\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nQuartz-Scheduler Hazelcast Job Store\n\nCopyright (c) 2004, The Apache Software Foundation\n\nSource code: https://github.com/FlavioF/quartz-scheduler-hazelcast-jobstore\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nQuartz\n\nCopyright (c) 2004, The Apache Software Foundation\n\nSource code: https://github.com/quartz-scheduler/quartz\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nAWS JAVA-SDK\n\nCopyright (c) 2004, The Apache Software Foundation\n\nSource code: https://aws.amazon.com/sdk-for-java\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nZIP4J\n\nCopyright (c) 2004, The Apache Software Foundation\n\nSource code: http://www.lingala.net/zip4j/\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.",
        "e333b2bf-3af4-41bd-b3bc-166fc5fd7416": "class determined.searcher.RemoteSearchRunnersearch_method: determined.searcher._search_method.SearchMethodcontext: determined.core._context.Context RemoteSearchRunner performs a search for optimal hyperparameter values, applying the provided SearchMethod (you will subclass SearchMethod and provide an instance of the derived class). RemoteSearchRunner executes on-cluster: it runs a meta-experiment using Core API. runexp_config: Union[Dict[str, Any], str]model_dir: Optional[str] = Noneincludes: Optional[Iterable[Union[str, pathlib.Path]]] = Noneint Run custom search as a Core API experiment (on-cluster). Parameters exp_config (dictionary, string) \u2013 experiment config filename (.yaml) or a dict. model_dir (string) \u2013 directory containing model definition. includes (Iterable[Union[str, pathlib.Path]], optional) \u2013 Additional files or directories to include in the model definition. (default: None)",
        "36962995-475f-41a6-896a-530da34b1c90": "Optional. The minimum number of slots required for a node to be scheduled during a trial. If gres_supported is false, specify slots_per_node in order to utilize more than one GPU per node. It is the user\u2019s responsibility to ensure that slots_per_node GPUs will be available on nodes selected for the job using other configurations such as targeting a specific resource pool with only GPU nodes or specifying a Slurm constraint in the experiment configuration.",
        "0bd68217-ea71-44bc-92e0-15d76175c895": "Required. The hostname or IP address of the Determined master.",
        "8d0c7951-564c-41a0-99be-584419cee5d2": "The Determined CLI has built-in help. Please see help for the top-level commands, as well as their subcommands: det user -h det user-group -h det rbac -h det rbac assign-role -h",
        "d9f26b0a-696a-4e4a-8eed-16b6451da4d6": "Download the complete code for this tutorial from mnist_pytorch.tgz. After downloading the file, open a terminal window, extract the file, and cd into the mnist_pytorch directory: tar xzvf mnist_pytorch.tgz cd mnist_pytorch Follow along with the code as you complete the tutorial.",
        "e759ace3-4693-4c3a-80cc-04c0601d332a": "gcr.io/mapr-252711/kubeflow/notebooks/jupyter-tensorflow-cuda-full:<image-tag> This image is packaged with data science packages and is integrated with\n                  TensorFlow machine learning libraries for GPU-based tasks. Use this image to perform data analysis, manipulation, and visualization for\n                  GPU-based machine learning tasks using TensorFlow library for faster model\n                  training and data processing. gcr.io/mapr-252711/kubeflow/notebooks/jupyter-data-science:<image-tag> This image is integrated with Tensorflow and PyTorch packages, including\n                  various other tools for data analysis, machine learning, and\n                  visualization. Use this image that is integrated with data science libraries to perform data\n                  science tasks requiring deep learning capabilities of TensorFlow and\n                  PyTorch. gcr.io/mapr-252711/kubeflow/notebooks/codeserver:<image-tag> This image enables you to run Visual Studio Code in the browser where you can\n                    edit and develop code in a remote server setup. This image features a VS Code environment, providing a code-server that runs\n                    Visual Studio Code in the browser, allowing for rich code editing and\n                    development experience in a remote server setup. In HPE Ezmeral Unified Analytics Software , the codeserver image includes VS\n                    Code and Python installation, and VS Code Python extension. Use this image to run Visual Studio Code in the browser where you can edit and\n                    develop code in a remote server setup. Image and Package Support For a list of supported notebook images and included packages in HPE Ezmeral Unified Analytics Software , see Notebook Images . On this page Image Format Supported Notebook Images Image and Package Support Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "a3d4eb39-0911-41c3-b29f-a319166ed4a8": "During __init__() of TorchBatchProcessor, we pass in a TorchBatchProcessorContext object, which contains useful methods that can be used within the TorchBatchProcessor class. TorchBatchProcessor is compatible with Determined\u2019s MetricReducer. You can pass MetricReducer to TorchBatchProcessor as follow:",
        "6e725b12-26d4-4b3a-a1a8-55178580b0f6": "When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. mirror A replica of a volume.",
        "30965e53-3637-43e7-8cb3-340cc5f5e291": "Observability Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Metering and Billing Describes metering and billing in HPE Ezmeral Unified Analytics Software . Monitoring Describes monitoring and alerting in HPE Ezmeral Unified Analytics Software . Logging Describes how logging works in HPE Ezmeral Unified Analytics Software and how to     access log files for the platform and applications. Audit Logging Describes auditing in HPE Ezmeral Unified Analytics Software and how to access audit logs. Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Observability Describes observability in HPE Ezmeral Unified Analytics Software . Metering and Billing Describes metering and billing in HPE Ezmeral Unified Analytics Software . Monitoring Describes monitoring and alerting in HPE Ezmeral Unified Analytics Software . Logging Describes how logging works in HPE Ezmeral Unified Analytics Software and how to     access log files for the platform and applications. Audit Logging Describes auditing in HPE Ezmeral Unified Analytics Software and how to access audit logs. Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "5c5f60a7-01cd-4d42-8932-d67bc5abc093": "The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage.",
        "be68a2c4-c7e3-443b-ada0-e2eb65ab3b01": "Configuring Email Notifications Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Configuring Email Notifications Describes how to configure the Simple Mail Transfer Protocol (SMTP) to send email     notifications from the Data Fabric UI to specified email     accounts. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . User and Role Management This page describes the roles supported by the HPE Ezmeral Data Fabric as-a-service platform. Viewing a List of Users Describes how to display a searchable list of Keycloak users that includes the names of     the users and their roles. Configuring Email Notifications Describes how to configure the Simple Mail Transfer Protocol (SMTP) to send email     notifications from the Data Fabric UI to specified email     accounts. Viewing and Editing Access Control Information Describes how to find and use the Access Control card that shows the access privileges     for users and groups. Access Control Expression Syntax This topic explains access control expression. Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Configuring Email Notifications Describes how to configure the Simple Mail Transfer Protocol (SMTP) to send email\n    notifications from the Data Fabric UI to specified email\n    accounts. The Data Fabric UI can notify you by email when alarms\n      are generated on a fabric. To configure email notifications, you must set up SMTP: Setting Up SMTP To set up SMTP: Sign in to the Data Fabric UI , and switch to the Fabric manager experience . Click Fabric administration . On the SMTP card, click Edit SMTP\n              settings . The Edit SMTP settings form is\n            displayed. Specify the following parameters: Parameter Description Example Provider* Select Office 365 , SMTP , or Other from the drop-down menu. If you select Office 365 , the SMTP server and port information is\n                      pre-filled for you. NOTE: Gmail is provided as an option, but is not currently\n                        supported because Gmail does not support unsecure emails from third-party\n                        applications. For more information, see this page . Office 365 SMTP server* The name of the mail server for the SMTP provider that you\n                      specified. smtp.office365.com This server requires an encrypted connection\n                      (SSL) Check this box if the connection to the SMTP server must be\n                      encrypted. N/A SMTP port* The SMTP port to use for sending mail. 587 Sender's full name* The name that the HPE Ezmeral Data Fabric should use\n                      when sending email. East Lab Data Fabric Sender's email address* The email address that the HPE Ezmeral Data Fabric should use when sending email. jennifer-huang87@outlook.com Sender's username (Optional) The user name that the HPE Ezmeral Data Fabric should use when logging on to the SMTP\n                      server. jennifer46 Sender's SMTP password (Optional) The password that the HPE Ezmeral Data Fabric should use when logging on to the SMTP\n                      server. mySMTP!pw Click Save . A message indicates if the configuration was\n            successful. See Setting Up Alarm Notifications .",
        "3d6d61f9-ce0b-47a1-b3bd-cd3be6fb3645": "Use the following steps to view fabric service status. Procedure Log on to the Data Fabric UI If you are a fabric user, click the Table View icon on the Resources card. If you are a fabric manager, select the Fabric\n                                manager option, and click Global namespace and check the table view. Click the link for the fabric under the Resource Name column. Navigate to the Services tab for the fabric. Results The details about the various fabric-related services along with the status of each\n                service is visible on the Services tab. (Topic last modified: 2023-11-01) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "225925f8-3e64-4ad5-aa09-e0b42c15e914": "org/projects/asm/\n\n-----------------------------------------------------------\n\nJLine (Java Library for Handling Console Input v. 2)\n\nSource Code: https://github.com/jline/jline2\n\nLicense: BSD License\n\nhttps://github.com/jline/jline2/blob/master/LICENSE.txt\n\n-----------------------------------------------------------\n\nOpenTSDB\n\nLGPL v2.1\nhttps://github.com/OpenTSDB/opentsdb/blob/master/COPYING.LESSER\n\n-----------------------------------------------------------\n\nApache Spark\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nSnappy 1.0.5\nNew BSD License\n\nhttp://opensource.org/licenses/BSD-3-Clause\n\n-----------------------------------------------------------\n\nHue\n\nCopyright (c) Cloudera\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nAnsible\n\nGPL\nhttps://github.com/ansible/ansible/blob/devel/COPYING\n\n-----------------------------------------------------------\n\nApache Drill\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\ngperftools 2.0\nNew BSD License\n\nhttp://opensource.org/licenses/BSD-3-Clause\n\n-----------------------------------------------------------\n\nApache ZooKeeper\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nSource code: http://zookeeper.apache.org\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nOpen\n\nApplication Interface (OJAI)\n\nCopyright (c) 2015 The Apache Software Foundation.\n\nSource code: https://github.com/ojai/ojai\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nApache Commons\n\nCopyright (c) 2003-2007 The Apache Software Foundation.\n\nSource code and additional copyright: http://commons.apache.org/\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nGoogle Collections (Guava)\n\nCopyright (c) 2007 Google Inc.\n\nSource code: http://code.google.com/p/guava-libraries/\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nApache Tomcat\n\nCopyright (c) 1999-2011 The Apache Software Foundation.\n\nSource code: http://tomcat.apache.org\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nJetty Web Container\n\nCopyright (c) 1995-2009 Mort Bay Consulting Pty Ltd.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nOpen Json\n\nAndroid JSON library\nCopyright (C) 2010 The Android Open Source Project\n\nSource code: https://github.com/tdunning/open-json\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nJUnit\n\nLicense: Common Public License - v 1.0\nhttp://www.junit.org/license\n\n-----------------------------------------------------------\n\nlog4j\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nJavaMail\n\nCopyright (c) 1997-2011, Oracle and/or its affiliates.\n\nSource code: http://www.oracle.com/technetwork/java/index-138643.html\n\nLicense: Oracle Corporation (\"ORACLE\") ENTITLEMENT for SOFTWARE\nSee below.\n\n-----------------------------------------------------------\n\nProtocol Buffers\n\nCopyright (c) 2008 Google Inc.\n\nSource code: http://protobuf.googlecode.com\n\nLicense: New BSD License\nhttp://www.opensource.org/licenses/bsd-license.php\n\n-----------------------------------------------------------\n\nuuid - DCE compatible Universally Unique Identifier library\n\nCopyright (C) 1996, 1997, 1998 Theodore Ts'o.\n\nLicense: below.",
        "8a6ea7a9-e848-4937-9f46-d264130a353e": "Transport Layer Security (TLS) is a protocol for secure network communication. TLS prevents the data being transmitted from being modified or read while it is in transit and allows clients to verify the identity of the server (in this case, the Determined master). Determined can be configured to use TLS for all connections made to the master. That means that all CLI and WebUI connections will be secured by TLS, as well as connections from agents and tasks to the master. Communication between agents that occur as part of distributed training will not use TLS, nor will proxied connections from the master to a TensorBoards or notebook instance. After the master and agent are configured to use TLS, no additional configuration is needed for tasks run in the cluster. In shells and notebooks, the Determined Python libraries automatically make connections to the master using TLS with the appropriate certificate.",
        "8ad96b04-0966-49cf-a35c-a507a81266cd": "Here are some suggested initial settings for adaptive_asha that typically work well. Search mode: mode: Set to standard. Resource budget: max_length: The maximum training length (see Training Units) of any trial that survives to the end of the experiment. This quantity is domain-specific and should roughly reflect the number of minibatches the model must be trained on for it to converge on the data set. For users who would like to determine this number experimentally, train a model with reasonable hyperparameters using the single search method. max_trials: This indicates the total number of hyperparameter settings that will be evaluated in the experiment. Set max_trials to at least 500 to take advantage of speedups from early-stopping. You can also set a large max_trials and stop the experiment once the desired performance is achieved. max_concurrent_trials: This field controls the degree of parallelism of the experiment. The experiment will have a maximum of this many trials training simultaneously at any one time. The adaptive_asha searcher scales nearly perfectly with additional compute, so you should set this field based on compute environment constraints. If this value is less than the number of brackets produced by the adaptive algorithm, it will be rounded up.",
        "2294ed07-104c-45f5-8a84-1d48c6e0cbbd": "Product Licensing Jump to main content Get Started Platform Administration Reference Home Reference Provides reference information for the HPE Ezmeral Data Fabric . Product Licensing Provides information related to product licensing. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Reference Provides reference information for the HPE Ezmeral Data Fabric . Release History Describes the currently released versions of the HPE Ezmeral Data Fabric as-a-service platform. Cloud Instance Specifications Compares different aspects of the supported cloud instances of the HPE Ezmeral Data Fabric . Third-Party Storage Solutions Describes global-namespace support for HPE partner storage technologies, including     Scality, WEKA, and VAST. Port Information Describes the ports used by HPE Ezmeral Data Fabric services. maprcli Commands in This Guide Describes how to use maprcli commands provided as reference links in     this guide. Operating System Support Matrix The tables on this page show the Linux operating-system versions that are supported for HPE Ezmeral Data Fabric releases. Doc Site Available as a PDF Provides a link to the downloadable PDF file containing all the information for the     current release. Product Licensing Provides information related to product licensing. Additional License Authorizations (ALA) Provides Additional License Authorizations for HPE Ezmeral Software, including HPE     Ezmeral Runtime Enterprise, HPE Ezmeral ML Ops, HPE Ezmeral Data Fabric, and Open Source     Software. Open-Source Software Acknowledgements (Release 7.6.0) Provides licensing information and acknowledges the use of open-source projects with     HPE software. Other Resources Provides links to additional resources such as on-demand training, videos, blogs, and     the HPE Ezmeral Data Fabric community. Contact HPE Provides a link to contact HPE Sales or Support. Product Licensing Provides information related to product licensing. Additional License Authorizations (ALA) Provides Additional License Authorizations for HPE Ezmeral Software, including HPE     Ezmeral Runtime Enterprise, HPE Ezmeral ML Ops, HPE Ezmeral Data Fabric, and Open Source     Software. Open-Source Software Acknowledgements (Release 7.6.0) Provides licensing information and acknowledges the use of open-source projects with     HPE software. (Topic last modified: 2023-04-20) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "bcc60c88-1cff-4577-bc57-09c27cd7d135": "Fit distributed jobs onto agents of different sizes. When enabled, we still prefer to fit jobs on same sized nodes but will fallback to allow heterogeneous fits. Sizes should be powers of two for the fitting algorithm to work.",
        "19aa5bd2-b4cc-4033-987a-9b1e8d743b84": "minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation. When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system.",
        "63bc8c5f-d9f1-485d-ac76-988464cc0b35": "det deploy aws down --cluster-id CLUSTER_ID Argument Description Default Value --cluster-id Unique ID for the cluster (used as the CloudFormation stack name). required --region AWS region deployed into. The default region for the AWS user --profile AWS profile used for deploying cluster resources. default",
        "c519d560-697d-4971-ae9c-c59369872079": "To install the master, we will launch an instance from the Determined master image. Let\u2019s start by navigating to the Compute Engine Dashboard of the GCP Console. Click \u201cCreate Instance\u201d and follow the instructions below: Choose Machine Type: we recommend a n1-standard-2 or more powerful. Configure Boot Disk: Choose Boot Disk Image: find the Determined master image in \u201cImages\u201d and click \u201cSelect\u201d. Set Boot Disk Size: set Size to be at least 100GB. If you have a previous Determined installation that you are upgrading, you want to use the snapshot or existing disk. This disk will be used to store all your experiment metadata and checkpoints. Configure Identity and API access: choose the service account according to GCP API Access. Configure Firewalls: choose or create a security group according to these Set up Internet Access. Check off Allow HTTP traffic. Review and launch the instance. SSH into the Determined master and edit the config at /usr/local/determined/etc/master.yaml according to the guide on Configuring the Cluster. Start the Determined master by entering make -C /usr/local/determined enable-master into the terminal.",
        "798c3020-1b01-4bdb-b502-ef84f1ff618e": "Creating a Local Repository on Ubuntu Jump to main content Get Started Platform Administration Reference Home Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. Creating a Local Repository for an Air-Gapped Installation Describes how to make installation packages available through a local repository for an     air-gapped installation. Creating a Local Repository on Ubuntu Describes how to create and use a local repository for Ubuntu. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Release Notes These notes contain information about release 7.6.0 of the HPE Ezmeral Data Fabric as-a-service platform. Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. Prerequisites for On-Premises Installation Describes fabric node and user prerequisites for on-premises installation of the HPE Ezmeral Data Fabric . AWS Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Amazon Web Services (AWS). Azure Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Microsoft Azure. GCP Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Google Cloud Platform (GCP). On-Premises Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric that is hosted on-site. Creating a Local Repository for an Air-Gapped Installation Describes how to make installation packages available through a local repository for an     air-gapped installation. Creating a Local Repository on RHEL, Rocky, or Oracle Linux Describes how to create and use a local repository for RHEL, Rocky, or Oracle     Linux. Creating a Local Repository on SLES Describes how to create and use a local repository for SLES. Creating a Local Repository on Ubuntu Describes how to create and use a local repository for Ubuntu. Troubleshooting Seed Node Installation Describes some common issues that can interfere with seed node     installation. Planning Worksheet for Cloud Deployments Print this worksheet, and use it to record configuration information for your cloud     deployment. Help for datafabric_container_setup.sh From the Docker command line, you can access the help text for the datafabric_container_setup.sh script. Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. SSO Using Keycloak Describes how single sign-on (SSO) is implemented by using       Keycloak. Setting Up Clients Summarizes the steps for enabling client communication with the HPE Ezmeral Data Fabric . Upgrade This section contains information that describes how to upgrade the HPE Ezmeral Data Fabric as-a-service platform. User Assistance Describes how to access different resources that can help you learn how to use the HPE Ezmeral Data Fabric . Creating a Local Repository on Ubuntu Describes how to create and use a local repository for Ubuntu. Ensure that you have access to the HPE internet repository so that you can download\n          package files. For more information, see Accessing the HPE Ezmeral Token-Authenticated Internet Repository . On the machine where you will set up the repository, log in as root . Change to the directory /root , and create the following directories\n          within it: ~/mapr\n|---dists\n|------binary\n|---------optional\n|------------binary-amd64\n|---mapr On a computer that is connected to the internet, download the following files,\n          substituting the appropriate <version> and\n          <datestamp>: https://package.ezmeral.hpe.com/releases/v7.x.x/ubuntu/mapr-<version>GA.deb.tgz\nhttps://package.ezmeral.hpe.com/releases/MEP/MEP-<version>/ubuntu/mapr-mep-<version>-<datestamp>.deb.tgz Copy the files to /root/mapr/mapr on the node, and extract them\n          there: tar -xvzf mapr-<version>GA.deb.tgz\ntar -xvzf mapr-mep-<version>-<datestamp>.deb.tgz Navigate to the /root/mapr directory. Use dpkg-scanpackages to create Packages.gz in the binary-amd64 directory: dpkg-scanpackages .",
        "9796bcb7-3a99-4771-82cb-1a67497b6384": "Single Sign-On (SSO) Support Jump to main content Get Started Platform Administration Reference Home Platform This section contains conceptual information that can help you to understand and use     the HPE Ezmeral Data Fabric . Single Sign-On (SSO) Support Describes how the HPE Ezmeral Data Fabric supports single sign-on     (SSO). HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Platform This section contains conceptual information that can help you to understand and use     the HPE Ezmeral Data Fabric . Data Fabric UI Describes the graphical user interface for the HPE Ezmeral Data Fabric . Global Namespace (GNS) Describes the data plane that connects all of your HPE Ezmeral Data Fabric deployments. Single Sign-On (SSO) Support Describes how the HPE Ezmeral Data Fabric supports single sign-on     (SSO). Iceberg Support Describes support for Iceberg in HPE Ezmeral Data Fabric 7.6.0. Fabric Resources Describes fabric resources. Data Storage Management Summarizes options that the HPE Ezmeral Data Fabric provides to         give you access to your data. AWS Architecture Notes Describes architectural considerations for the HPE Ezmeral Data Fabric software-as-a-service (SaaS) platform as deployed on     Amazon AWS. Single Sign-On (SSO) Support Describes how the HPE Ezmeral Data Fabric supports single sign-on\n    (SSO). Keycloak IAM Support The HPE Ezmeral Data Fabric supports SSO when configured with the\n        Keycloak identity and access management (IAM) solution. Other IAM solutions are not\n        currently supported. Keycloak Is Preinstalled and Preconfigured Starting with release 7.5.0, Keycloak is preinstalled and preconfigured whenever you create\n        a new fabric. You can create new users and roles easily and quickly by using the Keycloak\n        administration console. For more information, see SSO Using Keycloak . Limitation for Non-SSO Users SSO users with sufficient credentials can view and manage resources on all fabrics. Non-SSO\n        users can view and manage resources only on the fabric to which they are signed in. Non-SSO\n        users cannot view or manage resources on other fabrics. The Data Fabric UI does not display\n        these resources to non-SSO users because the UI cannot connect to other fabrics without the\n        same login information. (Topic last modified: 2023-10-29) On this page Keycloak IAM Support Keycloak Is Preinstalled and Preconfigured Limitation for Non-SSO Users \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "16f97a5d-4b8b-459e-8525-9958dc10c15e": "SAML integration applies only to Determined Enterprise Edition. Determined EE provides a SAML integration to allow users to use single sign-on (SSO) with their organization\u2019s identity provider (IdP) and to provide system administrators better control over access to resources. Currently, the only officially supported identity provider is Okta.",
        "8111086b-8694-4635-a618-9e68926b795c": "Install PostgreSQL 10 or greater. Debian Distributions On Debian distributions, use the following command: sudo apt install postgresql-10 Red Hat Distributions On Red Hat distributions, you\u2019ll need to configure the PostgreSQL yum repository as described in the Red Hat Linux documentation. Then, install version 10: sudo yum install postgresql-server -y sudo postgresql-setup initdb sudo systemctl start postgresql.service sudo systemctl enable postgresql.service The authentication methods enabled by default may vary depending on the provider of your PostgreSQL distribution. To enable the determined-master to connect to the database, ensure that an appropriate authentication method is configured in the pg_hba.conf file. When configuring the database connection as described in Configure and Start the Cluster, note the following: If you specify the db.hostname property, you must use a PostgreSQL host (TCP/IP) connection. If you omit the db.hostname property, you must use a PostgreSQL local (Unix domain socket) connection. Finally, create a database for Determined\u2019s use and configure a system account that Determined will use to connect to the database. For example, executing the following commands will create a database named determined, create a user named determined with the password determined-password, and grant the user access to the database: sudo -u postgres psql postgres=# CREATE DATABASE determined; postgres=# CREATE USER determined WITH ENCRYPTED PASSWORD 'determined-password'; postgres=# GRANT ALL PRIVILEGES ON DATABASE determined TO determined;",
        "1beaf01d-2e8e-4e44-a009-1d998fe7daaf": "Domain Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node.",
        "6bf2074b-8b5b-45d4-994f-4f1fa3e663f8": "Installing the Data Fabric Client on SLES Jump to main content Get Started Platform Administration Reference Home Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Setting Up Clients Summarizes the steps for enabling client communication with the HPE Ezmeral Data Fabric . Installing Clients on a Linux Host Describes how to install the client on a Linux host. Installing the Data Fabric Client on SLES This section describes how to install the Data Fabric Client on SLES. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Release Notes These notes contain information about release 7.6.0 of the HPE Ezmeral Data Fabric as-a-service platform. Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. SSO Using Keycloak Describes how single sign-on (SSO) is implemented by using       Keycloak. Setting Up Clients Summarizes the steps for enabling client communication with the HPE Ezmeral Data Fabric . Installing Clients on a Linux Host Describes how to install the client on a Linux host. Installing the Data Fabric Client on RHEL This section describes how to install the Data Fabric client on Red Hat Enterprise Linux (RHEL). Installing the Data Fabric Client on SLES This section describes how to install the Data Fabric Client on SLES. Installing the Data Fabric Client on Ubuntu This section describes how to install the Data Fabric client on Ubuntu. Setting up the Data Fabric Repository This section describes how to make packages available through the HPE Ezmeral Data Fabric repository. Installing Client Libraries Describes how to install the client libraries on a fabric to enable communication     between your Linux hosts and the HPE Ezmeral Data Fabric . Upgrade This section contains information that describes how to upgrade the HPE Ezmeral Data Fabric as-a-service platform. User Assistance Describes how to access different resources that can help you learn how to use the HPE Ezmeral Data Fabric . Installing the Data Fabric Client on SLES This section describes how to install the Data Fabric Client on SLES. Remove any previous Data Fabric software. You\n          can use rpm -qa | grep mapr to get a list of installed Data Fabric packages: rpm -qa | grep mapr Then type the package names\n            separated by spaces after the zypper rm command. For\n            example: zypper rm mapr-fileserver mapr-core Run the following command to install the Data Fabric client: zypper install mapr-edf-clients Open the Data Fabric UI to complete the\n          configuration, as described in Installing Client Libraries Step 6. (Topic last modified: 2024-02-02) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "9404d4bb-7a55-4eb8-8357-3bfd2175fe27": "Required. The name of the validation metric used to evaluate the performance of a hyperparameter configuration.",
        "fdb359ed-68c8-4a18-9851-7d0189817c8f": "Administering Storage Policies Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Data Tiering Conceptual information about data tiering. Administering Storage Policies Manage storage policies related to data tiering. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Creating a Standard Volume Procedure to create standard volume. Creating a Mirror Volume Procedure to create mirror volume. Converting Standard Volume to Mirror Volume Editing a Volume Edit accountable entity, volume access for accountable entity and volume hard         quota. Renaming a Volume Rename a volume. Viewing Volume Endpoint Info View volume endpoint information. Viewing Object Endpoint Info to Remotely Access Files as Objects View endpoint information for files in a volume to be able to access the files as         objects when accessed by S3 client. Downloading Volume Endpoint Information Download JSON file containing endpoint information for the selected volume endpoint         information. Deleting a Volume Delete a single volume. Administering Volume Snapshots Snapshot overview and administering snapshots. Data Tiering Conceptual information about data tiering. Schedules for Volume Data Tiering Describes schedules for data tiering of volume data Manually Offloading Data to a Cold Tier Recalling Data to the Data Fabric File System Administering Storage Policies Manage storage policies related to data tiering. Creating a Storage Policy Editing a Storage Policy Deleting Storage Policy Delete storage policy. Administering Remote Targets Administering Schedules Introduction to schedules. Mirroring Synopsis of mirrors and mirroring process. Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Administering Storage Policies Manage storage policies related to data tiering. You can configure a storage policy (or rules) for\n                data at the volume level. A storage policy simplifies the lifecycle\n                management of data in the volume including automated migration of files to low-cost\n                storage alternatives. A storage policy contains rules for files that have a\n                well-defined lifecycle or for files you want to switch to different storage tiers\n                during their lifecycle. You can specify the rules, at the volume level, to\n                selectively identify files to offload (such as file size, file owner, and file\n                modification time), the schedule for offloading the data (for example, two months\n                after file modification), and the settings for storing (such as the location and\n                credentials for the tier) and recalling the offloaded data. You can configure one\n                rule per volume. You can also associate a schedule to automatically offload data at\n                scheduled intervals based on the associated rules. Data offload is driven by\n                rules, which are configured per volume. Data offload rule can be based on size of\n                file ( s ), owner ( u , g , or p ) of the file, and/or file modification timestamp\n                    ( m ). You can apply one rule per volume. When a rule is\n                associated with a volume, the rule is first applied on the files in the\n                tiering-enabled volume.",
        "0390e871-2d3f-4d2b-8bbb-0aca8d235c19": "Using whylogs with Spark Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Spark Provides a brief overview of Apache Spark in HPE Ezmeral Unified Analytics Software . Using Spark Images Describes different types of Spark images supported by HPE Ezmeral Unified Analytics Software . List of Spark Images Lists the Spark images distributed by HPE Ezmeral Unified Analytics Software . These         images enables you to run the Spark applications in an air-gapped environment. Creating Spark Applications Describes how to create Spark applications using HPE Ezmeral Unified Analytics Software . Managing Spark Applications Describes how to view and manage Spark applications using HPE Ezmeral Unified Analytics Software . Configuring Memory for Spark Applications Describes how to set memory options for Spark applications. Creating Interactive Sessions Describes how to create interactive sessions in HPE Ezmeral Unified Analytics Software . Submitting Statements Describes how to submit statements in HPE Ezmeral Unified Analytics Software . Managing Interactive Sessions Describes how to view and manage Spark interactive sessions in HPE Ezmeral Unified Analytics Software . Spark History Server Provides an overview of Spark History Server. Using Spark SQL API Describes how to use Spark SQL API in HPE Ezmeral Unified Analytics Software . Enabling GPU Support for Spark Describes NVIDIA spark-rapids accelerator support for Spark, and how     to enable and allocate the GPU resources on Spark. Securely Passing Spark Configuration Values Describes how to pass the sensitive data to Spark configuration using the Kubernetes     Secret. Running Spark Applications in Namespaces Describes how namespaces work with regard to Spark applications in HPE Ezmeral Unified Analytics Software . Using whylogs with Spark Describes how to use whylogs with Spark. Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Using whylogs with Spark Describes how to use whylogs with Spark. Prerequisites Sign in to HPE Ezmeral Unified Analytics Software as a\n            member. About this task In HPE Ezmeral Unified Analytics Software , whylogs is integrated to work with Livy\n                sessions submitted through Kubeflow notebooks using the %manage_spark magic function. You can use whylogs with Spark to\n                profile, visualize, and monitor the data and detect. To use whylogs with Spark, refer to Data Validation example and WhyLogs Profiling example in GitHub. The basic steps are outlined as\n                    follows: Create a notebook or import your notebook into HPE Ezmeral Unified Analytics Software . See Creating and Managing Notebook Servers . Enter the %manage_spark command in your notebook and\n                        configure your Spark session through different tabs. You must select the\n                        authentication as Single Sign-On and the runtime language as Python. To\n                        learn about creating sessions by using %manage_spark , see %manage_spark . Enter the %config_spark magic in your notebook and update\n                        the value of spark.kubernetes.container.image property to gcr.io/mapr-252711/spark-3.5.0:202401221805R .\n                        Click Submit when done. To learn about using %config_spark , see %config_spark . Verify that your created session is in the Idle state. You can verify by clicking the Manage Sessions tab or by navigating to the Spark Interactive\n                            Sessions screen. See Managing Interactive Sessions . Once the session is in the Idle state, you can set\n                        the environment variables and import the required libraries and modules from\n                        whylogs. Create data frames to profile the data or validate the data with whylogs and\n                        run the notebook. Once you finish running your notebook, navigate back to the HPE Ezmeral Unified Analytics Software home screen. In the left navigation bar, go to Data Engineering > Data\n                            Sources . Click Browse . Go to the /shared/<spark-whylogs> folder which is a path\n                        set in your notebook to store the logs from whylogs. You can see that the\n                        data pro\ufb01les and the drift summary report are stored in the shared volume in\n                        the .html and .bin formats. To download a summary report, select Download from\n                        the Actions menu. Results You can analyze the summary report to detect drifts and monitor your data. Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "dd191a90-579a-43f1-8f1d-c91ddf116d20": "Release Date: January 26, 2022 Breaking Changes API: Routes with /api/v1/models/:id/* are replaced by /api/v1/models/:name/*. Spaces and special characters in a name must be URI-encoded. You can get a model by ID with /api/v1/models?id=<id>. API: On the list of models (/api/v1/models) the optional name parameter is now a case-sensitive match, unless you add the parameter name_case_insensitive=true. Python API: determined.experimental.client.Determined.get_model() now takes a name rather than an ID. Use determined.experimental.client.Determined.get_model_by_id() to get a model from its ID. Model Registry: New model names must not be blank, have a slash, have multiple spaces, only numbers, or be case-insensitive matches to an existing model name. Model Registry: Model names with a forward slash will replace the slash in the name with \u2018\u2013\u2018. Bug Fixes Master: Fix a bug in the priority scheduler where jobs with equal priority would be scheduled or preempted in an order not correctly respecting job submission time. Removed Features API: remove /experiment-list, /experiment-summaries, and /:experiment_id/kill endpoints from the legacy API. These functions are now replaced by the gRPC API (/api/v1/experiments) in the web UI, CLI, and tests.",
        "669a77b3-8b0a-449c-aae8-1479b5012e80": "Authorization system to use. Defaults to basic. See RBAC docs for further info.",
        "8013c582-5358-4413-afce-7587a60ecd41": "Optional. Whether to compress gradients when they are exchanged during distributed training. Compression may alter gradient values to achieve better space reduction. Defaults to false.",
        "d4c55b28-26ec-43c2-ace0-c26071a28074": "Step 2 validated that your Trial API calls work as expected. This step uses your code to run an actual Determined training loop with abbreviated workloads to make sure that it meets Determined requirements. This step assumes you have a working local environment for training. If you do not, skip to Step 4. Create an experiment using the following command: det experiment create myconfig.yaml my_model_dir --local --test The --local argument specifies that training occurs where you launched the experiment instead of occurring on a cluster. The --test argument runs abbreviated workloads to try to detect bugs sooner and exits immediately. The test is considered to have passed if the command completes successfully. Diagnose failures: Local test mode performs the following actions: Builds a model. Runs a single batch of training data. Evaluates the model. Saves a checkpoint to a dummy location. If your per-method checks in Step 2 passed but local test mode fails, your Trial subclass might not be implemented correctly. Double-check the documentation. It is also possible that you have found a bug or an invalid assumption in the Determined software and should file a GitHub issue or contact Determined on Slack.",
        "761303c1-25be-406f-88b9-2f970221245c": "Viewing Fabric Settings Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Viewing Fabric Settings Describes how to view and change the fabric settings, which include fabric auditing,     data auditing, and gateway information. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Configuring a Proxy Server for Data Fabric Access to the Internet Describes the procedure to configure an https or http proxy for scenarios where         communication between the internet and Data Fabric must happen over a proxy         server. Creating a Fabric Fabrics make it possible for you to create volumes, buckets, and topics in a cloud or     on-premises deployment. If your organization has multiple departments or multiple use cases to     support, you can create multiple fabrics. This page describes the basic steps to create a new     fabric for any of the supported fabric providers (AWS, Azure, GCP, and on-premises). Importing a Fabric This section provides the steps to import an as-a-service fabric into the global     namespace. Viewing the Fabric Status Describes how to use the Global namespace card. Viewing Fabric Settings Describes how to view and change the fabric settings, which include fabric auditing,     data auditing, and gateway information. Viewing the Fabric Endpoint Describes how to view the endpoint for a fabric on the Data Fabric UI. Viewing the Software Version Describes several ways to identify the core software version for a fabric. Generating S3 Access Keys for the Global Namespace Describes how to obtain an S3       user access key and secret key that can be used to perform operations on S3 resources anywhere       in the global namespace. Setting a Quota for a User Set quota for an individual user. Setting a Quota for a Group Set quota for an individual group. Viewing Fabric-Related Metrics Explains the various fabric-related metrics visible on the overview/data_fabric_ui.html . Setting Default Quotas for Users/Groups Set default values for user and group quotas on a fabric via the Data Fabric UI . Viewing the Fabric Service Status View status of various services running on a fabric. View Capacity Usage by User on Fabric Describes how to check the capacity used by various internal volumes, buckets,             topics , and binary tables created by the user that is logged         in to the Data Fabric UI. SSH Access to a Cloud-Based Fabric Describes how to obtain a fabric-specific .pem file that enables SSH     access to a cloud-based fabric. Deleting a Fabric Delete a remote fabric from the global namespace. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Viewing Fabric Settings Describes how to view and change the fabric settings, which include fabric auditing,\n    data auditing, and gateway information. About the Fabric Settings Fabric settings currently include: Setting Default Value Description Fabric auditing Off Auditing of fabric-management operations and fabric administration. Data auditing Off Auditing of data-access operations. Gateway N/A The Gateway parameter is not currently supported. For table replication,\n                    generate a DNS record that specifies the location of the gateways in the fabric,\n                    which can copy and paste into the zone file for your domain.",
        "2c2f71a3-2bbf-4775-9692-4261254f128b": "The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage.",
        "b0b46e4b-0d82-49d0-a3fe-d8286753752b": "data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster. The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components.",
        "1672493a-6b4b-40dd-9d68-33b3d8aa2ea6": "The activation key file is a signed\n                  XML file. Service activation and billing in connected environments is mostly automated. The only\n        manual process that the administrator performs is going to MY HPE SOFTWARE\n          CENTER and downloading the activation key file and then uploading the file into Unified Analytics to activate the product.\n        The activation key is valid for the length of the contract, typically one, three, or five\n        years unless the contract is made invalid, such as product cancellation or failure to meet\n        the contractual agreement. To activate Unified Analytics , an\n        administrator completes the following steps: Install and deploy Unified Analytics .\n            For connected environments, select the Connected option during installation. The system\n            provides the URL to access Unified Analytics . Go to the Unified Analytics UI URL\n            provided. The window displays a Platform ID and requests an activation key. You cannot\n            proceed with activation until you provide the activation key file. Copy the unique Platform ID. After purchasing HPE Ezmeral Unified Analytics Software , the activation key is made available to you through the Activate your products button in the HPE Subscription Electronic Receipt email that you receive from HPE. This receipt directs you to MY HPE\n              SOFTWARE CENTER where you can activate your product. On the Activate EON page, enter the Platform ID (copied in step 3) in the Platform ID\n            field. Once activation is completed, download the Unified Analytics activation key file. Return to the Unified Analytics URL and\n            upload the activation key file. Billing Process in Connected Environments When the activation key is uploaded, the cluster registers with the HPE billing service.\n        Consumption data is uploaded to the HPE billing service on an hourly basis. Consumption data\n        is based on the vCPU used by applications every hour. Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "89b6e383-49e7-4cd0-9540-369229faa093": "desired replication factor Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster. The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs.",
        "0daef4e3-5d16-47ef-87c4-538edc596727": "OAuth 2.0 applies only to Determined Enterprise Edition. Only the SCIM endpoints are supported. Determined EE allows requests to certain endpoints to be authenticated using OAuth 2.0 with the authorization code flow.",
        "31e75b09-4aa2-48a2-8d46-e7ad07063b6d": "Install Prometheus on any node in the monitored cluster. Launch Prometheus with the provided prometheus.yml configuration file. To replace the placeholder master address, you\u2019ll need to edit the Prometheus configuration file. The metric_relabel_configs parameter edits certain label names in jobs for joining in PromQL. The scrape_interval parameter values can be modified to optimize for resolution/size/time. The $PATH_TO_TOKEN specifies a path to an authorization token for the Determined master. This can be kept in a local file by running the token-refresh.sh script in the same directory with a CRON job (set to run daily).",
        "a5fa01b4-aa4d-410b-b560-77b00a45cb36": "In the Experimental section of your user settings, you can turn experimental features on or off. However, if you don\u2019t know what the feature is referring to or the possible impact, you likely should not turn it on. Experimental features are pre-release features. They can be changed or removed at any time.",
        "a25f6450-c11c-4a5d-89cb-cb860db8f93f": "Release Date: January 25, 2021 New Features Update experiment details pages to include a learning curve visualization. This will enable a comparison of hyperparameter performance among many different trials within an experiment. Support Elasticsearch as an alternative backend for logging. Read more about Elasticsearch-backed logging to see if it\u2019s appropriate for your Determined deployment. Improvements Breaking Change: REST API: Update trial logs API to return string IDs. WebUI: Enable filtering of trial logs by agent, container, rank, log level, and timestamp. WebUI: Improve section contrast on all pages. Deployment: Add the command det-deploy aws list, which shows all the CloudFormation stacks that are managed by det-deploy aws (using the tag managed-by: determined). This only applies to new deployments since this version, not previous deployments. Update examples to use the new PyTorch APIs. Deprecated Features The old PyTorch API was deprecated in 0.12.13 and will be removed in the next release. See the PyTorch migration guide for details on updating your PyTorch model code to use the new API.",
        "e184e4eb-9291-43d0-a332-84decf599d83": "Defines the default pod spec which will be applied to all CPU-only tasks when running on Kubernetes. See Customize a Pod for details.",
        "1f668b2f-1918-4f06-9d94-c66ce02bc541": "-----------------------------------------------------------\n\n\"prismjs@1.29.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/PrismJS/prism\",\n    \"licenseUrl\": \"https://github.com/PrismJS/prism/blob/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"property-information@5.6.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/wooorm/property-information\",\n    \"licenseUrl\": \"https://github.com/wooorm/property-information/blob/main/license\",\n\n-----------------------------------------------------------\n\n\"refractor@3.6.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/wooorm/refractor\",\n    \"licenseUrl\": \"https://github.com/wooorm/refractor/blob/main/license\",\n\n-----------------------------------------------------------\n\n\"space-separated-tokens@1.1.5\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/wooorm/space-separated-tokens\",\n    \"licenseUrl\": \"https://github.com/wooorm/space-separated-tokens/blob/main/license\",\n\n-----------------------------------------------------------\n\n\"xtend@4.0.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/Raynos/xtend\",\n    \"licenseUrl\": \"https://github.com/Raynos/xtend/blob/master/LICENSE\",\n\n-----------------------------------------------------------\n\ncommons-beanutils\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\ncommons-configuration\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\njoda-time\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\njna\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\ncommons-lang\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nehcache-core\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nannotations\n\nLicense: GNU Lesser Public License\nhttp://www.gnu.org/licenses/lgpl.html\n\n-----------------------------------------------------------\n\nhazelcast\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\njersey-server\n\nLicense: CDDL+GPL License\nhttp://glassfish.java.net/public/CDDL+GPL_1_1.html\n\n-----------------------------------------------------------\n\nlibpam4j\n\nLicense: The MIT license\nhttp://www.opensource.org/licenses/mit-license.php\n\n-----------------------------------------------------------\n\nlombok\n\nLicense: The MIT License\nhttps://projectlombok.org/LICENSE\n\n-----------------------------------------------------------\n\nspring-security-core\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nspring-security-kerberos-core\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nswagger-annotations\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nApache Ranger\n\nCopyright 2014-2022 The Apache Software Foundation\n\nLicense: Apache License Version 2.0, January 2004\nhttp://www.apache.org/licenses/LICENSE-2.0\n\n-----------------------------------------------------------\n\nApache NiFi\n\nCopyright 2014-2022 The Apache Software Foundation\n\nLicense: Apache License Version 2.0, January 2004 \nhttp://www.apache.org/licenses/LICENSE-2.0\n\n-----------------------------------------------------------\n\nApache Airflow\n\nCopyright 2016-2021 The Apache Software Foundation\n\nLicense: Apache License  Version 2.0, January 2004  \nhttp://www.apache.org/licenses/LICENSE-2.0",
        "a8a78463-0dc6-4ee4-acd4-cd225089e6d7": "AWS Fabric Configuration Parameters Jump to main content Get Started Platform Administration Reference Home Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. AWS Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Amazon Web Services (AWS). HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Release Notes These notes contain information about release 7.6.0 of the HPE Ezmeral Data Fabric as-a-service platform. Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. Prerequisites for On-Premises Installation Describes fabric node and user prerequisites for on-premises installation of the HPE Ezmeral Data Fabric . AWS Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Amazon Web Services (AWS). Azure Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Microsoft Azure. GCP Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Google Cloud Platform (GCP). On-Premises Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric that is hosted on-site. Creating a Local Repository for an Air-Gapped Installation Describes how to make installation packages available through a local repository for an     air-gapped installation. Troubleshooting Seed Node Installation Describes some common issues that can interfere with seed node     installation. Planning Worksheet for Cloud Deployments Print this worksheet, and use it to record configuration information for your cloud     deployment. Help for datafabric_container_setup.sh From the Docker command line, you can access the help text for the datafabric_container_setup.sh script. Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. SSO Using Keycloak Describes how single sign-on (SSO) is implemented by using       Keycloak. Setting Up Clients Summarizes the steps for enabling client communication with the HPE Ezmeral Data Fabric . Upgrade This section contains information that describes how to upgrade the HPE Ezmeral Data Fabric as-a-service platform. User Assistance Describes how to access different resources that can help you learn how to use the HPE Ezmeral Data Fabric . AWS Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new\n    fabric using Amazon Web Services (AWS). Parameters with an asterisk (*) are required. Before you can initiate the Create process, you must specify all required parameters. Name* Name of the fabric. Use a name that is unique across all of your fabrics and is from 1\n          to 40 characters. The name: Must start with a letter (either lowercase or uppercase). Can contain lowercase letters, uppercase letters, numbers, and hyphens. Must not contain consecutive hyphens. Must include a letter or a number as the final character. Provider The cloud provider on which to create the fabric. Select Amazon Web Services\n            (AWS) . Access key ID* AWS credential Access Key. The user must have \"AmazonEC2FullAccess\" permission. Secret key* AWS credential Secret Access Key. Region* The AWS region in which to provision the fabric. Storage Tier* The consumption baseline that you specified in your license for the fabric. Your actual\n          storage consumption can exceed this level. Select from these tiers: 1 TB 10 TB 100 TB 1 PB Data-at-rest encryption Data on disk (or data at rest) on a secure fabric can be encrypted, enabling you to\n          protect the data if a disk is compromised. Encryption of data at rest not only prevents\n          unauthorized users from accessing sensitive data, but it also protects against data theft\n          via sector-level disk access. Data-at-rest encryption is ON by default. Nodes The number of nodes allocated based on the Storage tier you selected. You do not need to\n          specify a number. The nodes are populated automatically. Virtual Private Cloud (VPC) ID* The AWS Virtual Private Cloud (VPC) ID to use in the selected region.",
        "679a2f8b-2959-4460-b98a-6eff8c05f3ce": "log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage. minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation. When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node.",
        "5f2406f4-1f72-486b-a28e-c4d4317945c4": "At a high level, SHA prunes (\u201chalves\u201d) a set of trials in successive rounds we call rungs. SHA starts with an initial set of trials. (A trial means one model, with a fixed set of hyperparameter values.) SHA trains all the trials for some length and the trials with the worst validation performance are discarded. In the next rung, the remaining trials are trained for a longer period of time, and then trials with the worst validation performance are pruned once again. This is repeated until the maximum training length is reached. First, an example of SHA. Rung 1: SHA creates N initial trials; the hyperparameter values for each trial are randomly sampled from the hyperparameters defined in the experiment configuration file. Each trial is trained for 1 epoch, and then validation metrics are computed. Rung 2: SHA picks the N/4 top-performing trials according to validation metrics. These are trained for 4 epochs. Rung 3: SHA picks the N/16 top-performing trials according to validation metrics. These are trained for 16 epochs. At the end, the trial with best performance has the hyperparameter setting the SHA searcher returns. In the example above, we generalize \u201chalving\u201d with a field called divisor, which determines what fraction of trials are kept in successive rungs, as well as the training length in successive rungs. max_length is 16 epochs, which is the maximum length a trial is trained for. In general, SHA has a fixed divisor d. In the first rung, it generates an initial set of randomly chosen trials and runs until each trial has trained for the same length. In the next rung, it keeps 1/d of those trials and closes the rest. Then it runs each remaining trial until it has trained for d times as long as the previous rung. ASHA iterates this process until some stopping criterion is reached, such as completing a specified number of rungs or having only one trial remaining. The total training length, rungs, and trials within rungs are fixed within each SHA searcher, but vary across different calls to SHA by the adaptive algorithm. Note that although the name \u201cSHA\u201d includes the phrase \u201chalving\u201d, the fraction of trials pruned after every rung is controlled by divisor.",
        "1b9da0d0-21bc-4534-ac8f-781bd934b987": "This user guide provides step-by-step instructions for installing Determined using Linux packages. Determined releases Debian and RPM packages for installing the Determined master and agent as systemd services on machines running Linux. You have two options for installing the Determined master and agent: Using Debian packages on Ubuntu 16.04 or 18.04, or Using Red Hat 7-based Linux distributions (e.g., Red Hat Enterprise Linux, CentOS, Oracle Linux, and Scientific Linux).",
        "79c78929-a746-45ce-a00b-06447fdd9a1b": "Format: entrypoint: model_def:TrialClass The entry point field expects a predefined or custom script, but also supports legacy file and trial class definitions. When you specify a trial class as the entry point, it must be a subclass of a Determined trial class. Each trial class is designed to support one deep learning application framework. When training or validating models, the trial might need to load data from an external source so the training code needs to define data loaders. A TrialClass is located in the model_def filepath and launched automatically. This is considered legacy behavior. By default, this configuration automatically detects distributed training, based on slot size and the number of machines, and launches with Horovod for distributed training. If used in a distributed training context, the entry point is: python3 -m determined.launch.horovod --trial model_def:TrialClass",
        "6aa34b6e-7f4e-4f1c-9995-65c16c6d477f": "Audit Logging Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Metering and Billing Describes metering and billing in HPE Ezmeral Unified Analytics Software . Monitoring Describes monitoring and alerting in HPE Ezmeral Unified Analytics Software . Logging Describes how logging works in HPE Ezmeral Unified Analytics Software and how to     access log files for the platform and applications. Audit Logging Describes auditing in HPE Ezmeral Unified Analytics Software and how to access audit logs. Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Audit Logging Describes auditing in HPE Ezmeral Unified Analytics Software and how to access audit logs. Auditing provides a chronological set of records that document the events that occur in an HPE Ezmeral Unified Analytics Software cluster. Auditing records user, application, and control plane events (that occur through the UI and\n      programmatic access via APIs or CLIs) in audit logs. Audit logs maintain records of actions\n      for accountability, tracking, and compliance purposes. Auditing provides the following information about actions in HPE Ezmeral Unified Analytics Software : Type of action User or application that triggered the action Timestamp (time the action occurred) Status of the action (Failed, Started, Success) Audited Actions The following tables lists the actions that auditing captures in the audit logs: Area Description Platform Captures successful and failed login attempts by users. Administration Captures the add/delete/modify user actions performed by a user assigned the\n                  administrator role. Billing & Licensing Captures the license related actions performed by the platform\n                  administrator. Captures the billing and activation related actions performed by the platform\n                    administrator, including: Creation of billing credentials and signing-key Creation billing and license credentials and signing-key in airgapped\n                        environments Downloading of metering usage in airgapped environments Uploading of metering usage Renewal of billing and license credentials Keycloak Captures Keycloak realm updates when the product is deactivated or activated\n                  (triggered from enabled to disabled and vice versa). Kubeflow Captures the creation of a notebook in Kubeflow. The audit message contains\n                  information about the name/namespace and whether the API call was a dry\n                  run. Captures the deletion of a notebook in Kubeflow. The audit message contains\n                  information about the name/namespace and whether the API call was a dry\n                  run. Captures the creation of a Create KServe Inference Service in Kubeflow. The\n                  audit message contains information about the name/namespace and whether the API\n                  call was a dry run. Captures the deletion of a Create KServe Inference Service in Kubeflow. The\n                  audit message contains information about the name/namespace and whether the API\n                  call was a dry run. Spark Spark application submitted using Spark operator. Spark application deleted using Spark operator. Scheduled Spark application submitted using Spark operator. Scheduled spark application deleted using Spark operator. NOTE Livy doesn't support audit logging. Airflow User disabled or enabled a DAG. Captures DAG ID and username. User started DAG execution. Captures DAG execution time, DAG ID, and\n                  username. DAG task scheduled after triggering the DAG. Captures DAG run ID, DAG ID, and\n                  task ID. DAG task running after scheduling. Captures DAG execution time, DAG ID, task\n                  ID, and username. DAG task succeeded after running. Captures DAG execution time, DAG ID, task\n                  ID, and username. DAG task failed after running. Captures DAG execution time, DAG ID, task ID,\n                  and username. EzPresto Query completed event. Audits the user, query, timestamp, status, type of\n                  query, and client-ip. Audits the data source name, data source type, user, timestamp, and\n                  status. Audits the user, create view query, timestamp, and status. Audtis the user, cache table details, remote table details, and\n                  status.",
        "1c08df56-35cb-4de3-9c68-515abd7b98b1": "TCP port for the Fluent Bit daemon to listen on. Defaults to port 24224. Should be unique when running multiple agents on the same node.",
        "4be28801-8564-4016-8008-b3936adbd973": "Customize a Pod",
        "1ef7f3da-2cd4-4c13-9216-7891cb51b499": "Optional. Whether to use mixed precision training with PyTorch during distributed training. Setting O1 enables mixed precision and loss scaling. Defaults to O0 which disables mixed precision training. This configuration setting is deprecated; users are advised to call context.configure_apex_amp in the constructor of their trial class instead.",
        "cabebb15-71ce-45f8-a9ec-9e1be3a28914": "Do: Use framework abstractions to implement learning rate scheduling instead of directly changing the learning rate. See tf.keras.optimizers.schedules.LearningRateSchedule and determined.pytorch.LRScheduler as examples. For code that needs to download artifacts (e.g., data, configurations, pretrained weights), download to a tempfile.TemporaryDirectory unique to the Python process. This will avoid race conditions when using distributed training, in which Determined executes multiple Python processes in the same task container. To learn more about distributed training with Determined, visit the conceptual overview or the intro to implementing distributed training. Do not use instance attributes on a trial class to save any state over time (e.g., storing metric history in a self attribute). The Trial instance will only save and restore model weights and optimizer state over time; self attributes may be reset to their initial state at any time if the Determined cluster reschedules the trial to another task container.",
        "018123df-5ab6-405b-aaaa-ec555e19fe6a": "For\n                            example: gcr.io/mapr-252711/spark-gpu-3.4.0:v3.4.0 Images for HPE Ezmeral Unified Analytics Software 1.3.0 The following images are required in order to install and run Spark and Spark based services: Spark Operator Images gcr.io/mapr-252711/spark-operator-1.3.8:1.3.8.4-hpe\ngcr.io/mapr-252711/autoticketgen-2.0.2:202401101515MG Livy Server Images gcr.io/mapr-252711/livy-0.8.0:202401202202R Spark History Server Images gcr.io/mapr-252711/spark-hs-3.5.0:202401050731R HPE-Curated Spark 3.5.0 Workload\n                            Images (Supporting Data Fabric Services) gcr.io/mapr-252711/spark-base-3.5.0:202401202202R\ngcr.io/mapr-252711/spark-3.5.0:202401202202R\ngcr.io/mapr-252711/spark-gpu-3.5.0:202401202202R\ngcr.io/mapr-252711/spark-py-3.5.0:202401202202R\ngcr.io/mapr-252711/spark-r-3.5.0:202401202202R Spark OSS 3.5.0 Images (Not\n                            Supporting Data Fabric Services) gcr.io/mapr-252711/apache-spark:3.5.0\ngcr.io/mapr-252711/apache-spark:3.5.0-py\ngcr.io/mapr-252711/apache-spark:3.5.0-r\ngcr.io/mapr-252711/apache-spark:3.5.0-gpu (Default) HPE-Curated Spark Images\n                            When Using GUI (Supporting Data Fabric Services) gcr.io/mapr-252711/spark-3.5.0:v3.5.0\ngcr.io/mapr-252711/spark-gpu-3.5.0:v3.5.0\ngcr.io/mapr-252711/spark-py-3.5.0:v3.5.0\ngcr.io/mapr-252711/spark-r-3.5.0:v3.5.0 Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "b7881248-db52-4e78-b632-fab4f03e1e23": "data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage. minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation.",
        "dbd587a3-38db-4ed3-be2f-394422a85983": "data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage. minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation.",
        "1aaad332-acb1-4cd6-ab07-1dd98e4d41b0": "PostgreSQL Connection Parameters Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Accessing Data in External S3 Object Stores Describes how to access data in external object stores from clients, such as Spark and     Kubeflow notebooks. EzPresto Describes the EzPresto SQL query engine     and its featues. Connect Data Sources Provides instructions for connecting HPE Ezmeral Unified Analytics Software to external data sources. Delta Connection Parameters List of Delta connection parameters, descriptions, default values, and supported data     types. Delta Thrift Connection Parameters List of  Delta Thrift connection parameters, descriptions, default values, and     supported data types. Hive Connection Parameters List of Hive connection parameters, descriptions, default values, and supported data     types. Hive Glue Metastore Connection Parameters List of Hive Glue Metastore connection parameters, descriptions, default values, and     supported data types. Hive Thrift Metastore Connection Parameters List of Hive Thrift Metastore connection parameters, descriptions, default values, and     supported data types. Hive Discovery Metastore Connection Parameters Lists Hive discovery metastore connection parameters, parameter descriptions, default     values, and supported data types. MySQL Connection Parameters List of MySQL connection parameters, descriptions, default values, and supported data     types. Oracle Connection Parameters List of Oracle connection parameters, descriptions, default values, and supported data     types. Snowflake Connection Parameters List of  Snowflake connection parameters, descriptions, default values, and supported     data types. SQL Server Connection Parameters List of SQL Server connection parameters, descriptions, default values, and supported     data types. Teradata Connection Parameters List of  Teradata connection parameters, descriptions, default values, and supported     data types. PostgreSQL Connection Parameters List of PostgreSQL connection parameters, descriptions, default values, and supported     data types. Iceberg Connection Parameters List of Iceberg connection parameters, descriptions, default values, and supported data     types. Configuring a Hive Data Source with Kerberos Authentication Describes the required prerequisite steps to complete before you connect HPE Ezmeral Unified Analytics Software to a Hive data source     that uses Kerberos authentication. Connect to CSV and Parquet Data in an External S3 Data Source via Hive Connector Describes how to use the Hive connector with Presto in HPE Ezmeral Unified Analytics Software to connect to CSV and     Parquet data in S3-based external data sources. EzPresto/connect-external-s3-data-source.html Connect to External Applications via JDBC Describes how to connect external applications and BI tools, such as Tableau and     PowerBI, to EzPresto through the EzPresto JDBC endpoint. Using Spark to Query EzPresto Describes how to use Spark to query EzPresto . Connect to EzPresto via Python Client Provides information for connecting to EzPresto from a Python client. Cache Data Describes data caching and provides the steps for caching data in HPE Ezmeral Unified Analytics Software . Submitting Presto Queries from Notebook Describes how to submit Presto queries from the notebook. Airflow Provides an overview of Apache Airflow in HPE Ezmeral Unified Analytics Software . Superset Provides a brief overview of  Superset in HPE Ezmeral Unified Analytics Software . Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. PostgreSQL Connection Parameters List of PostgreSQL connection parameters, descriptions, default values, and supported\n    data types. The following tables list the required and optional PostgreSQL connection parameters. Required Connection Parameters Parameter Description Default Value Data Type Connection Url JDBC connection url. null STRING Connection User Specifies the login name of the user for the connection. null STRING Connection Password Specifies the password of the user for the connection. null STRING Enable Local Snapshot Table Enable Caching while querying. true BOOLEAN Optional Connection Parameters Parameter Description Default Value Data Type Allow Drop Table Allow connector to drop tables. false BOOLEAN Case Insensitive Name Matching Match schema and table names case insensitively. false BOOLEAN Case Insensitive Name Matching Cache Ttl Duration for which remote dataset and table names will be cached. Set to\n                      0ms to disable the cache. 1m DURATION Generic Cache Table Ttl TTL for cache table expiry in minutes. 1440 INTEGER Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "170c8ba7-066c-4088-9b5e-19d94a2bbe6d": "Additional Slurm options when launching trials with sbatch. See environment.slurm for more details.",
        "8c60bc1d-d4be-4a53-be8c-97fade97efe6": "You can bind or unbind a resource pool to a workspace. By default, all resource pools are unbound, making them globally available to all workspaces in the cluster. Default resource pools cannot be bound to a workspace.",
        "b7dfb680-e3d5-401b-8fc1-4ec675719155": "Contact HPE Jump to main content Get Started Platform Administration Reference Home Reference Provides reference information for the HPE Ezmeral Data Fabric . Contact HPE Provides a link to contact HPE Sales or Support. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Reference Provides reference information for the HPE Ezmeral Data Fabric . Release History Describes the currently released versions of the HPE Ezmeral Data Fabric as-a-service platform. Cloud Instance Specifications Compares different aspects of the supported cloud instances of the HPE Ezmeral Data Fabric . Third-Party Storage Solutions Describes global-namespace support for HPE partner storage technologies, including     Scality, WEKA, and VAST. Port Information Describes the ports used by HPE Ezmeral Data Fabric services. maprcli Commands in This Guide Describes how to use maprcli commands provided as reference links in     this guide. Operating System Support Matrix The tables on this page show the Linux operating-system versions that are supported for HPE Ezmeral Data Fabric releases. Doc Site Available as a PDF Provides a link to the downloadable PDF file containing all the information for the     current release. Product Licensing Provides information related to product licensing. Other Resources Provides links to additional resources such as on-demand training, videos, blogs, and     the HPE Ezmeral Data Fabric community. Contact HPE Provides a link to contact HPE Sales or Support. Contact HPE Provides a link to contact HPE Sales or Support. Contact HPE (Topic last modified: 2024-01-16) Recommended Resources Training Modules for HPE Ezmeral \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "89d2e1f2-1124-41f7-8a93-f01a81c9d93d": "data-fabric gateway Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster. The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication.",
        "af749cc2-cf6e-4e4a-b458-3266fd983fad": "To learn more, see Batch Framework Updates . Individual framework updates : To upgrade frameworks sequentially(one at a\n              time) when new versions are available, use individual framework updates. To learn\n              more, see Individual Framework Updates . Batch Framework Updates If frameworks have updates available for a new version, every hour HPE Ezmeral Unified Analytics Software bundles those updates\n        and displays the update bundle on the Available Updates table as Tools & Frameworks . NOTE The table in Available\n            Updates is updated every hour whereas if there are any new versions of\n          frameworks, you can see the Upgrade button is enabled for that\n          framework immediately in the Tools & Frameworks screen for\n          individual framework updates. Once you see the Tools & Frameworks update bundle, you can click\n        the bundle to view details. In the Details dialog box, you can see\n        the name, description, the current version of the framework and chart, and the new available\n        version for the framework and chart. Once you see the new available versions for the update,\n        you can perform the following actions by clicking on the Actions menu. Update To batch update frameworks immediately, follow these steps: Click Update in the Actions menu. This will open an Update Now dialog box and you can\n                    compare the current and new available versions of frameworks for upgrade. Click Update Now to immediately start framework\n                    updates. Wait for framework updates to be in the In\n                      Progress status. NOTE You cannot cancel framework updates once it\n                      is in the In Progress status. Result: You can navigate to the Tools &\n                  Frameworks screen to see frameworks are now in the Upgrading status. Schedule To schedule batch framework updates for later, follow these steps: Click Schedule in the Actions menu. This will open a Schedule Update dialog box and you can compare the\n                    current and new available versions of frameworks for upgrade. Select a date and time to schedule the update. Wait for framework updates to\n                    be in the Scheduled status. Once the framework updates are in the Scheduled status,\n                    you can perform the following actions from the Actions menu. Cancel You can cancel the scheduled updates any time before update starts or\n                            if updates are not in the In Progress status\n                            yet. Reschedule You can reschedule the scheduled updates any time before update starts\n                            or if updates are not in the In Progress status\n                            yet. Update You can update frameworks immediately even though it has been scheduled\n                            for a later date and time. Viewing Update History Once your updates are complete, the Tools & Frameworks update details will be displayed in the Update History table. You can click Tools & Frameworks to view details. In the Details dialog box, you can see the name, description, the\n                current version of the framework and chart, and the new available version for the\n                framework and chart. Individual Framework Updates To update frameworks one at a time, follow these steps: Click Tools & Frameworks in the left navigation bar. If there are any new versions of frameworks, you can see the Upgrade button is enabled for that framework. An enabled Upgrade button only appears if the version of the framework\n              currently installed is older than the version available. For example, if a new version\n              of Airflow is available, you see the Upgrade button enabled in the application tile for Airflow . Click Upgrade to complete the upgrade for that framework. Repeat steps 1 and 2 to update all frameworks of your choice. Failure and Rollback When you are upgrading frameworks, if one of the framework updates fails, the application\n        tile for that framework will be in the Error status and the failed\n        application will be rollbacked to the previous version from which you were upgrading to the\n        new version. For example: In HPE Ezmeral Unified Analytics Software , if you upgraded ten frameworks and nine frameworks are upgraded and\n        in the Ready status, and if one farmework upgrade failed and is in\n        the Error status with a warning message on the framework tile, then\n        only that failed application is rolled back to the previous version whereas nine frameworks\n        are successfully upgraded to new versions. If for some reason the rollback fails and the framework is in the error state, you must\n        contact HPE support to resolve this issue. On this page Batch Framework Updates Individual Framework Updates Failure and Rollback Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "914a4ec8-afe4-476c-ba8d-2993a1c23ca2": "To view the experiment progress in your browser: Enter the following URL: http://localhost:8080/. This is the cluster address for your local training environment. Accept the default username of determined, and click Sign In. A password is not required.",
        "ccdf7fbb-defc-4a96-a396-e453bc674e49": "This config only takes effect with\n                      hive.immutable-partitions set to true FALSE BOOLEAN Hive Create Empty Bucket Files For Temporary Table Create empty files when there is no data for temporary table\n                      buckets FALSE BOOLEAN Hive Enable Parquet Batch Reader Verification enable optimized parquet reader FALSE BOOLEAN Hive Create Empty Bucket Files For Temporary Table Create empty files when there is no data for temporary table\n                      buckets FALSE BOOLEAN Hive Min Bucket Count To Not Ignore Table Bucketing Ignore table bucketing when table bucket count is less than the value\n                      specified, otherwise, it is controlled by property\n                      hive.ignore-table-bucketing 0 INTEGER Hive Partition Statistics Based Optimization Enabled Enables partition statistics based optimization, including partition\n                      pruning and predicate stripping FALSE BOOLEAN Hive Experimental Optimized Partition Update Serialization\n                      Enabled Serialize PartitionUpdate objects using binary SMILE encoding and\n                      compress with the ZSTD compression FALSE BOOLEAN Hive Materialized View Missing Partitions Threshold Materialized views with missing partitions more than this threshold falls\n                      back to the base tables at read time 100 INTEGER Hive S3select Pushdown Max Connections The maximum number of client connections allowed for those operations\n                      from worker nodes 500 INTEGER Min(1) Hive Temporary Staging Directory Enabled Should use (if possible) temporary staging directory for write\n                      operations TRUE BOOLEAN Hive Temporary Staging Directory Path Location of temporary staging directory for write operations. Use ${USER}\n                      placeholder to use different location for each user. /tmp/presto-${USER} STRING Hive Temporary Table Storage Format The default file format used when creating new tables. ORC STRING possibleValues(ORC, DWRF, PARQUET, AVRO, RCBINARY, RCTEXT, SEQUENCEFILE,\n                      JSON, TEXTFILE, CSV, PAGEFILE) Hive Temporary Table Compression Codec The compression codec to use when writing files for temporary\n                      tables SNAPPY STRING possibleValues(NONE, SNAPPY, LZ4, ZSTD, GZIP) Hive Use Pagefile For Hive Unsupported Type Automatically switch to PAGEFILE format for materialized exchange when\n                      encountering unsupported types TRUE BOOLEAN Hive Parquet Pushdown Filter Enabled Enable complex filter pushdown for Parquet FALSE BOOLEAN Hive Range Filters On Subscripts Enabled enable pushdown of range filters on subscripts (a[2] = 5) into ORC column\n                      readers FALSE BOOLEAN Hive Adaptive Filter Reordering Enabled Enable adaptive filter reordering TRUE BOOLEAN Hive Parquet Batch Read Optimization Enabled Is Parquet batch read optimization enabled FALSE BOOLEAN Hive Enable Parquet Dereference Pushdown Is dereference pushdown expression pushdown into Parquet reader\n                      enabled FALSE BOOLEAN Hive Max Metadata Updater Threads Maximum number of metadata updated threads 100 INTEGER Min(1) Hive Partial_aggregation_pushdown_enabled enable partial aggregation pushdown FALSE BOOLEAN Hive Manifest Verification Enabled Enable verification of file names and sizes in manifest / partition\n                      parameters FALSE BOOLEAN Hive Undo Metastore Operations Enabled Enable undo metastore operations TRUE BOOLEAN Hive Verbose Runtime Stats Enabled Enable tracking all runtime stats. Note that this may affect query\n                      performance FALSE BOOLEAN Hive Prefer Manifests To List Files Prefer to fetch the list of file names and sizes from manifests rather\n                      than storage FALSE BOOLEAN Hive Partition Lease Duration Partition lease duration 0.00s DURATION Hive Size Based Split Weights Enabled Enable estimating split weights based on size in bytes TRUE BOOLEAN Hive Minimum Assigned Split Weight Minimum weight that a split can be assigned when size based split weights\n                      are enabled 0.05 DOUBLE Min(0, inclusive=false), Max(1) Hive Use Record Page Source For Custom Split Use record page source for custom split. By default, true. Used to query\n                      MOR tables in Hudi. TRUE BOOLEAN Hive Split Loader Concurrency Number of maximum concurrent threads per split source 4 INTEGER Min(1) Hive Domain Compaction Threshold Maximum ranges to allow in a tuple domain without compacting it 100 INTEGER Min(1) Hive Max Concurrent File Renames Maximum concurrent file renames 20 INTEGER Hive Max Concurrent Zero Row File Creations Maximum number of zero row file creations 20 INTEGER Min(1) Hive Recursive Directories Enable reading data from subdirectories of table or partition locations.\n                      If disabled, subdirectories are ignored. FALSE BOOLEAN Hive User Defined Type Encoding Enabled Enable user defined type FALSE BOOLEAN Hive Loose Memory Accounting Enabled When enabled relaxes memory accounting for queries violating memory\n                      limits to run that previously honored memory thresholds FALSE BOOLEAN Hive Max Outstanding Splits Size Maximum amount of memory allowed for split buffering for each table scan\n                      in a query, before the query is failed 256MB DATASIZE Min(1MB) Hive Max Split Iterator Threads Maximum number of iterator threads 1000 INTEGER Hive Allow Corrupt Writes For Testing Allow Hive connector to write data even when data will likely be\n                      corrupt FALSE BOOLEAN Hive Create Empty Bucket Files Should empty files be created for buckets that have no data?",
        "18b2cbca-41b0-4f77-9ce7-12020f4b6d71": "Optional. For multi-GPU training, whether to average the training metrics across GPUs instead of only using metrics from the chief GPU. This impacts the metrics shown in the Determined UI and TensorBoard, but does not impact the outcome of training or hyperparameter search. This option is currently supported for PyTorchTrial and TFKerasTrial instances. Defaults to true.",
        "81f733f7-5a70-49e3-825e-8f0ee27d4ddc": "To execute arbitrary Python code during the lifecycle of a PyTorchTrial, implement the PyTorchCallback and supply them to the PyTorchTrial by implementing build_callbacks().",
        "e98f912d-8917-4178-bcaa-866872e7ea9c": "determined.pytorch.deepspeed.overwrite_deepspeed_configbase_ds_config: Union[str, Dict]source_ds_dict: Dict[str, Any]Dict[str, Any] Overwrite a base_ds_config with values from a source_ds_dict. You can use source_ds_dict to overwrite leaf nodes of the base_ds_config. More precisely, we will iterate depth first into source_ds_dict and if a node corresponds to a leaf node of base_ds_config, we copy the node value over to base_ds_config. Parameters base_ds_config (str or Dict) \u2013 either a path to a DeepSpeed config file or a dictionary. source_ds_dict (Dict) \u2013 dictionary with fields that we want to copy to base_ds_config Returns The resulting dictionary when base_ds_config is overwritten with source_ds_dict.",
        "44520ea3-038a-459f-ac18-e5d21fcc2165": "Min number of Determined agent instances. Defaults to 0.",
        "9c8527a4-db5b-4289-8526-37e1c425b92b": "GCP Fabric Configuration Parameters Jump to main content Get Started Platform Administration Reference Home Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. GCP Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Google Cloud Platform (GCP). HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Release Notes These notes contain information about release 7.6.0 of the HPE Ezmeral Data Fabric as-a-service platform. Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. Prerequisites for On-Premises Installation Describes fabric node and user prerequisites for on-premises installation of the HPE Ezmeral Data Fabric . AWS Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Amazon Web Services (AWS). Azure Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Microsoft Azure. GCP Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Google Cloud Platform (GCP). On-Premises Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric that is hosted on-site. Creating a Local Repository for an Air-Gapped Installation Describes how to make installation packages available through a local repository for an     air-gapped installation. Troubleshooting Seed Node Installation Describes some common issues that can interfere with seed node     installation. Planning Worksheet for Cloud Deployments Print this worksheet, and use it to record configuration information for your cloud     deployment. Help for datafabric_container_setup.sh From the Docker command line, you can access the help text for the datafabric_container_setup.sh script. Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. SSO Using Keycloak Describes how single sign-on (SSO) is implemented by using       Keycloak. Setting Up Clients Summarizes the steps for enabling client communication with the HPE Ezmeral Data Fabric . Upgrade This section contains information that describes how to upgrade the HPE Ezmeral Data Fabric as-a-service platform. User Assistance Describes how to access different resources that can help you learn how to use the HPE Ezmeral Data Fabric . GCP Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new\n    fabric using Google Cloud Platform (GCP). Parameters with an asterisk (*) are required. Before you can initiate the Create process, you must specify all required parameters. Name* Name of the fabric. Use a name that is unique across all of your fabrics and is from 1\n          to 40 characters. The name: Must start with a lowercase letter. Can contain lowercase letters, numbers, and hyphens. Must not contain consecutive hyphens. Must include a lowercase letter or a number as the final character. Provider The cloud provider on which to create the fabric. Select Google Cloud\n            Platform (GCP) . Service account key file* A file containing your GCP service account credentials. For more information, see Create and delete service account keys . Zone* The GCP zone in which to provision the fabric. Storage Tier* The consumption baseline that you specified in your license for the fabric. Your actual\n          storage consumption can exceed this level. Select from these tiers: 1 TB 10 TB 100 TB 1 PB Data-at-rest encryption Data on disk (or data at rest) on a secure fabric can be encrypted, enabling you to\n          protect the data if a disk is compromised. Encryption of data at rest not only prevents\n          unauthorized users from accessing sensitive data, but it also protects against data theft\n          via sector-level disk access. Data-at-rest encryption is ON by default. Nodes The number of nodes allocated based on the Storage tier you selected. You do not need to\n          specify a number. The nodes are populated automatically. VPC network* The identifier for the VPC. The VPC must have an internet gateway attached. Subnetwork* The identifier for the public subnet. (Topic last modified: 2023-08-27) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "db85e443-70d5-43e6-b393-f78383166dae": "Submitting Spark Applications by Using DAGs Describes how to submit the Spark applications by using DAGs in Airflow. Defining RBACs on DAGs Describes role-based access controls (RBACs) with respect to Airflow in HPE Ezmeral Unified Analytics Software and how to define RBACs     to permit access to DAGs. Using whylogs with Airflow Describes how to use whylogs with Airflow DAGs. More information Financial Time Series Workflow MNIST Digits Recognition Workflow On this page Airflow Functionality Airflow Architecture Airflow Components Airflow Limitations Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "dae3ee7f-79bc-4c7f-804f-0fc07e4f33fd": "Enroot uses XDG_RUNTIME_DIR which is not provided to the compute jobs by Slurm/PBS by default. The error mkdir: cannot create directory \u2018/run/enroot\u2019: Permission denied indicates that the environment variable XDG_RUNTIME_DIR is not defined on the compute nodes. See Podman Requirements for recommendations. Enroot requires manual download and creation of containers. The error [ERROR] No such file or directory: /home/users/test/.local/share/enroot/determinedai+environments+cuda-11.1-base-gpu-mpi-0.18.5 indicates the user test has not created an Enroot container for Docker image determinedai/environments:cuda-11.1-base-gpu-mpi-0.18.5. Check the available containers using the enroot list command. See Enroot Requirements for guidance on creating Enroot containers. Enroot does not provide a mechanism for sharing containers. Each user must create any containers needed by their Determined experiments prior to creating the experiment. Some Docker features do not have an exact replacement in Enroot, and therefore the associated Determined features are not supported. Feature Description resources.devices Managed via Enroot configuration files. resources.shm_size Managed via Enroot configuration files. environment.registry_auth.server No equivalent setting in Enroot. environment.registry_auth.email No equivalent setting in Enroot.",
        "91957f63-5880-41c6-85f9-869929214866": "The adaptive_asha search method employs an Asynchronous version of the Successive Halving Algorithm (ASHA), which is suitable for large-scale experiments with hundreds or thousands of trials.",
        "e2f5d2a2-d572-4dc6-891f-1d002c40ec40": "volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are\n            normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but\n            tuning the size correctly is important. Files inherit the chunk size settings of the\n            directory that contains them, as do subdirectories on which chunk size has not been\n            explicitly set. Any files written by a Hadoop application, whether via the file APIs or\n            over NFS, use chunk size specified by the settings for the directory where the file is\n            written. (Topic last modified: 2023-04-07) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "f19d0bd4-9d3d-4c04-bca5-93912ba7a21b": "The master can be configured to use systemd socket activation, allowing it to be started automatically on demand (e.g., when a client makes a network connection to the port) and restarted with reduced loss of connection state. To switch to socket activation, run the following commands: sudo systemctl disable --now determined-master sudo systemctl enable --now determined-master.socket When socket activation is in use, the port on which the master listens is configured differently; the port listed in the master config file is not used, since systemd manages the listening socket. The default socket unit for Determined is configured to listen on port 8080. To use a different port, run: sudo systemctl edit determined-master.socket which will open a text editor window. To change the listening port, insert the following text (with the port number substituted appropriately) into the editor and then exit the editor: [Socket] ListenStream= ListenStream=0.0.0.0:<port> For example, you might want to configure the master to listen on port 80 for HTTP traffic or on port 443 if using TLS. After updating the configuration, run the following commands to put the change into effect (this will restart the master): sudo systemctl stop determined-master sudo systemctl restart determined-master.socket See the systemd documentation on socket unit files or systemctl for more information.",
        "03cc1e64-e740-45c5-bc74-bc7f01b6dc1c": "The Model Registry is a way to group conceptually related checkpoints (including ones across different experiments), store metadata and long-form notes about a model, and retrieve the latest version of a model for use or further development. The Model Registry can be accessed through the WebUI, Python SDK, REST API, or CLI, though the WebUI has some features that the others are missing. The Model Registry is a top-level option in the navigation bar. This will take you to a page listing all of the models that currently exist in the registry, and allow you to create new models. You can select any of the existing models to go to the Model Details page, where you can view and edit detailed information about the model. There will also be a list of every version associated with the selected model, and you can go to the Version Details page to view and edit that version\u2019s information. For more information about how to use the model registry, see Organizing Models in the Model Registry",
        "4754ac48-148b-42a5-8b67-656b3c24deae": "GPU Resource Management Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Installation Provides links to HPE Ezmeral Unified Analytics Software installation and service activation topics. Identity and Access Management Describes identity and access management in HPE Ezmeral Unified Analytics Software . Expanding the Cluster Describes how to add additional user-provided hosts to the management cluster to increase resource capacity and how     to expand the cluster to include the additional user-provided hosts. Shutting Down an HPE Ezmeral Unified Analytics Software Cluster Describes how to gracefully shut down an HPE Ezmeral Unified Analytics Software cluster when you want to perform maintenance or upgrade     tasks. Importing Applications and Managing the Application Lifecycle Describes how to import, manage, and secure applications and frameworks in HPE Ezmeral Unified Analytics Software . Connecting to External S3 Object Stores Describes how to connect HPE Ezmeral Unified Analytics Software to external S3 object storage in AWS, MinIO, and HPE Ezmeral Data Fabric Object Store . Connecting to External HPE Ezmeral Data Fabric Clusters Describes how to connect HPE Ezmeral Unified Analytics Software to an external HPE Ezmeral Data Fabric cluster. Configuring Endpoints Describes the endpoints in HPE Ezmeral Unified Analytics Software and how to configure them. GPU Support Provides information about support for NVIDIA GPU, MIG partitioning, preparing hosts     for GPU-enabled environment, adding hosts and enabling GPU in HPE Ezmeral Unified Analytics Software . GPU Resource Management Describes the GPU idle reclaim policy used for GPU resource management. Configuring GPU Idle Reclaim Describes how to configure the GPU idle reclaim, view pod details, and view GPU         usage. GPU Scheduling Workload Scenarios Describes GPU scheduling workload scenarios and the notebook example for GPU idle     reclaim. Troubleshooting Describes how to identify and debug issues in HPE Ezmeral Unified Analytics Software . Support Matrix The tables on this page show the tools and frameworks, HPE Ezmeral Data Fabric ,operating system       versions , and GPU models that are supported for HPE Ezmeral Unified Analytics Software releases. Release Notes This document provides a comprehensive overview of the latest updates and enhancements     in HPE Ezmeral Unified Analytics Software (version     1.3.0), including new features, improvements, bug fixes, and known issues. Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. GPU Resource Management Describes the GPU idle reclaim policy used for GPU resource management. GPU resource management enables you to optimize the analytical workloads by distributing the\n      GPU resources to various workloads so that each workload receives the necessary computing\n      power. HPE Ezmeral Unified Analytics Software implements the GPU idle reclaim feature to maximize\n      GPU utilization by dynamically allocating and deallocating resources to different frameworks\n      and workloads as needed. This prevents overallocation and underutilization of the GPU\n      resources and increases efficiency. GPU resource management uses a priority policy to ensure that critical workloads get the\n      resources they need while also allowing lower-priority workloads to utilize the GPU when it is\n      available. When a workload or framework is finished using its GPU resources, HPE Ezmeral Unified Analytics Software initiates GPU resource reclamation. This involves\n      deallocating the resources and making them available for other workloads. Custom Scheduler HPE Ezmeral Unified Analytics Software runs its own scheduler which functions\n        independently and is not connected to the default Kubernetes scheduler. Note that the default Kubernetes scheduler is still available alongside this custom\n        scheduler. The custom scheduler is an enhanced version of the default Kubernetes scheduler\n        that includes the GPU idle reclaim plugins and the preemption tolerance. The custom scheduler plugin governs all GPU workloads and is installed in the hpe-plugin-scheduler namespace. This namespace consists of a controller\n        and a scheduler module. The scheduler is responsible for scheduling and reclaiming. There are two pods in the scheduler namespace.",
        "cb640bf8-a1a1-485b-bf32-d1b641d1617d": "There are two types of continuations: pausing and reactivating training using the WebUI or clicking Continue Trial after the experiment completes. These two types of continuations have different behaviors. While you always want to preserve the model\u2019s state, you do not always want to preserve the batch index. When you pause and reactivate you want training to continue from the same batch index, but when starting a fresh experiment you want training to start with a fresh batch index. You can save the trial ID in the checkpoint and use it to distinguish the two types of continuations. To distinguish between the two types of continuations, you can save the trial ID in the checkpoint. Enable Pausing and Resuming an Experiment To enable pausing an experiment, enable preemption: # NEW: Detect when the experiment is paused by the WebUI. if core_context.preempt.should_preempt(): return Define a load_state function for restarting model training from existing checkpoint: # NEW: Define load_state function for restarting model training from # existing checkpoint. Returns (.pt, int). # Also update load_state header to take trial info object as an argument. def load_state(checkpoint_directory, trial_id): checkpoint_directory = pathlib.Path(checkpoint_directory) with checkpoint_directory.joinpath(\"checkpoint.pt\").open(\"rb\") as f: model = torch.load(f) with checkpoint_directory.joinpath(\"state\").open(\"r\") as f: epochs_completed, ckpt_trial_id = [int(field) for field in f.read().split(\",\")] # Docs snippet start: compare checkpoint and current trial IDs # If trial ID does not match our current trial ID, we'll ignore # epochs completed and start training from epoch_idx = 0 if ckpt_trial_id != trial_id: epochs_completed = 0 # Docs snippet end: compare checkpoint and current trial IDs return model, epochs_completed If checkpoint exists, load it and assign it to model state prior to resuming training: # NEW: If checkpoint exists, load it and assign it to model state # prior to resuming training. info = det.get_cluster_info() assert info is not None, \"this example only runs on-cluster\" latest_checkpoint = info.latest_checkpoint if latest_checkpoint is None: epochs_completed = 0 else: with core_context.checkpoint.restore_path(latest_checkpoint) as path: model, epochs_completed = load_state(path, info.trial.trial_id) Enable Continuing the Trial To enable continuing the trial after the experiment completes, save the trial ID. One way to do this is to load the checkpoint and save the checkpoint in a file in the checkpoint directory. Open the checkpoint.pt file in binary mode and compare ckpt_trial_id with the current trial_id: # If trial ID does not match our current trial ID, we'll ignore # epochs completed and start training from epoch_idx = 0 if ckpt_trial_id != trial_id: epochs_completed = 0 Save the checkpoint in the checkpoint.pt file: # NEW: Save checkpoint. checkpoint_metadata_dict = {\"steps_completed\": steps_completed} # NEW: Here we are saving multiple files to our checkpoint # directory. 1) a model state file and 2) a file includes # information about the training loop state. with core_context.checkpoint.store_path(checkpoint_metadata_dict) as (path, storage_id): torch.save(model.state_dict(), path / \"checkpoint.pt\") with path.joinpath(\"state\").open(\"w\") as f: f.write(f\"{epochs_completed},{info.trial.trial_id}\") Detect when the experiment is paused by the WebUI: # NEW: Detect when the experiment is paused by the WebUI. if core_context.preempt.should_preempt(): return",
        "6ca4b8e0-2ed6-4ea4-b717-2dbb840f6bc2": "Optionally, search or filter the data sets to find the data set(s) that you want to\n            cache. Click + Select for each of the data sets that you want to cache. Click Selected Datasets . The Selected Datasets drawer opens and displays\n            the selected data sets. Click Cache Datasets . The Manage Datasets screen appears. Each data set\n            that you selected appears on its own tab. Optionally, modify the data set(s). Use the pencil icon to modify data set and column names. Use the check boxes next to the column names to remove columns from the\n                data set. Use the Schema dropdown to change the schema or add a new schema. If you have selected multiple data sets, use the connector icon next to the\n                schema dropdown to apply the schema to all of the selected data sets. Click Cache Overview and compare the original data sets (Input Assets) to the\n            modified data sets (Output Assets) to verify the changes. If the changes to a data set are incorrect, click the pencil icon to edit the\n            data set. To cache the data set(s), click Save to cache . The system displays the\n            following message: Successfully initiating cache If an error\n            appears, correct the issue and continue. To view the cached data sets, go to Data Engineering > Cached Assets in the\n            left navigation bar of the HPE Ezmeral Unified Analytics Software UI. NOTE Depending on the size of the data sets, it may take a\n              minute or so for them to appear as cached assets. Enable or Disable a Cache You can enable or\n        disable caching through the Enable Local Snapshot Table option when you create a data\n        source connection. See Connect Data Sources . You cannot disable caching\n      by setting the TTL to zero. If the TTL is set to zero, the cache expires immediately but still\n      consumes resources. On this page How to Cache Data Enable or Disable a Cache Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "3df6dc88-1b97-4d93-9938-149eeb269c50": "\"repository\": \"https://github.com/reduxjs/redux\",\n    \"licenseUrl\": \"https://github.com/reduxjs/redux/raw/master/LICENSE.md\",\n\n-----------------------------------------------------------\n\n\"react-highcharts@16.1.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/kirjs/react-highcharts\",\n    \"licenseUrl\": \"https://github.com/kirjs/react-highcharts/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"lodash@4.17.21\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/lodash/lodash\",\n    \"licenseUrl\": \"https://github.com/lodash/lodash/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"highcharts@7.2.2\"\n\n    \"licenses\": \"https://www.highcharts.com/license\",\n    \"repository\": \"https://github.com/highcharts/highcharts-dist\",\n\n-----------------------------------------------------------\n\n\"highcharts@9.1.0\"\n\n    \"licenses\": \"https://www.highcharts.com/license\",\n    \"repository\": \"https://github.com/highcharts/highcharts-dist\",\n\n-----------------------------------------------------------\n\n\"pegjs@0.10.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/pegjs/pegjs\",\n    \"licenseUrl\": \"https://github.com/pegjs/pegjs/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-overlays@0.7.3\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/react-bootstrap/react-overlays\",\n    \"licenseUrl\": \"https://github.com/react-bootstrap/react-overlays/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"jquery@3.6.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/jquery/jquery\",\n    \"licenseUrl\": \"https://github.com/jquery/jquery/raw/main/LICENSE.txt\",\n\n-----------------------------------------------------------\n\n\"react-bootstrap-typeahead@1.4.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/ericgio/react-bootstrap-typeahead\",\n    \"licenseUrl\": \"https://github.com/ericgio/react-bootstrap-typeahead/raw/main/LICENSE.md\",\n\n-----------------------------------------------------------\n\n\"@reduxjs/toolkit@1.5.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/reduxjs/redux-toolkit\",\n    \"licenseUrl\": \"https://github.com/reduxjs/redux-toolkit/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\"@reduxjs/toolkit@1.8.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/reduxjs/redux-toolkit\",\n    \"licenseUrl\": \"https://github.com/reduxjs/redux-toolkit/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-table@6.11.5\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/TanStack/table\",\n    \"licenseUrl\": \"https://github.com/TanStack/table/raw/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"json-structure-validator@1.2.1\"\n\n    \"licenses\": \"none\",\n    \"repository\": \"https://github.com/AntJanus/JSON-structure-validator\",\n\n-----------------------------------------------------------\n\n\"keycode@2.2.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/timoxley/keycode\",\n    \"licenseUrl\": \"https://github.com/timoxley/keycode/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-intl@2.4.0\"\n\n    \"licenses\": \"BSD-3-Clause\",\n    \"repository\": \"https://github.com/formatjs/formatjs\",\n\n-----------------------------------------------------------\n\n\"intl-messageformat@2.1.0\"\n\n    \"licenses\": \"BSD-3-Clause\",\n    \"repository\": \"https://github.com/formatjs/formatjs\",\n\n-----------------------------------------------------------\n\n\"intl-messageformat@9.6.16\"\n\n    \"licenses\": \"BSD-3-Clause\",\n    \"repository\": \"https://github.com/formatjs/formatjs\",\n\n-----------------------------------------------------------\n\n\"rc-slider@8.3.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/react-component/slider\",\n    \"licenseUrl\": \"https://github.com/react-component/slider/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"graphql-tag@2.12.6\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/apollographql/graphql-tag\",\n    \"licenseUrl\": \"https://github.com/apollographql/graphql-tag/raw/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-notification-system@0.2.",
        "b9369b4f-9853-4de9-b47b-5457f0e7949d": "Optional. Instructs how frequent to perform system operations, such as periodic checkpointing and preemption, in the unit of batches. The number of records in a batch is controlled by the global_batch_size hyperparameter. Defaults to 100. Setting this value too small can increase the overhead of system operations and decrease training throughput. Setting this value too large might prevent the system from reallocating resources from this workload to another, potentially more important, workload. As a rule of thumb, it should be set to the number of batches that can be trained in roughly 60\u2013180 seconds.",
        "ed407016-a7bd-4b9a-8af5-3aff7627e30e": "On-Premises Fabric Configuration Parameters Jump to main content Get Started Platform Administration Reference Home Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. On-Premises Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric that is hosted on-site. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Release Notes These notes contain information about release 7.6.0 of the HPE Ezmeral Data Fabric as-a-service platform. Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. Prerequisites for On-Premises Installation Describes fabric node and user prerequisites for on-premises installation of the HPE Ezmeral Data Fabric . AWS Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Amazon Web Services (AWS). Azure Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Microsoft Azure. GCP Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Google Cloud Platform (GCP). On-Premises Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric that is hosted on-site. Creating a Local Repository for an Air-Gapped Installation Describes how to make installation packages available through a local repository for an     air-gapped installation. Troubleshooting Seed Node Installation Describes some common issues that can interfere with seed node     installation. Planning Worksheet for Cloud Deployments Print this worksheet, and use it to record configuration information for your cloud     deployment. Help for datafabric_container_setup.sh From the Docker command line, you can access the help text for the datafabric_container_setup.sh script. Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. SSO Using Keycloak Describes how single sign-on (SSO) is implemented by using       Keycloak. Setting Up Clients Summarizes the steps for enabling client communication with the HPE Ezmeral Data Fabric . Upgrade This section contains information that describes how to upgrade the HPE Ezmeral Data Fabric as-a-service platform. User Assistance Describes how to access different resources that can help you learn how to use the HPE Ezmeral Data Fabric . On-Premises Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new\n    fabric that is hosted on-site. Parameters with an asterisk (*) are required. Before you can initiate the Create process, you must specify all required parameters. Creating an on-premises fabric requires that you provide host nodes before starting\n      fabric creation. These nodes must meet certain prerequisites. Before creating an on-premises\n      fabric, review Prerequisites for On-Premises Installation . Name* Name of the fabric. Use a name that is unique across all of your fabrics and is from 1\n          to 40 characters. The name: Must start with a letter (either lowercase or uppercase). Can contain lowercase letters, uppercase letters, numbers, and hyphens. Must not contain consecutive hyphens. Must include a letter or a number as the final character. Provider The cloud provider on which to create the fabric. Select On-premises . Username* The SSH username. Password* The SSH password. Airgap repository The repository for the Installer to use if your installation cannot access the internet.\n          The repository must contain nested folders. For example: ./installer/redhat . You must create this repository before installing\n          an air-gapped fabric. See Creating a Local Repository for an Air-Gapped Installation . Data-at-rest encryption Data on disk (or data at rest) on a secure fabric can be encrypted, enabling you to\n          protect the data if a disk is compromised. Encryption of data at rest not only prevents\n          unauthorized users from accessing sensitive data, but it also protects against data theft\n          via sector-level disk access. Data-at-rest encryption is ON by default. Nodes* The recommended minimum number of nodes that should be allocated. The form provides this\n          information based on the Storage tier you selected.",
        "809336da-5320-43af-9aa0-0b739d927e46": "2\"\n\n    \"licenses\": \"Unlicense\",\n    \"repository\": \"https://github.com/text-mask/text-mask\",\n    \"licenseUrl\": \"https://github.com/text-mask/text-mask/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-select@1.3.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/JedWatson/react-select/tree/master/packages/react-select\",\n    \"licenseUrl\": \"https://github.com/JedWatson/react-select/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-dock@0.2.4\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/reduxjs/redux-devtools\",\n    \"licenseUrl\": \"https://github.com/reduxjs/redux-devtools/raw/main/LICENSE.md\",\n\n-----------------------------------------------------------\n\n\"css-toggle-switch@4.1.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/ghinda/css-toggle-switch\",\n    \"licenseUrl\": \"https://github.com/ghinda/css-toggle-switch/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"dompurify@2.3.8\"\n\n    \"licenses\": \"MPL-2.0 OR Apache-2.0\",\n    \"repository\": \"https://github.com/cure53/DOMPurify\",\n    \"licenseUrl\": \"https://github.com/cure53/DOMPurify/raw/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-copy-to-clipboard@5.0.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/nkbt/react-copy-to-clipboard\",\n    \"licenseUrl\": \"https://github.com/nkbt/react-copy-to-clipboard/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-duallist@1.1.6\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/jyotirmaybanerjee/react-duallist\",\n    \"licenseUrl\": \"https://github.com/jyotirmaybanerjee/react-duallist/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"redux-devtools-extension@2.13.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/zalmoxisus/redux-devtools-extension\",\n    \"licenseUrl\": \"https://github.com/zalmoxisus/redux-devtools-extension/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"axios-mock-adapter@1.19.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/ctimmerm/axios-mock-adapter\",\n    \"licenseUrl\": \"https://github.com/ctimmerm/axios-mock-adapter/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"axios@0.21.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/axios/axios\",\n    \"licenseUrl\": \"https://github.com/axios/axios/raw/v0.x/LICENSE\",\n\n-----------------------------------------------------------\n\n\"axios@0.27.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/axios/axios\",\n    \"licenseUrl\": \"https://github.com/axios/axios/raw/v0.x/LICENSE\",\n\n-----------------------------------------------------------\n\n\"codemirror@5.62.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/codemirror/basic-setup\",\n    \"licenseUrl\": \"https://github.com/codemirror/basic-setup/raw/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"codemirror@5.65.12\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/codemirror/basic-setup\",\n    \"licenseUrl\": \"https://github.com/codemirror/basic-setup/raw/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"grommet-icons@4.9.0\"\n\n    \"licenses\": \"Apache-2.0\",\n    \"repository\": \"https://github.com/grommet/grommet-icons\",\n    \"licenseUrl\": \"https://github.com/grommet/grommet-icons/raw/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"grommet-icons@4.10.0\"\n\n    \"licenses\": \"Apache-2.0\",\n    \"repository\": \"https://github.com/grommet/grommet-icons\",\n    \"licenseUrl\": \"https://github.com/grommet/grommet-icons/raw/master/LICENSE\",\n-----------------------------------------------------------\n\n\"grommet@2.25.1\"\n\n    \"licenses\": \"Apache-2.0\",\n    \"repository\": \"https://github.com/grommet/grommet\",\n    \"licenseUrl\": \"https://github.",
        "649a029e-6f7a-44ad-a632-410015ab0d48": "The Determined agent uses Docker to run your workloads. For more information, visit Docker for Mac installation instructions. By default, Determined will store checkpoints in $(brew --prefix)/var/determined/data, which is typically /usr/local/var/determined/data or /opt/homebrew/var/determined/data. Make sure to configure it as a shared path for Docker for Mac in Docker -> Preferences\u2026 -> Resources -> File Sharing. When installing on a different machine than the master, add Homebrew tap. brew tap determined-ai/determined Install determined-agent package. brew install determined-agent When installing on a different machine than the master, edit $(brew --prefix)/etc/determined/agent.yaml and change master_host and container_master_host to your master network hostname, and master_port to your master network port. Start the determined-agent service. brew services start determined-agent",
        "5687f7a2-72c7-49d8-9160-31fd086f97b3": "The description of the resource pool.",
        "93d689c4-c61e-47ce-be1c-6bb5851a0ba0": "\"repository\": \"https://github.com/sindresorhus/object-assign\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/sindresorhus/object-assign/main/license\",\n\n-----------------------------------------------------------\n\n\"react-fast-compare@3.2.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/FormidableLabs/react-fast-compare\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/FormidableLabs/react-fast-compare/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-is@18.2.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/facebook/react\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/facebook/react/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-joyride@2.5.3\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/gilbarbara/react-joyride\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/gilbarbara/react-joyride/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-proptype-conditional-require@1.0.4\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/beefancohen/react-proptype-conditional-require\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/beefancohen/react-proptype-conditional-require/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-query@3.39.3\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/TanStack/query\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/TanStack/query/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"react-side-effect@2.1.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/gaearon/react-side-effect\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/gaearon/react-side-effect/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"scheduler@0.23.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/facebook/react\",\n    \"licenseUrl\": \"https://github.com/facebook/react/raw/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"scroll@3.0.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/michaelrhodes/scroll\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/michaelrhodes/scroll/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"scrollparent@2.0.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/olahol/scrollparent.js\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/olahol/scrollparent.js/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"shallowequal@1.1.0\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/dashed/shallowequal\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/dashed/shallowequal/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"@mswjs/cookies@0.2.2\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/mswjs/cookies\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/mswjs/cookies/main/LICENSE.md\",\n\n-----------------------------------------------------------\n\n\"@open-draft/until@1.0.3\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/open-draft/until\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/open-draft/until/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"@xmldom/xmldom@0.8.7\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/xmldom/xmldom\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/xmldom/xmldom/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"available-typed-arrays@1.0.5\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/inspect-js/available-typed-arrays\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/inspect-js/available-typed-arrays/main/LICENSE\",\n\n-----------------------------------------------------------\n\n\"base64-js@1.5.1\"\n\n    \"licenses\": \"MIT\",\n    \"repository\": \"https://github.com/beatgammit/base64-js\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/beatgammit/base64-js/master/LICENSE\",\n\n-----------------------------------------------------------\n\n\"buffer@6.0.",
        "88ec0fc9-c3a9-4ea5-9588-f83e27b31b48": "When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. MAST Gateway A gateway that serves as a centralized entry point for all the operations that\n            need to be performed on tiered storage.",
        "795b0576-0a0b-4282-bc18-4d62f35426b2": "Tutorials Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . About Provides an overview of HPE Ezmeral Unified Analytics Software . Tutorials Provides a set of tutorials that you can use to experience HPE Ezmeral Unified Analytics Software and the included     applications, such as tutorials for data science and data analytics workflows with notebooks and     applications like Spark, MLflow, Feast, Airflow, and EzPresto. Preparing the Tutorial Environment Describes how to prepare the environment for the Financial Time Series and MNIST Digits     Recognition tutorials. Data Source Connectivity and Exploration Provides basic steps for using the Data Engineering space within HPE Ezmeral Unified Analytics Software . BI Reporting (Superset) Basics Provides basic steps for using the BI Reporting (Superset) space within HPE Ezmeral Unified Analytics Software . Candy Sharing Tutorial (Kale) Describes how Kale converts Notebook to pipeline by applying notebook         annotations. Feast Ride Sharing Use Case Provides an end-to-end workflow using Feast in HPE Ezmeral Unified Analytics Software to generate training data and perform online         model inference for the ride-sharing driver satisfaction model. Financial Time Series Workflow Describes how to use HPE Ezmeral Unified Analytics Software to run a Spark application from an Airflow DAG and then run a     Jupyter notebook to analyze and visualize data that the Spark application puts into a shared     directory in the shared volume that the data scientist\u2019s notebook is mounted to. MLflow Bike Sharing Use Case Provides an end-to-end workflow in HPE Ezmeral Unified Analytics Software for an MLflow prediction model to determine bike rentals per hour     based on weather and time. MNIST Digits Recognition Workflow Provides an end-to-end workflow in HPE Ezmeral Unified Analytics Software for an MNIST digits recognition example. Retail Store Analysis Dashboard (Superset) Provides an end-to-end workflow example for a retail store analysis scenario in HPE Ezmeral Unified Analytics Software using EzPresto and Superset. Submitting a Spark Wordcount Application Provides an end-to-end example for creating and submitting a wordcount Spark         Application in HPE Ezmeral Unified Analytics Software . Resources Provides links to additional resources such as product licensing information, on-demand     training, videos, blogs, and HPE Ezmeral Unified Analytics Software community. Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Tutorials Provides a set of tutorials that you can use to experience HPE Ezmeral Unified Analytics Software and the included\n    applications, such as tutorials for data science and data analytics workflows with notebooks and\n    applications like Spark, MLflow, Feast, Airflow, and EzPresto. The following sections describe the tutorials and provide links to access the\n      complete tutorials in GitHub. Fraud Detection Use Case Overview In this tutorial, data scientists and machine learning engineers inspect fraudulent\n                transactions using the Bankism dataset. This synthetically created dataset is a\n                combination of various customer payments, made at different intervals and in varying\n                amounts. This tutorial covers everything from data processing, through model development, to\n                the final stage of model deployment. By the end of this tutorial, you will learn how to detect and curtail fraudulent\n                activities with high accuracy. Tools This tutorial uses the following components from HPE Ezmeral Unified Analytics Software : Kale to enable the transformation of a Jupyter Notebook into a Kubeflow\n                    Pipeline. Kubeflow Pipelines to scale the training and deployment process in a\n                    reproducible way. MinIO to store the training artifacts. KServe as a fully trained machine learning model. GitHub Link To complete this tutorial, follow the instructions outlined in the fraud detection tutorial . Question-Answering Use Case Overview In this tutorial, data engineers explore the deployment of LLMs (Large Language\n                Models) to write the code that serves the model, processes the user requests, and\n                packages everything in a custom Docker. This tutorial utilizes an open-source Large Language Model (LLM) that can answer\n                questions over a corpus of private documentation.",
        "8dce7131-1f95-4e5e-9385-af2aeb409416": "data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage. minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation.",
        "508976e9-0496-4b76-a1dc-e78a178b8e58": "class determined.searcher.ExitedReasonvalue The reason why a trial exited early The following reasons are supported: ERRORED: The Trial encountered an exception USER_CANCELLED: The Trial was manually closed by the user INVALID_HP: The hyperparameters the trial was created with were invalid",
        "189f9b6b-7c34-4bf1-ba55-c715a63ce693": "This section describes how Determined runs on Amazon Web Services (AWS). For installation, see Install Determined. A master node (a single, non-GPU instance) manages the cluster, provisioning and terminating agent nodes dynamically as new workloads are started by users. The master stores metadata in an external database; using AWS Aurora or RDS is recommended. Users interact with the cluster by using a CLI or visiting the WebUI hosted on the master. Nodes in the cluster communicate with one another over a Virtual Private Cloud (VPC); users interact with the master via a designated external port configured during installation. Following the diagram, a standard execution would be: User submits experiment to master Master creates one or more agents (depending on experiment) if they don\u2019t exist Agent accesses required data, images, etc. Agent completes experiment and communicates completion to master Master shuts down agents that are no longer needed This section provides details on the core resources, which are required to run Determined, and peripheral resources, which are optionally configurable based on user requirements.",
        "b5ba4e17-096b-4293-8c9b-00dc10862ef7": "class determined.experimental.client.TrainingMetricstotal_batches: Optional[int] = None**kwargs: Any @deprecated: Use TrialMetrics instead. Specifies a training metric report that the trial reported.",
        "3abd8b18-4464-4810-b7ff-195f2a7dcc23": "You can start Jupyter Notebooks from the WebUI.",
        "913d7e1c-2e19-4bc7-9d21-6f0a918d9009": "For example, once you have generated the keys for the fabric\n        manager, you can use MinIO client ( mc ) commands to set an alias for the\n        fabric manager. Then you can use the same alias to perform operations on all fabrics in the\n        namespace. For information about the supported mc commands, see MinIO Client (mc) Commands . You can generate the keys only twice for the same global namespace. More attempts to\n        generate keys result in an error message. Use these steps: Sign in to your local fabric as a fabric manager. The Global\n              namespace screen appears. In the Global namespace card, click the Graph view . Click the icon for the global namespace. Click View access points : The Access points screen is\n            displayed. Click the S3 servers tab. The screen displays the Access keys\n            and S3 server details: Click Generate key . A confirmation dialog box asks if you want\n            to generate a new S3 key. Click Generate key . A dialog box displays the access key and\n            secret key information. For example: Click Download JSON if you want to download the keys for the\n            global namespace as a JSON file. Click Download if you want to download the S3 end points\n            information as a JSON file. (Topic last modified: 2023-11-05) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "cfff2ff5-f5d9-4293-ab06-a1202a5c7a74": "container location database (CLDB) Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster.",
        "bfa94d24-6002-434e-b352-bbed75f68335": "data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage. minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation.",
        "b6f377d1-bdb6-4fce-ac93-ecadebc5c45c": "These kubectl commands list and delete pods which are running Determined tasks: # Get all pods that are running Determined tasks. kubectl get pods -l=determined # Delete all Determined task pods. Users should never have to run this, # unless they are removing a deployment of Determined. kubectl get pods --no-headers=true -l=determined | awk '{print $1}' | xargs kubectl delete pod",
        "e7c085ab-d524-470e-bb55-cd45672979f3": "Using the -h or --help argument on objects or actions prints a help message and exits the CLI. For example, to print usage for the deploy command, run the following: det deploy -h Similarly, you can get help for a subcommand. For example, to get help for deploy aws: det deploy aws -h",
        "d31e5dc6-3bac-4d59-a875-e5608f9f5262": "When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical\n            namespace that is organized like a standard file system.",
        "2a67b9a5-bc48-4b16-ba90-01a2beacc34f": "MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. (Topic last modified: 2020-07-02) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "b3219cc8-cd1a-4b3f-ae0b-3c2f105e2fb0": "Use the value from the experiment configuration tags list (if no matching tags, nothing is passed to workload manager). If a tag in the list begins with the specified prefix, remove the prefix and use the remainder as the value for the WCKey/Project. If multiple tag values begin with prefix, the remainders are concatenated with a comma (,) separator for Slurm or underscore (_) for PBS. If a prefix is not specified or empty, all tags will be matched (and therefore concatenated). Workload managers do not generally support multiple WCKey/Project values so it is recommended that prefix is configured to match a single label to enable use of the workload manager reporting tools that summarize usage by each WCKey/Project value.",
        "00994c7b-19d5-4b15-8ad9-7dfff15ffbdc": "Muting/Dismissing Alarms Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Muting/Dismissing Alarms Mute or dimiss an alarm via HPE Ezmeral Data Fabric UI. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Viewing Alarms View alarms on the overview/data_fabric_ui.html . Muting/Dismissing Alarms Mute or dimiss an alarm via HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Muting/Dismissing Alarms Mute or dimiss an alarm via HPE Ezmeral Data Fabric UI. Prerequisites You must have access to Data Fabric UI and the permission to mute or dismiss an\n            alarm. About this task You can mute or dismiss an alarm that is visible on the Data Fabric UI. An alarm can\n                be muted for 24 hours, 6 hours or 1 hour. Follow the steps given below to mute/dismiss an alarm. Procedure Log on to the Data Fabric UI . Click the bell icon at the top right corner next to the help icon. Click View all to view all alarms. To mute an alarm, click Mute and select the duration for which you wish\n                    to mute the alarm. Alternatively, click Dismiss to dismiss the\n                    alarm. Results The alarm is muted for the specified duration or the alarm\n            is dimissed, depending on the action you have selected. Related maprcli Commands To implement the features described on this page, the\n                Data Fabric UI relies on the following maprcli command. The\n                command is provided for general reference. For more information, see maprcli Commands in This Guide . alarm mute (Topic last modified: 2023-10-18) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary"
    },
    "relevant_docs": {
        "e16be2e7-20c0-40b2-8d23-9d38dae4a265": [
            "23941117-0b3c-41b1-ac81-b2b745f84243"
        ],
        "6c96852b-cdd7-406a-9ed6-743be496cbb7": [
            "23941117-0b3c-41b1-ac81-b2b745f84243"
        ],
        "fab959d6-a0ce-4812-98b7-2995a93dbb4c": [
            "e4e6ff92-8f29-4afb-9945-bd5dd760a101"
        ],
        "4fa1c75e-32db-4769-8fb6-d885b3e0f7ff": [
            "e4e6ff92-8f29-4afb-9945-bd5dd760a101"
        ],
        "3b58d492-555b-48f4-8bbc-64cc55a2b072": [
            "15260de1-40d3-4706-8c65-b2031ce36cfa"
        ],
        "3b844b41-5417-4981-9236-3c0f76578a43": [
            "15260de1-40d3-4706-8c65-b2031ce36cfa"
        ],
        "3e70d613-0cc4-4876-97f6-2f5847c7b79c": [
            "b561a67d-165f-4991-8e68-bbdbc6a230c9"
        ],
        "5b2b6a36-606a-4db5-81f8-f9b69e7147d0": [
            "b561a67d-165f-4991-8e68-bbdbc6a230c9"
        ],
        "ecd07a30-3860-42b5-bfc5-6450ab4c9494": [
            "f71f2d1d-e40f-46be-93eb-900102ad4a41"
        ],
        "0fc6051c-d5a3-481a-895c-54d71592e3a4": [
            "f71f2d1d-e40f-46be-93eb-900102ad4a41"
        ],
        "bd8a4e99-73e5-430c-9b8e-c5ea4c03e68e": [
            "e55be723-bc6b-4c6e-9859-656025f71dec"
        ],
        "faeb67b4-cbfc-467d-bf05-2fd011b1f301": [
            "e55be723-bc6b-4c6e-9859-656025f71dec"
        ],
        "2a63a233-015f-405f-9d91-f606237809a8": [
            "c5ab7301-4df1-4281-80f0-dd7cf3b97d43"
        ],
        "4130260e-9a43-49b0-9c68-e21a2306fb7d": [
            "c5ab7301-4df1-4281-80f0-dd7cf3b97d43"
        ],
        "1c9835b8-d91d-4883-9944-433542653b76": [
            "c394b048-173e-4b0e-9228-991570e70e3b"
        ],
        "6383976e-5a63-4268-86be-c20c89f45490": [
            "c394b048-173e-4b0e-9228-991570e70e3b"
        ],
        "f24e6311-fa54-4097-a645-59905be4193c": [
            "6bbdba0a-4d08-40e2-aaa5-167b9515375a"
        ],
        "234f82cd-a0bd-4e23-b9fb-310ed7776fcf": [
            "6bbdba0a-4d08-40e2-aaa5-167b9515375a"
        ],
        "28ba0fdc-b55d-4839-955b-912a0829406e": [
            "60d018ec-ad87-44f8-8c84-050a35da5b91"
        ],
        "73ed4d49-91fc-43ff-bfa5-a206c77c1d56": [
            "60d018ec-ad87-44f8-8c84-050a35da5b91"
        ],
        "bd8b6919-a525-4f9c-99b5-e66753909a9a": [
            "792fed08-fdd6-46f4-ba92-60f565025f5b"
        ],
        "22924aaf-84ab-4727-bdda-820965c0fce2": [
            "792fed08-fdd6-46f4-ba92-60f565025f5b"
        ],
        "7d0f3023-66dd-4dec-9c46-c820004cdc1a": [
            "db75bbb2-1846-4d2f-98ea-b2a1e957135f"
        ],
        "0f29695a-c7f3-41a7-9fb3-608cb1682775": [
            "db75bbb2-1846-4d2f-98ea-b2a1e957135f"
        ],
        "71b3f015-e118-4cb9-9558-b7630bb7dba1": [
            "13742b1d-a1f3-4b6d-9982-b8ccabe2ec10"
        ],
        "aa104ddc-bd3a-42d6-85e7-84ea3ec63fa3": [
            "13742b1d-a1f3-4b6d-9982-b8ccabe2ec10"
        ],
        "ea0f0272-3b62-48c3-96fb-af1e64902fdd": [
            "776e095a-83b8-4cf2-bb74-543b64650795"
        ],
        "1fe80530-e24e-4cdc-9174-9c4ba24d0243": [
            "776e095a-83b8-4cf2-bb74-543b64650795"
        ],
        "f175ffc1-9a42-4eeb-99e6-21894fb8e1bd": [
            "7eb7018c-7c30-4669-b6c7-5e53f1402c5d"
        ],
        "6ead00b9-8fc3-40a5-9b0b-35e459360644": [
            "7eb7018c-7c30-4669-b6c7-5e53f1402c5d"
        ],
        "08e44157-ac3d-4948-83f4-2f04db787b15": [
            "87f6a9f6-f666-47b0-8b7a-2719eb18e0de"
        ],
        "15f01977-9ace-4490-ac4d-4187c0c7d4b9": [
            "87f6a9f6-f666-47b0-8b7a-2719eb18e0de"
        ],
        "f034384d-4399-489c-924f-0db331c9e041": [
            "63395af6-8dad-44cb-a6e2-d9220978a71b"
        ],
        "bd77dc77-b13c-4445-a3b8-c762a1c86cc5": [
            "63395af6-8dad-44cb-a6e2-d9220978a71b"
        ],
        "2d1a4239-85df-4d2e-8cd9-b8668acff4f6": [
            "29314430-7c0e-46fd-b7da-4bac086feacf"
        ],
        "ae456eab-d8ce-4db4-9271-0e8c2ad6b2ea": [
            "29314430-7c0e-46fd-b7da-4bac086feacf"
        ],
        "288370cc-09cf-4908-a967-55107882d678": [
            "3de39877-c8dc-475e-b616-4bb488095cfc"
        ],
        "64aad730-ae3c-4799-b6e2-b44e6c3e58fc": [
            "3de39877-c8dc-475e-b616-4bb488095cfc"
        ],
        "24df5c47-1c0c-4687-969e-18047f0623f8": [
            "7eeddea0-a5cf-4552-8c15-45c6d4de9352"
        ],
        "b306ad27-5672-4270-863d-05f201f5e73f": [
            "7eeddea0-a5cf-4552-8c15-45c6d4de9352"
        ],
        "8c1734fe-d81a-49ad-a5a7-ac4c9180bc4f": [
            "8cfe60b3-adc2-4585-83ed-e1696065a5d2"
        ],
        "30932457-aaa9-4ad5-ab8a-6f1f339c5b50": [
            "8cfe60b3-adc2-4585-83ed-e1696065a5d2"
        ],
        "1c01b111-0548-4110-a985-7ea44b23c55e": [
            "15011b9e-a5c7-438f-9e96-f8b1a5432c07"
        ],
        "6c615213-1eaf-4536-b7d6-1ea350ffc46f": [
            "15011b9e-a5c7-438f-9e96-f8b1a5432c07"
        ],
        "880e9929-2b91-450c-94ec-03103236c4f0": [
            "10b4f892-64b8-45f0-8c5f-1f08c7eb1f05"
        ],
        "3e560e71-df91-4bc7-bed8-72cecf70ddd9": [
            "10b4f892-64b8-45f0-8c5f-1f08c7eb1f05"
        ],
        "3e715607-003b-49e3-9850-461bdf284d9e": [
            "29bff331-071c-4290-8376-29a84877f793"
        ],
        "3b92a7ca-3038-4747-8b17-f5c1d0137a30": [
            "29bff331-071c-4290-8376-29a84877f793"
        ],
        "ad8afcdb-6d81-4776-9ff6-150bea48a693": [
            "0b23b381-e119-4ff1-a446-3e362d34a89b"
        ],
        "fd7a9701-84b0-468d-b65e-888df67b3c4f": [
            "0b23b381-e119-4ff1-a446-3e362d34a89b"
        ],
        "a54459dd-5eae-4a63-b9da-a3bf8502366e": [
            "aeb9e64f-b315-4549-849f-1eb7c268de75"
        ],
        "f12e45d1-cb37-48ff-9d00-a0fcda45a65a": [
            "aeb9e64f-b315-4549-849f-1eb7c268de75"
        ],
        "c23ee1be-6c43-4173-9811-9486f8fdf570": [
            "4b7ea1c2-367c-45e2-a766-47d6dd44de93"
        ],
        "46c78383-4249-4d13-9016-4adf7fb4e939": [
            "4b7ea1c2-367c-45e2-a766-47d6dd44de93"
        ],
        "dc557c6a-5a6e-40c6-a687-21fa975a3aa7": [
            "b92687d2-ee8f-4253-b754-7b9c432c59a4"
        ],
        "6975b8a4-534e-4faf-a17c-b3f1030833be": [
            "b92687d2-ee8f-4253-b754-7b9c432c59a4"
        ],
        "54d4ead9-2cca-456f-adab-933d72b69366": [
            "5c86d74e-4ad6-4c9a-bf4e-598a3e68ac20"
        ],
        "3b9b02f9-7f1f-4416-bd02-e49e8f2420d8": [
            "5c86d74e-4ad6-4c9a-bf4e-598a3e68ac20"
        ],
        "3e1ea7da-002c-469e-bd00-1b5f4427e91e": [
            "e4961f0f-26de-43a8-b46f-3a2c8ee853c8"
        ],
        "c1b67de6-05c8-4ee8-87be-bd848d03eb69": [
            "e4961f0f-26de-43a8-b46f-3a2c8ee853c8"
        ],
        "15f3c3ff-ba86-4b54-ba08-ee70b5341c40": [
            "6a1a91cf-ca0c-4a74-a3fe-3b8da3134888"
        ],
        "ca675616-3a3c-40f9-8fc6-7561beb0c658": [
            "6a1a91cf-ca0c-4a74-a3fe-3b8da3134888"
        ],
        "0b99a4f9-a8f6-4613-8b12-610a964450ca": [
            "cd73e8c0-42fb-4fb9-8963-9e7839eddbb3"
        ],
        "c0bea6b7-0db0-49fd-bef5-1d15459d87de": [
            "cd73e8c0-42fb-4fb9-8963-9e7839eddbb3"
        ],
        "62af850a-e607-4e40-a34c-7fc8b389b59f": [
            "f97b2b6d-5359-4950-8639-fa110d5b22c1"
        ],
        "e17166f5-8687-4fb0-a753-baf4ad2c6d77": [
            "f97b2b6d-5359-4950-8639-fa110d5b22c1"
        ],
        "462d7c21-4269-494e-be28-8925e1ac8efd": [
            "2bd57364-512d-4dab-bb3d-e7a67f9ff709"
        ],
        "83fd3d84-d567-4654-826b-7d891e0f7910": [
            "2bd57364-512d-4dab-bb3d-e7a67f9ff709"
        ],
        "2320fa27-c1cc-415f-9bc3-39ca803d71cb": [
            "0ba7827e-24db-465e-8986-be1596699b49"
        ],
        "e00429d1-b806-4296-a6f7-8b26e7a7ad9c": [
            "0ba7827e-24db-465e-8986-be1596699b49"
        ],
        "a58b015e-c8ce-4679-927b-0d40e8202b3e": [
            "12ecce6d-ee38-40af-bc98-be13befca34b"
        ],
        "b631d8db-0e07-41fa-8ff4-fb5ca82e2ae5": [
            "12ecce6d-ee38-40af-bc98-be13befca34b"
        ],
        "e4e92c33-7014-4b4d-8143-d141b4257e72": [
            "648c6097-c160-4c15-8ffb-bfa7c2d80836"
        ],
        "0c52db23-2f05-4de2-84ee-060cd70ff7ef": [
            "648c6097-c160-4c15-8ffb-bfa7c2d80836"
        ],
        "53c184cf-e6a5-4d81-92e6-70fcd732d187": [
            "9427cd1d-3bbe-4b83-b9b4-be2d19b6f8df"
        ],
        "240cf2a1-b0dc-4f73-8199-7b6a228507e3": [
            "9427cd1d-3bbe-4b83-b9b4-be2d19b6f8df"
        ],
        "6c84bda6-2517-41c8-8425-848161d7a245": [
            "6fc19e12-19ef-451a-b84e-4703a2727bb6"
        ],
        "2c8b0cdb-65c0-4538-a5b3-7414579a3e92": [
            "6fc19e12-19ef-451a-b84e-4703a2727bb6"
        ],
        "53da31de-6c15-4ce0-b3e1-20d04540dc84": [
            "e1c2d986-16ca-49ac-a1b2-7757c8d58ab1"
        ],
        "39af0ef9-d661-49db-af64-c1eb6adfdf8f": [
            "e1c2d986-16ca-49ac-a1b2-7757c8d58ab1"
        ],
        "01126020-c4b9-4408-945f-0fa8b85ab68b": [
            "609ca34d-5390-4890-9c9a-9871c8905db0"
        ],
        "99f3f504-d033-4bb5-bb23-eaaa3f130e12": [
            "609ca34d-5390-4890-9c9a-9871c8905db0"
        ],
        "3217dcaf-fbd2-4693-b29f-39df350602a6": [
            "beecc191-7bf2-40b9-a5e9-275921b94cdc"
        ],
        "4337254e-a7a3-4e6d-962c-532bc4e5a79b": [
            "beecc191-7bf2-40b9-a5e9-275921b94cdc"
        ],
        "a88e3301-ca20-4040-8bac-04cff4e97f23": [
            "906267a0-1d64-4072-b673-95d09b639581"
        ],
        "bd0f8ae3-9709-4675-bd20-37d1e3faec2f": [
            "906267a0-1d64-4072-b673-95d09b639581"
        ],
        "9696c869-d720-4b05-b5a9-92c2bbb5d226": [
            "c95c30da-bc56-4a5b-b2e6-da6db3292cbb"
        ],
        "9ae3b7e3-4cec-4c95-bde3-42c56dc90c04": [
            "c95c30da-bc56-4a5b-b2e6-da6db3292cbb"
        ],
        "7a1e3d61-bf5f-4cfa-8a4d-1cb644a2d66d": [
            "35178e46-ad37-42ba-ab4b-aa8b2cc8d9a4"
        ],
        "5ee66a9d-bcae-444d-be02-7e25c4dc9be5": [
            "35178e46-ad37-42ba-ab4b-aa8b2cc8d9a4"
        ],
        "801bf04d-f3c5-4cd9-960c-4ee4f64dbe00": [
            "c6b4d4f8-cf4e-418e-a225-d351680e4dbc"
        ],
        "f6efe481-97fb-4f03-9aec-5114c30a8b00": [
            "c6b4d4f8-cf4e-418e-a225-d351680e4dbc"
        ],
        "f9898e6e-e56d-4e32-a068-729ad33d33c6": [
            "71359944-ae21-46ab-8779-ebe706609e31"
        ],
        "a541dd02-1b39-4cf4-90cb-7d7d7918f2df": [
            "71359944-ae21-46ab-8779-ebe706609e31"
        ],
        "5647b8bb-3a6d-4238-ba7f-ef0d2e42dba5": [
            "57494e20-97b3-4653-b3e8-852c42cf85d5"
        ],
        "43abbdc0-678c-4490-93bb-e32008dc8e30": [
            "57494e20-97b3-4653-b3e8-852c42cf85d5"
        ],
        "1c20d7d0-0dab-4d0a-bd59-22de5a536b14": [
            "1f1cc489-ea48-45fb-a24e-fa0be09161aa"
        ],
        "d1d00485-e1c2-4561-83cd-716c33d3c7c8": [
            "1f1cc489-ea48-45fb-a24e-fa0be09161aa"
        ],
        "d16b52cd-e5da-4031-83e5-98e40e1f99a0": [
            "8ad7f83e-02ba-43b2-9cbe-0168908f6a2b"
        ],
        "63c0d971-2d6b-40ae-8440-04945df7809f": [
            "8ad7f83e-02ba-43b2-9cbe-0168908f6a2b"
        ],
        "4f150953-a469-44fa-8c6a-872a7ecf5d1e": [
            "f884b9a7-16e7-422d-a576-a5ffff897fd3"
        ],
        "5d5dd7a9-afb6-4724-9723-26149aa79cbe": [
            "f884b9a7-16e7-422d-a576-a5ffff897fd3"
        ],
        "1aa84708-3d2e-419e-a630-d75f9526fd3d": [
            "63e0adfa-d702-472e-8a4a-7296875a0339"
        ],
        "7fcb609b-9898-4213-abc4-1cc16ee1e80e": [
            "63e0adfa-d702-472e-8a4a-7296875a0339"
        ],
        "7de50878-698c-4607-a35d-a248dc05fe96": [
            "9cb68f5b-5b44-4ee5-8d80-b4f016b8a393"
        ],
        "2112d612-b7ab-440e-8c66-f6f0a4c91f71": [
            "9cb68f5b-5b44-4ee5-8d80-b4f016b8a393"
        ],
        "597cf057-ff85-4235-b220-2ea316f74043": [
            "df2f9d55-1c8c-4e96-88cb-8d1462cb650a"
        ],
        "7f7381bc-6a6a-422f-a432-09a532736d0d": [
            "df2f9d55-1c8c-4e96-88cb-8d1462cb650a"
        ],
        "4fff5728-045f-4d39-bed5-6e67a9d68822": [
            "778aa07b-7328-4679-9caa-ef8b103fa22c"
        ],
        "05ca946b-f615-4675-b199-d5727f525702": [
            "778aa07b-7328-4679-9caa-ef8b103fa22c"
        ],
        "4321fd16-c50f-4483-8a8d-1a36e8bb7c92": [
            "3f453cfa-7872-4a24-93fa-a03411ee7779"
        ],
        "067a5479-77fb-491b-bceb-f505a924647e": [
            "3f453cfa-7872-4a24-93fa-a03411ee7779"
        ],
        "46f28aac-b7d6-4202-bd31-d9456a1d1c51": [
            "24d9b223-83a5-4fa2-b525-2f33364ab7f7"
        ],
        "2f814be1-7be3-4c17-abf1-c0f9273c4957": [
            "24d9b223-83a5-4fa2-b525-2f33364ab7f7"
        ],
        "bf2845bb-d53c-42ee-a7a6-336cc8be6c0a": [
            "5b07b4d0-8090-42c9-855e-7ee0d14d12f6"
        ],
        "d8ae1d66-cc0f-4917-8e51-1205d0349ed4": [
            "5b07b4d0-8090-42c9-855e-7ee0d14d12f6"
        ],
        "32e5cb72-d670-4efc-825a-222c6a7c0c63": [
            "a0e06fd1-cbec-4dfc-8b52-c566d9e81b44"
        ],
        "e6e7855c-2d76-45eb-b669-5a5b1b10271e": [
            "a0e06fd1-cbec-4dfc-8b52-c566d9e81b44"
        ],
        "da0db22e-8806-4e39-89e5-bd0100adb41b": [
            "f1035dc0-6d06-4a53-91a5-f764c8f5cda0"
        ],
        "f8777fe6-e6c3-4845-9a50-fbe382eaafc6": [
            "f1035dc0-6d06-4a53-91a5-f764c8f5cda0"
        ],
        "acda3487-2468-4a99-9572-d0e178dc38e8": [
            "35938174-d478-4cd0-bd9a-5c47894776ce"
        ],
        "7d4a6ceb-6057-4826-b775-b03f1996f629": [
            "35938174-d478-4cd0-bd9a-5c47894776ce"
        ],
        "66867757-5458-433c-a910-51782d0fb5cc": [
            "dfe6ff9e-79a0-4be3-8575-8be9725fe20d"
        ],
        "0e00fd6f-a761-4d16-b014-3d236a50f5f7": [
            "dfe6ff9e-79a0-4be3-8575-8be9725fe20d"
        ],
        "ef33ccf9-76a6-485f-a10c-ef9edff714b4": [
            "30929f92-553a-4d77-add0-18d0147b8356"
        ],
        "6eb31920-30e7-42ba-9605-d7a4fcfe0cff": [
            "30929f92-553a-4d77-add0-18d0147b8356"
        ],
        "f1e58ed5-ebd2-4ac5-a0f3-fd4d02928a74": [
            "30929f92-553a-4d77-add0-18d0147b8356"
        ],
        "127e7b9e-0e45-44b2-9131-34083dae08a3": [
            "30929f92-553a-4d77-add0-18d0147b8356"
        ],
        "f06b3a5f-bdea-4561-ae7f-fdad794c5248": [
            "30929f92-553a-4d77-add0-18d0147b8356"
        ],
        "c653e2ec-abe3-4398-8e46-446c069ca397": [
            "30929f92-553a-4d77-add0-18d0147b8356"
        ],
        "61460b54-5764-43e8-af29-b23dbec6a3b3": [
            "30929f92-553a-4d77-add0-18d0147b8356"
        ],
        "8e0c7f9d-ee21-4680-9c3c-39a8967186aa": [
            "30929f92-553a-4d77-add0-18d0147b8356"
        ],
        "7a7c3676-bdb5-4720-8672-7865b724acee": [
            "30929f92-553a-4d77-add0-18d0147b8356"
        ],
        "02346efd-b581-44b9-87cf-f275bcb36d56": [
            "30929f92-553a-4d77-add0-18d0147b8356"
        ],
        "b546f2ed-e745-45fa-bad8-5f68adc3a609": [
            "30734e2a-ef15-4b5f-b333-8f63905fcad3"
        ],
        "fb343c3e-c18f-41c5-8eba-50889e998aee": [
            "30734e2a-ef15-4b5f-b333-8f63905fcad3"
        ],
        "638146eb-cb70-44b1-bddb-074c49162a92": [
            "97299f71-7aa4-4d5c-9773-f102b7097ddc"
        ],
        "207f1e89-371c-4d7f-bf40-efa7da02a41c": [
            "97299f71-7aa4-4d5c-9773-f102b7097ddc"
        ],
        "0212ba69-36ee-4933-8cab-5c0925ea6d71": [
            "5f486e0b-a6ca-4e9b-ac4d-921e2fd1c97e"
        ],
        "b5d9109c-2c99-4c79-86a4-7675ec5f0093": [
            "5f486e0b-a6ca-4e9b-ac4d-921e2fd1c97e"
        ],
        "4ccae256-f934-4ebe-b4cf-13e110b6087d": [
            "9d354c46-4c15-4606-8655-7754ced8af93"
        ],
        "d6919cfb-7cec-4cd3-84fe-e1cee994226a": [
            "9d354c46-4c15-4606-8655-7754ced8af93"
        ],
        "73bcf15b-fd27-43d4-8238-6d9000a66b21": [
            "71fd215b-1c30-42de-bc7e-e37a78f0f215"
        ],
        "9dc1da74-4651-42bd-848c-99efba882470": [
            "71fd215b-1c30-42de-bc7e-e37a78f0f215"
        ],
        "d6ce35b1-6951-49a6-afd8-99f56a8ae03b": [
            "66f331c0-24a9-4763-bb41-959e836cdd98"
        ],
        "914abb4b-3bba-43c8-ac0f-006065ac9629": [
            "66f331c0-24a9-4763-bb41-959e836cdd98"
        ],
        "7dccf381-22d8-489e-a60c-60ec2bc9c319": [
            "7e6e5300-c746-4d0f-b82c-0965165c053c"
        ],
        "da4def65-b9ea-459a-819d-c0fc4b5e0b56": [
            "7e6e5300-c746-4d0f-b82c-0965165c053c"
        ],
        "7bc0c0d8-9efc-444d-b757-57ac8ed92d16": [
            "36555817-f139-4cd5-b6aa-828fdedcdd7b"
        ],
        "6f092b35-50af-4ce0-ac61-0267ca44cd60": [
            "36555817-f139-4cd5-b6aa-828fdedcdd7b"
        ],
        "cb24ad5c-dec0-482d-85e3-db7b1960fdfa": [
            "5e785c55-110b-4337-b9d1-0e94906a04f5"
        ],
        "072b54c6-19df-49e8-ba51-29beed867b64": [
            "5e785c55-110b-4337-b9d1-0e94906a04f5"
        ],
        "693b8470-6bcc-4ed4-95ee-7b7677fee3c6": [
            "1bfacc2c-4baa-46f8-8d97-3ac0bb0de9c9"
        ],
        "7463c6eb-2932-4135-9c42-80a81407f4c4": [
            "1bfacc2c-4baa-46f8-8d97-3ac0bb0de9c9"
        ],
        "959e6118-9309-4612-9d05-de28016ccc3e": [
            "d20125a1-5577-423d-9ac4-76edb6a1e5de"
        ],
        "7c56dcc5-3be1-4dbf-9fa7-00f6a7eeffc0": [
            "d20125a1-5577-423d-9ac4-76edb6a1e5de"
        ],
        "5daad02e-b1ba-489a-af90-9700409088a2": [
            "04c38740-b39c-4d98-85b4-beb7e3002e68"
        ],
        "54a93722-7e8a-4ed5-8320-13fa443a542b": [
            "04c38740-b39c-4d98-85b4-beb7e3002e68"
        ],
        "7634ab24-b917-4143-86ec-76748202f77f": [
            "0eb13f88-8153-4dfa-84b2-5a405e53a51e"
        ],
        "43adb5c1-2135-448d-a829-b50ca28bcdd2": [
            "0eb13f88-8153-4dfa-84b2-5a405e53a51e"
        ],
        "06675714-40e7-415f-9a64-c5f4eb2b1b56": [
            "8c272b39-04c2-499f-ac82-78048e56c012"
        ],
        "5429bf1e-02fa-4c36-ba72-fa3a7417e1f2": [
            "8c272b39-04c2-499f-ac82-78048e56c012"
        ],
        "282f4043-6217-488a-b7fd-857c83b1806c": [
            "a45bdff9-ef6e-474f-b765-8e94bf2e79af"
        ],
        "71b44de1-cbc5-47c8-9bd5-29e629a5d290": [
            "a45bdff9-ef6e-474f-b765-8e94bf2e79af"
        ],
        "12cd3fd7-84cc-47ad-bba6-078ea7403403": [
            "7db097fe-b93d-44f1-b3e4-48c5df72d8cb"
        ],
        "bad87a97-c787-4090-9384-a45c40c7111f": [
            "7db097fe-b93d-44f1-b3e4-48c5df72d8cb"
        ],
        "ff08c482-0709-40c1-8694-dc89c1b3ba29": [
            "2baeb6a2-ada1-4280-9157-be395ef45a8d"
        ],
        "944740fb-ff00-4330-b639-37a515aa02cd": [
            "2baeb6a2-ada1-4280-9157-be395ef45a8d"
        ],
        "eec153aa-6058-47e9-84b4-a4a2abad8b54": [
            "cbabce88-74a1-444c-91c6-9ac95ad96a32"
        ],
        "86a54a7a-763d-43aa-bcc0-81bf783cdfd1": [
            "cbabce88-74a1-444c-91c6-9ac95ad96a32"
        ],
        "81b62d9d-76b2-46c0-98cf-0258cfbd5b65": [
            "745f7248-1c3b-4f80-be54-32f50b7f79bb"
        ],
        "7bf4c2a6-d83c-4dcf-a2ca-98d11221893b": [
            "745f7248-1c3b-4f80-be54-32f50b7f79bb"
        ],
        "4606474e-0e89-47c4-b1fc-b7ff7f9f3f8b": [
            "68688c66-3da9-4a87-9d92-86aacc1de7b1"
        ],
        "d1cf3889-a01d-4e8d-8a1d-1cfcd2d7b03e": [
            "68688c66-3da9-4a87-9d92-86aacc1de7b1"
        ],
        "cefe0b1b-28e8-4fca-8e2b-6e402bcdbf5c": [
            "e758bcc0-4617-428e-a252-f73844385664"
        ],
        "8e7d07d8-1b15-4244-8f22-285f7224f1ca": [
            "e758bcc0-4617-428e-a252-f73844385664"
        ],
        "d7210808-2790-40e3-96f4-683d91ac9c72": [
            "3d832b59-8fe7-4da4-810d-c7057203ae35"
        ],
        "59c8766e-d1ff-4390-abd5-57b179355d94": [
            "3d832b59-8fe7-4da4-810d-c7057203ae35"
        ],
        "c2b2b54b-565f-4d67-9c6e-fd6210bc14c4": [
            "b231217d-9b15-4f74-97f3-3099dfdb9dc5"
        ],
        "7a135a11-deb2-4372-a8f3-aac96efcb9c4": [
            "b231217d-9b15-4f74-97f3-3099dfdb9dc5"
        ],
        "f3bf5f5a-c553-4049-a4e7-d0616ac97d7c": [
            "1a00904f-c392-47bf-8ad2-be104dd150e8"
        ],
        "33e7be56-6859-43b2-9d32-a06df06dc186": [
            "1a00904f-c392-47bf-8ad2-be104dd150e8"
        ],
        "1c03180c-f4af-4071-bbc3-46e5edb427b1": [
            "969be782-3f0f-4dbe-bdb0-a286dbffdb99"
        ],
        "ced7e140-68cb-4b54-9737-1f361a158c01": [
            "969be782-3f0f-4dbe-bdb0-a286dbffdb99"
        ],
        "b2e6e773-d4f3-4193-a485-f42f4909e3bf": [
            "18a13e96-ac1f-4352-b537-2dddb2048c48"
        ],
        "017565d7-bca7-4f7e-9f38-bb008f6be195": [
            "18a13e96-ac1f-4352-b537-2dddb2048c48"
        ],
        "f80af23d-bff1-4230-9ecd-d4b032f1834b": [
            "971458a1-70f3-46b6-bcbe-cdf5e79cbb17"
        ],
        "655ef9f5-19a3-4969-a5c5-2c725ff6ef60": [
            "971458a1-70f3-46b6-bcbe-cdf5e79cbb17"
        ],
        "6ba631ee-57a2-4ff9-b857-a6e7d61e8046": [
            "2c34534a-bca9-4cf3-92d3-9edb42f84ad7"
        ],
        "6b261721-3754-47cb-b38b-f263e3883077": [
            "2c34534a-bca9-4cf3-92d3-9edb42f84ad7"
        ],
        "3b4c92ce-251a-458a-a029-2f2d2eff19f7": [
            "b88f9f33-c3bf-413f-8050-84210fef69e4"
        ],
        "25fa805d-c9c7-4330-9183-ca81244d4b14": [
            "b88f9f33-c3bf-413f-8050-84210fef69e4"
        ],
        "65376742-1ec8-4062-957d-11f169975dc0": [
            "194aca6a-7b04-4d36-b5d7-22099a6a4a05"
        ],
        "83473034-6060-4c9d-a5a6-53314b5c3438": [
            "194aca6a-7b04-4d36-b5d7-22099a6a4a05"
        ],
        "3c08e470-8e32-4a13-b67f-8f506f111b72": [
            "aa572938-32ce-4e52-8283-7bc73f152c3d"
        ],
        "d072cb1e-896d-4d82-93a6-18619b951d56": [
            "aa572938-32ce-4e52-8283-7bc73f152c3d"
        ],
        "81f2c6d6-e9fe-42f8-a776-418b3e653108": [
            "d096c550-69b5-4c0b-998b-8eaefe8bb83b"
        ],
        "5a70d845-a167-4aa3-aadc-c4fc650e0b2c": [
            "d096c550-69b5-4c0b-998b-8eaefe8bb83b"
        ],
        "cf7fd2f3-b14f-4ef6-a527-f5e94e90e019": [
            "8b4545c1-3b8b-4756-8c32-fcbaacfbe031"
        ],
        "4d8668b6-dcf9-433f-b68c-cc5406e926f7": [
            "8b4545c1-3b8b-4756-8c32-fcbaacfbe031"
        ],
        "cdfb0861-84d4-4a70-bebd-7240bb436c00": [
            "21cddac2-a36d-4f5a-a31c-d3177d57ee92"
        ],
        "66e0712d-eb3e-4874-8be8-ceb705b12753": [
            "21cddac2-a36d-4f5a-a31c-d3177d57ee92"
        ],
        "0864413c-f290-4751-9f64-121fa94b3486": [
            "d05e4834-d9d4-4e34-9336-50ebf734aabe"
        ],
        "34940a7d-ed9d-48e8-8b23-7c667e8d746f": [
            "d05e4834-d9d4-4e34-9336-50ebf734aabe"
        ],
        "eb477fb0-9c57-499f-9caf-306fd0aa1932": [
            "f974f5a1-4d31-4924-a8e1-bf41ad68e5e7"
        ],
        "058969e5-129f-4b4f-81af-8adff9014a4e": [
            "f974f5a1-4d31-4924-a8e1-bf41ad68e5e7"
        ],
        "4ce288cd-5b6c-40cb-8edf-b668cbde5af0": [
            "a389b6d0-7dc6-40e9-aeb2-380aed7dfd90"
        ],
        "50c2a564-6186-47f6-8b93-6872ee4acd0a": [
            "a389b6d0-7dc6-40e9-aeb2-380aed7dfd90"
        ],
        "deb55470-c6aa-4af5-ab3b-b36dc5c071d1": [
            "ac128152-2281-4815-9bd6-620c3e69d984"
        ],
        "13e0d35f-5154-43c6-a381-e8bbd13cdedc": [
            "ac128152-2281-4815-9bd6-620c3e69d984"
        ],
        "b2db12a0-f3bc-4fdb-b5ee-51d9cf28c7ac": [
            "49588d3b-b942-4627-b4c7-106c07ddd7d4"
        ],
        "8c2f3de1-fcae-4691-8a99-9632e79dd4d9": [
            "49588d3b-b942-4627-b4c7-106c07ddd7d4"
        ],
        "4882a58d-fdc9-4e6f-b64c-26967346d44c": [
            "18b7ac47-e516-4594-9fd2-94d3c621cfc4"
        ],
        "40928ce3-01b2-4245-9958-2c58c9f1e036": [
            "18b7ac47-e516-4594-9fd2-94d3c621cfc4"
        ],
        "47d023b2-58cb-4bb8-9aa4-98651ae5d313": [
            "4d3fb314-91d1-4522-b1a5-e79700c22972"
        ],
        "bd0128d6-d764-4c45-9d24-d0a130a95509": [
            "4d3fb314-91d1-4522-b1a5-e79700c22972"
        ],
        "ad8dece1-2399-4935-aea1-76b8f4c58e52": [
            "68503127-1353-46e3-a61f-4d84850973dc"
        ],
        "4c9fdfbc-0949-441b-a31d-f2873f69aa5d": [
            "68503127-1353-46e3-a61f-4d84850973dc"
        ],
        "3131bd54-c273-4978-80f5-ed0e01679b25": [
            "ab4be9e8-eee0-4c90-a10f-32a1a1b21663"
        ],
        "0cbd277f-af73-4d4c-87d5-de4206ab8847": [
            "ab4be9e8-eee0-4c90-a10f-32a1a1b21663"
        ],
        "d09cb46b-f94e-412d-96a9-28e1a17b8ca6": [
            "405a7072-7acf-4f05-b904-0ee00f4a2e50"
        ],
        "9acb2529-7f23-42b2-9956-60aa4309a8dc": [
            "405a7072-7acf-4f05-b904-0ee00f4a2e50"
        ],
        "040d392d-8a44-45df-a218-7980b941b1b8": [
            "1b176057-6174-408e-a20d-cd9097793b6b"
        ],
        "9880fe38-8747-4258-b23d-a245c53aea30": [
            "1b176057-6174-408e-a20d-cd9097793b6b"
        ],
        "9196e0b8-8898-454b-b440-45b2160c72b1": [
            "cabb9676-fd13-4a3e-9053-c8f8970e6a17"
        ],
        "83dc43bb-03b1-4c40-afaf-b2ba77c065a5": [
            "cabb9676-fd13-4a3e-9053-c8f8970e6a17"
        ],
        "36a0fa18-6af3-43f1-90ad-426c268d4f12": [
            "9de63f02-6181-430f-a9ca-66a738169de0"
        ],
        "f3b4eaf3-9dd5-42fb-8270-22e073f8d9d8": [
            "9de63f02-6181-430f-a9ca-66a738169de0"
        ],
        "de067880-58b3-4091-b85c-d24dba36648c": [
            "5e0567bd-6f09-4b3f-bb15-547eca5a1d03"
        ],
        "0fd6ef45-ec89-403a-89f3-08f630bb66ee": [
            "5e0567bd-6f09-4b3f-bb15-547eca5a1d03"
        ],
        "1f276d21-9ce7-489b-bae1-960755e3bb37": [
            "916dc445-3b02-4209-a916-84c393f10a5a"
        ],
        "599b9b62-f31a-4a81-8577-82f93024743b": [
            "916dc445-3b02-4209-a916-84c393f10a5a"
        ],
        "24ed398e-6cbb-4816-a0f4-8703f1967669": [
            "f1e1e70e-9fa8-4b15-9c04-396fa08c46ab"
        ],
        "bfad6992-f14a-4095-a1a2-cfea6c014b37": [
            "f1e1e70e-9fa8-4b15-9c04-396fa08c46ab"
        ],
        "f2752333-24b2-4073-ad3f-3cca618b8c9b": [
            "ffe4a32d-41df-4a59-a408-0017ae3dc174"
        ],
        "9723ec76-3487-4fa3-a69d-6ef24ce3f18b": [
            "ffe4a32d-41df-4a59-a408-0017ae3dc174"
        ],
        "2131edfc-d7a9-4360-9533-716acff9e3fe": [
            "9dd2dc04-53e1-4917-8cb3-7eba87c5db6f"
        ],
        "d3fe8c4f-dde2-4e86-8a96-08a414dea0b0": [
            "9dd2dc04-53e1-4917-8cb3-7eba87c5db6f"
        ],
        "7dd67a03-9220-40b6-9053-99f3e9b544b0": [
            "1e84fdc2-66f1-4b42-b2c6-4ba2019806c1"
        ],
        "14c1121e-f03f-43fb-892b-d240d0793130": [
            "1e84fdc2-66f1-4b42-b2c6-4ba2019806c1"
        ],
        "25b65994-bed2-4a8a-9a5b-b9bbe00fd1e1": [
            "e6ad817d-1e98-4ec1-aed8-998f8b4e537c"
        ],
        "f36e8bc4-57ca-41a9-9e02-3c9e8ea683be": [
            "e6ad817d-1e98-4ec1-aed8-998f8b4e537c"
        ],
        "2c179cf6-206c-42d5-9dab-15925f8f65fc": [
            "b38f4982-10a9-4cb7-8bfd-106e38a35526"
        ],
        "d8606758-f7fc-41a1-b1c9-a6912c5c7b8f": [
            "b38f4982-10a9-4cb7-8bfd-106e38a35526"
        ],
        "e8bf5f7a-5b60-48fd-a7fa-f7740fb4dee7": [
            "a6a60919-4ebd-4670-ba37-46352f81441a"
        ],
        "bffada8c-2151-4b7a-8c88-b34d3f927c36": [
            "a6a60919-4ebd-4670-ba37-46352f81441a"
        ],
        "6948e2ff-9934-419f-9fdf-7cdcc854106c": [
            "1f9fa402-ca18-4d0f-b766-2ec538973f0a"
        ],
        "b0af4d91-951d-4d8f-b6ee-2650823d877d": [
            "1f9fa402-ca18-4d0f-b766-2ec538973f0a"
        ],
        "d2eae39e-2058-4d0a-a26b-0f446a7543e7": [
            "8a55d086-e003-4699-9531-c3e67b5bdb7c"
        ],
        "582b6fd7-017b-4d6a-afad-469d18840d34": [
            "8a55d086-e003-4699-9531-c3e67b5bdb7c"
        ],
        "547f6b26-8b16-4acf-8651-1e4e0fb96887": [
            "3f7d8c75-4ed4-4749-b09c-4f3c91f7e5de"
        ],
        "9910a987-d3a8-447b-abd6-34ab35363f64": [
            "3f7d8c75-4ed4-4749-b09c-4f3c91f7e5de"
        ],
        "9b9a5eb1-802c-4788-9558-07e40ec73f9f": [
            "b5cd4494-8f31-4620-adaf-7966d59a6ed3"
        ],
        "7584bd23-c951-4e99-a6d1-dfb5525c5f4d": [
            "b5cd4494-8f31-4620-adaf-7966d59a6ed3"
        ],
        "10478766-f279-4a8f-bf7d-74c6d84e8c17": [
            "4e55c5b0-479d-43de-94b7-00dbc4416601"
        ],
        "c435ca8d-e7b9-4eba-97b1-43879764eff6": [
            "4e55c5b0-479d-43de-94b7-00dbc4416601"
        ],
        "b819f1c1-eebc-4518-8532-80c607575e56": [
            "2c6d9c3d-3737-4358-9507-ff69747df419"
        ],
        "a3cd2ff9-cdfc-420b-97ae-bad50ea6adac": [
            "2c6d9c3d-3737-4358-9507-ff69747df419"
        ],
        "5f0941f2-b8b0-4dfb-a589-0e67611e68ad": [
            "e6b81ec5-b07e-4492-a220-944e04f04d8a"
        ],
        "15cfe82e-c4fc-4aa5-a3e2-3aafbe1d746a": [
            "e6b81ec5-b07e-4492-a220-944e04f04d8a"
        ],
        "25a2dc05-74db-4852-9877-826c8c9514bf": [
            "211ca6ad-a540-48d8-b5b2-9dd1b5ca2a1b"
        ],
        "f85352f4-3a12-4a74-ad53-cd9b17572783": [
            "211ca6ad-a540-48d8-b5b2-9dd1b5ca2a1b"
        ],
        "58e3bb25-6d95-43c0-ac04-5934c44d5e90": [
            "0b7a2024-988e-4eaa-8d0e-835f6fa947f9"
        ],
        "123350c2-a44b-495f-b140-8a185b247d62": [
            "0b7a2024-988e-4eaa-8d0e-835f6fa947f9"
        ],
        "158f2777-4563-49da-a524-b011c8d8cee3": [
            "6b8688fb-9b5e-40e9-be63-f08fcbfe0a56"
        ],
        "d2e18868-c7ef-4bee-91a4-4ac223f9b8dc": [
            "6b8688fb-9b5e-40e9-be63-f08fcbfe0a56"
        ],
        "33357757-946b-444d-8727-12210d44bbc1": [
            "0293c79d-1be4-44fa-8d20-fb6286c53ef3"
        ],
        "4f576b0e-3e70-46be-a4d2-becdaebf9307": [
            "0293c79d-1be4-44fa-8d20-fb6286c53ef3"
        ],
        "56874aa4-a45a-4a8a-8bd4-52eec54dc313": [
            "b96dd3de-8cf6-4a5b-bd1f-608df312fba9"
        ],
        "a4e7e28c-158b-4650-8375-385106fbcd00": [
            "b96dd3de-8cf6-4a5b-bd1f-608df312fba9"
        ],
        "e48b6bd7-983a-431b-9005-802630a5d2f1": [
            "d4ab7704-e8dd-4e0f-b7dd-4faedd328453"
        ],
        "008ca114-c7f7-402a-96e8-b3cc22d4fa65": [
            "d4ab7704-e8dd-4e0f-b7dd-4faedd328453"
        ],
        "cfd1f96a-d079-490e-9e3e-e20c90d7f0e5": [
            "0d5db34f-1ec9-44bd-9c40-874d9e75c733"
        ],
        "15d7d6f3-c9be-4fbc-b995-eed3aadce732": [
            "0d5db34f-1ec9-44bd-9c40-874d9e75c733"
        ],
        "e14e4d57-a674-4414-977d-573fa129bec9": [
            "07a09ee8-a7da-443c-a3f4-3fc1e46fa3f1"
        ],
        "80309861-89d5-49d1-9024-8adedc85eb50": [
            "07a09ee8-a7da-443c-a3f4-3fc1e46fa3f1"
        ],
        "f828eaed-bc10-4547-9dfb-1f8386141c89": [
            "597209cf-c3a5-49e7-925f-d524e2dadea5"
        ],
        "aff9d775-117f-4909-948a-7cb4093ed122": [
            "597209cf-c3a5-49e7-925f-d524e2dadea5"
        ],
        "f3f52cbb-27ee-4cf9-b9bd-b437360d1ada": [
            "6443ef6d-9d25-4b19-84d3-a1cb86b5602e"
        ],
        "25ff8a11-73d6-43b6-a7d9-b032acbe39c1": [
            "6443ef6d-9d25-4b19-84d3-a1cb86b5602e"
        ],
        "aa6f93d8-3dd4-4d01-98a0-526be6d8d156": [
            "621d0f98-bba8-41da-8d64-ecdcef988fd0"
        ],
        "62fccb36-85d9-4c08-8639-3c7bc5f7d846": [
            "621d0f98-bba8-41da-8d64-ecdcef988fd0"
        ],
        "4d25a475-8a9d-4dfb-86a8-52fd8db06786": [
            "5ad1a5c2-72de-437d-b91b-71958cd48683"
        ],
        "cc34e90d-0e73-411b-8571-26c435644179": [
            "5ad1a5c2-72de-437d-b91b-71958cd48683"
        ],
        "bcef19c2-b119-4372-8002-0ebe2a07570a": [
            "41221b7b-92a8-44ee-a15e-32a5ccda10fa"
        ],
        "75e316de-6ae4-4c0b-b41d-fb30f6245572": [
            "41221b7b-92a8-44ee-a15e-32a5ccda10fa"
        ],
        "a318d046-b67b-4cbc-8ff6-438705bfb886": [
            "7073ad07-8b70-4718-a58e-4b64a51332b5"
        ],
        "53573885-cafd-4886-a422-dac4999afe80": [
            "7073ad07-8b70-4718-a58e-4b64a51332b5"
        ],
        "6f8b26dc-60d5-4924-af86-55ac13d23156": [
            "cff143a9-feca-4571-8946-b33139e7d4c0"
        ],
        "b27689be-a7d9-4e39-bd20-1e4f98581045": [
            "cff143a9-feca-4571-8946-b33139e7d4c0"
        ],
        "b9234162-c37e-46b6-a12d-079a5f6dc9b3": [
            "dbafc8ad-1bfa-43f0-8e30-2f6407fbfb6e"
        ],
        "ad7ac415-ddf7-4169-abcc-5eea9ae5a56d": [
            "dbafc8ad-1bfa-43f0-8e30-2f6407fbfb6e"
        ],
        "2cbc67c3-bb4c-4d49-b9c7-11b4785be8b3": [
            "544c4fac-79ec-400f-ac7f-96ff44575348"
        ],
        "a2daf3dc-6a61-4e8c-9d06-9ca829aa45d9": [
            "544c4fac-79ec-400f-ac7f-96ff44575348"
        ],
        "093b259e-20df-4a57-99ae-c75bdeb2e18c": [
            "d9bce74f-cad1-496e-817f-acb36b4cc686"
        ],
        "1c9f58c9-511e-4a04-a232-96590064d38f": [
            "d9bce74f-cad1-496e-817f-acb36b4cc686"
        ],
        "b98c283d-b4f9-41c7-9108-ce8ce8036bc8": [
            "0d708795-94d6-44d4-b46d-a6d2ef4d2c93"
        ],
        "749f9afc-a699-4f18-a69a-9dacb219532b": [
            "0d708795-94d6-44d4-b46d-a6d2ef4d2c93"
        ],
        "6ed79d95-37b9-4805-8851-6d6289faba46": [
            "c0ab2f5e-db21-4d16-ae68-7a801a032a5f"
        ],
        "d1423b7d-4a37-4620-9c5f-2999d4a4a589": [
            "c0ab2f5e-db21-4d16-ae68-7a801a032a5f"
        ],
        "484f5c94-f32a-46c5-8cc7-94d65c624810": [
            "323ab9ce-1be1-4e8d-b721-3156e45b9f6a"
        ],
        "73c6c301-8402-4ba7-b3ca-8f5bc6f60d4f": [
            "323ab9ce-1be1-4e8d-b721-3156e45b9f6a"
        ],
        "d1d0790e-f014-4e12-aa8c-c61ce891b30e": [
            "32b37005-53a7-4d77-88d6-46567fdcd253"
        ],
        "59933338-65bb-408a-9772-bce5bd50f99a": [
            "32b37005-53a7-4d77-88d6-46567fdcd253"
        ],
        "cfa9d316-859d-44d3-ad50-8086c1818560": [
            "e333b2bf-3af4-41bd-b3bc-166fc5fd7416"
        ],
        "fbc20381-0137-4b9b-bdfb-f2d1a6f8b042": [
            "e333b2bf-3af4-41bd-b3bc-166fc5fd7416"
        ],
        "959a637c-7f19-4d72-bbb1-621821f4fb61": [
            "36962995-475f-41a6-896a-530da34b1c90"
        ],
        "b53a13a5-6165-43ae-9ca8-214fff743b50": [
            "36962995-475f-41a6-896a-530da34b1c90"
        ],
        "4e17fe70-e070-457c-828c-7b39c763f2f1": [
            "0bd68217-ea71-44bc-92e0-15d76175c895"
        ],
        "e82e763a-377d-43b5-b9f1-4443d2b153eb": [
            "0bd68217-ea71-44bc-92e0-15d76175c895"
        ],
        "7d26dbf1-e3f5-47bd-89b6-e1fc10e1ec1a": [
            "8d0c7951-564c-41a0-99be-584419cee5d2"
        ],
        "2b06b379-3527-43e0-8fb7-c9ff651c761b": [
            "8d0c7951-564c-41a0-99be-584419cee5d2"
        ],
        "d711cb65-65c2-4f5e-9870-dab1820ac069": [
            "d9f26b0a-696a-4e4a-8eed-16b6451da4d6"
        ],
        "3ee449ce-71c6-4305-ad4a-3c4e3ef1322c": [
            "d9f26b0a-696a-4e4a-8eed-16b6451da4d6"
        ],
        "b8a06e25-1a22-417d-b0ac-da4fcdc82d82": [
            "e759ace3-4693-4c3a-80cc-04c0601d332a"
        ],
        "c510c00f-d10b-4d89-8ccd-b1a15d67cebe": [
            "e759ace3-4693-4c3a-80cc-04c0601d332a"
        ],
        "501d9a60-db33-4249-8275-3597ca378646": [
            "a3d4eb39-0911-41c3-b29f-a319166ed4a8"
        ],
        "8f41fbed-1e61-4a92-9584-b6f28a4db1b6": [
            "a3d4eb39-0911-41c3-b29f-a319166ed4a8"
        ],
        "b945d7ea-8de5-4e3a-9102-bbd3701fdb10": [
            "6e725b12-26d4-4b3a-a1a8-55178580b0f6"
        ],
        "950f784f-4849-42c4-871d-9452ce4d338c": [
            "6e725b12-26d4-4b3a-a1a8-55178580b0f6"
        ],
        "e765c8a1-bb94-449e-acfc-68dd1d170449": [
            "30965e53-3637-43e7-8cb3-340cc5f5e291"
        ],
        "5e3606dd-8c65-4f12-89c7-ce665c06bbba": [
            "30965e53-3637-43e7-8cb3-340cc5f5e291"
        ],
        "6b980f00-fd4b-42ea-ba57-d0a5b19dd500": [
            "5c5f60a7-01cd-4d42-8932-d67bc5abc093"
        ],
        "3709b522-efdf-4774-bdcc-ca84dc9827a7": [
            "5c5f60a7-01cd-4d42-8932-d67bc5abc093"
        ],
        "672b37eb-bd2b-4da2-a25b-c3e1eda714b3": [
            "be68a2c4-c7e3-443b-ada0-e2eb65ab3b01"
        ],
        "a6a238fd-a80a-4e09-a64a-90cc1e763ef2": [
            "be68a2c4-c7e3-443b-ada0-e2eb65ab3b01"
        ],
        "3ed57b96-de1f-4b3c-8181-5b912275f39d": [
            "3d6d61f9-ce0b-47a1-b3bd-cd3be6fb3645"
        ],
        "026ab58f-b79b-47b2-9852-5e166abcc382": [
            "3d6d61f9-ce0b-47a1-b3bd-cd3be6fb3645"
        ],
        "c41a2ab8-4286-4a2c-902b-4939c9cc205c": [
            "225925f8-3e64-4ad5-aa09-e0b42c15e914"
        ],
        "a05a6984-e9d9-4429-97d6-fdaf919c4ecd": [
            "225925f8-3e64-4ad5-aa09-e0b42c15e914"
        ],
        "e60285f7-a071-4292-9e1e-2b3d9ce58f46": [
            "8a6ea7a9-e848-4937-9f46-d264130a353e"
        ],
        "c3774433-6852-4f64-9367-22e3f5b82384": [
            "8a6ea7a9-e848-4937-9f46-d264130a353e"
        ],
        "50ad8095-4f67-4de3-bd6f-1031f601142b": [
            "8ad96b04-0966-49cf-a35c-a507a81266cd"
        ],
        "1fc762ca-6dfa-4307-ad60-ae327dafde85": [
            "8ad96b04-0966-49cf-a35c-a507a81266cd"
        ],
        "fcae82c8-a7af-41ca-9f49-b3f5d9fa4c17": [
            "2294ed07-104c-45f5-8a84-1d48c6e0cbbd"
        ],
        "86bf1cb9-ef11-4de2-abe6-56df1e8f6a15": [
            "2294ed07-104c-45f5-8a84-1d48c6e0cbbd"
        ],
        "fb8c75a0-bbbc-4a9d-a69e-3fedfb0ea42c": [
            "bcc60c88-1cff-4577-bc57-09c27cd7d135"
        ],
        "3ebe147e-f1a4-4e61-9577-30348a3aaf94": [
            "bcc60c88-1cff-4577-bc57-09c27cd7d135"
        ],
        "a4242024-89e0-4444-aa25-ed0eaac73d54": [
            "19aa5bd2-b4cc-4033-987a-9b1e8d743b84"
        ],
        "74d46fdc-684d-4baa-a884-abf57d222cbd": [
            "19aa5bd2-b4cc-4033-987a-9b1e8d743b84"
        ],
        "454743cc-c7a9-4ee8-bd99-249f660b7814": [
            "63bc8c5f-d9f1-485d-ac76-988464cc0b35"
        ],
        "5e67b280-06c5-45fa-8a8f-612c2f6b3950": [
            "63bc8c5f-d9f1-485d-ac76-988464cc0b35"
        ],
        "e687d470-acd4-451a-98b2-50f8f81bee62": [
            "c519d560-697d-4971-ae9c-c59369872079"
        ],
        "64976f01-19e2-49ae-bf02-bb43ccf036d0": [
            "c519d560-697d-4971-ae9c-c59369872079"
        ],
        "f1dfad32-4278-44f6-b68c-fdad86680801": [
            "798c3020-1b01-4bdb-b502-ef84f1ff618e"
        ],
        "1a2e43ab-d425-4385-85e0-d07381878090": [
            "798c3020-1b01-4bdb-b502-ef84f1ff618e"
        ],
        "f4d70abd-5870-46c4-ad35-a165e40e2f9a": [
            "9796bcb7-3a99-4771-82cb-1a67497b6384"
        ],
        "c3b31295-1cf4-4f2e-87b4-afc87228d077": [
            "9796bcb7-3a99-4771-82cb-1a67497b6384"
        ],
        "26d8f959-2aac-4423-a104-6f250de97a17": [
            "16f97a5d-4b8b-459e-8525-9958dc10c15e"
        ],
        "3c6e651f-50a9-45ed-b0eb-5991532c11d5": [
            "16f97a5d-4b8b-459e-8525-9958dc10c15e"
        ],
        "15d93156-a811-4514-a081-b2efa2222436": [
            "8111086b-8694-4635-a618-9e68926b795c"
        ],
        "128eb153-d5ba-43b2-aaed-681a6350927b": [
            "8111086b-8694-4635-a618-9e68926b795c"
        ],
        "79eab579-ca20-4b77-90e4-94724734fc42": [
            "1beaf01d-2e8e-4e44-a009-1d998fe7daaf"
        ],
        "a65017bf-7c5f-44a8-b47d-847faf7f665c": [
            "1beaf01d-2e8e-4e44-a009-1d998fe7daaf"
        ],
        "860890f6-7cb9-4fd9-b63c-c64663e3c8ef": [
            "6bf2074b-8b5b-45d4-994f-4f1fa3e663f8"
        ],
        "f025156e-f064-4c36-b6ac-0240b0b09972": [
            "6bf2074b-8b5b-45d4-994f-4f1fa3e663f8"
        ],
        "9e93e93f-0238-4f52-81a6-7ec1f335ba4c": [
            "9404d4bb-7a55-4eb8-8357-3bfd2175fe27"
        ],
        "ba07280f-12b8-4549-9aae-c113e240d587": [
            "9404d4bb-7a55-4eb8-8357-3bfd2175fe27"
        ],
        "a1868ffc-c43c-44c2-b522-a08e578c11a1": [
            "fdb359ed-68c8-4a18-9851-7d0189817c8f"
        ],
        "7e7452d1-fe1f-498b-8d35-71954979eca6": [
            "fdb359ed-68c8-4a18-9851-7d0189817c8f"
        ],
        "02c348ee-0958-4cbd-8776-c777bea768c1": [
            "0390e871-2d3f-4d2b-8bbb-0aca8d235c19"
        ],
        "8bbf6af9-df62-469e-8ac6-5c5770129d6e": [
            "0390e871-2d3f-4d2b-8bbb-0aca8d235c19"
        ],
        "449b0c37-0c54-48b5-b3cd-2881ff80f13b": [
            "dd191a90-579a-43f1-8f1d-c91ddf116d20"
        ],
        "a9d6a794-b0c8-48ed-ab19-0df9f9268c18": [
            "dd191a90-579a-43f1-8f1d-c91ddf116d20"
        ],
        "fa8d3fd1-fa51-4ee4-aab2-e70304cb003c": [
            "669a77b3-8b0a-449c-aae8-1479b5012e80"
        ],
        "2daf030c-446f-478b-a24b-cb5173d9c4d2": [
            "669a77b3-8b0a-449c-aae8-1479b5012e80"
        ],
        "c5a80d2b-690e-4084-b6bb-1e3c441aeed9": [
            "8013c582-5358-4413-afce-7587a60ecd41"
        ],
        "cd2c461b-509e-4b74-bb0d-e2e5d783a998": [
            "8013c582-5358-4413-afce-7587a60ecd41"
        ],
        "1bcc64e7-2510-4444-afe9-96ef00c322bc": [
            "d4c55b28-26ec-43c2-ace0-c26071a28074"
        ],
        "15645701-007c-4d25-a7db-5ccdfbab886e": [
            "d4c55b28-26ec-43c2-ace0-c26071a28074"
        ],
        "7a65733e-d267-4770-acba-b76f9436e051": [
            "761303c1-25be-406f-88b9-2f970221245c"
        ],
        "bb9ad3d0-a8fc-46cb-acfd-53a91b3f5894": [
            "761303c1-25be-406f-88b9-2f970221245c"
        ],
        "0c370b8f-62be-404f-a732-bfe963abd084": [
            "2c2f71a3-2bbf-4775-9692-4261254f128b"
        ],
        "fedabece-9125-4110-ad87-3ccc7e91c724": [
            "2c2f71a3-2bbf-4775-9692-4261254f128b"
        ],
        "9f5a6866-a4a9-491d-916b-4c56978d581c": [
            "b0b46e4b-0d82-49d0-a3fe-d8286753752b"
        ],
        "fc2b570f-38ab-4d16-ae34-f55e0d2a62c7": [
            "b0b46e4b-0d82-49d0-a3fe-d8286753752b"
        ],
        "7c104137-9025-4458-8915-6beb1914fe4c": [
            "1672493a-6b4b-40dd-9d68-33b3d8aa2ea6"
        ],
        "e7c7eee6-3d1e-4c08-8664-b7c00c09a7c8": [
            "1672493a-6b4b-40dd-9d68-33b3d8aa2ea6"
        ],
        "76fde596-daff-4af0-b4cd-e8a8eaa32714": [
            "89b6e383-49e7-4cd0-9540-369229faa093"
        ],
        "358a3261-d9a6-407d-907f-68d0a82ccd01": [
            "89b6e383-49e7-4cd0-9540-369229faa093"
        ],
        "ebd58645-3ed7-4723-a884-caf7f42cc243": [
            "0daef4e3-5d16-47ef-87c4-538edc596727"
        ],
        "530e595b-7c0b-4b2e-8c33-fb595f8636f1": [
            "0daef4e3-5d16-47ef-87c4-538edc596727"
        ],
        "67b60a64-129c-4350-b4f2-a3deaac7dbf1": [
            "31e75b09-4aa2-48a2-8d46-e7ad07063b6d"
        ],
        "6ffd4ed5-6a75-4c46-a784-17f25424362d": [
            "31e75b09-4aa2-48a2-8d46-e7ad07063b6d"
        ],
        "da2a62fa-79be-4fff-bf72-94615dc20f00": [
            "a5fa01b4-aa4d-410b-b560-77b00a45cb36"
        ],
        "c0178c53-4cd3-4ee1-ac65-08afe11a2a23": [
            "a5fa01b4-aa4d-410b-b560-77b00a45cb36"
        ],
        "2ee93810-34c1-4640-89fd-86e0330cc290": [
            "a25f6450-c11c-4a5d-89cb-cb860db8f93f"
        ],
        "e73ac179-c823-40ef-a202-e587787a451f": [
            "a25f6450-c11c-4a5d-89cb-cb860db8f93f"
        ],
        "a8744cef-5764-4e1b-a59a-e822f15eb117": [
            "e184e4eb-9291-43d0-a332-84decf599d83"
        ],
        "0c95fc24-ddbd-4719-a63f-c3957aceb000": [
            "e184e4eb-9291-43d0-a332-84decf599d83"
        ],
        "2d40fb65-befa-4c9c-9b7c-207db7ea3c14": [
            "1f668b2f-1918-4f06-9d94-c66ce02bc541"
        ],
        "b503012d-a601-4729-b244-84589df1403a": [
            "1f668b2f-1918-4f06-9d94-c66ce02bc541"
        ],
        "898bc91e-48f5-4162-9db5-eb71c15a307e": [
            "a8a78463-0dc6-4ee4-acd4-cd225089e6d7"
        ],
        "44001874-bd06-47d4-a183-63a8cabbfe46": [
            "a8a78463-0dc6-4ee4-acd4-cd225089e6d7"
        ],
        "892e4378-02cf-4c04-a74a-45721203a139": [
            "679a2f8b-2959-4460-b98a-6eff8c05f3ce"
        ],
        "d17b954d-c414-46cd-b55d-fabadc617afa": [
            "679a2f8b-2959-4460-b98a-6eff8c05f3ce"
        ],
        "31f5a29c-c24a-45b7-80be-1e4cef6f557f": [
            "5f2406f4-1f72-486b-a28e-c4d4317945c4"
        ],
        "9191567b-f892-456a-95e9-5f031bf5f1ef": [
            "5f2406f4-1f72-486b-a28e-c4d4317945c4"
        ],
        "d86f2134-8b7f-41a5-b680-766c5e8468f6": [
            "1b9da0d0-21bc-4534-ac8f-781bd934b987"
        ],
        "f63b930c-1e53-424b-a65e-5112c6a8e8f6": [
            "1b9da0d0-21bc-4534-ac8f-781bd934b987"
        ],
        "87120f6e-342c-44d7-8718-c56c329a0f74": [
            "79c78929-a746-45ce-a00b-06447fdd9a1b"
        ],
        "0920267a-ca66-4933-99e2-2542703cfc2a": [
            "79c78929-a746-45ce-a00b-06447fdd9a1b"
        ],
        "1d9ac16b-3f50-43b1-b871-174d074774d0": [
            "6aa34b6e-7f4e-4f1c-9995-65c16c6d477f"
        ],
        "2fb74fbe-f461-4592-a8c8-124c4b01c968": [
            "6aa34b6e-7f4e-4f1c-9995-65c16c6d477f"
        ],
        "cf802a33-5bd7-44a9-a268-0b30a83e4381": [
            "1c08df56-35cb-4de3-9c68-515abd7b98b1"
        ],
        "22b78bb5-9a64-4dda-9002-21dca7f32bf6": [
            "1c08df56-35cb-4de3-9c68-515abd7b98b1"
        ],
        "0d2bd6f8-856e-4284-a252-102687a6a24e": [
            "4be28801-8564-4016-8008-b3936adbd973"
        ],
        "68ed16a4-9953-4a6f-9f04-d4af6a3499e9": [
            "4be28801-8564-4016-8008-b3936adbd973"
        ],
        "7864e08c-1145-462a-8984-4b9384f5e199": [
            "1ef7f3da-2cd4-4c13-9216-7891cb51b499"
        ],
        "bd139ccd-ab91-4e97-83aa-b23aea86a6a8": [
            "1ef7f3da-2cd4-4c13-9216-7891cb51b499"
        ],
        "68356234-5145-4231-ba0e-4e53f5b289a2": [
            "cabebb15-71ce-45f8-a9ec-9e1be3a28914"
        ],
        "d188b9fb-7486-4696-b9b4-a2032f6ec95e": [
            "cabebb15-71ce-45f8-a9ec-9e1be3a28914"
        ],
        "bd2510df-6020-4cca-8a97-6e500569273b": [
            "018123df-5ab6-405b-aaaa-ec555e19fe6a"
        ],
        "42628921-8608-4959-a23c-b7ccaecd395e": [
            "018123df-5ab6-405b-aaaa-ec555e19fe6a"
        ],
        "7ba8ee55-35f5-43fc-b3f6-5e37daed3c0f": [
            "b7881248-db52-4e78-b632-fab4f03e1e23"
        ],
        "b2750dce-eb1c-4746-aea6-4ed4731730b3": [
            "b7881248-db52-4e78-b632-fab4f03e1e23"
        ],
        "7501f44c-4e9d-442d-be42-90847d6e393d": [
            "dbd587a3-38db-4ed3-be2f-394422a85983"
        ],
        "50dd1a84-790d-4128-90f0-bf11b7f2deec": [
            "dbd587a3-38db-4ed3-be2f-394422a85983"
        ],
        "80fbe5cb-16d2-46e5-90c0-cf3f9880589d": [
            "1aaad332-acb1-4cd6-ab07-1dd98e4d41b0"
        ],
        "5c2601cd-c122-4bec-83ad-c426925d8174": [
            "1aaad332-acb1-4cd6-ab07-1dd98e4d41b0"
        ],
        "aa693da4-448e-4794-8056-4178e73cfe4c": [
            "170c8ba7-066c-4088-9b5e-19d94a2bbe6d"
        ],
        "bcea2274-2dcb-4c85-b005-e4bbd861f9b1": [
            "170c8ba7-066c-4088-9b5e-19d94a2bbe6d"
        ],
        "39d1504b-d469-44ac-974a-9a7d814fa2a9": [
            "8c60bc1d-d4be-4a53-be8c-97fade97efe6"
        ],
        "b22d9539-55f1-4b4a-8a55-f083d5901ce8": [
            "8c60bc1d-d4be-4a53-be8c-97fade97efe6"
        ],
        "9ab2e647-a3dd-4dd7-8f1c-382ead02343c": [
            "b7dfb680-e3d5-401b-8fc1-4ec675719155"
        ],
        "bc936965-478d-4fc1-a9d9-975c468e286f": [
            "b7dfb680-e3d5-401b-8fc1-4ec675719155"
        ],
        "5cc2e4c9-e508-4369-845a-58f95b3e0afd": [
            "89d2e1f2-1124-41f7-8a93-f01a81c9d93d"
        ],
        "abedf745-bd0e-4929-a77f-7f0153163c3d": [
            "89d2e1f2-1124-41f7-8a93-f01a81c9d93d"
        ],
        "a8234754-54c4-477e-9a20-182b6a2ab359": [
            "af749cc2-cf6e-4e4a-b458-3266fd983fad"
        ],
        "84fee029-9f07-4d53-93e6-af3432f88b0e": [
            "af749cc2-cf6e-4e4a-b458-3266fd983fad"
        ],
        "7ddd1499-6e84-4376-8ae7-3ebfaa8960cb": [
            "914a4ec8-afe4-476c-ba8d-2993a1c23ca2"
        ],
        "6b7cd185-eda1-43e2-a9db-14fcee549125": [
            "914a4ec8-afe4-476c-ba8d-2993a1c23ca2"
        ],
        "3e178ed1-7f26-4cd5-bc9c-6ef07c0a844a": [
            "ccdf7fbb-defc-4a96-a396-e453bc674e49"
        ],
        "7701b686-1461-46e7-9e79-25f8d286c8e1": [
            "ccdf7fbb-defc-4a96-a396-e453bc674e49"
        ],
        "d766111b-df30-4dab-b8c1-df0864e8aa8d": [
            "18b2cbca-41b0-4f77-9ce7-12020f4b6d71"
        ],
        "7d1a7f2e-28bb-43d9-b14c-c5fd79b53761": [
            "18b2cbca-41b0-4f77-9ce7-12020f4b6d71"
        ],
        "266b3841-f471-49f4-b26c-d8691bcd7215": [
            "81f733f7-5a70-49e3-825e-8f0ee27d4ddc"
        ],
        "35bf41c0-031b-4888-bd6c-275413f47641": [
            "81f733f7-5a70-49e3-825e-8f0ee27d4ddc"
        ],
        "b9ab834e-a34d-4ade-bdf8-267af3e71a80": [
            "e98f912d-8917-4178-bcaa-866872e7ea9c"
        ],
        "7fb0e828-f51a-461d-af47-fd64e31d7e76": [
            "e98f912d-8917-4178-bcaa-866872e7ea9c"
        ],
        "e9868d42-2191-478a-b4ff-7f2a7356c212": [
            "44520ea3-038a-459f-ac18-e5d21fcc2165"
        ],
        "3625e480-bfa1-4561-abd0-11a03520467d": [
            "44520ea3-038a-459f-ac18-e5d21fcc2165"
        ],
        "f251605a-fa5d-4070-9b50-61eb044c234c": [
            "9c8527a4-db5b-4289-8526-37e1c425b92b"
        ],
        "e0c4695e-d235-4ad1-9da1-aab08539ff8e": [
            "9c8527a4-db5b-4289-8526-37e1c425b92b"
        ],
        "dbda5546-feb9-4da7-96cc-901c69261c5a": [
            "db85e443-70d5-43e6-b393-f78383166dae"
        ],
        "a65916b0-9482-47ec-85e8-3fe3e880b01f": [
            "db85e443-70d5-43e6-b393-f78383166dae"
        ],
        "c30f6fd8-569f-43e7-9a84-10c6aa0b9f9d": [
            "dae3ee7f-79bc-4c7f-804f-0fc07e4f33fd"
        ],
        "92679f2d-85c0-436d-8120-ff7cce476095": [
            "dae3ee7f-79bc-4c7f-804f-0fc07e4f33fd"
        ],
        "14ee20c9-3a7f-4399-a576-db98a2e3999a": [
            "91957f63-5880-41c6-85f9-869929214866"
        ],
        "aac9324a-a144-4818-9d62-a83346a3fdc8": [
            "91957f63-5880-41c6-85f9-869929214866"
        ],
        "138d2219-d608-4d4a-9446-d0d293755ad7": [
            "e2f5d2a2-d572-4dc6-891f-1d002c40ec40"
        ],
        "674c3e17-37ab-4e34-b80d-0c5c9923de93": [
            "e2f5d2a2-d572-4dc6-891f-1d002c40ec40"
        ],
        "a9101a89-e7d4-4de0-bef0-de11f2bec0b4": [
            "f19d0bd4-9d3d-4c04-bca5-93912ba7a21b"
        ],
        "d20151e8-fab4-403e-a1a4-01fdd5e5d9a9": [
            "f19d0bd4-9d3d-4c04-bca5-93912ba7a21b"
        ],
        "05d3cd8a-1e2a-43c6-9ae0-2f5d86ebf8fd": [
            "03cc1e64-e740-45c5-bc74-bc7f01b6dc1c"
        ],
        "b7fb07d6-cbd6-4675-8549-c8a83b972540": [
            "03cc1e64-e740-45c5-bc74-bc7f01b6dc1c"
        ],
        "4762c26d-6dcd-4cbd-915c-2d4a52dba143": [
            "4754ac48-148b-42a5-8b67-656b3c24deae"
        ],
        "903872fe-d298-4256-ad56-81d2af364f39": [
            "4754ac48-148b-42a5-8b67-656b3c24deae"
        ],
        "32615a14-5208-40aa-b8dc-3757115e278c": [
            "cb640bf8-a1a1-485b-bf32-d1b641d1617d"
        ],
        "58750c26-771d-4aa3-aaaf-5a9e46fa1031": [
            "cb640bf8-a1a1-485b-bf32-d1b641d1617d"
        ],
        "3cc1cdaa-36bd-4ed6-b65e-811e77e4b94e": [
            "6ca4b8e0-2ed6-4ea4-b717-2dbb840f6bc2"
        ],
        "ef73d320-18bc-41e4-ae5d-6e6a8733343e": [
            "6ca4b8e0-2ed6-4ea4-b717-2dbb840f6bc2"
        ],
        "d9b4ae8a-b115-44ae-91b6-a3ba3bb3920e": [
            "3df6dc88-1b97-4d93-9938-149eeb269c50"
        ],
        "3aaa71fe-64cb-458e-a887-db7ca41ea046": [
            "3df6dc88-1b97-4d93-9938-149eeb269c50"
        ],
        "7809803c-6524-4959-b737-32194ae9d50b": [
            "b9369b4f-9853-4de9-b47b-5457f0e7949d"
        ],
        "1132fb93-6a20-4be3-9da3-a28af2314359": [
            "b9369b4f-9853-4de9-b47b-5457f0e7949d"
        ],
        "2b22e832-1611-4601-9268-691e17c1b50b": [
            "ed407016-a7bd-4b9a-8af5-3aff7627e30e"
        ],
        "a94540ca-367e-468b-ac17-63a9d81a010e": [
            "ed407016-a7bd-4b9a-8af5-3aff7627e30e"
        ],
        "dbe32245-4aac-4801-a920-04e008e8d307": [
            "809336da-5320-43af-9aa0-0b739d927e46"
        ],
        "82f8dba0-c848-485e-9703-908d96ccb728": [
            "809336da-5320-43af-9aa0-0b739d927e46"
        ],
        "cfbf042c-c64c-432a-8b5f-17911a1385ee": [
            "649a029e-6f7a-44ad-a632-410015ab0d48"
        ],
        "e8e7b709-edf2-4290-8217-7b5e03fd3db5": [
            "649a029e-6f7a-44ad-a632-410015ab0d48"
        ],
        "d6707ca2-702e-4e2c-bb18-6fe161d39778": [
            "5687f7a2-72c7-49d8-9160-31fd086f97b3"
        ],
        "2c466fbc-b702-4142-9c95-5371778a1cae": [
            "5687f7a2-72c7-49d8-9160-31fd086f97b3"
        ],
        "0fcfdec2-7797-4695-8b76-b0e50231eca1": [
            "93d689c4-c61e-47ce-be1c-6bb5851a0ba0"
        ],
        "90342b19-b259-456d-a706-f805051671a2": [
            "93d689c4-c61e-47ce-be1c-6bb5851a0ba0"
        ],
        "d4e6c7bf-47f7-46dc-8d7b-3c5050bec646": [
            "88ec0fc9-c3a9-4ea5-9588-f83e27b31b48"
        ],
        "1f295612-1535-4512-a87f-c8db5e5f200e": [
            "88ec0fc9-c3a9-4ea5-9588-f83e27b31b48"
        ],
        "b914be89-37e8-441b-b4cd-ffb722a179e2": [
            "795b0576-0a0b-4282-bc18-4d62f35426b2"
        ],
        "f906642b-ce1e-447e-8e2c-124ded5b444f": [
            "795b0576-0a0b-4282-bc18-4d62f35426b2"
        ],
        "9db13d44-c205-46da-8757-7e129b93b415": [
            "8dce7131-1f95-4e5e-9385-af2aeb409416"
        ],
        "cef091d8-ab0d-4eca-b324-0dd5df44fcab": [
            "8dce7131-1f95-4e5e-9385-af2aeb409416"
        ],
        "76df1522-495c-41c2-8c34-97628102cf17": [
            "508976e9-0496-4b76-a1dc-e78a178b8e58"
        ],
        "beedece7-bfff-4ff6-a334-40c725829f9f": [
            "508976e9-0496-4b76-a1dc-e78a178b8e58"
        ],
        "7220a2f5-0290-432f-aa7f-58c67cca155d": [
            "189f9b6b-7c34-4bf1-ba55-c715a63ce693"
        ],
        "663fd9a2-c034-419f-a9a7-fab41d15784b": [
            "189f9b6b-7c34-4bf1-ba55-c715a63ce693"
        ],
        "41133952-c3bf-4b34-b9d1-0758485f8c61": [
            "b5ba4e17-096b-4293-8c9b-00dc10862ef7"
        ],
        "f2f7daf3-ebec-4ad5-b9f1-cbe942d39ff9": [
            "b5ba4e17-096b-4293-8c9b-00dc10862ef7"
        ],
        "3d0c7d1f-b56f-4aef-a9d5-45eadade157f": [
            "3abd8b18-4464-4810-b7ff-195f2a7dcc23"
        ],
        "39c90a70-86e5-45b2-83df-4a82c0c9f69c": [
            "3abd8b18-4464-4810-b7ff-195f2a7dcc23"
        ],
        "44ca536f-54db-4336-b661-b1865165e66c": [
            "913d7e1c-2e19-4bc7-9d21-6f0a918d9009"
        ],
        "538263d6-60c2-4af1-88af-fed019ea3a2e": [
            "913d7e1c-2e19-4bc7-9d21-6f0a918d9009"
        ],
        "b8392dd6-2390-4cb4-a012-30a7fd115c31": [
            "cfff2ff5-f5d9-4293-ab06-a1202a5c7a74"
        ],
        "e9b0d3db-5a6d-486c-b37c-6ce9877f9462": [
            "cfff2ff5-f5d9-4293-ab06-a1202a5c7a74"
        ],
        "85a659e6-66db-4091-a72f-ff29f043e973": [
            "bfa94d24-6002-434e-b352-bbed75f68335"
        ],
        "d8dab93c-9402-40ec-ab45-c72f95f360b4": [
            "bfa94d24-6002-434e-b352-bbed75f68335"
        ],
        "d80c88d5-2225-4847-a01c-8fed2081ff7e": [
            "b6f377d1-bdb6-4fce-ac93-ecadebc5c45c"
        ],
        "88da89da-b7ed-432c-876b-f81d2f828a79": [
            "b6f377d1-bdb6-4fce-ac93-ecadebc5c45c"
        ],
        "0bd852bc-8229-4854-914e-181c60c3723a": [
            "e7c085ab-d524-470e-bb55-cd45672979f3"
        ],
        "c58a670d-f2bb-4121-8e6a-5bc7ff2dfb30": [
            "e7c085ab-d524-470e-bb55-cd45672979f3"
        ],
        "956ec280-e7f7-4e53-9a20-03bf24460616": [
            "d31e5dc6-3bac-4d59-a875-e5608f9f5262"
        ],
        "18a5f683-f4d9-4d31-8ebe-6f133e41a6f0": [
            "d31e5dc6-3bac-4d59-a875-e5608f9f5262"
        ],
        "35e99d72-ec7e-4a68-b78c-b05c9d23158f": [
            "2a67b9a5-bc48-4b16-ba90-01a2beacc34f"
        ],
        "f48fe1d0-c9b5-4b0f-8dae-cc1359239ad1": [
            "2a67b9a5-bc48-4b16-ba90-01a2beacc34f"
        ],
        "0e632782-d822-49e4-8d4c-e0c63515c42d": [
            "b3219cc8-cd1a-4b3f-ae0b-3c2f105e2fb0"
        ],
        "160a5506-37c7-4d5e-bff5-7d6ad9bb362c": [
            "b3219cc8-cd1a-4b3f-ae0b-3c2f105e2fb0"
        ],
        "5991bc6f-85c5-4991-923e-27e329c9de7a": [
            "00994c7b-19d5-4b15-8ad9-7dfff15ffbdc"
        ],
        "5719b65e-aa55-43d4-a716-fbdc12040095": [
            "00994c7b-19d5-4b15-8ad9-7dfff15ffbdc"
        ]
    },
    "mode": "text"
}