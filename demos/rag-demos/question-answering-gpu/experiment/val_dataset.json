{
    "queries": {
        "d1380f1e-342e-4b7c-a606-446bbe5186ce": "What are the key components of a model's metadata?",
        "86d73b0a-4409-4292-90ee-bdb979d7cb8c": "Can you provide an example of a model version within the given JSON representation?",
        "a3e19e47-b776-4d5a-83e8-50f817b0889f": "How can the Job Queue be accessed and modified in the WebUI?",
        "87660fa6-3bec-4009-b517-393901130bc9": "What is the purpose of viewing and modifying the Job Queue in the WebUI?",
        "570b19f6-f7a8-4aa7-b71a-4aa3b3390306": "How does the allocation of resources to tasks vary based on their requirements and weights?",
        "7b65ebef-6772-47eb-8382-2ed29e9985bc": "Can you explain the concept of proportional allocation of resources to tasks in the given context?",
        "19778f85-32f0-4bd0-a833-ae9b0f8ac437": "How does enabling the Cloud Filestore API benefit a GCP project?",
        "008531ca-08c8-4b86-be6b-c3b8b3332dc5": "What is the role of the Cloud Resource Manager API in a GCP project?",
        "b4221bb6-0d15-48e4-93e8-6f535e71c5e1": "How can you display a searchable list of Keycloak users in the Data Fabric UI? What information does this list include?",
        "ece8c69c-e841-4b39-a237-78f86aef0ab4": "What are the different roles that can be assigned to users in the HPE Ezmeral Data Fabric as-a-service platform? How can a fabric manager edit the roles for a user?",
        "b283a80d-dd3e-4df3-a699-cb781405321e": "What are the steps involved in uploading Spark application files to the user or shared directory?",
        "663a87ea-b5f6-4a55-b313-a0960b11d373": "How can you configure Spark to use S3 as the data source?",
        "6c254016-fa58-4734-aaa8-349b6ee6a655": "What are the required arguments for creating a DistributedContext object?",
        "8e61ad36-0fdb-4a09-be4c-068ff84b11c6": "How can you determine the rank information using the Horovod module?",
        "915f0a35-2075-448e-9f11-3649d18bf67b": "What components are included in a PyTorch checkpoint?",
        "c9ef7f79-ffb5-4fb9-bbe4-71bb6c1d235f": "How are PyTorch trials checkpointed and what does the state-dict.pth file contain?",
        "07da2a7e-45b0-4ba1-a747-f72e6f64bcde": "What is the purpose of attaching an internet gateway to a VPC in the context of network setup?",
        "64ab02fb-8097-4468-b3a5-76ad263b906e": "How can you ensure that a subnet in a VPC is accessible from the internet?",
        "cadaf077-7cb6-4d6d-b07a-bb89a7ec687c": "What improvements have been made in the latest release regarding error messages for 404 or 'Not Found' codes? How does RBAC affect these error messages?",
        "f71033a2-fa5b-4b4b-beb2-ca029b0754fc": "What is the deprecated feature mentioned in the context information? What alternative is recommended for PyTorch Lightning users?",
        "8f907eab-1b3d-45fd-9093-c2db30fc4c50": "How are batch metrics collected and stored in the database at the end of every training workload?",
        "e5ae023c-965c-475f-b780-5d2ac1322756": "In TensorBoard, how are batch metrics represented and what does the x-axis of each plot correspond to?",
        "0ba37409-75ec-43f3-b3a3-7a8f571b0ea9": "How are bind mounts used in container experiments, and why are they important?",
        "e82aaf21-01e5-44cd-a028-9ae97781cca8": "What precautions should users take when specifying host paths for bind mounts in container experiments?",
        "e0f142cc-0503-48d4-ad69-ef2589fdf101": "What is the purpose of designating a fabric as the global policy master in HPE Ezmeral Data Fabric? How does it relate to security policies and access control?",
        "25a3e718-33a3-433f-8f4a-e75f0e377989": "How does the process of assigning and unassigning security policies to volumes work in HPE Ezmeral Data Fabric? Explain the steps involved and the implications of enabling or disabling a security policy.",
        "ab1e4300-1c05-450a-a00d-1ed0fea57eca": "What are the two types of credentials that can be used to access AWS CloudFormation APIs?",
        "6c22baaa-d85d-490e-b940-41e41588db9b": "How can the default EC2 instance limits on GPU instances be increased?",
        "efb4df2a-7f81-4531-b72e-d7ebd80b9f33": "How can you access the Keycloak administration console to manage Keycloak and your SSO users?",
        "5512cf76-8908-4883-82c2-0e469fbb25a1": "Why is it important to change the default Keycloak admin password soon after signing in?",
        "2bc451e2-7743-4dc0-98df-292f6f1d2e4b": "What is the purpose of the Global Namespace (GNS) in the HPE Ezmeral Data Fabric? How does it connect different deployments of the data fabric?",
        "e14f901f-08c1-4714-b1a9-41aa129e14c5": "Can you explain the support for Iceberg in HPE Ezmeral Data Fabric 7.6.0? What are the key features or functionalities provided by Iceberg in this version?",
        "61580c9a-6337-45bc-a9e5-779440ec326f": "How can the WorkspaceCreator role be assigned to users? Can it be assigned globally or only to specific workspaces?",
        "1579d579-a5d8-46d8-8ae8-25466c7328ae": "What is the default behavior when a user creates a workspace? Do they automatically get assigned the WorkspaceAdmin role or is there an option to configure this behavior?",
        "35d5af25-8ccf-4bbc-92b1-0297c43bb9d9": "What are the different units in which the advisory quota and hard quota can be expressed?",
        "532b28db-1392-47b9-8102-168b75823eb0": "How can you set the quota for a group in the Data Fabric UI?",
        "c2d3084a-e8c5-4e9f-a931-eb339b6ea015": "What is the default value for labeling the Determined agent instances if the master is on GCP?",
        "d2bf479d-ebae-474e-acfb-24b0015b2c61": "How is the determined agent instance labeled if the master is not on GCP?",
        "3d9dc61e-7fde-488c-846b-9767dde2907e": "What is the purpose of a data node in a data-fabric cluster? How does it contribute to the overall functioning of the cluster?",
        "63b6ef3f-5a41-42ca-bbd6-6a3573f084c9": "Explain the concept of a global namespace in the context of HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "bc961ed0-ffe3-4f47-92e3-02d07724a0f9": "What is the recommended way to create an experiment, and what are the limitations on the size of the context directory?",
        "9103ead4-1fb8-435d-a601-8ecc3341ed47": "How can users exclude certain files from the model definition when creating an experiment, and what types of files are always ignored?",
        "338a3230-5e1f-44ce-9ff0-b5f6f663671f": "How does the default Kubernetes scheduler handle distributed deep learning workloads that require multiple pods to be scheduled before execution starts?",
        "a682a1d5-438f-42f7-a7e3-93f1b86fefe6": "What are the two scheduling features that Determined includes to address the limitations of the default Kubernetes scheduler?",
        "5bffa338-d566-47f1-86cf-a8d71bc1af8d": "How can the wait time for agents to start up be customized in the given system?",
        "ffee9408-4722-494f-9a78-e15be946ab9a": "What is the default wait time for agents to start up in the system?",
        "8baa6086-417c-4de8-8f43-1d51c496fb09": "What is the purpose of the InferenceProcessor class in the given context?",
        "36e2bf56-5227-4177-8a15-b103414490ec": "How does the process_batch() function in the InferenceProcessor class handle inference logic?",
        "3d5d7305-6273-4532-818a-d62d3a986bad": "How can you customize environment variables for different types of tasks in a trial container?",
        "96f2eb2f-c991-4714-a833-7e5bc659ecbe": "What are the different types of tasks for which you can customize environment variables in a trial container?",
        "fa867d89-83f9-42cd-996d-1c2b845ce95c": "What is the purpose of specifying defaults for all task containers in a scheduling system? How does it benefit the overall management of tasks?",
        "49624bf4-a015-45e4-80ea-8ac6bbbe93e1": "Can you provide examples of different types of schedulable units that can be represented by a task? How does the concept of a task help in organizing and executing these units efficiently?",
        "642f87b5-33bd-418c-ace1-b7574c62d38d": "How does YARN allocate memory for each map or reduce task?",
        "bcffa628-ab6c-42a0-a929-6a08d7057cf7": "What is the purpose of ZooKeeper in distributed applications?",
        "03f5f4a6-7489-4656-861e-2a78053bc305": "What are the advantages of using Determined's built-in resource management compared to Kubernetes or Slurm for teams of any size?",
        "85a0c6be-3002-49a0-884e-d15a367f7247": "How does Determined's resource management support both on-prem clusters and cloud auto-scaling?",
        "1c041fa7-e447-4344-8d5e-426a2dbf0717": "What is the default user that Determined runs task containers as? Can this default user be changed?",
        "0b337334-4cce-4eaa-b4a3-87dcef4ad99d": "How can a Determined user be associated with a Unix user and group? What is the benefit of doing so?",
        "e822812e-77fb-41ff-af6b-a9832353da8c": "What is the purpose of the minimum replication factor in a data-fabric cluster? How does it affect the cluster's operation?",
        "45bf325d-8a8d-4205-a827-1eed642e291a": "Explain the concept of an object store in the HPE Ezmeral Data Fabric. How does it leverage the capabilities of the file system for performance and scalability?",
        "4a028393-02ed-4f5e-b3c5-fa6516415191": "What are the two types of packages that can be installed in Kubeflow notebooks in HPE Ezmeral Unified Analytics Software environments?",
        "70380aa8-b847-45e6-80d1-67b2f6b5c715": "How can custom packages be installed in Kubeflow notebooks in single-user mode in HPE Ezmeral Unified Analytics Software?",
        "3942b5d8-06cb-4356-90fc-21cff281c6c7": "What is the purpose of the top_checkpoint() method in the ExperimentReference class? How does it determine the order of checkpoints?",
        "1492797b-a70c-4ad6-bfac-b80bda97b2f8": "How can you sort checkpoints by a specific metric using the top_checkpoint() method in the determined.experimental client? Can you provide an example?",
        "5d6f31d5-3d06-4452-923c-e21e8620ce2a": "How does the det deploy tool utilize AWS CloudFormation to deploy and configure a Determined cluster?",
        "540fc24b-5285-490c-a8a8-2b2c57c5dc16": "What is the purpose of using CloudFormation to build the necessary components for Determined into a single stack?",
        "3837f3e2-af01-481e-81f6-635af2a51654": "What is the purpose of the `overwrite_deepspeed_args` field in the hyperparameters section of the DeepSpeedTrial configuration?",
        "8794dfdb-d985-4911-a472-5ac5bc853b95": "How can you enable pipeline parallelism with a given model in DeepSpeed?",
        "38aa9633-887a-4318-8eff-713434d3a5ec": "What is the significance of setting the slotType and slotResourceRequests.cpu values in a CPU-only configuration? How does it affect the allocation of resources in Kubernetes?",
        "6e3d81e7-e7b5-4e63-b3d3-293075b1a47e": "How would you determine the appropriate value for slotResourceRequests.cpu in a CPU-only configuration, considering the limitations of the number of CPUs allocatable by Kubernetes? Provide an example scenario to illustrate your answer.",
        "02118535-555b-4077-8751-8f973d2cf36b": "What is the purpose of OpenTelemetry (OTel) in HPE Ezmeral Data Fabric? How does it contribute to monitoring and managing fabric deployments?",
        "70ed849f-5566-4fb1-90a3-3fecee8f4056": "How can an external NFS server be associated with Data Fabric? What is the benefit of this integration in sharing data across clusters in the global namespace?",
        "b95053fc-bab4-4924-9b75-18a3e00fb3cd": "What is the purpose of the replication factor in a data-fabric cluster? How does it affect the availability of writes in the cluster?",
        "8a9fb3ae-a9ef-4aa8-bbab-c2068524cd5a": "Explain the role of the ResourceManager (RM) in a YARN cluster. How does it manage cluster resources and schedule applications?",
        "e0292707-4740-44dd-9ed3-8251e1bd234d": "What is the purpose of the replication factor in a data-fabric cluster? How does it affect the write operations in the cluster?",
        "971ef10d-c425-4881-876f-43254a294435": "Explain the role of the ResourceManager (RM) in a YARN cluster. How does it manage cluster resources and schedule applications?",
        "deb827e7-526d-4656-acb2-7056e8312b21": "What are the potential configuration changes that may be required in the storage.conf file when using Podman in rootless mode on an HPC cluster?",
        "b61bdc36-ae80-4554-841c-2c7af9b87abc": "How can you resolve the permission error issue when XDG_RUNTIME_DIR is not defined and /run/user is not writable by a non-root user in Podman?",
        "b72c4745-3d37-4e26-9ccf-9cc3b8e896ce": "What is the purpose of data recall in the HPE Ezmeral Data Fabric File System? How does it ensure data integrity and manage storage resources effectively?",
        "f1bb0039-9ff7-49c2-9d64-a803de76202c": "Explain the steps involved in manually recalling data from a volume to a cold tier in the HPE Ezmeral Data Fabric. Why is it necessary to recall data before running analytics jobs on a cold tiering volume?",
        "875e1393-1e59-4900-9522-79669bf50cb8": "What are the steps to view or download topic connection properties in the Data Fabric UI?",
        "9749f926-64ea-482e-815e-394be777afb6": "How can you associate an external NFS server with Data Fabric to share data across clusters in the global namespace?",
        "bc146735-0efa-4285-9053-5cdbc65370f3": "What are the different versions of Determined and how do they utilize Docker containers?",
        "1b2f0338-2f46-4244-8273-bcc542378ac0": "What are the default image tags referenced in this version of Determined and what are their corresponding Docker Hub locations?",
        "13e1d320-a8bf-422e-8cef-aedaa073b599": "What is the purpose of a storage pool in a data-fabric system? How does it contribute to the overall storage capacity and accessibility for users?",
        "ce4f5874-8b87-4617-8ed0-939ef07e6765": "How can a storage pool be created using multiple storage devices? Provide an example of how combining multiple disk drives can result in a larger storage capacity.",
        "614bb638-6ae0-472b-83d0-bb77d8f24462": "How does the use of a credential impact the security of an account_url?",
        "95a83c33-d007-4e43-b039-8d19acce116e": "In what ways can diverse questions enhance the effectiveness of a quiz or examination?",
        "8d7dab68-2c0e-4fd4-9c8f-908ab355f758": "How does the new release of Determined support deploying on Kubernetes? What are the limitations when using Determined with Kubernetes?",
        "8f5fd7be-19ea-413d-a5df-c98026cd3939": "What is the new feature in Determined that allows running multiple distributed training jobs on a single agent? How does this feature improve resource allocation in smaller clusters?",
        "8c0d5b1f-910a-459f-93fa-75e05c98b375": "What is the purpose of an access control list (ACL) in the MapR Converged Data Platform? How does it determine user permissions for specific actions on an object?",
        "b7a2e66f-1625-4c5b-be73-c9b1590bd5e6": "Explain the concept of a data container in a data-fabric cluster. What are the different types of data containers and how do they contribute to data replication and storage in the cluster?",
        "0c39d8b8-f114-44b1-b837-81232ab29d80": "How does the optional parameter \"batch\" in the profiling process affect the beginning of profiling? Provide an example to illustrate your answer.",
        "c37d5ba3-5337-4721-93c4-733d74646978": "As a teacher, how would you explain the significance of specifying the batch for profiling? Discuss the potential benefits and drawbacks of using this optional parameter.",
        "d3446723-3592-4150-a950-b225a002040b": "How does the support for the \"okta\" IdP differ from other identity providers?",
        "84c23a17-58ce-4a5c-b581-4da17a95943f": "As a teacher, how would you explain the significance of the \"okta\" IdP in the context of authentication and authorization?",
        "d6541e91-5ee7-4ce0-9d75-08f94f188d56": "What activities are covered by the license described in the context information? Are there any activities that are explicitly excluded from the license?",
        "af22187b-27e5-4fc7-899c-b565e55b925d": "Under what conditions can you copy and distribute the complete source code of the Library? What requirements must be met in order to modify and distribute a work based on the Library?",
        "dc1a5313-5892-4414-889d-35bebbcb1f0b": "How can the Core API be used to adapt model training code? Provide an example of how it can be applied to the PyTorch MNIST model.",
        "1266e30e-3597-4ab1-92b7-862b60908b0c": "As a teacher, how would you explain the purpose and functionality of the Core API to your students? Use the context information provided to support your explanation.",
        "8d1aa15b-f350-497a-baca-e5eadb1866f6": "What are some actions that can be performed in the Data Fabric UI's Fabric manager view?",
        "c64f81e7-aeb6-45ec-ab29-730e37240b88": "What are the different fabric status values and their descriptions?",
        "251a76ac-6970-4ab5-8f33-daa4a6ee02d6": "What are the different types of APIs mentioned in the context information?",
        "7794943b-1f87-450e-8ec4-8bdbdd540a87": "How can the training APIs be used in the document?",
        "f7504dea-e5e0-42b1-9f9c-61842ee810f8": "What are the key features and functionality provided by HPE Ezmeral Unified Analytics Software?",
        "9b7e42ce-6b7d-423e-b4b9-c3bebe6520ef": "Which compute components are included in HPE Ezmeral Unified Analytics Software and what are their respective roles in data analytics tasks?",
        "cffba0c4-2727-4ead-ad8a-f62f14ca2e88": "What bug was fixed in the CLI related to det task logs?",
        "df48684d-8e72-4c4f-8916-ffcdeaa3755c": "What improvement was made to the Scheduler in the latest release?",
        "8b5ed1da-5629-43d7-962c-18b9083de548": "What is the role of the super user in a data-fabric cluster? How does their administrative access impact the cluster's operations?",
        "59c2f2a9-560b-4881-b8c8-88d819527071": "How does the concept of a super user align with the principles of data security and access control? Discuss the potential risks and benefits associated with granting administrative access to a data-fabric cluster.",
        "4c91c86f-644a-42d4-967d-6639f7dd6164": "How can the test_mode parameter be used to debug code or write automated tests around model code?",
        "cabcdd20-ef03-4695-98cb-c1f32fcb4947": "What are the purposes of the max_length, checkpoint_period, and validation_period parameters in the trainer.fit() function?",
        "3b993edf-9e8e-4228-bc74-f7e8b51e5541": "How can you integrate Spark with an external metastore in HPE Ezmeral Unified Analytics Software? What are the steps involved in setting up this integration?",
        "04d3e716-3e96-4e63-b026-c645131c0ab9": "What is a temporary view in the Spark DataFrame API? How can you create a temporary view for data in HPE Ezmeral Unified Analytics Software?",
        "c107977c-e638-4bc3-9391-626cf0a7d402": "How does Determined Enterprise Edition (EE) enhance user and group provisioning through its SCIM integration?",
        "2c45f0d1-6517-49c5-ab92-1255776d6e18": "Which identity provider (IdP) is currently officially supported by Determined EE for SCIM integration, and what is the expected compatibility with other IdPs that adhere to RFC 7644?",
        "00941685-bfc2-496e-8d92-d5229e20c6b0": "How does the \"resource_pool\" configuration option contribute to the resource scheduling in the experiment?",
        "2887dcf2-8264-4119-8c04-6ef22ccba1c5": "What is the purpose of the \"slots_per_trial\" configuration option in the experiment setup?",
        "f33626c8-e230-4540-b090-c1d66e94c1a0": "What is the purpose of the \"store_path\" function in the given script?",
        "ec6f0f7b-b3e9-4dab-9ef6-758d674aed60": "How does the script save checkpoints during the training process?",
        "65a06c98-7baa-4b07-b6ed-d6129e93906b": "What is the recommended container platform for AMD/ROCm support in Determined?",
        "9062ab4c-369f-446d-be5b-ac0de4d2f934": "How can you resolve the error \"No HIP GPUs are available\" when launching experiments with slot_type: rocm?",
        "62d14ec5-3298-449e-a895-3068274299d1": "How does the system handle namespace changes in Spark applications? Provide an example scenario.",
        "7bf695c6-cbc4-40f5-be50-3db6f35c2928": "What is the purpose of adding a security context to Spark applications in HPE Ezmeral Unified Analytics Software? Explain the consequences of not setting the security context.",
        "2c056569-f41a-419c-92f6-a3b6a1716463": "What information do you need to configure your IdP for users to SSO to Determined?",
        "8c4b7651-4b6d-4370-ac54-2290ebc2bf04": "How should the audience URL be set when configuring the IdP for SSO to Determined?",
        "db4dfa08-6cf1-4978-927e-665705f9713b": "How does the task_container_defaults setting in a resource pool affect the defaults for all tasks launched in that pool? Explain with an example.",
        "1c090b37-aa50-4690-8120-cdbd24566a2e": "Can you explain the concept of partition_overrides in the context of Slurm/PBS? How does it relate to the task_container_defaults setting?",
        "845d150c-097a-4afd-8116-e34d251509fe": "What are the different operations that can be performed related to tables in HPE Ezmeral Data Fabric?",
        "55df4551-3f2a-480b-83cb-ea839a496fd7": "How can you view the list of tables created on a fabric in HPE Ezmeral Data Fabric?",
        "be1c8ea2-4e13-49d9-be76-3e2148ac8301": "How does fluentd help in managing log data in a system?",
        "92d94e92-39e8-4e4e-ac5e-cb914296fe35": "What are the key components of fluentd and how do they work together to collect and process log data?",
        "f1a7df6b-708a-4d76-ac6d-e8150b154b21": "How can you enable incoming webhooks for a Slack application?",
        "575adee2-1e30-42f1-9252-a8a2fa5cfcf5": "What is the purpose of configuring incoming webhooks for a Slack application?",
        "a1a4e280-b8f8-4650-887a-3ccc93832de1": "How does the bug fix in version 0.17.14 address the issue of the resource pool and resource manager crashing after submitting a command with a non-default priority?",
        "6d2c483f-e831-4a19-b3b1-50afe8a0a515": "Why is it recommended for all users on versions 0.17.12 and 0.17.13 to update to version 0.17.14 or later?",
        "ad9977ad-6af9-4332-b41b-fbe111ac914c": "How can you increase the resource capacity of an HPE Ezmeral Unified Analytics Software cluster by adding additional user-provided hosts?",
        "8dd627db-fa73-453b-9186-9d93f8735264": "What steps should be followed to import an application in HPE Ezmeral Unified Analytics Software after fixing the application chart?",
        "a87a20f6-59bc-49c3-86fd-3bbead3119a6": "How can you configure a command to run when a specific event occurs? Provide an example of the syntax used to specify the command and its arguments.",
        "0c3c5626-7d37-4407-bc2b-9f5085b84f6b": "In the given context, what is the purpose of the configuration for commands to run when certain events occur? Explain why it is important to specify the command and its arguments in this configuration.",
        "33db4107-5458-4fe0-8f0e-447208555da5": "What is the purpose of a data node in a data-fabric cluster? How does it contribute to the overall functioning of the cluster?",
        "f0ff8d8d-8ab6-4489-b2b7-efe287f69612": "Explain the concept of a global namespace in the context of HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data?",
        "8d8036b6-d94b-464d-b4cd-96b781f86a9c": "What are the additional requirements for installing the launcher on a node?",
        "9ac763a6-2431-409d-9918-fde0b59bfd07": "Why is it important for the installation node to support an RPM or Debian-based package installer?",
        "62922525-ea88-40e9-bccd-5460233c5bdc": "How does the environment section contribute to the execution of workloads in an experiment?",
        "bd850e1d-2cf4-459a-abab-4f0f4aca6a61": "What are some ways in which the trial environment can be customized for a specific experiment?",
        "ad8e1efe-4458-46f8-ad31-5365d703a7ea": "How can you restore a volume from a volume snapshot in HPE Ezmeral Data Fabric? Provide a step-by-step procedure.",
        "bcd64ac0-f413-4f44-a321-fa9e866c0661": "What is the purpose of preserving a volume snapshot in HPE Ezmeral Data Fabric? Explain its significance in the context of data restoration.",
        "17e0fc6f-efde-4f58-aef0-03debfecd739": "How does the adaptive_asha search method differ from other search methods when it comes to handling large-scale experiments?",
        "3cf40bda-7f29-428f-88ba-29ae3349d42f": "Can you explain the concept of the asynchronous successive halving algorithm (ASHA) and how it is utilized in the adaptive_asha search method?",
        "f47e14db-8c96-48ca-913b-3b1baf95dcb8": "What is the purpose of introducing hyperparameter search and executing full training for each trial in Step 7 of the experiment configuration?",
        "d3f134e2-ed81-48c4-8704-10e4ffbc1773": "How can errors occur when loading from a checkpoint in an experiment with searcher.source_trial_id set?",
        "02e48dd8-41a9-4adf-9fa1-930fd7d670e0": "How does the Determined master schedule a TensorBoard instance within the cluster?",
        "be3c9a80-b654-4ea7-aba1-aa4d80299184": "What is the purpose of the TensorBoard web interface in the Determined CLI?",
        "bcab10f0-8e86-4171-bca3-c6ed851e0d5e": "How can you modify a storage policy in HPE Ezmeral Data Fabric? Provide at least three modifications that can be made to a storage policy.",
        "d37d4d4c-c2ec-4c52-a8a5-205d4d89a62c": "What are the different expressions that can be used to modify an advanced rule in a storage policy? Explain each expression and provide an example for each.",
        "5ea6938d-c744-427a-81e9-2e79ae00e23c": "What are the three modes of early stopping in the context of hyperparameter optimization? How do they differ from each other?",
        "1b227bc9-add2-474e-860d-a6f59eeb51f9": "As a teacher, explain the trade-off between aggressive and conservative modes of early stopping in hyperparameter optimization. Which mode would you recommend using and why?",
        "cb080b9d-ac4f-4bc1-a04f-d6217a32b1d8": "What is the order of enforcement for security policies in the HPE Ezmeral Data Fabric? Explain how the system evaluates and enforces ACEs at different levels.",
        "f9efb1e9-fc21-4baa-998e-aad25916b291": "How can administrators manage security policies in the HPE Ezmeral Data Fabric? Describe the steps involved in creating, editing, enabling, disabling, and assigning security policies to volumes.",
        "e2c1b590-b266-497c-a3c9-7c9cfa6c75ec": "What is the role of the data-fabric user in a cluster? How does it differ from other users in terms of privileges and control?",
        "38906cc4-97ec-4a36-ac22-ccacd22334b9": "Explain the concept of the global namespace in the HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "dd34e3a5-83e7-4486-aafd-3a7d26f289a4": "What bug was fixed in the September 8, 2020 release deployment?",
        "0d0b18f4-9bd6-433a-bd80-3daaa47aa637": "How was the bug related to experiment labels on the WebUI fixed?",
        "7ba65e5f-3e61-4731-85e1-7b1ad216c65d": "What is the default duration in seconds before idle TensorBoard instances are automatically terminated?",
        "4ce2dffb-8849-48fd-9460-3508ed1481ec": "How is an idle TensorBoard instance defined in terms of HTTP traffic?",
        "29a0674a-5a05-4d03-b841-a0fb0f46fa2a": "How can you enable metrics in the Ray dashboard?",
        "d8ac2800-15a0-4a0d-aa0d-90b64dd4c2ed": "What are the prerequisites for enabling metrics in the Ray dashboard?",
        "58b4029e-e862-45c8-8da1-ef454f693cb1": "How can Determined agents access Docker Hub for tasks such as building new images for user workloads?",
        "4b9bb9b4-317c-4795-a180-a55176236f23": "What additional steps may be required to ensure that instances in a VPC on a public cloud provider can access the Internet when using Determined master and agents?",
        "65c61499-d066-4eec-98dd-6633aae9a207": "What are the steps involved in importing an as-a-Service fabric into the Data Fabric UI?",
        "09cd95dc-f3dd-4c21-a0a9-7161551e8b64": "How can you ensure that the imported fabric is part of a cluster group and listed as part of the global namespace?",
        "368d52c5-ec6e-4f06-8510-c4d7cf8362ba": "How can you view the core software version for a fabric in HPE Ezmeral Data Fabric? Provide two different methods.",
        "7d8016f6-2b01-4f40-952d-40561413db1f": "What are the steps involved in creating a new fabric in HPE Ezmeral Data Fabric?",
        "6ff4fccd-e235-4d5d-a566-aab066957fa2": "What is the purpose of the `PyTorchTrialContext` in the given context information? How does it relate to the `PyTorchTrial`?",
        "c711e7f6-9052-48ca-926f-db9106266952": "How does the `aggregation_frequency` parameter affect the distributed training process in the given context?",
        "f9084cdb-8fef-4484-aad2-86b955bc3493": "What is the significance of removing the EstimatorTrial in the upcoming release of Determined?",
        "87353a51-03e6-4393-8fca-e77f677e39f3": "How does the bug fix related to trial logs impact users who created trials prior to Determined version 0.17.0?",
        "60d8730f-cb0f-49e3-ae78-3d1525036582": "What are the steps to install the Data Fabric client on Red Hat Enterprise Linux (RHEL)?",
        "9053a462-4399-4f41-9e25-3f0439fc7366": "How can you make packages available through the HPE Ezmeral Data Fabric repository?",
        "bfed2ed0-5d86-4d62-9317-4f9f70c8caad": "What are the steps involved in the workflow for using HPE Ezmeral Unified Analytics Software?",
        "3cc55e2d-786e-458e-9fe7-c0f016f8ced0": "How can you connect external data sources to HPE Ezmeral Unified Analytics Software?",
        "1bed8fab-98c8-4e45-87d7-f15e1b679856": "What changes were made to the experiment list page in terms of user interface and functionality?",
        "ddf60625-e47a-45a6-851b-ba2e30d3eef1": "How was the trial detail page enhanced with new features and improvements?",
        "735d3ba7-e9bc-4c08-9332-917c02a778cc": "What are the steps involved in installing, configuring, and upgrading a deployment of Determined with dynamic agents on GCP?",
        "f6fae989-e6f6-4806-addd-7fd30486f2f1": "Can you explain the concept of elastic infrastructure in Determined and how it relates to the deployment of dynamic agents on GCP?",
        "f4c3acc3-53bf-4bbd-882b-909305d4dcc8": "What is a resource pool and how is it used in job assignment?",
        "635ebcac-be98-4b85-a8b5-360cd40bbf0f": "Can you explain the purpose and functionality of resource pools in computational environments?",
        "9ed1cafd-7986-4ffe-9fde-22e28966e942": "How are CPU resources allocated in the given context?",
        "0d025ff5-0033-4b35-8f07-52d85f51b5a9": "What is the purpose of representing partitions as a resource pool with slot type cpu in this scenario?",
        "880c657f-77c2-4ebf-a455-1e373ba793b6": "What is the purpose of the section mentioned in the context information? How does it contribute to the free software distribution system?",
        "f1378d56-bc57-40da-b37e-12b41249111b": "In what circumstances can the original copyright holder add a geographical distribution limitation to the Library? How does this limitation affect the distribution of the software?",
        "094977a3-01d5-4487-824a-972dfe151415": "What is the purpose of the new top-level navigation option \"model registry\" in the WebUI? How does it allow users to interact with existing models?",
        "59ed6b75-ba62-438a-9e1a-8d52ab164496": "Explain the experimental support for Multi-Instance GPUs (MIGs) in agent-based setups. What are the limitations of using MIG instances for distributed training?",
        "a85d908e-5800-4ff8-9202-5db6b088c9cc": "What is the purpose of the `loss_scale` parameter in the `amp.initialize` function?",
        "45985897-a3c2-4167-8672-92eca7e778ca": "How can the `set_profiler` function be used to profile GPU and CPU activities during training?",
        "641edbf4-d762-4740-9030-2560348e9193": "What are the benefits of using the Determined library as an alternative to the HuggingFace Trainer Class in the Transformers context?",
        "a0f90a00-c2bd-42a8-aab7-b2a88c5f0562": "How does the MMDetection library serve as an alternative to the trainer used by MMDetection, and what are the advantages of accessing the Determined benefits in this context?",
        "a72b1384-a98d-4853-b28a-59baed39bf1f": "How does the security policy server enforce policies and manage security policy metadata in the fabric?",
        "04302f51-9bf9-421c-9df9-b72d2f26f9cd": "What is the name of the internal volume that stores the security policy metadata in the fabric?",
        "3ecb5941-9bfb-437e-8e9e-1469c7909b2a": "How can Determined agents access packages, data, or other resources hosted on the public Internet? Can agents use proxies for this purpose?",
        "bd248b7c-d14a-4151-8061-f5a04372b2c5": "What additional steps might need to be taken when using VPCs on a public cloud provider to ensure that instances in the VPC can access the Internet? Specifically, what configurations are required on GCP and AWS?",
        "0fde5cd0-678b-4b54-97a6-98d65e580520": "What is the default port for the Determined master when TLS is enabled? How does this differ when TLS is not enabled?",
        "c60a03c5-eeda-4de2-b669-8060807ca384": "In what scenarios would the Determined master use port 443? And in what scenarios would it use port 80?",
        "23519186-2c73-4533-8e46-e92578b3b225": "What is the default value for the master hostname that containers started by this agent will connect to?",
        "5a714d67-f05c-4b8e-8a8b-e64c7fb23eae": "How can the master hostname for containers started by this agent be overridden?",
        "95cbbfea-64f5-4aee-b565-95643e9a9421": "Why is Docker a dependency for several Determined system components?",
        "5b348abb-57cd-4671-b7ec-4a1cad4987be": "What is the specific role of Docker in enabling agent nodes to run containerized workloads in Determined?",
        "26c5cd41-5282-4a1c-a015-00bcbdeef85d": "What are the different units in which quotas can be specified for users and groups in the fabric?",
        "6b1bc975-92e6-40a2-a3bf-066068055509": "How is the size of a disk space quota calculated for a user or volume in the fabric?",
        "be8c8dfe-c040-4fb9-81fa-a40d4b25d6d7": "How does the scheduler configuration on each resource pool in Determined affect task scheduling?",
        "d60e75d6-53da-4bed-a805-62de096eb3ff": "Can you explain the relationship between the global scheduler configuration and the scheduler configuration on each resource pool in Determined?",
        "9ce08b8b-f936-4c0c-8b3f-253e16594761": "What is the purpose of the Docker image in the Determined agents setup? How does it contribute to the functioning of the agents?",
        "744917ef-419f-4d37-aa13-1916ea89b2cb": "What is the default Docker image used for Determined agents? Can you explain the significance of specifying a repository and tag in the image format?",
        "5b2cbdd2-7d89-44b8-ac0f-120c6dac656a": "What is the purpose of an access control list (ACL) in the HPE Ezmeral Data Fabric? How does it determine user permissions for specific actions on an object?",
        "e9bff3ce-7760-4c5f-882d-9bff9776babc": "Explain the concept of a data container in a data-fabric cluster. What are the different types of data containers and their roles in replication?",
        "0544f6aa-ab02-4708-85de-9c806d44aa61": "What is the purpose of the \"--error --output\" option in the generated batch file? How does it assist in diagnosing problems during job startup failure?",
        "9e3fd956-3aad-416c-b3dd-873e4cebb603": "How does Determined handle checkpoint and restart for its experiments? What is the significance of the \"--no-requeue\" option in SLURM?",
        "5c39d981-d298-451e-af3b-5058d33090ea": "What is the purpose of the AMI ID in the context of the Determined agent?",
        "c2040d6e-c562-41fc-85a6-936ce793c152": "How does the default GCP agent image contribute to the setup of the Determined agent?",
        "4aef93ff-112f-4a73-adac-f12bf336be00": "How is the experiment seed used in the Determined experiment framework?",
        "4efb1b27-4eb7-4e2d-83e5-857159d2227b": "What is the purpose of the trial seed in the Trial interface of the Determined experiment framework?",
        "0655cb4a-8717-4621-93b7-1acad840cc98": "How is the AWS secret key used in the context of AWS services?",
        "1586df19-8839-4f46-b368-ef00badedd0f": "What are the potential risks or security concerns associated with the exposure or misuse of the AWS secret key?",
        "7f7000a8-1d00-4f0c-bf39-c417541b00e6": "How are trial logs handled in the given context?",
        "9878b8f8-f5dc-4514-835e-da951e815253": "What is the default storage system for trial logs if no other option is specified?",
        "526293d4-2977-46f3-ad98-6c8d395dbd71": "What is the purpose of the TrialReference class in the determined.experimental.client module?",
        "6df3b7d8-d2fb-49e4-99bc-3515a9a0a086": "How can you obtain an iterable of log lines from a trial using the TrialReference class?",
        "a534d406-c9a1-4bc0-aa9e-8a5156487484": "How does the latest release of the Determined platform improve the scheduling and scaling behavior of CPU tasks?",
        "5169979e-f7bc-4564-a969-221d667b3639": "What bug was fixed in the latest release that caused active trials to not be restored properly on a master restart when max_restarts is greater than 0?",
        "2423f6b3-19cf-4979-b732-2099b99cfbd5": "How does the policy server manage security policies and composite IDs?",
        "f0e6fbf2-064c-46ef-b727-13d1862146d2": "What are the different services provided by the policy server?",
        "0adbd153-05b5-4059-aa1a-875c98a2887f": "How does the file system path on each agent relate to the trial container in the context of the given information?",
        "572d4718-b6aa-4ac3-b79f-4b51182e1967": "What is the purpose of mounting the directory to /determined_shared_fs inside the trial container in the given scenario?",
        "37078e3b-d7e4-4159-9918-ce80021298ac": "How can you remotely access files as objects in HPE Ezmeral Data Fabric using S3 client?",
        "14d57825-f062-465f-89ca-daacc6c5d9d4": "What steps should be followed to view object endpoint information for files in a volume in Data Fabric UI?",
        "aeabed37-6607-4368-9604-0113fccb1e93": "How does the Object Store in HPE Ezmeral Data Fabric efficiently store data for fast access?",
        "2f0109a3-898c-405f-aef7-26bbf04a43ed": "What are the key benefits of leveraging the capabilities of the patented HPE Ezmeral Data Fabric file system in the Object Store?",
        "1a998dc1-a62e-4be6-a555-b8e9f97966b1": "What is the purpose of specifying the endpoint for S3 clones in the given context information?",
        "bcda3565-a3e3-45ca-96ac-59615aea6097": "How does the option to use Amazon S3 as the endpoint differ from using a specified endpoint for S3 clones?",
        "2d8f0fa0-4948-4149-9008-61310abcecf0": "In the context of the experiment configuration, explain the importance of having diverse questions for a quiz or examination. How does it benefit the students' learning experience?",
        "23874358-d062-4f0e-856a-9886c3f40dbd": "Based on the given context information, propose a hypothetical scenario where the experiment configuration could be applied in a real-life educational setting. Discuss the potential challenges that teachers/professors might face while implementing this configuration and suggest strategies to overcome them.",
        "209c20a6-3e34-4b65-b075-15f6571a6a2b": "How can you ensure that your work is saved even if your notebook gets terminated on Determined clusters?",
        "a10275ec-702c-4bf5-a629-333828064988": "What is the default location where JupyterLab takes a checkpoint every 120 seconds?",
        "81b8681c-f253-4909-8432-aeae064a5f74": "What is the purpose of using Model Hub Transformers in the context of core transformers tasks?",
        "c81416ae-f0dd-4207-b154-1fb0f9d88c80": "How can Model Hub Transformers Trials be customized or built according to individual requirements?",
        "6cb615e6-dbca-461b-9f1b-be8af5765164": "What is the purpose of a developer preview in the MapR Converged Data Platform? How should it be used and what precautions should be taken?",
        "28fb1f44-b85a-45d1-a739-140c2d87b6b0": "Explain the concept of an access control list (ACL) in the context of the HPE Ezmeral Data Fabric. How does an ACL specify user permissions and what actions can be performed on an object?",
        "c941c1c4-5fd2-43d4-8dfc-d15946d459fc": "What is the role of a data-fabric gateway in a cluster? How does it facilitate communication between source and destination clusters?",
        "d21a0b72-404f-4024-aca0-f0f58d693b58": "Explain the concept of a global namespace in the HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "3438cbc3-3f16-4bea-9a0f-4a2ba0d67e49": "What is the purpose of enabling the \"Mysql Auto Reconnect\" feature in Presto?",
        "b92252e1-d3c7-4846-98f4-f204fcc879f2": "How many connection retries will Presto attempt when the \"Mysql Max Reconnects\" value is set to 3?",
        "ea9754e7-63b2-4cc7-9f2c-a4e0c13236db": "What is the purpose of deleting a remote target in HPE Ezmeral Data Fabric? How can you ensure the data on the remote target is backed up before deletion?",
        "2d1fd114-b6ff-40a8-8352-38fede04c6e0": "What are the prerequisites for deleting a remote target in HPE Ezmeral Data Fabric? How does deleting a remote target affect the accessibility of data on the cold tier and its availability for data tiering?",
        "ccbf780b-dbd6-4da9-b555-f0c1182fe9f5": "How does configuring slots_per_trial to be greater than max_slots affect the training process in data parallelism?",
        "e5b79413-0791-4fc5-a382-d1c4807e9b30": "According to the PyTorch documentation, how can enabling data parallel training using slots_per_trial alter the behavior of certain models?",
        "8c26c318-d091-47e8-aab4-aab924bf7357": "What is the purpose of installing Determined on Kubernetes according to the provided document?",
        "add28ba5-01f2-4ad7-b4de-ea2033bf4644": "Can you explain the steps involved in setting up Determined on Kubernetes as outlined in the document?",
        "d05c2027-d36a-4e69-8809-d89c90de13e1": "What are the two different ways to connect to a Ray cluster in HPE Ezmeral Unified Analytics Software?",
        "3fcf217c-c04c-49e0-b1c5-cde882791988": "Why does Hewlett Packard Enterprise recommend using JobSubmissionClient instead of Ray Client to submit Ray jobs?",
        "e7f2a4d9-5999-4999-ad8f-c568ebc3262e": "How can RBAC be enabled on an existing Determined installation?",
        "b6842672-68b4-42af-8dc2-c5e9857c08f7": "What are the default user accounts included in a brand new Determined installation and what permissions do they have?",
        "1476d511-0966-475b-8352-9247f9c5357b": "How has the security been improved in the latest release of Determined?",
        "8bc70c1d-c007-49ff-8b90-0b7eeb585f12": "What new feature has been introduced in the web version of Determined to support various themes?",
        "1debdfb9-ab3e-4e84-9323-63115d1d9a98": "How can you access the HPE Ezmeral Unified Analytics Software home page after updating your DNS settings?",
        "601e14d2-0d26-4255-8cfd-8fc042e33293": "In an air-gapped environment, why is it necessary to manually set the HTTP proxy or configure Airflow to point to an internal GitHub repository for a successful Airflow installation?",
        "411dcec7-74a1-466d-a68b-a692a29b7ebc": "Which Linux operating system versions are supported for HPE Ezmeral Data Fabric releases?",
        "ed452670-7c8c-4e0b-844c-5a62d8ae8763": "What are the different third-party storage solutions supported by HPE Ezmeral Data Fabric, and how do they enhance global-namespace support?",
        "bb6389d3-f442-47ee-95e0-b673f622d751": "What is the purpose of a shared hierarchical namespace in a data fabric system? How is it organized and structured?",
        "0287d221-6dd3-4064-a6c3-de71d17fc262": "Who is the typical user that cluster services run as in a data fabric system? What is the significance of this user in the overall functioning of the system?",
        "bb3ebc95-6f2f-4169-9654-7c47af58dce4": "What are the steps to follow if you cannot reach the Determined master after installing Determined on Kubernetes?",
        "71d07595-0b1f-483b-81f5-0e1a1dfc9811": "How can you check the IP address and port assigned to the Determined master by looking up the master service?",
        "63cf47f5-3d7d-4f79-9a83-21d2761865a6": "How does Determined support arbitrary training and validation metrics reduction during distributed training?",
        "45fb27d3-b234-42dc-a576-308396a6fa79": "What options are available for defining custom reducers in Determined?",
        "0501d9f9-390d-457f-873e-b88429b9b869": "What command should be used in the CLI to list OAuth clients?",
        "f0a50587-71ef-4040-90d9-c825a2bddf71": "How can you retrieve a list of OAuth clients using the command line interface (CLI)?",
        "be6a97de-075d-4eed-8f0f-6caa807ce0ae": "What is the purpose of a validation metric in evaluating the performance of a hyperparameter configuration?",
        "e4df0231-0072-427c-a837-8e5654eda8fc": "Can you provide an example of a validation metric that is commonly used to evaluate the performance of hyperparameter configurations?",
        "b7efbd9e-2616-4d11-90a0-f34ffae375b5": "What is the purpose of the evaluate_batch() method in the given context? How does it contribute to the overall validation process?",
        "ba3db739-5ea6-4fb1-b527-0831e3b79182": "Can you explain the role of the evaluation_reducer() function in the context of evaluating the model's performance? How does it help in summarizing the evaluation metrics?",
        "df2aab19-03c6-4388-b44f-ab571b0a5cae": "What is the purpose of a data node in a data-fabric cluster? How does it contribute to the overall functioning of the cluster?",
        "5e830980-fe0b-4a33-bc4d-57055a290174": "Explain the concept of a global namespace in the context of HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "e7cf90e1-0982-4f1e-a16b-bccff7d558ae": "How can administrators control resource pool availability within a cluster?",
        "317043fa-15cf-44a9-b41f-7e9ba6d068b3": "How does binding and unbinding resource pools affect the availability of resource pools in a cluster?",
        "04132e60-ab65-4f7c-bcbc-acc984284439": "What specific permission is required for an instance to upload logs to CloudWatch?",
        "a77c7f7a-fd5d-4f32-9ecd-594a6b79e496": "What is the namespace that instances will upload their metrics to when using CloudWatch?",
        "8273c485-63be-460b-894a-dcecf83b2392": "What is the purpose of installing Determined on WSL using Docker Desktop?",
        "1988dd35-a255-43a6-b53d-1fe84b8f8696": "Can you explain the steps involved in installing Determined on WSL using Docker Desktop?",
        "9b903d4e-1fa6-4573-afa7-56b9a5af83d4": "What are the possible values for the \"Iceberg Catalog Cached Catalog Num\" property?",
        "a4cf92b0-6a4b-4415-a654-a9a6831b204c": "What is the purpose of the \"Enable Local Snapshot Table\" option in the context information?",
        "163eaa99-572c-44f6-bfd7-d555b8bc99fe": "What are some of the registered trademarks or trademarks mentioned in the context information?",
        "2b6490fa-95e4-4676-933d-3f5ecfe95c2d": "How can partners support the HPE Ezmeral Unified Analytics Software?",
        "90116b47-7dfe-4da6-a520-275965f7f4f3": "What is the purpose of the sudo configuration file /etc/sudoers.d/zz_launcher in the context of Slurm/PBS job management?",
        "fae812d5-fc10-430a-b0d8-c3a8c4a6eae6": "Explain the process of generating and applying the sudo configuration necessary for the launcher service to perform Slurm/PBS job management.",
        "13dd4fe2-caa9-4eeb-88de-aff7911587d5": "What is a security policy domain and what is its purpose in the HPE Ezmeral Data Fabric platform? How does it relate to the global policy master and member fabrics?",
        "d2c14117-32ff-4e7e-8d18-7f7ee14279de": "How are security policies created and managed in the HPE Ezmeral Data Fabric platform? Explain the process of designating a fabric as the global policy master and the steps involved in creating, editing, enabling, and disabling security policies.",
        "7f92649b-64fb-404a-a214-d8399fac23bb": "How can you copy multiple images to a local filesystem using the ezua-airgap-util command? Provide the necessary command syntax and specify the destination path.",
        "f818b904-ff2c-43c8-8936-0841fb442648": "What options are available for copying multiple images to a remote container registry using the ezua-airgap-util command? Explain how to provide the destination URL and credentials for the container registry.",
        "cfba3857-8105-494a-ba57-42fec9a44f02": "How can you configure the master to use TLS? What options need to be set and what should be the paths to the TLS certificate file and key file?",
        "aa68486c-fbb3-49f3-8ea6-5c5001da4933": "What is the default TCP port that the master listens on when TLS is enabled? How does this differ from the default port when TLS is not in use?",
        "0aeb7c53-abb7-4a43-91aa-5e99ac65a2d9": "How does the built-in model registry in Determined help in managing trained models and their versions?",
        "24680821-ac44-4281-a65f-6a1c61a81c58": "As a teacher, how would you explain the importance of a model registry in the context of machine learning and training models?",
        "95593b28-fe29-4b75-8400-9f709242cace": "How can the X-Determined-AI-Signature-Timestamp header be used to verify a webhook request?",
        "2840daa3-ff3e-4742-852d-0e31d83e41ac": "Explain the process of generating a signed request payload using the X-Determined-AI-Signature-Timestamp and the request body.",
        "6590d773-76ab-4807-8b92-7eaeed10f93a": "How can you specify the ports in the experiment or task config?",
        "ee6d2317-6d1c-4932-ac74-8f308f0d8e9a": "What command can be used to start a tunnel and proxy localhost:8265 to the task container?",
        "527e7929-f9d9-4cc7-a4d0-9c625972c45f": "What are the differences between the internal OpenLDAP server and external AD/LDAP servers in HPE Ezmeral Unified Analytics Software? How do these differences impact the configuration options during installation?",
        "54612f80-2622-4ae3-a239-6848536e292e": "How does the user management operator in HPE Ezmeral Unified Analytics Software set up local resources for users? What role does it play in enabling user access to the software?",
        "461603f7-17b1-4e45-8c20-68fd19517c5e": "What is the purpose of re-replication in the context of data replication?",
        "507b185c-87da-4ef3-9052-5b08329bf952": "How does the timeout specified in the cldb.fs.mark.rereplicate.sec parameter affect the re-replication process?",
        "f8718e95-fe9c-4e77-af15-ac44b22a9bc1": "How does enabling TLS enhance the security of a cluster? Explain the significance of enabling TLS in the given context.",
        "93eb1ae6-abeb-4b06-8be0-276efd5abede": "What is the purpose of the \"skip_verify\" setting in TLS-related configuration? How does it impact server certificate verification?",
        "e076b910-1722-49db-9fec-5a623151618c": "What are the required arguments for creating a DistributedContext object?",
        "3f863240-ff88-4cce-b3fb-a32daf2b7fd9": "How can you determine the rank information using the Horovod module?",
        "003ed9a4-746d-4686-9ba9-46415f0699ec": "What is the purpose of the \"Hive S3 Multipart Min File Size\" configuration parameter in the context of EC2 IAM roles?",
        "7695070f-4877-414c-a0fe-d9402b30dd58": "How does the \"Hive S3 Sse Type\" configuration parameter affect the server-side encryption of S3 data in Presto?",
        "f56fd3ff-99e5-42a7-8d5d-ff7bf51044d7": "How does the model_def_metrics.py script differ from the model_def.py script?",
        "63c28a4f-5d0a-4817-9d5c-d73c01f0311a": "What is the purpose of the steps_completed variable in the model_def_metrics.py script?",
        "65199081-4704-41e0-8887-75fd9bbfe8d3": "How does Determined make it easy to launch and manage Jupyter Notebooks?",
        "7044a773-17cc-4626-b7c5-0430c71b1a3a": "What happens to Jupyter Notebooks in Determined if they are idle for a configurable duration?",
        "ab2314b6-46f1-4ddf-b5f4-ede22f7369b4": "How does the hierarchical namespace in the shared file system organize the access policies for user accounts and IAM users?",
        "78fef877-1a9e-4db5-9529-a305af6ba3cf": "What is the purpose of associating access policies with accounts, users, buckets, and objects in the file system?",
        "77c124af-a4e4-431a-befa-eb93ec14bda5": "What is the default value for the maximum number of times intermediate results are evaluated for a trial before terminating poorly performing trials?",
        "3a4fef42-7418-4773-be87-82f9e6b82a9f": "Who should consider changing the default value of the maximum number of times intermediate results are evaluated for a trial and terminate poorly performing trials?",
        "f824c7f1-4a4b-44ac-96a0-4626f9c1f11b": "How does Determined make it easy to manage dependencies when using Jupyter Notebooks?",
        "96d10fb5-46ea-4f0c-9a1b-0a8977431984": "What is the importance of configuring the cluster to mount persistent directories into the container when using Jupyter Notebooks in Determined?",
        "b5592d9e-bda7-4a00-88e4-91cc40af0049": "What are the prerequisites for creating an on-premises fabric?",
        "ff518189-de14-4902-ab9c-7901815c3f9a": "How can you monitor the progress of fabric creation?",
        "21a19951-c697-4e14-8116-ec08fec304b4": "What is the purpose of the SearcherState class in the determined.searcher module?",
        "26c91efa-03e1-4561-8b19-9dd9366fd85b": "How can the SearcherState be used by a SearchMethod to inform event handling?",
        "05d017dc-e91b-432d-96ec-fa81005cf263": "How can you configure the Helm chart to use an external Postgres database instead of deploying an instance of Postgres on the same Kubernetes cluster?",
        "61547668-0d75-4e32-aa6d-d9fa389b7e7d": "What happens when the db.hostAddress is configured in the Determined Helm chart?",
        "6bddf994-c87d-46f7-bf00-550faf0fe38f": "What is the validation loss for the checkpoint with UUID \"8d45f621-8652-4268-8445-6ae9a735e453\"?",
        "2ec9c900-79b5-43cc-a517-2eb27ea4e2d0": "How many inputs were used for validation in the checkpoint with UUID \"62131ba1-983c-49a8-98ef-36207611d71f\"?",
        "5803ccd7-ee2d-484c-97ee-8d7573d20789": "What are the specific software and hardware requirements that need to be met in order to install the system?",
        "089acf19-bc50-460c-8f15-5270248f7455": "Can you provide a detailed description of the installation requirements for the software and hardware?",
        "712bcd70-a94a-488e-a216-59b838c5c6d9": "How can you use the load() method in TensorFlow to load a trained model with weights?",
        "8e79efdb-6fd8-4a76-ac0f-ed77617b83df": "What are the two formats in which TensorFlow checkpoints are saved, and how are they loaded as trackable objects?",
        "55245fb7-2ff7-4a65-bd2b-4b1f835dea10": "What is the purpose of the Integration Agreement mentioned in the context information? How does it affect the relationship between the parties involved?",
        "491f3f7b-0479-4780-9938-f26f4f05bef6": "Explain the permissions and restrictions outlined in the ZLIB License and the D3.js License. How do these licenses impact the use and distribution of the respective software?",
        "b18ca0ce-355e-41b2-b904-37eb3c2ded9a": "What is the purpose of the minimum replication factor in a data-fabric cluster? How does it affect the cluster's operation?",
        "ad6b8719-2005-43e6-babb-38340bd34ae0": "Explain the role of the Object Store in the HPE Ezmeral Data Fabric. How does it leverage the capabilities of the file system for performance and scalability?",
        "f89b6c39-2c0d-4ee9-83c5-cc73e61e3269": "What is the purpose of the \"--cluster-id\" argument in the given context information?",
        "c217c053-4bb7-404f-8a5a-270b440216e5": "Why is the \"--project-id\" argument marked as \"required\" in the context information?",
        "ac8c34c4-64ad-4f45-9ef1-7e22549173ce": "What is the maximum number of partitions per writer in Hive?",
        "281dd1c0-c61d-4baf-8212-9ac28c472dbc": "Should Presto user be impersonated when communicating with the Hive Metastore?",
        "c48edb7d-9ffd-44e4-82c6-333231c9b897": "How does the \"slots_per_node\" parameter help in utilizing more than one GPU per node when \"gres_supported\" is set to false?",
        "d890b24e-11b4-424c-98c3-451c627227e6": "What are some possible ways to ensure that the required number of GPUs specified by \"slots_per_node\" will be available on the selected nodes for a job?",
        "52fe0d28-cdf9-4932-8291-3d6243eb7706": "What is the default username for signing in to Determined once it is installed and Docker is running?",
        "6d5052f8-744a-48cb-bca2-217e3d7fee04": "What is the URL to access the sign-in page for Determined after it is installed and Docker is running?",
        "03d9a8fd-c4d9-42cd-8723-dd9ee586ef67": "What is the procedure to view CPU and memory utilization by fabric in the Data Fabric UI?",
        "2c05fd32-b729-48ef-b66f-e5c01d4a91a0": "How can an infrastructure admin or fabric manager view the CPU and memory utilization of a fabric in the Data Fabric UI?",
        "d7567fa8-5d0b-4fb3-a098-c15a1ca02404": "How does the new feature in the Determined Enterprise Edition allow for custom resource pools to be defined on an HPC cluster?",
        "6bf97804-2150-4b60-abf7-66136f077f6d": "What is the alternative container platform supported by Determined Enterprise Edition, replacing Apptainer/Singularity/Podman?",
        "e2a61bdf-0297-45e6-9cb1-054feac92d4c": "What is the purpose of GPU support in HPE Ezmeral Unified Analytics Software and how does it enhance the performance of various tasks?",
        "37e35818-409d-48dc-817e-a5d7834d87d9": "How does MIG partitioning in HPE Ezmeral Unified Analytics Software contribute to higher resource utilization and cost efficiency?",
        "382b6cdd-0fc4-4e38-b22b-ce03c897f628": "How can performing an initial validation before training be beneficial when fine-tuning a model on a new dataset?",
        "3be2046e-2788-4368-9ec4-2f793282b1fd": "As a teacher, how would you explain the importance of determining a baseline through initial validation before training a model on a new dataset?",
        "ddde1cbd-86cd-4c0d-ac1a-ce6a7c119828": "What is the purpose of the \"Configuring Per-Task Pod Specs\" section in the Customize a Pod guide?",
        "e30cd312-6f3e-4e21-8d0c-f4e625d77bcc": "How does the example provided in the section demonstrate the customization of pod specifications for individual tasks?",
        "234fa71b-3388-4298-b497-da61fea05db9": "What are the limitations of using your own open-source Spark images in HPE Ezmeral Unified Analytics Software?",
        "f6fe6df3-ea3b-4b57-81c7-d0e72d291936": "How can you use your own open-source Spark images to submit Spark applications in HPE Ezmeral Unified Analytics Software?",
        "b1e17e0a-5ab5-4fc3-b724-82783bb5c944": "What are the recommended versions of HPE Ezmeral Data Fabric that can be connected to HPE Ezmeral Unified Analytics Software externally in the latest release?",
        "818a1797-0344-4127-9ad4-4dd33d5de42e": "Which operating system versions are supported by HPE Ezmeral Unified Analytics Software, and which specific version is recommended for GPU hosts?",
        "050053fa-ebae-4a67-9880-d9a9a53e375d": "What are the required fields for specifying Docker registry credentials when pulling a custom base Docker image?",
        "1bf91e78-7180-4256-b60e-fb8bd25c6410": "How can the default Docker registry credentials be overridden in the experiment config?",
        "47dcd3b2-5d61-4963-9651-be0528a9d7b7": "How does the latest release of the software improve command configurations?",
        "516c4330-b779-45a7-8a03-68a81ffa7427": "What bug was fixed in the API related to the WebUI's log viewer?",
        "fe31679c-b55a-4762-a141-be80bbe7e68c": "What is the purpose of enabling the optional feature in the given context information?",
        "6cc0dfaf-1254-4e78-9b07-81d2da42e223": "How does the default setting of the optional feature affect the tensor_fusion_threshold and tensor_fusion_cycle_time?",
        "c916a689-35eb-4d39-89f3-c1a53331aa0a": "What is the default size, in bytes, of /dev/shm for Determined task containers?",
        "6dafbdc5-7483-4262-9796-f1b8ae939405": "Can the size of /dev/shm for Determined task containers be modified? If so, what is the default value and how can it be changed?",
        "b26003c6-2926-4ab2-ba63-8f71264003cb": "How does the priority scheduler handle experiments with different priority values? How does this differ when using Kubernetes?",
        "82898b92-1072-48f0-97b5-f323be53f5a6": "In what scenarios is the priority value ignored and managed by the configured workload manager instead?",
        "7ae3978d-afae-472d-84f9-ea67f85249e9": "What is the purpose of using the CLI command \"det oauth client remove <client ID>\"?",
        "454f4d2e-a11c-49ee-8119-84fcdf384e9a": "How can OAuth clients be removed using the CLI?",
        "3fb8b07c-0fc9-41d3-a5a7-40721a0dcd70": "How does the Editor role differ from the Viewer role in terms of permissions and responsibilities?",
        "8f47cd9c-6507-4c34-9c2f-3a9bc29339bc": "What actions can an Editor perform within their scope, and how do these actions supersede the permissions of the Viewer role?",
        "0b366ba0-0167-4904-971c-73e41aafc004": "How does the IdP's certificate contribute to the validation of assertions in the context of authentication and authorization?",
        "81e556c0-01c5-4311-abbc-66e1dfd5f325": "What is the significance of the path to the IdP's certificate in the process of validating assertions?",
        "c8dcab86-9f47-4619-accc-ed54b7e07b1b": "What are the differences between the CPU and GPU environments mentioned in the context information?",
        "f6efbac1-3989-457d-a0fa-a17453454a80": "How do the versions of PyTorch and TensorFlow differ across the three environments mentioned in the context information?",
        "9292b668-17b0-42ea-98aa-5f89c2a7521e": "How can you create folders on a bucket to store objects in the Data Fabric UI?",
        "64ad6641-0877-4e7b-aa9c-25e8dd10752a": "What maprcli command can be used to implement the features described in the context information?",
        "faba35fb-fdc8-4a0c-98d4-e59a6e913a62": "What are the steps involved in preparing the Determined repository to run devcluster?",
        "18195a24-015a-46d3-9f20-c7eee8543361": "What are the configuration settings that need to be specified in the ~/.devcluster.yaml file for running Determined?",
        "a638a524-7a5e-4b49-8131-6e7d3e714378": "What is the purpose of log compaction in the HPE Ezmeral Data Fabric? How does it ensure efficient storage and retrieval of messages?",
        "9538fb36-8ce8-4d3c-9652-400a8a966e40": "Explain the concept of a global namespace in the HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "f75216d3-a030-46f4-b0f3-9b15d7b34e65": "How can you increase the resource capacity of an HPE Ezmeral Unified Analytics Software cluster by adding additional user-provided hosts?",
        "c377b66f-2f46-4032-bfcf-2791bb21da62": "What steps should be taken to identify and debug issues related to logging in HPE Ezmeral Unified Analytics Software?",
        "e705282a-f1d0-4620-a60c-dc2af7325dd6": "What are the required connection parameters for connecting to an Oracle database in HPE Ezmeral Unified Analytics Software? Provide a brief description of each parameter.",
        "3cdb5e4b-3393-43b5-87f7-07c1bc9fad09": "How can you enable caching while querying an Oracle database in HPE Ezmeral Unified Analytics Software? What is the default value for this parameter?",
        "7f5ea351-1a4d-4426-87fb-85c5231bb1db": "What is the purpose of an edge node in a MapR Converged Data Platform environment? How does it differ from a client node?",
        "9933124e-86eb-445a-9fa4-c8f2f91b6b9e": "Explain the concept of an access control list (ACL) in the context of HPE Ezmeral Data Fabric. How does an ACL specify user permissions for performing actions on an object?",
        "398c1028-83a8-471f-8d8c-fe5a393214df": "What are the prerequisites for setting up a GKE cluster?",
        "2ff864c8-0501-4035-a80a-4a22976f70b7": "Why is it important to have Google Cloud SDK and kubectl installed before setting up a GKE cluster?",
        "9280a5c7-234f-4653-918e-2ca5a3c78b0e": "How can the key for tagging the Determined agent instances be customized?",
        "aff99e86-87d6-4aba-b23e-8be37b2dccb3": "As a teacher, how would you ensure that the questions for the quiz/examination are diverse in nature based on the given context information?",
        "89deaf43-8b45-4129-82fa-e713bd40efa6": "How does the shared hierarchical namespace in the file system contribute to organizing data in a structured manner?",
        "56ceebde-c856-4436-b048-d81a9a237696": "In what ways can the shared hierarchical namespace be beneficial for users in managing and accessing their files and data?",
        "2ea9947a-e0fb-4286-9b10-a5ffc32a9342": "How can the computational overhead in the training loop be reduced when using a GPU?",
        "e1c1bf19-d34d-4f69-8019-c2d0293db398": "What optimizations does Determined use to reduce communication overhead in the third step of the training loop?",
        "ee134c66-c90d-4ad0-a60c-749ba5e11179": "What is the license for the \"commons-beanutils\" library mentioned in the context information?",
        "d5ae3199-4915-4c8e-be8e-16459a30ae96": "Which library in the context information is licensed under the GNU Lesser Public License?",
        "781fd761-0fa6-4b54-b6b4-3bf0d26f24cf": "How does YARN allocate memory for each map or reduce task?",
        "83b8e911-9d47-47d0-b537-3c3bfe042f26": "What is the purpose of ZooKeeper in distributed applications?",
        "bf585583-544a-44ba-b7fc-34778527a73c": "How can you expand the resource capacity of an HPE Ezmeral Unified Analytics Software cluster by adding additional user-provided hosts to the management cluster?",
        "3d2fc287-2d18-4e5a-ac7e-1ea8f987f957": "What steps should be taken to identify and debug issues related to monitoring in HPE Ezmeral Unified Analytics Software, specifically when alerts and notifications fail to display?",
        "a085b76a-0243-40e2-9c94-666346972c00": "What is EzPresto and how does it optimize query performance in HPE Ezmeral Unified Analytics Software?",
        "f6617eff-7c86-408e-9cbc-b325e2172971": "How can users connect EzPresto to multiple data sources in HPE Ezmeral Unified Analytics Software and what are the benefits of caching data sets?",
        "0abb20d5-203e-4770-af18-46dee53a19e2": "What is the purpose of a data node in a data-fabric cluster? How does it contribute to the overall functioning of the cluster?",
        "eee4c273-2ace-4e33-bf0a-d1bb8e545818": "Explain the concept of a global namespace in the context of HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "d3a84f2c-657c-4725-95c1-79cd488d3cdb": "What is the purpose of manually offloading data to a cold tier in HPE Ezmeral Data Fabric? How does this process help in managing disk space?",
        "e3e7c89a-501b-456b-9543-ef5f1a1d27f9": "How can a fabric user manually offload data from a volume to a cold tier in HPE Ezmeral Data Fabric? Explain the steps involved in triggering the offload operation.",
        "f51c2a33-4565-4fdb-9d6c-1cc2cb728db1": "What types of data does Data Fabric support tiering for?",
        "15b297df-bd29-4f9c-8d8d-fc6e4a1da4b1": "Can tables and streams be tiered in Data Fabric?",
        "45d667b7-9b89-4688-bae9-4fa48998e8c9": "How does Determined support individual users or user groups as security principals?",
        "3d57cdbe-fa78-4ca7-bc28-0d5014ada93b": "Can you provide an example of a security principal performing an action on a resource in the context of Determined?",
        "895fe686-98f0-4dbe-a5d9-34cf8012d299": "What is the purpose of the `load_model_from_checkpoint_pathpath` function in the determined.keras module?",
        "2a01a260-c6ba-4a58-a0e6-0f9666848f2e": "How can you specify which tags to load from the TensorFlow SavedModel when using the `load_model_from_checkpoint_pathpath` function?",
        "7fdd71f6-a77e-44b8-80ef-a122818fdbf8": "What is the purpose of a data node in a data-fabric cluster? How does it contribute to the overall functioning of the cluster?",
        "8b118cc2-49b3-4c1a-8fb6-c99b793961b1": "Explain the concept of a global namespace in the context of HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "9963b123-cdd1-408c-a425-4d46aacab3c5": "What is the default value for using spot instances?",
        "43947d77-e745-44f8-91d6-8dcf05348655": "Can you explain the concept of spot instances and their use in more detail?",
        "50a1f00a-2814-4ba4-b56c-942ef2283456": "What is the purpose of visiting the \"Common Configuration Options\" in the WebUI?",
        "a58a389b-c837-4c2b-bae8-d71803426316": "How can the WebUI collect anonymous information and what kind of information does it collect?",
        "884e7a12-bd1a-4c95-8da9-269ad5491136": "What are the two options available in the Data Access dialog when selecting \"Change to private access\"? How does each option affect the data source?",
        "4aa3acfa-cab4-453e-abda-3ced31662e2b": "How can a member be granted access to a data source? How does this differ from granting public access to a data source?",
        "115aa5f5-3c1f-4869-8ed7-eb2088bb0bb8": "What is the purpose of an access control list (ACL) in the MapR Converged Data Platform? How does it determine user permissions for specific actions on an object?",
        "9453770b-1869-4daa-9d61-3d562cf66ba8": "Explain the concept of a data container in a data-fabric cluster. What are the different types of data containers and how do they function in terms of replication roles?",
        "10337334-3290-40f3-9301-1303e3a34eb7": "What is the purpose of the Determined master port?",
        "5cf6b118-1d08-48ca-b4f2-dd2931028620": "Can you explain the significance of the Determined master port in the overall functioning of the system?",
        "6ab869c1-85db-4bd5-bb4b-12383805068a": "What is the purpose of clicking the Table view icon in the given context information?",
        "0353a183-e3e4-4061-a5fe-87b3b9aecbc5": "How can you access the fabric details page in the given context information?",
        "5880ea7a-a2df-4a7d-8241-f75531f16e2e": "What is the purpose of viewing the top fabrics that have consumed the maximum storage capacity in the Data Fabric?",
        "a23d2408-c078-4835-a5c8-5f7425d6e67c": "How can you navigate to the Resource page to view the fabric resources after clicking on the fabric area on the Fabric Storage card?",
        "8ee69c11-f6df-4769-9a95-631be2b3cb03": "What are the prerequisites for an on-premises installation of the HPE Ezmeral Data Fabric?",
        "5aadb765-cbfa-4a17-9fcb-6477ab4b33ae": "How can you create and use a local repository for RHEL, Rocky, or Oracle Linux?",
        "1d3b0b9f-5582-4aec-a745-8cbc5178c6c1": "What is the purpose of a Reducer in the determined.pytorch library? How does it contribute to the evaluation process?",
        "925077c2-2ae9-4bfb-931c-03c96b0ba9fc": "Explain the different types of aggregation methods available in the Reducer class of determined.pytorch. How can these methods be used to summarize evaluation metrics?",
        "e8c9a59d-7a28-4087-a20e-72a238067147": "How can you stop the mirroring of data that is in progress on a mirror volume in HPE Ezmeral Data Fabric? Provide a step-by-step procedure.",
        "17e9a111-b2c6-43e6-9ce0-1870d69de97f": "What is the purpose of the Mirrors tab in the Data Fabric UI? Explain how you can access this tab and perform actions related to mirroring volumes.",
        "fcd98428-a325-4e90-88a8-0ae024623c45": "What is the purpose of setting the network interface during distributed training in Determined? How does Determined automatically determine the network interface to use?",
        "5f197416-2e12-48f1-a2dc-9c497a0a128a": "How can the dtrain_network_interface option be used in Determined to explicitly set the network interface? Provide an example of how it can be used.",
        "f69abe2a-7386-432f-9ba7-3878fa630835": "How can you determine the fabric storage consumption trends by individual users and groups using the Data Fabric UI?",
        "be057e2b-605e-46bc-a49f-e724cd49a20f": "What is the aggregated storage size of volumes and topics owned by a user or group in the Data Fabric UI?",
        "2aa59382-86c0-4d3f-a38d-3eca8c38019d": "How does Determined currently support reproducibility in deep learning trials?",
        "b4062ed0-fc8a-4bb1-9d6a-ce2adf7da945": "Why is achieving perfect reproducibility challenging in deep learning using the hardware and software stack typically used?",
        "603b73ba-7e85-4263-be77-fa16edb84e47": "How does the DeepSpeedTrialContext class in the DeepSpeedTrial API ensure gradient aggregation, checkpointing, and fault tolerance in a Determined workflow?",
        "0048faae-c492-4933-9fd1-7d0436b96c3a": "What is the purpose of the disable_dataset_reproducibility_checks() method in the DeepSpeedTrialContext class?",
        "7551122a-6247-4920-aace-6385c3f7a2ab": "How does enabling SCIM contribute to the overall functionality of Determined? Explain the steps involved in enabling SCIM and setting the authentication mode and credentials.",
        "5350d03e-c5f6-4f0e-9bfb-bbcf7fc8ebad": "In what ways can the authentication mode and necessary credentials be customized in Determined? Provide examples of different authentication modes and the corresponding credentials that can be set.",
        "7d6c89de-583b-4c56-a38b-0c1264e49b10": "What is the purpose of the \"det\" command line tool and how can it be installed?",
        "7d7621ca-3032-4e25-8e12-e7942dca09bd": "What is the main component included in the determined library that allows users to interact with it through the command line?",
        "eaae6d76-767c-4530-bb50-ccf5b8c95fff": "What is the purpose of setting up a DNS record in the given context?",
        "a63409af-3dbc-4332-9fca-2644ad42606d": "Can you explain the information provided by Certbot after receiving the certificate?",
        "92330c4b-8d4c-4480-9369-19cc327919f9": "What are the different options available for launching training code on a cluster in the context of running experiments?",
        "a36f1221-111b-463c-a6d3-a2e4cd2e43dd": "Why is it considered good practice to separate the launcher that starts distributed workers from the training script in distributed training?",
        "65810abc-a08d-45d7-8c3a-1615ed04f437": "What are the different components that a user with the Viewer role can access within its scope?",
        "b6bcde95-f097-47bc-ae58-0d69fd4118a8": "Can a user with the Viewer role view experiment metadata and artifacts?",
        "c227bafd-3359-41c7-aa23-24e4f92218cc": "How does the optional parameter \"source_checkpoint_uuid\" differ from \"source_trial_id\" in the given context?",
        "31926098-bd9f-4dd0-ade2-09314aba272c": "As a teacher, how would you explain the importance of initializing weights from an arbitrary checkpoint in the context of this task?",
        "53b257eb-9d46-4dfb-89c3-b42deb25aed9": "What is the purpose of a mirror schedule in data storage systems? How does it help in disaster recovery?",
        "e9684185-bef0-4b11-98f0-b955f5c87d5a": "How does setting the mirror schedule impact the recovery point objective (RPO) of an organization? Explain with an example.",
        "88dc7259-0b2e-4ff0-b8b1-787dc03c1fef": "How can you query model checkpoints from trials and experiments using Determined's APIs?",
        "cc4bb986-b2d5-41b7-b7ff-803c67b0331b": "What are the different ways to load model checkpoints in a Python process with Determined?",
        "88d0a54f-a0cd-407c-a0f2-905acb75b74c": "What are the different roles that can be assigned to a user in the HPE Ezmeral Unified Analytics Software UI?",
        "f317be14-6d0c-4555-b617-775b599acff2": "How can a user be removed from the system in the HPE Ezmeral Unified Analytics Software UI?",
        "645d9ae2-7d27-4a1c-bd7e-6709f326ad07": "How can administrators add and remove users in HPE Ezmeral Unified Analytics Software?",
        "e57aeb9a-77fe-40ea-a954-b273ca7cf5c1": "What are the steps to onboard a user programmatically through the Kubernetes API in HPE Ezmeral Unified Analytics Software?",
        "b4f51d2b-3261-4a79-8b7c-f64f4cd970bf": "How does enabling OpenTelemetry impact the functionality of a system? Provide examples to support your answer.",
        "066daf4f-6d08-4041-9baf-4b4db3d1d31d": "Explain the default setting of OpenTelemetry and its significance. How does it affect the behavior of a system?",
        "58172f4f-1f84-459b-9915-8221fd1f2842": "What breaking changes were introduced in the latest release of the software? How do these changes affect the deployment process and the Model Registry APIs?",
        "6f78d5ab-844d-4e96-8a87-a2cd83a38fe7": "How does the new release of the software address the issue of representing infinite and NaN metric values in JSON and the WebUI? Explain the changes made and their impact on reporting and visualization of metrics.",
        "84f006d5-ec63-4f06-8e2b-edf491abecf3": "How can you edit your script to report epoch-based metrics to the Determined master?",
        "6029fd16-21c5-4e4c-b385-6c8c00b86073": "What tutorials are available to learn the basics of working with Determined and how to port existing code to the Determined environment?",
        "93693842-914d-44fd-a271-5d59aad64f34": "What is the purpose of tainting nodes in a Kubernetes cluster? How does it restrict the scheduling of pods?",
        "e77118c0-8ad4-438d-8d02-90ab855ac936": "How can taints be added to an existing resource in a Kubernetes cluster? Explain the role of tolerations in Pods and how they can be included in the Pod specification.",
        "91e09e82-3973-48cb-b1f4-72de2ccc5944": "True or False: By default, Prometheus is enabled. Is this statement correct?",
        "5f4d8f1f-d6d2-4d7b-9fe6-13c284d6c960": "As a teacher, explain why it is important to consider whether Prometheus is enabled or not in a system configuration.",
        "6500afc4-7544-4a93-8c8b-6af4a5aa97f4": "What are the key steps involved in setting up a new training environment in Determined?",
        "f7acdc80-8a5c-4aa2-a198-3c612ccabbfd": "How can Determined be helpful for someone who has never used it before and wants to quickly set up a training environment?",
        "3c098a84-8769-4872-af0e-ef8de129f0b5": "How can you access the Presto UI from the HPE Ezmeral Unified Analytics Software UI?",
        "dc8aec40-42b3-493c-91d8-f832bbf75526": "What steps should be taken to view the stack trace for failed queries in the Presto UI?",
        "84d549ef-37d9-4325-bac5-2a1f2712f5a6": "What is an int hyperparameter and how is it defined in the context of this document?",
        "b3da6f58-1e03-41b7-8e90-36260b71a233": "How does the count key in a grid search relate to the minval and maxval keys in defining the grid points for an int hyperparameter?",
        "f9b0efb1-2d84-4b31-aa6b-7f8ce1f02524": "What is the purpose of using kubectl with AKS after creating a cluster?",
        "00cc7df8-73a1-480c-929a-796ac7bc43c8": "How can users create or update the cluster kubeconfig in order to use kubectl with AKS?",
        "6b930611-c825-4af7-a9ad-6e5700ee24ad": "What is the purpose of the replication factor in a data-fabric cluster? How does it affect the availability of writes to the cluster?",
        "8f2d7c28-ee11-4963-a918-73b1a243f2d0": "Explain the concept of re-replication in a data-fabric cluster. What are some possible reasons for re-replication to occur?",
        "23762d6e-3f44-4e86-81f4-ca215fa2b6c6": "How can you create a local repository for an air-gapped installation of the HPE Ezmeral Data Fabric? Explain the steps involved and the purpose of setting up a local repository in this scenario.",
        "84563ebe-30b9-40a6-ada9-1f16f00e1d3b": "Compare and contrast the process of creating a local repository for RHEL, Rocky, or Oracle Linux with creating a local repository for SLES and Ubuntu. Highlight any key differences or similarities in the setup and usage of the local repositories for these operating systems.",
        "e223b986-92a0-4f6f-bff5-08d55c297bf0": "How can multiple security policies be assigned to one or more volumes in HPE Ezmeral Data Fabric? Provide a step-by-step procedure to accomplish this task using the Data Fabric UI.",
        "d7505d98-124f-426d-9c9e-8233db7b22e2": "What are the prerequisites for assigning multiple security policies to volumes in HPE Ezmeral Data Fabric? Explain the role of a fabric manager in performing this operation and describe the steps involved in assigning security policies to volumes using the Data Fabric UI.",
        "806b24f7-54d6-4929-b87d-5cc16935d3de": "How did the recent update address issues related to uploads to Google Cloud Storage?",
        "38491b3d-3faa-44f6-914e-4b31085335c6": "What bug was fixed in the CLI that prevented secure websocket connections to the master?",
        "36d1dbbb-a1b4-468f-b847-a5b16a842486": "What is the purpose of a data node in a data-fabric cluster? How does it contribute to the overall functioning of the cluster?",
        "2b37e814-032a-4c19-9caf-18c88b795e9f": "Explain the concept of a global namespace in the context of HPE Ezmeral Data Fabric. How does it enable the management of globally deployed data as a single resource?",
        "76258636-b936-4eaf-95fe-0084fa5182b5": "Explain the concept of log compaction and its significance in a data-fabric cluster. How does log compaction ensure the retention of the latest version of messages published to a topic partition?",
        "8b3c8a00-ef4c-41be-b1b4-e2241a8299a9": "Discuss the role and importance of the ResourceManager (RM) in a YARN cluster. How does the ResourceManager manage cluster resources and schedule applications?"
    },
    "corpus": {
        "94a7ba5e-4969-4373-a93d-df835de6e750": "A model has a unique name, an optional description, user-defined metadata, and zero or more model versions. A model\u2019s metadata can contain arbitrary information about the model. The following is an example JSON representation of a model for illustration. { \"mnist_cnn\": { \"description\": \"a character recognition model\", \"metadata\": { \"dataset_url\": \"http://yann.lecun.com/exdb/mnist/\", \"git_url\": \"http://github.com/user/repo\" }, \"versions\": [] } }",
        "f6642fdd-d314-4ae8-86cb-7ba37ab77f7d": "To find out how to view and modify the Job Queue in the WebUI, start with View the Job Queue.",
        "75895fdb-a1aa-4ff5-9853-50ca24bd5e38": "Tasks receive a proportional amount of the available resources depending on the resource they require and their weight.",
        "e14c9567-705f-4187-97a9-835350ee463a": "To get started on GCP, you will need to create a project. The following GCP APIs must be enabled on your GCP project: Cloud Filestore API Cloud Resource Manager API Cloud SQL Admin API IAM API Service Networking API Cloud Logging API",
        "18069236-4588-46a0-b697-66f707cd4fbb": "Viewing a List of Users Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Viewing a List of Users Describes how to display a searchable list of Keycloak users that includes the names of     the users and their roles. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . User and Role Management This page describes the roles supported by the HPE Ezmeral Data Fabric as-a-service platform. Viewing a List of Users Describes how to display a searchable list of Keycloak users that includes the names of     the users and their roles. Configuring Email Notifications Describes how to configure the Simple Mail Transfer Protocol (SMTP) to send email     notifications from the Data Fabric UI to specified email     accounts. Viewing and Editing Access Control Information Describes how to find and use the Access Control card that shows the access privileges     for users and groups. Access Control Expression Syntax This topic explains access control expression. Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Viewing a List of Users Describes how to display a searchable list of Keycloak users that includes the names of\n    the users and their roles. The User management card can display only a small number of Keycloak\n      users. Clicking the View all button displays a searchable,\n      configurable, full-page listing of Keycloak users who have access to the Data Fabric UI . If you are a fabric manager, you can also edit\n      the roles for a user. To display the full list of Keycloak users: Sign in to the Data Fabric UI , and switch to the Fabric manager or Infrastructure admin experience. Click Security administration . Scroll down to the User management card. Click View all . The list of Keycloak users is displayed. (Topic last modified: 2024-01-15) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "3d77431d-dba4-4f6a-be0b-f6e56f2dc62e": "On the Data Volumes tab, select your\n                                                  user directory or the shared directory. NOTE The following image shows an\n                                                  example of a user ( bob ) directory\n                                                  and a shared directory: If you do\n                                                  not see your user directory or the shared directory, contact your\n                                                  administrator. Click Upload to upload the Spark\n                                                  application files to the user/ or shared/ directory. Return to the browser you were working in with\n                                                  the Configure Spark\n                                                  Application wizard. Click Browse , and\n                                                  navigate to the location where you uploaded files. Select the Spark application files. Using S3 To use S3 as the source,\n                                                  manually set the Spark configurations for S3 in\n                                                  Spark application YAML file. For example: Update your Spark application YAML\n                                                  with the following\n                                                  configurations: sparkConf:\n    spark.hadoop.fs.s3a.access.key: <S3-ACCESS_KEY>\n    spark.hadoop.fs.s3a.connection.ssl.enabled: \"true\"\n    spark.hadoop.fs.s3a.endpoint: <S3-endpoint>\n    spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n    spark.hadoop.fs.s3a.path.style.access: \"true\"\n    spark.hadoop.fs.s3a.secret.key: <S3-SECRET-KEY> Alternatively, you can set ACCESS_KEY and\n                                                  SECRET_KEY as environment\n                                                  variables: spark.kubernetes.driverEnv.AWS_ACCESS_KEY_ID: <ACCESS_KEY>\nspark.kubernetes.driverEnv.AWS_ACCESS_KEY_ID: <SECRET_KEY>\nspark.kubernetes.executorEnv.AWS_ACCESS_KEY_ID: <ACCESS_KEY>\nspark.kubernetes.executorEnv.AWS_SECRET_ACCESS_KEY: <SECRET_KEY> NOTE When you are using these options,\n                                                  tokens from the Secret mounted into the pod are\n                                                  not automatically refreshed. To learn more\n                                                  about configuration options, see Hadoop S3\n                                                  Client . However, for HPE-Curated Spark , you\n                                                  can use EzSparkAWSCredentialProvider which automatically sets credentials from the file\n                                                  automatically mounted in driver and executor pods\n                                                  by Kyverno policy. sparkConf:\n    spark.hadoop.fs.s3a.endpoint: <S3-endpoint>\n    spark.hadoop.fs.s3a.connection.ssl.enabled: \"true\"\n    spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.s3a.aws.credentials.provider: \"org.apache.spark.s3a.EzSparkAWSCredentialProvider\" spark.hadoop.fs.s3a.path.style.access: \"true\" If you are using Amazon S3, add the following\n                                                  configurations in addition to the previous\n                                                  configurations options to the sparkConf section. spark.driver.extraJavaOptions: -Djavax.net.ssl.trustStore=/etc/pki/java/cacerts\nspark.executor.extraJavaOptions: -Djavax.net.ssl.trustStore=/etc/pki/java/cacerts If S3 server is behind the proxy server, add\n                                                  the following configurations in addition to the\n                                                  previous configurations\n                                                  options. spark.hadoop.fs.s3a.proxy.host: \"web-proxy.corp.hpecorp.net\"\nspark.hadoop.fs.s3a.proxy.port: \"8080\" To create the Kubernetes Secret with\n                                                  Base64-encoded values for AWS_ACCESS_KEY_ID\n                                                  (username) and AWS_SECRET_ACCESS_KEY (password),\n                                                  use notebook. See Creating and Managing Notebook Servers . For example: Run kubectl apply\n                                                  -f for the following\n                                                  YAML: apiVersion: v1 \nkind: Secret\ndata: \n  AWS_ACCESS_KEY_ID: <Base64-encoded value; example: dXNlcg== >\n  AWS_SECRET_ACCESS_KEY: <Base64-encoded value; example:cGFzc3dvcmQ= > \nmetadata: \n  name: <K8s-secret-name-for-S3> \ntype: Opaque To learn more, see Securely Passing Spark Configuration Values . Using Other Select Other as the data\n                                                  source, to reference other locations of the\n                                                  application file. For example, to refer to main application files\n                                                  and dependency files, or to refer to a file inside\n                                                  the specific Spark image, use the local:// schema. local:///opt/mapr/spark/spark-3.2.0/examples/jars/spark-examples_2.12-3.2.0.16-eep-810.jar File Name: Manually enter the location and file name of the\n                                            application for the S3 and Other sources. For example: s3a://apps/my_application.jar For User Directory and Shared Directory , click Browse , and browse and select\n                                            files. NOTE Ensure the extension of the main application file\n                                                matches the selected application type. The extension\n                                                must be .py for Python, .jar for Java and Scala, and .r for R applications.",
        "9c4ac2d7-3018-4e89-b2d0-2f3a43385da7": "class determined.core.DistributedContext*rank: intsize: intlocal_rank: intlocal_size: intcross_rank: intcross_size: intchief_ip: Optional[str] = Nonepub_port: int = 12360pull_port: int = 12376port_offset: int = 0force_tcp: bool = False DistributedContext provides useful methods for effective distributed training. A DistributedContext has the following required args: rank: the index of this worker in the entire job size: the number of workers in the entire job local_rank: the index of this worker on this machine local_size: the number of workers on this machine cross_rank: the index of this machine in the entire job cross_size: the number of machines in the entire job Additionally, any time that cross_size > 1, you must also provide: chief_ip: the ip address to reach the chief worker (where rank==0) DistributedContext has .allgather(), .gather(), and .broadcast() methods, which are easy to use and which can be useful for coordinating work across workers, but it is not a replacement for the allgather/gather/broadcast operations in your particular distributed training framework. classmethod from_horovodhvd: Anychief_ip: Optional[str] = Nonedetermined.core._distributed.DistributedContext Create a DistributedContext using the provided hvd module to determine rank information. Example: import horovod.torch as hvd hvd.init() distributed = DistributedContext.from_horovod(hvd) The IP address for the chief worker is required whenever hvd.cross_size() > 1. The value may be provided using the chief_ip argument or the DET_CHIEF_IP environment variable. classmethod from_deepspeedchief_ip: Optional[str] = Nonedetermined.core._distributed.DistributedContext Create a DistributedContext using the standard deepspeed environment variables to determine rank information. The IP address for the chief worker is required whenever CROSS_SIZE > 1. The value may be provided using the chief_ip argument or the DET_CHIEF_IP environment variable. classmethod from_torch_distributedchief_ip: Optional[str] = Nonedetermined.core._distributed.DistributedContext Create a DistributedContext using the standard torch distributed environment variables to determine rank information. The IP address for the chief worker is required whenever CROSS_SIZE > 1. The value may be provided via the chief_ip argument or the DET_CHIEF_IP environment variable. get_rankint Return the rank of the process in the trial. The rank of a process is a unique ID within the trial. That is, no two processes in the same trial are assigned the same rank. get_local_rankint Return the rank of the process on the agent. The local rank of a process is a unique ID within a given agent and trial; that is, no two processes in the same trial that are executing on the same agent are assigned the same rank. get_sizeint Return the number of slots this trial is running on. get_num_agentsint Return the number of agents this trial is running on. gatherstuff: AnyOptional[List] Gather stuff to the chief. The chief returns a list of all stuff, and workers return None. gather() is not a replacement for the gather functionality of your distributed training framework. gather_localstuff: AnyOptional[List] Gather stuff to the local chief. The local chief returns a list of all stuff, and local workers return None. gather_local() is not a replacement for the gather functionality of your distributed training framework. allgatherstuff: AnyList Gather stuff to the chief and broadcast all of it back to the workers. allgather() is not a replacement for the allgather functionality of your distributed training framework. allgather_localstuff: AnyList Gather stuff to the local chief and broadcast all of it back to the local workers. allgather_local() is not a replacement for the allgather functionality of your distributed training framework. broadcaststuff: AnyAny Every worker gets the stuff sent by the chief. broadcast() is not a replacement for the broadcast functionality of your distributed training framework. broadcast_localstuff: Optional[Any] = NoneAny Every worker gets the stuff sent by the local chief. broadcast_local() is not a replacement for the broadcast functionality of your distributed training framework.",
        "ea27a9d3-052d-41a1-893c-bc28f7376e46": "A checkpoint includes the model definition (Python source code), experiment configuration file, network architecture, and the values of the model\u2019s parameters (i.e., weights) and hyperparameters. When using a stateful optimizer during training, checkpoints will also include the state of the optimizer (i.e., learning rate). You can also embed arbitrary metadata in checkpoints via a Python SDK. PyTorch trials are checkpointed as a state-dict.pth file. This file is created in a similar manner to the procedure described in the PyTorch documentation, but instead of the fields in that documentation, the dictionary will have four keys: models_state_dict, optimizers_state_dict, lr_schedulers_state_dict, and callbacks, which are the state_dict of the models, optimizers, LR schedulers, and callbacks respectively.",
        "d2834df4-0584-4feb-a5d0-3712e6a58532": "For example: vpc-0b5177b19511ee301 . You must provide a VPC, and the VPC must have an\n          internet gateway attached. Public subnet ID* The subnet ID to use in the selected VPC. For example: subnet-0445a49217546b101 . The public subnet must be accessible from the internet. (Topic last modified: 2023-08-27) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "f0a30b5c-0449-4833-997f-5960c5a6027f": "Release Date: June 21, 2023 Improvements Errors: Errors that return 404 or \u2018Not Found\u2019 codes now have standardized messaging using the format \u201c<task/trial/workspace etc.> <ID> not found\u201d. In addition, if RBAC is enabled, the error message includes a suffix to remind users to check their permissions. This is because with RBAC enabled, permission denied errors and not found errors both return a \u2018Not Found\u2019 response. Deprecated Features LightningAdapter is deprecated and will be removed in a future version. We recommend that PyTorch Lightning users migrate to the Core API. Bug Fixes Users: Resolved an issue that was causing an error when attempting to create a new user with a username that was previously used by a renamed user.",
        "17ff5505-676a-4c43-9a69-e3de3dd59cd4": "At the end of every training workload, batch metrics are collected and stored in the database, providing a granular view of model metrics over time. Batch metrics will appear in TensorBoard under the Determined group. The x-axis of each plot corresponds to the batch number. For example, a point at step 5 of the plot is the metric associated with the fifth batch seen.",
        "506466d9-bbe2-45e8-828b-d9f51ef233d9": "The bind_mounts section specifies directories that are bind-mounted into every container launched for this experiment. Bind mounts are often used to enable trial containers to access additional data that is not part of the model definition directory. This field should consist of an array of entries; each entry has the form described below. Users must ensure that the specified host paths are accessible on all agent hosts (e.g., by configuring a network file system appropriately).",
        "670717b9-1d1d-4fa0-80b5-96e0ce522bed": "Designating a Fabric as Global Policy Master Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Security Policies Add, edit, delete, and manage state of security policies. Designating a Fabric as Global Policy Master Designate a fabric in the global namespace as the global policy master. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. About Security Policy Domain Describes a security policy domain. Security Policy Implementation Workflow Describes the security policy workflow, in general, and the steps in implementing a         security policy. Security Policy Enforcement Process Describes the steps followed during security policy enforcement on         volumes. Understanding Access Control in a Security Policy The implications of permissions assigned to users and groups in a security         policy. Managing File and Directory ACEs Describes the implications of setting access control expressions on files and             directories. Security Policy Permissions Permissions define which administrative users can create, view, and modify security     policies. Administrators set the permissions on security policies through cluster-level and     security policy-level ACLs. Designating a Fabric as Global Policy Master Designate a fabric in the global namespace as the global policy master. Creating a Security Policy Add a security policy on the global policy master. Viewing a Security Policy View security policy details. Viewing All Security Policies View all security policies on the Data Fabric UI. Editing a Security Policy Make changes to a security policy. Assigning a Security Policy to One or More Volumes Assigning Multiple Security Policies to One or More Volumes Describes how to assign multiple security policies to volumes. Unassigning One or More Security Policies from a Volume Unassign a policy from a volume to which it has been previously assigned. Disabling a Security Policy Describes how to disable a security policy. Enabling a Security Policy Describes how to enable a security policy. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Designating a Fabric as Global Policy Master Designate a fabric in the global namespace as the global policy master. About this task A security policy domain is a group of fabrics that share data and use the same\n                security policies to control access to the data. A security policy domain consists\n                of a global policy master and zero or more member fabrics that constitute a global\n                security policy namespace. Before you can create security policies, one fabric must\n                be set as the global policy master, and security policies must be created and\n                managed only on the fabric that is designated as the global policy master. The primary fabric is auto-designated as the global policy master.\n                Hence, it is not required to explicitly designate a fabric as a global policy\n                master. NOTE: Every 15 minutes, the policies created on the global policy\n                master are mirrored on the member fabrics in the same global namespace. (Topic last modified: 2024-01-22) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "a26ee83f-a3f8-4382-92df-a58411c810fe": "Either AWS credentials or an IAM role with permissions to access AWS CloudFormation APIs. See the AWS Documentation for information on how to use AWS credentials. An AWS EC2 Keypair. You may also want to increase the EC2 instance limits on your account \u2014 the default instance limits on GPU instances are zero. The default configuration for det deploy can result in launching up to 5 g4dn.metal instances (which have 94 vCPUs each), which would exceed the default quota. AWS instance limits can be increased by submitting a request to the AWS Support Center.",
        "2d41d8d3-3be6-4e91-b9bd-5f2c41868f43": "Accessing the Keycloak Administration Console Jump to main content Get Started Platform Administration Reference Home Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . SSO Using Keycloak Describes how single sign-on (SSO) is implemented by using       Keycloak. Accessing the Keycloak Administration Console Describes how to start the Keycloak administration console so you can manage Keycloak       and your SSO users. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Release Notes These notes contain information about release 7.6.0 of the HPE Ezmeral Data Fabric as-a-service platform. Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. SSO Using Keycloak Describes how single sign-on (SSO) is implemented by using       Keycloak. Accessing the Keycloak Administration Console Describes how to start the Keycloak administration console so you can manage Keycloak       and your SSO users. Changing the Keycloak admin Password Describes how to change the default Keycloak admin password to prevent       unauthorized access to Keycloak and your Data Fabric user       information. Adding New Users to Keycloak Describes how to add new users in Keycloak so you can use them to sign in to the Data Fabric UI . Adding a Group to Keycloak Describes how to add a Keycloak user group. Integrating Your LDAP Directory with Keycloak Keycloak can interface with an external LDAP directory so that LDAP users can access       the Data Fabric UI . Completing SSO Setup Using the Data Fabric UI Describes how to configure the HPE Ezmeral Data Fabric to work       with your SSO server. Resetting the SSO Configuration Describes how to update your single sign-on (SSO) configuration information using the Data Fabric UI . Identifying All CLDB Nodes Explains how you can identify all the CLDB nodes in an HPE Ezmeral Data Fabric . Setting Up Clients Summarizes the steps for enabling client communication with the HPE Ezmeral Data Fabric . Upgrade This section contains information that describes how to upgrade the HPE Ezmeral Data Fabric as-a-service platform. User Assistance Describes how to access different resources that can help you learn how to use the HPE Ezmeral Data Fabric . Accessing the Keycloak Administration Console Describes how to start the Keycloak administration console so you can manage Keycloak\n      and your SSO users. If a new fabric has been created, you can access the Keycloak administration console by\n            using these steps: In a browser, append port 6443 to the URL for your first fabric. This is the URL\n                  provided by the seed node procedure following successful fabric creation. For\n                  example: https://<FQDN_for_host_node_first_fabric>:6443 Click Administration Console : The Sign In page is\n                  displayed: Sign in using the default credentials: Username: admin Password: p@ssw0rd IMPORTANT: HPE recommends that you change the password for the admin user soon after sign in. See Changing the Keycloak admin Password . (Topic last modified: 2023-11-27) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "81d699b3-666a-47ac-86da-390bbdb16ebe": "Platform Jump to main content Get Started Platform Administration Reference Home Platform This section contains conceptual information that can help you to understand and use     the HPE Ezmeral Data Fabric . HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Platform This section contains conceptual information that can help you to understand and use     the HPE Ezmeral Data Fabric . Data Fabric UI Describes the graphical user interface for the HPE Ezmeral Data Fabric . Global Namespace (GNS) Describes the data plane that connects all of your HPE Ezmeral Data Fabric deployments. Single Sign-On (SSO) Support Describes how the HPE Ezmeral Data Fabric supports single sign-on     (SSO). Iceberg Support Describes support for Iceberg in HPE Ezmeral Data Fabric 7.6.0. Fabric Resources Describes fabric resources. Data Storage Management Summarizes options that the HPE Ezmeral Data Fabric provides to         give you access to your data. AWS Architecture Notes Describes architectural considerations for the HPE Ezmeral Data Fabric software-as-a-service (SaaS) platform as deployed on     Amazon AWS. Platform This section contains conceptual information that can help you to understand and use\n    the HPE Ezmeral Data Fabric . IMPORTANT: To view platform information for the HPE Ezmeral Data Fabric \u2013 Customer Managed platform, see this\n        website . Data Fabric UI Describes the graphical user interface for the HPE Ezmeral Data Fabric . Global Namespace (GNS) Describes the data plane that connects all of your HPE Ezmeral Data Fabric deployments. Single Sign-On (SSO) Support Describes how the HPE Ezmeral Data Fabric supports single sign-on     (SSO). Iceberg Support Describes support for Iceberg in HPE Ezmeral Data Fabric 7.6.0. Fabric Resources Describes fabric resources. Data Storage Management Summarizes options that the HPE Ezmeral Data Fabric provides to         give you access to your data. AWS Architecture Notes Describes architectural considerations for the HPE Ezmeral Data Fabric software-as-a-service (SaaS) platform as deployed on     Amazon AWS. (Topic last modified: 2023-04-23) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "1c1e0330-6b22-4646-b430-990b8c41651f": "The WorkspaceCreator role grants the single permission to create new workspaces. It can only be assigned globally. By default, when a user creates a workspace, they automatically get assigned the WorkspaceAdmin role. This behavior can be configured using master config: security: authz: workspace_creator_assign_role: enabled: true role_id: ROLE_ID where ROLE_ID is the integer role identifier, as listed in det rbac list-roles. To disable the assignment of any roles to the newly created workspace, set enabled: false.",
        "b3513020-5035-4a74-b12c-50ac09451a9a": "Advisory quota and hard quota can be expressed in\n                            megabytes (MB), gigabytes (GB), or terabytes (TB). GB is the default\n                            unit. The advisory quota must be less than the hard quota. Follow the steps given below to set quota for a group. Procedure Log on to the Data Fabric UI . Select Fabric manager option from\n                    the dropdown on the Home page. Click Fabric Administration . Select the groups option in the dropdown for Quota by . Click Edit fabric quotas on the Quota by groups card. On the Edit Fabric Quotas dialog box, enter the values for advisory\n                    quota, hard quota. Change the unit, as required. Specify the Fabric reserve limit . Click Update. Alternatively, you can edit the group quota for an individual group from the Settings tab for a fabric. To get to the Settings tab, Select the Fabric user on\n                            the Home page. Click the table view icon on the Resources card and click the fabric\n                            name on the table view of resources. Results The specified quota is saved for the individual group. NOTE: The default quota specified for the fabric applies to other groups, unless a\n                    group-specific quota is set for any other group on the fabric. (Topic last modified: 2023-11-02) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "bf272d54-8e7f-4942-9021-42ffd3b4b168": "Value for labeling the Determined agent instances. Defaults to the master instance name if the master is on GCP, otherwise determined-ai-determined.",
        "77d62c62-aad5-4169-bef8-18da0dfdcf11": "data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage. minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation.",
        "7c687745-5b89-4aa3-8038-32f3a5afc311": "The CLI is the recommended way to create an experiment, although you can also use the WebUI to create from an existing experiment or trial. To create an experiment: $ det experiment create <configuration file> <context directory> The Experiment Configuration file is a YAML file that controls your experiment. The context directory contains relevant training code, which is uploaded to the master. The total size of the files in the context cannot exceed 95 MB. As a result, only very small datasets should be included. Instead, set up data loaders to read data from an external source. Refer to the Prepare Data section for more data loading options. Because project directories can include large artifacts that should not be packaged as part of the model definition, including data sets or compiled binaries, users can specify a .detignore file at the top level, which lists the file paths to be omitted from the model definition. The .detignore file uses the same syntax as .gitignore. Byte-compiled Python files, including .pyc files and __pycache__ directories, are always ignored.",
        "7d89aec4-413e-4042-a056-3ce8419b8c23": "By default, the Kubernetes scheduler does not support gang scheduling or preemption. This can be problematic for distributed deep learning workloads that require multiple pods to be scheduled before execution starts. Determined includes built-in support for the lightweight coscheduling plugin, which extends the default Kubernetes scheduler to support gang scheduling. Determined also includes support for priority-based preemption scheduling. Neither are enabled by default. For more details and instructions on how to enable the coscheduling plugin, refer to Gang Scheduling and Priority Scheduling with Preemption.",
        "341710f1-e7b4-4dba-89ff-8219e7dc0a4f": "How long to wait for agents to start up before retrying. This string is a sequence of decimal numbers, each with optional fraction and a unit suffix, such as \u201c30s\u201d, \u201c1h\u201d, or \u201c1m30s\u201d. Valid time units are \u201cs\u201d, \u201cm\u201d, \u201ch\u201d. The default value is 20m.",
        "80e98c90-44c0-41f3-8bae-8dbab35a34d2": "The first step is to define an InferenceProcessor. You should initialize your model in the __init__() function of the InferenceProcessor. You should implement process_batch() function with inference logic. You can optionally implement on_checkpoint_start() and on_finish() to be run before every checkpoint and after all the data has been processed, respectively. For an example of how to accomplish this, visit our Torch Batch Process Embeddings example. \"\"\" Define custom processor class \"\"\" class InferenceProcessor(TorchBatchProcessor): def __init__(self, context): self.context = context self.model = context.prepare_model_for_inference(get_model()) self.output = [] self.last_index = 0 def process_batch(self, batch, batch_idx) -> None: model_input = batch[0] model_input = self.context.to_device(model_input) with torch.no_grad(): with self.profiler as p: pred = self.model(model_input) p.step() output = {\"predictions\": pred, \"input\": batch} self.output.append(output) self.last_index = batch_idx def on_checkpoint_start(self): \"\"\" During checkpoint, we persist prediction result \"\"\" if len(self.output) == 0: return file_name = f\"prediction_output_{self.last_index}\" with self.context.upload_path() as path: file_path = pathlib.Path(path, file_name) torch.save(self.output, file_path) self.output = []",
        "8d952cee-7502-4eda-892a-10e63b18537f": "Optional. A list of environment variables that will be set in every trial container. Each element of the list should be a string of the form NAME=VALUE. See Environment Variables for more details. You can customize environment variables for CUDA (NVIDIA GPU), CPU, and ROCm (AMD GPU) tasks differently by specifying a dict with cuda (gpu prior to 0.17.6), cpu, and rocm keys.",
        "1518b00a-baa8-483c-9437-f51d9a7263fe": "Specifies defaults for all task containers. A task represents a single schedulable unit, such as a trial, command, or TensorBoard.",
        "7a8e19b2-27c2-403a-aa50-c5966afa9af9": "YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. data fabric A collection of nodes that work together under a unified architecture, along with the\n            services or technologies running on that architecture. A fabric is similar to a Linux\n            cluster. Fabrics help you manage your data, making it possible to access, integrate,\n            model, analyze, and provision your data seamlessly. (Topic last modified: 2023-06-16) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "81d5ab64-31f5-41e8-96ce-b42f54490070": "Use Determined\u2019s built-in resource management. This is an easier alternative to installing and administering via Kubernetes or Slurm. Ideal for teams of any size to share dedicated compute resources. Compatible with on-prem clusters and cloud auto-scaling (AWS and GCP). Deploy on Prem Install Determined Using Linux Packages Install Determined Using Docker Deploy on AWS Deploy on GCP",
        "cf2438d9-c669-4de2-b01b-0bbd048255d0": "By default, Determined runs task containers as root. However, it is possible to associate a Determined user with a Unix user and group, provided that the Unix user and group already exist. Tasks initiated by the associated Determined user will run under the linked Unix user rather than root. For more information, see: Run Tasks as Specific Agent Users.",
        "ee1e8e7e-3a1e-4ae8-a6a9-b96a98ce415b": "minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation. When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system.",
        "8501a294-95f9-4e4c-9619-8391817c1b88": "Installing Custom Packages in Kubeflow Notebooks at Runtime Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Notebook Images Overview Describes notebook images available HPE Ezmeral Unified Analytics Software and their     uses. Creating and Managing Notebook Servers Describes how to create and manage notebook servers in HPE Ezmeral Unified Analytics Software . Creating GPU-Enabled Notebook Servers Describes how to create and deploy the GPU-enabled notebook servers. Building Custom Kubeflow Jupyter Notebook Image Describes how to build the custom Kubeflow Jupyter notebook image. Installing Custom Packages in Kubeflow Notebooks at Runtime Describes how to install custom packages in existing Kubeflow notebooks that persist     between restarts. Enabling Kale Extension in Kubeflow Notebook Describes how to enable and use the Kale extension, and specify GPU       resources using Kale extension in a Kubeflow notebook. Notebook Magic Functions Jupyter notebook magic functions, also known as magics, are special commands that     provide notebook functions that might not be easy for you to program using Python. HPE Ezmeral Unified Analytics Software supports line magics and cell magics. Creating the Conda Environment Describes how to create a conda environment in HPE Ezmeral Unified Analytics Software . Accessing MinIO S3 using Boto3 Describes how to use Boto3 to interact wtih MinIO from a Jupyter Notebook. Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Installing Custom Packages in Kubeflow Notebooks at Runtime Describes how to install custom packages in existing Kubeflow notebooks that persist\n    between restarts. You can only install custom packages in a Kubeflow notebook in connected HPE Ezmeral Unified Analytics Software environments only for two types of packages \u2013 conda packages and pip packages. You cannot install custom packages in air-gapped environments. Packages installed to the base\n      environment do not persist; the packages are removed after the notebook restarts. By default, the base conda environment is activated for all notebook users. All notebook\n      users can perform the following tasks: Install packages to the base environment Create and install your own conda environment Use the conda environment of another notebook user, if permitted by the environment\n          owner You can install packages that persist (save between restarts). You can also install packages\n      that do not persist (do not save between restarts). If packages do not have to persist between restarts, install the packages to the\n          base conda environment. This applies to both single-user and multi-user modes. If packages must persist between restarts, create an individual conda\n          environment. This applies to both single-user and multi-user modes. The following sections describe how to create an individual conda environment where you can\n      install packages that persist between restarts in single and multi-user modes: NOTE Run\n        commands in the notebook terminal. Single-User Mode Complete the following steps in the notebook if you want to install custom packages that\n        persist between restarts in single-user mode: Create an individual conda\n            environment: conda create --prefix ~/.conda/envs/kf-users-env --clone base Activate the conda environment: conda activate kf-users-env Multi-User Mode Any user with access to the notebook, typically the owner, can create the conda\n        environment. The conda environment is shared with other users (between contributors). All\n        users get equivalent permissions. Users can use the existing packages, as well as install\n        and remove the packages.",
        "674835fc-6600-4c52-aa9d-83c8ab548ffe": "The ExperimentReference class is a reference to an experiment. The reference contains the top_checkpoint() method. Without arguments, the method will check the experiment configuration searcher field for the metric and smaller_is_better values. These values are used to sort the experiment\u2019s checkpoints by validation performance. The searcher settings in the following snippet from an experiment configuration file will result in checkpoints being sorted by the loss metric in ascending order. searcher: metric: \"loss\" smaller_is_better: true The following snippet of Python code can be run after the specified experiment has generated a checkpoint. It returns an instance of Checkpoint representing the checkpoint that has the best validation metric. from determined.experimental import client checkpoint = client.get_experiment(id).top_checkpoint() Checkpoints can be sorted by any metric using the sort_by keyword argument, which defines which metric to use, and smaller_is_better, which defines whether to sort the checkpoints in ascending or descending order with respect to the specified metric. from determined.experimental import client checkpoint = ( client.get_experiment(id).top_checkpoint(sort_by=\"accuracy\", smaller_is_better=False) ) You may also query multiple checkpoints at the same time using the top_n_checkpoints() method. Only the single best checkpoint from each trial is considered; out of those, the checkpoints with the best validation metric values are returned in sorted order, with the best one first. For example, the following snippet returns the top five checkpoints from distinct trials of a specified experiment. from determined.experimental import client checkpoints = client.get_experiment(id).top_n_checkpoints(5) This method also accepts sort_by and smaller_is_better arguments. TrialReference is used for fine-grained control over checkpoint selection within a trial. It contains a top_checkpoint() method, which mirrors top_checkpoint() for an experiment. It also contains select_checkpoint(), which offers three ways to query checkpoints: best: Returns the best checkpoint based on validation metrics as discussed above. When using best, smaller_is_better and sort_by are also accepted. latest: Returns the most recent checkpoint for the trial. uuid: Returns the checkpoint with the specified UUID. The following snippet showcases how to use the different modes for selecting checkpoints. from determined.experimental import client trial = client.get_trial(id) best_checkpoint = trial.top_checkpoint() most_accurate_checkpoint = trial.select_checkpoint( best=True, sort_by=\"accuracy\", smaller_is_better=False ) most_recent_checkpoint = trial.select_checkpoint(latest=True) specific_checkpoint = client.get_checkpoint(uuid=\"uuid-for-checkpoint\")",
        "1c304f43-82f2-4304-98bb-0857469f2b14": "The det deploy tool is provided by the determined Python package. It uses AWS CloudFormation to automatically deploy and configure a Determined cluster. CloudFormation builds the necessary components for Determined into a single CloudFormation stack.",
        "33c0ea29-e6fe-494e-9367-2154942a0d70": "After configuration, you can initialize the model engine in the DeepSpeedTrial. The following example corresponds to the experiment configuration above, with a field in the hyperparameters section named overwrite_deepspeed_args. class MyTrial(DeepSpeedTrial): def __init__(self, context: DeepSpeedTrialContext) -> None: self.context = context self.args = AttrDict(self.context.get_hparams()) model = Net(self.args) ds_config = overwrite_deepspeed_config( self.args.deepspeed_config, self.args.get(\"overwrite_deepspeed_args\", {}) ) parameters = filter(lambda p: p.requires_grad, model.parameters()) model_engine, __, __, __ = deepspeed.initialize( model=model, model_parameters=parameters, config=ds_config ) self.model_engine = self.context.wrap_model_engine(model_engine) After the model engine is initialized, you need to register it with Determined by calling wrap_model_engine(). Differing from PyTorchTrial, you do not need to register the optimizer or learning rate scheduler with Determined because both are attributes of the model engine. If you want to use pipeline parallelism with a given model, pass layers of the model for partitioning to the DeepSpeed PipelineModule before creating the pipeline model engine: net = ... net = deepspeed.PipelineModule( layers=get_layers(net), loss_fn=torch.nn.CrossEntropyLoss(), num_stages=args.pipeline_parallel_size, ..., )",
        "e20bd75c-2aa4-423d-be54-27ea6f77ca45": "For CPU-only configurations, you need to set slotType: cpu as well as slotResourceRequests.cpu: <number of CPUs per slot> in values.yaml. Please note that the number of CPUs allocatable by Kubernetes may be lower than the number of \u201chardware\u201d CPU cores. For example, an 8-core node may provide 7.91 CPUs, with the rest allocated for the Kubernetes system tasks. If slotResourceRequests.cpu was set to 8 in this example, the pods would fail to allocate, so it should be set to a lower number instead, such as 7.5. Then, similarly to GPU-based configuration, maxSlotsPerPod needs to be set to the greatest common divisor of all the node sizes. For example, if you have 16-core nodes with 15 allocatable CPUs, it\u2019s reasonable to set maxSlotsPerPod: 1 and slotResourceRequests.cpu: 15. If you have some 32-core nodes and some 64-core nodes, and you want to use finer-grained slotResourceRequests.cpu: 15, set maxSlotsPerPod: 2.",
        "2b1ebf09-4d45-4eff-bbbf-e9a13f36cf32": "Monitoring Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Adding an OTel Endpoint Describes how to add an OTel endpoint to a fabric. Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . OpenTelemetry (OTel) is an observability framework that allows you to instrument,\n            generate, collect, and export telemetry data. For more information on OTel, see the official OpenTelemetry documentation . The OTel endpoint provides centralized monitoring for your HPE Ezmeral Data Fabric deployments. Use OTel to generate metrics and\n            logs for your fabrics, manage your OTel deployments through the Data Fabric UI , and view the generated telemetry data\n            through EZ Central. Adding an OTel Endpoint Describes how to add an OTel endpoint to a fabric. (Topic last modified: 2023-10-27) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "85804159-fd19-432f-be9c-46c68b28fe4e": "When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. volume A tree of files and directories grouped for the purpose of applying a policy or set of\n        policies to all of them at once.",
        "cd3df421-759e-4b17-9bdf-0071ba086acd": "When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. super user The user that has administrative access to the data-fabric cluster.",
        "6236d16a-1514-4eef-9c54-c303d46f5d8f": "When Determined is configured to use Podman, the containers are launched in rootless mode. Your HPC cluster administrator should have completed most of the configuration for you, but there may be additional per-user configuration that is required. Before attempting to launch Determined jobs, verify that you can run simple Podman containers on a compute node. For example: podman run hello-world If you are unable to do that successfully, then one or more of the following configuration changes may be required in your $HOME/.config/containers/storage.conf file: Podman does not support rootless container storage on distributed file systems (e.g. NFS, Lustre, GPSF). On a typical HPC cluster, user directories are on a distributed file system and the default container storage location of $HOME/.local/share/containers/storage is therefore not supported. If this is the case on your HPC cluster, configure the graphroot option in your storage.conf to specify a local file system available on compute nodes. Alternatively, you can request that your system administrator configure the rootless_storage_path in /etc/containers/storage.conf on all compute nodes. Podman utilizes the directory specified by the environment variable XDG_RUNTIME_DIR. Normally, this is provided by the login process. Slurm and PBS, however, do not provide this variable when launching jobs on compute nodes. When XDG_RUNTIME_DIR is not defined, Podman attempts to create the directory /run/user/$UID for this purpose. If /run/user is not writable by a non-root user, then Podman commands will fail with a permission error. To avoid this problem, configure the runroot option in your storage.conf to a writeable local directory available on all compute nodes. Alternatively, you can request your system administrator to configure the /run/user to be user-writable on all compute nodes. Create or update $HOME/.config/containers/storage.conf as required to resolve the issues above. The example storage.conf file below uses the file system /tmp, but there may be a more appropriate file system on your HPC cluster that you should specify for this purpose. [storage] driver = \"overlay\" graphroot = \"/tmp/$USER/storage\" runroot = \"/tmp/$USER/run\" Any changes to your storage.conf should be applied using the command: podman system migrate",
        "657719e8-580e-4101-a167-4a1ece850592": "Recalling Data to the Data Fabric File System Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Data Tiering Conceptual information about data tiering. Recalling Data to the Data Fabric File System HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Creating a Standard Volume Procedure to create standard volume. Creating a Mirror Volume Procedure to create mirror volume. Converting Standard Volume to Mirror Volume Editing a Volume Edit accountable entity, volume access for accountable entity and volume hard         quota. Renaming a Volume Rename a volume. Viewing Volume Endpoint Info View volume endpoint information. Viewing Object Endpoint Info to Remotely Access Files as Objects View endpoint information for files in a volume to be able to access the files as         objects when accessed by S3 client. Downloading Volume Endpoint Information Download JSON file containing endpoint information for the selected volume endpoint         information. Deleting a Volume Delete a single volume. Administering Volume Snapshots Snapshot overview and administering snapshots. Data Tiering Conceptual information about data tiering. Schedules for Volume Data Tiering Describes schedules for data tiering of volume data Manually Offloading Data to a Cold Tier Recalling Data to the Data Fabric File System Administering Storage Policies Manage storage policies related to data tiering. Administering Remote Targets Administering Schedules Introduction to schedules. Mirroring Synopsis of mirrors and mirroring process. Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Recalling Data to the Data Fabric File System Prerequisites You must be a fabric manager or a fabric user to perform this operation. Data tiering must have been enabled on the volume during the volume creation, to\n                    be able to offload/recall data. Offloaded data must be present on the cold tier. About this task When you read data that has been offloaded to a remote target (or cold tier), data is\n                automatically recalled to the file system to allow the read to succeed. The recalled data is automatically: Purged based on the expiration time period set at the volume level for\n                        recalled data if there are no changes (for example, read operation). Offloaded based on the rule and the expiration time period set at the volume\n                        level for recalled data if there are changes (for example, overwrite\n                        operation). For a cold tiering volume, you must explicitly recall the volume before running any\n                analytics jobs. Follow the steps given below to recall data manually from a volume to a cold\n                tier. Procedure Log on to the Data Fabric UI . Select Fabric user on the Home\n                    Page. Click the Table view icon on the Resources card. In the tabular list of\n                    fabrics, click the down arrow for the fabric that contains the volume. Click the ellipsis under Actions for the required volume. Select Recall data . Results The data recall operation begins and data is recalled on to\n            the fabric file system. (Topic last modified: 2023-11-01) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "63d6cc12-cad7-428c-b2b2-6e188d5a1720": "Viewing or Downloading Topic Connection Properties Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Viewing or Downloading Topic Connection Properties View and download connection properties related a topic for Apache Kafka Wire         Protocol. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Creating a Topic Create a topic for Apache Kafka Wire Protocol. Editing a Topic Edit a topic for Apache Kafka Wire Protocol. Deleting a Topic Delete a topic for Apache Kafka Wire Protocol. Viewing or Downloading Topic Connection Properties View and download connection properties related a topic for Apache Kafka Wire         Protocol. Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Viewing or Downloading Topic Connection Properties View and download connection properties related a topic for Apache Kafka Wire\n        Protocol. Prerequisites You must be a fabric user or a fabric manager to view and\n            download connection properties for topic. About this task You can view and/or download topic connection properties from the Data Fabric UI . Use the following steps to view or download\n                connection properties. Procedure Log on to the Data Fabric UI . Under the default Fabric user ,\n                    click the Table View icon on the Resources card. In the tabular list of\n                    fabrics, click the down arrow for the fabric that contains the topic. Click the View Connection Properties in the Endpoint column for\n                    the topic, to view the connection properties. If you want to download the connection properties, click Download on the Connection Properties dialog box. Results The connection properties file is downloaded to the default\n            downloads folder on the local machine. (Topic last modified: 2023-11-02) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "19f5fcfe-9da3-4843-8ff8-53a72eaed9cd": "Each version of Determined utilizes specifically-tagged Docker containers. The image tags referenced by default in this version of Determined are described below. Environment File Name CPUs determinedai/environments:py-3.8-pytorch-1.12-tf-2.11-cpu-2b7e2a1 NVIDIA GPUs determinedai/environments:cuda-11.3-pytorch-1.12-tf-2.11-gpu-2b7e2a1 AMD GPUs determinedai/environments:rocm-5.0-pytorch-1.10-tf-2.7-rocm-2b7e2a1 See Set Environment Images for the images Docker Hub location, and add each tagged image needed by your experiments to the image cache.",
        "56212782-f5bf-4da5-b085-34b16470b472": "It provides a shared hierarchical             namespace that is organized like a standard file system. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three\n            disks. For high-volume reads and writes, you can create larger storage pools when\n            initially formatting storage during cluster creation. NOTE: Storage pool refers to the combined storage capacity that is\n            obtained by combining one or more storage devices. Storage devices can be anything from\n            a very small disk drive to large arrays of  disk drives (each containing 20-30 drives). A storage pool is created to get a very large capacity of GBs/TBs/PBs available,\n                from which users are provided needed amounts of storage For example, one can\n            combine 10 hard disk drives of 4TB each, totaling to 40TBs. Now, one can either directly\n            use the 40TB as a single device or partition the space out to many smaller storage\n            capacities such as 100GB, 1TB and so on from this 40TB and provide that access to\n            different users. (Topic last modified: 2020-07-13) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "8e69b638-0df2-42e8-8379-c38c9799c067": "Optional. The credential to use with the account_url.",
        "4b6fca26-f282-4847-941b-46fd92e0dde9": "Release Date: September 3, 2020 New Features Support deploying Determined on Kubernetes. Determined workloads run as a collection of pods, which allows standard Kubernetes tools for logging, metrics, and tracing to be used. Determined is compatible with Kubernetes >= 1.15, including managed Kubernetes services such as Google Kubernetes Engine (GKE) and AWS Elastic Kubernetes Service (EKS). When using Determined with Kubernetes, we currently do not support fair-share scheduling, priority scheduling, per-experiment weights, or gang-scheduling for distributed training experiments; workloads will be scheduled according the behavior of the default Kubernetes scheduler. Users can configure the behavior of the pods that are launched for Determined workloads by specifying a custom pod spec. A default pod spec can be configured when installing Kubernetes, but a custom pod spec can also be specified on a per-task basis (e.g., via the environment.pod_spec field in the experiment configuration file). Support running multiple distributed training jobs on a single agent. In previous versions of Determined, a distributed training job could only be scheduled on an agent if it was configured to use all of the GPUs on that agent. In this release, that restriction has been lifted: for example, an agent with 8 GPUs can now be used to run two 4-GPU distributed training jobs. This feature is particularly useful as a way to improve utilization and fair resource allocation for smaller clusters. Improvements WebUI: Update primary navigation. The primary navigation is all to one side, and is now collapsible to maximize content space. WebUI: Trial details improvements: Update metrics selector to show the number of metrics selected to improve readability. Add the \u201cHas Checkpoint or Validation\u201d filter. Persist the \u201cHas Checkpoint or Validation\u201d filter setting across all trials, and persist the \u201cMetrics\u201d filter on trials of the same experiment. WebUI: Improve table pagination behavior. This will improve performance on Determined instances with many experiments. WebUI: Persist the sort order and sort column for the experiments, tasks, and trials tables to local storage. WebUI: Improve the default axes\u2019 ranges for metrics charts. Also, update the range as new data points arrive. Add a warning when the PyTorch LR scheduler incorrectly uses an unwrapped optimizer. When using PyTorch with Determined, LR schedulers should be constructed using an optimizer that has been wrapped via the wrap_optimizer() method. Add a reminder to remove sys.exit() if SystemExit exception is caught. Bug Fixes WebUI: Fix an issue where the recent task list did not apply the limit filter properly. Fix Keras and Estimator wrapping functions not returning the original objects when exporting checkpoints. Fix progress reporting for adaptive_asha searches that contain failed trials. Fix an issue that was causing OOM errors for some distributed EstimatorTrial experiments.",
        "91eeb6f6-59c6-4f2b-9bbc-ade958ff8206": "object Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster.",
        "739d2b94-66cf-49ea-b6ea-db5d81a9f58e": "Optional. Specifies the batch on which profiling should begin.",
        "48e9fba2-d5a7-493a-bff7-e1a9d8cf1bd8": "The name of the IdP. Currently (officially) supported: \u201cokta\u201d.",
        "c473e430-6bf5-4cd6-a295-fd6b0ca98c82": "Activities other than copying, distribution and modification are not\ncovered by this License; they are outside its scope.  The act of\nrunning a program using the Library is not restricted, and output from\nsuch a program is covered only if its contents constitute a work based\non the Library (independent of the use of the Library in a tool for\nwriting it).  Whether that is true depends on what the Library does\nand what the program that uses the Library does.\n\n\n  1. You may copy and distribute verbatim copies of the Library's\ncomplete source code as you receive it, in any medium, provided that\nyou conspicuously and appropriately publish on each copy an\nappropriate copyright notice and disclaimer of warranty; keep intact\nall the notices that refer to this License and to the absence of any\nwarranty; and distribute a copy of this License along with the\nLibrary.\n\n\n  You may charge a fee for the physical act of transferring a copy,\nand you may at your option offer warranty protection in exchange for a\nfee.\n\n\n  2. You may modify your copy or copies of the Library or any portion\nof it, thus forming a work based on the Library, and copy and\ndistribute such modifications or work under the terms of Section 1\nabove, provided that you also meet all of these conditions:\n\n\n    a) The modified work must itself be a software library.\n\n\n    b) You must cause the files modified to carry prominent notices\n    stating that you changed the files and the date of any change.\n\n\n    c) You must cause the whole of the work to be licensed at no\n    charge to all third parties under the terms of this License.\n\n\n    d) If a facility in the modified Library refers to a function or a\n    table of data to be supplied by an application program that uses\n    the facility, other than as an argument passed when the facility\n    is invoked, then you must make a good faith effort to ensure that,\n    in the event an application does not supply such function or\n    table, the facility still operates, and performs whatever part of\n    its purpose remains meaningful.\n\n\n    (For example, a function in a library to compute square roots has\n    a purpose that is entirely well-defined independent of the\n    application.  Therefore, Subsection 2d requires that any\n    application-supplied function or table used by this function must\n    be optional: if the application does not supply it, the square\n    root function must still compute square roots.)\n\n\nThese requirements apply to the modified work as a whole.  If\nidentifiable sections of that work are not derived from the Library,\nand can be reasonably considered independent and separate works in\nthemselves, then this License, and its terms, do not apply to those\nsections when you distribute them as separate works.  But when you\ndistribute the same sections as part of a whole which is a work based\non the Library, the distribution of the whole must be on the terms of\nthis License, whose permissions for other licensees extend to the\nentire whole, and thus to each and every part regardless of who wrote\nit.\n\n\nThus, it is not the intent of this section to claim rights or contest\nyour rights to work written entirely by you; rather, the intent is to\nexercise the right to control the distribution of derivative or\ncollective works based on the Library.\n\n\nIn addition, mere aggregation of another work not based on the Library\nwith the Library (or with a work based on the Library) on a volume of\na storage or distribution medium does not bring the other work under\nthe scope of this License.\n\n\n  3. You may opt to apply the terms of the ordinary GNU General Public\nLicense instead of this License to a given copy of the Library.  To do\nthis, you must alter all the notices that refer to this License, so\nthat they refer to the ordinary GNU General Public License, version 2,\ninstead of to this License.  (If a newer version than version 2 of the\nordinary GNU General Public License has appeared, then you can specify\nthat version instead if you wish.)  Do not make any other change in\nthese notices.\n\n\n  Once this change is made in a given copy, it is irreversible for\nthat copy, so the ordinary GNU General Public License applies to all\nsubsequent copies and derivative works made from that copy.\n\n\n  This option is useful when you wish to copy part of the code of\nthe Library into a program that is not a library.",
        "457c2990-dc76-4b10-8eb1-f11ea2c0b5a5": "This guide will help you get up and running with the Core API. Visit the API reference det.core API Reference In this user guide, we\u2019ll show you how to adapt model training code to use the Core API. As an example, we\u2019ll be working with the PyTorch MNIST model.",
        "0889a0cc-0b2d-422c-9260-86625e43c690": "These actions can include: Viewing fabric status and error information Viewing fabric access points Importing a fabric Creating a fabric Registering a fabric Adding an activation key Setting the billing model Importing an external S3 server Importing an external NFS server Upgrading fabric software (if a new software version is available) Deleting a fabric Reinitiating (retrying) an upgrade operation Viewing the Fabric Status To view the fabric status: Sign in to the Data Fabric UI , and switch to the Fabric manager view. Click Global namespace . Click the Table view icon to display the resource table with\n            status values. Fabric Status Information Fabric status values include: Status Description Active An activation key has been added as described in Adding an Activation Key . Deleting Fabric removal, as described in Deleting a Fabric , is\n                  currently in progress. Expired The activation key for the fabric is no longer valid. Inactive An activation key has not been added for the fabric. See Adding an Activation Key . Install Failed There was a problem during installation, and the fabric was not successfully\n                  installed. Installed Fabric installation completed successfully. Installing Fabric installation is currently in progress. Upgrade Failed There was a problem during the software upgrade, and the fabric was not\n                  successfully upgraded. Upgrading A software upgrade is currently in progress. (Topic last modified: 2023-11-05) On this page About the Global namespace Card Viewing the Fabric Status Fabric Status Information \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "5daaacbb-6ddc-4d70-9e65-206198a0bb2a": "Training API Guides: Learn how to use the training APIs including the Core API and the High-Level APIs. Inference API Guides: Learn how to use the batch processing API.",
        "56594034-dd9d-4785-b51c-faab5a528dc0": "About Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . About Provides an overview of HPE Ezmeral Unified Analytics Software . Tutorials Provides a set of tutorials that you can use to experience HPE Ezmeral Unified Analytics Software and the included     applications, such as tutorials for data science and data analytics workflows with notebooks and     applications like Spark, MLflow, Feast, Airflow, and EzPresto. Resources Provides links to additional resources such as product licensing information, on-demand     training, videos, blogs, and HPE Ezmeral Unified Analytics Software community. Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. About Provides an overview of HPE Ezmeral Unified Analytics Software . HPE Ezmeral Unified Analytics Software is usage-based\n      Software-as-a-Service (SaaS) that fully manages, supports, and maintains hybrid and\n      multi-cloud modern analytical workloads through open-source tools. HPE Ezmeral Unified Analytics Software separates compute and\n      storage for flexible, cost-efficient scalability to securely access data stored in multiple\n      data platforms through a simple user interface, which is easily installed and deployed on\n      private, public, and on-premises infrastructure. Features and Functionality HPE Ezmeral Unified Analytics Software provides the\n        following features and functionality in a single UX: Access data anywhere and manage it in one place Connect bidirectionally to multiple data platforms and join data to create a\n              federated data mesh that you manage in one place. Includes authentication,\n              authorization, logging, metrics collection, and monitoring. Robust, integrated storage layer Includes an integrated, scalable data fabric storage layer with data-mesh like\n              capabilities as the ephemeral storage for all types of data, including structured and\n              unstructured data, files, objects, and streams. Analytical workloads Support for the most common enterprise analytics use cases ranging from traditional\n              BI/Reporting (via PrestoDB and SparkSQL interfaces) to emerging workloads, such as\n              exploratory data science, real-time analytics, and machine learning workflows. Self-service data access All users, including administrators, data engineers, data analysts, and data\n              scientists can directly access data from HPE Ezmeral Unified Analytics Software . Built-in access to BI dashboards and data science tools Includes built-in BI dashboards for analytics and operational reporting, Also\n              includes web-based notebook interfaces, such as Jupyter Lab and Visual Studio, for\n              data science workflows (model training and serving frameworks). Built-in SSO Supports single sign-on experience; users sign in to access HPE Ezmeral Unified Analytics Software and compute\n              components integrate with the storage platform infrastructure to pass the identity of\n              each user. Performance Distributed, in-memory caching ( explicit) that accelerates\n              federated queries on commonly used datasets. Compute Components The compute components included in HPE Ezmeral Unified Analytics Software enable users to get up and running in minutes. Components\n        connect to each other at start-up and use pre-defined storage areas in the built-in data\n        fabric. When applicable, compute components can automatically take advantage of GPUs. The following list describes the compute components included in HPE Ezmeral Unified Analytics Software : Spark Spark is a primary engine for data analytics tasks. EzPresto EzPresto is a distributed SQL query\n              engine with a built-in query federation capability (distributed in-memory caching and\n              pushdown optimizations) for fast analytic queries on data of any size. Kubeflow Kubeflow as an ML framework focused on model training that includes Notebooks,\n              Pipelines (Airflow), Experiments, Kserve, and various distributed training\n              operators. Airflow Airflow for data engineering and task automation. Notebooks Jupyter notebooks for performing varied data science tasks, such as cleaning data,\n              labeling features, testing toy models, and launching distributed training models. Dashboard Frameworks Dashboard frameworks for building data models and visualizations.",
        "93be7c77-4b51-4565-a247-3d86a5add810": "Release Date: June 14, 2022 Bug Fixes Web: Update task cards to only truncate task UUIDs and leave experiment IDs alone. CLI: Fix an issue for det task logs where trial task IDs and checkpoint GC task IDs could not be used. Agent: Fix being unable to use control-C to cancel the agent when it is connecting to master. Trial: Fix a bug where the rendezvous timeout warning could be printed erroneously. Commands: Fix an issue for commands where setting an environment variable as FOO instead of FOO=bar in environment.environment_variables causes the agent to panic. Fixes Prevent certain hangs when using one of Determined\u2019s built-in launchers, which begin in release 0.18.0. These hangs were caused by wrapper processes seeing SIGTERM but not passing it to their child process. Supports running in containers that do not have a /bin/which path, such as python-slim. The error was caused by accidentally hardcoding /bin/which instead of letting the shell find which on the path. Automatically add a determined_version key to the metadata of checkpoints created by any of the Trial APIs. This automatic key was accidentally dropped in release 0.18.0. Note that Core API checkpoints have full control over their checkpoint metadata and so are unaffected. Improvements Scheduler: Tasks now release resources as they become free instead of holding them until all resources are free. CLI: det deploy aws up, det deploy aws down, and det deploy gcp down now take --yes to skip prompts asking for confirmation. --no-prompt is still usable. Experiments: When attempting to delete an experiment, if the delete fails it is now retryable. Agents: Improve behavior and observability when agents lose WebSocket messages due to network failures. Trials: Trial logs will report some system events such as when a trial gets canceled, paused, killed, or preempted. New Features Kubernetes: Specifying observability.enable_prometheus in Helm will now correctly enable Prometheus monitoring routes. Kubernetes: Users may now specify a checkpointStorage.prefix in the Determined Helm chart if using S3 buckets for checkpoint storage. Checkpoints will now be uploaded with the path prefix whereas before it was ignored. CLI: Add new command det experiment logs <experiment-id> to get logs of the first trial of an experiment. Flags from det trial logs are supported. Configuration: Add support for checkpointStorage.prefix in master and experiment configuration for Google Cloud Storage (gcs). Security Fixes API: Endpoints under /debug/pprof now require authentication.",
        "8d304caa-9e33-4ba0-bf34-438809e5114a": "super user The user that has administrative access to the data-fabric cluster. (Topic last modified: 2020-07-13) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "5a1a92cb-faf2-4dc6-a021-d4a7e983b4a6": "Trainer accepts a test_mode parameter which, if true, trains and validates your training code for only one batch, checkpoints, then exits. This is helpful for debugging code or writing automated tests around your model code. trainer.fit( max_length=pytorch.Epoch(1), checkpoint_period=pytorch.Batch(100), validation_period=pytorch.Batch(100), + test_mode=True )",
        "074593ce-2e20-4579-8e78-e0d07d8d17b7": "Using Spark SQL API Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Spark Provides a brief overview of Apache Spark in HPE Ezmeral Unified Analytics Software . Using Spark Images Describes different types of Spark images supported by HPE Ezmeral Unified Analytics Software . List of Spark Images Lists the Spark images distributed by HPE Ezmeral Unified Analytics Software . These         images enables you to run the Spark applications in an air-gapped environment. Creating Spark Applications Describes how to create Spark applications using HPE Ezmeral Unified Analytics Software . Managing Spark Applications Describes how to view and manage Spark applications using HPE Ezmeral Unified Analytics Software . Configuring Memory for Spark Applications Describes how to set memory options for Spark applications. Creating Interactive Sessions Describes how to create interactive sessions in HPE Ezmeral Unified Analytics Software . Submitting Statements Describes how to submit statements in HPE Ezmeral Unified Analytics Software . Managing Interactive Sessions Describes how to view and manage Spark interactive sessions in HPE Ezmeral Unified Analytics Software . Spark History Server Provides an overview of Spark History Server. Using Spark SQL API Describes how to use Spark SQL API in HPE Ezmeral Unified Analytics Software . Enabling GPU Support for Spark Describes NVIDIA spark-rapids accelerator support for Spark, and how     to enable and allocate the GPU resources on Spark. Securely Passing Spark Configuration Values Describes how to pass the sensitive data to Spark configuration using the Kubernetes     Secret. Running Spark Applications in Namespaces Describes how namespaces work with regard to Spark applications in HPE Ezmeral Unified Analytics Software . Using whylogs with Spark Describes how to use whylogs with Spark. Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Using Spark SQL API Describes how to use Spark SQL API in HPE Ezmeral Unified Analytics Software . In HPE Ezmeral Unified Analytics Software , you can use the Spark SQL API in two different\n      ways: External Metastore NOTE There will be some limitations to integration with external metastore. To integrate Spark with external metastore, follow these steps: Set the metastore URI with the spark.hive.metastore.uris config option.\n          This URI should be public and accessible from your Spark applications. Set the value of spark.sql.warehouse.dir property to the same value as\n          that of external metastore. For example: if you want to query a managed table then the\n          path to that managed table must match in both metastore and Spark runtime. Verify that the metastore host can accept external connections so that Spark can connect\n          to the metastore. Configure the gateway rules for securing the metastore as the metastore\n          doesn\u2019t have authentication and authorization. Verify that your Spark applications are querying the data from locations accessible\n          within the Spark runtime. Temporary Views The temporary view is a feature in the Spark DataFrame API. You can read data and create a\n        temporary view for the data by using the temporary view feature. These views are not global\n        and cannot be shared between any two Spark applications. You can use a temporary view in the\n        following two scenarios: If the schema is available for your data, use DataFrame:create[OrReplace]TempView . Some file formats already include\n          schema, for example, parquet files or CSV files with the header. You can read the file,\n          create a DataFrame and then call the create[OrReplace]TempView function\n          and give it the view name and finally, you can query data using Spark SQL API. If the schema is not available for your data, you can set it while creating or\n          converting the DataFrame, then create the temporary view. By default, Spark sets aliases\n          for the column names like underscore 1, underscore 2, and so on, however, you can set your\n          own column names. On this page External Metastore Temporary Views Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "07496ed7-46cb-44d8-ad24-f8211eeb98cd": "SCIM integration applies only to Determined Enterprise Edition. Determined EE provides a System for Cross-domain Identity Management (SCIM) integration to allow administrators to easily and securely provision users and groups through their standard identity provider (IdP). Currently, the only officially supported provider is Okta; however, Determined implements a minimal working subset of the protocol as specified by RFC 7644 and is expected to work with most IdPs that adhere to this RFC.",
        "e55b9a24-cee0-4d02-bc64-a841d961ebfd": "The experiment contributes the following resource scheduling configuration. See the Experiment Configuration Reference for the full details for these configuration options. resource_pool: Identifies the queue/partition to be used. slots_per_trial: The number of slots to use for each trial of this experiment. slurm: sbatch_args: Array of Slurm options added as #SBATCH options in the generated batch script. slots_per_node: The minimum number of slots required for a node to be scheduled during a trial. gpu_type: The Slurm gres type of the GPU to be injected into any generated --gpus/\u2013gres expressions. By default, no type is specified. pbs: pbsbatch_args: Array of PBS options added as #PBS options in the generated batch script. slots_per_node: The minimum number of slots required for a node to be scheduled during a trial.",
        "ec070273-1571-4549-aade-bd150b3690ae": "To save checkpoints, add the store_path function to your script: # NEW: Save checkpoint. checkpoint_metadata_dict = {\"steps_completed\": steps_completed} # NEW: Here we are saving multiple files to our checkpoint # directory. 1) a model state file and 2) a file includes # information about the training loop state. with core_context.checkpoint.store_path(checkpoint_metadata_dict) as (path, storage_id): torch.save(model.state_dict(), path / \"checkpoint.pt\") with path.joinpath(\"state\").open(\"w\") as f: f.write(f\"{epochs_completed},{info.trial.trial_id}\")",
        "afa2ea98-217d-4ada-acc9-58c4954e52b2": "AMD/ROCm support is available only with Singularity containers. While Determined does add the proper Podman arguments to enable ROCm GPU support, the capabilities have not yet been verified. Launching experiments with slot_type: rocm, may fail with the error RuntimeError: No HIP GPUs are available. Ensure that the compute nodes are providing ROCm drivers and libraries compatible with the environment image that you are using and that they are available in the default locations, or are added to the path and/or ld_library_path variables in the slurm configuration. Depending upon your system configuration, you may need to select a different ROCm image. See Set Environment Images for the images available. Launching experiments with slot_type: rocm, may fail in the AMD/ROCm libraries with with the error terminate called after throwing an instance of 'boost::filesystem::filesystem_error' what(): boost::filesystem::remove: Directory not empty: \"/tmp/miopen-.... A potential workaround is to disable the per-container /tmp by adding the following bind mount in your experiment configuration or globally by using the task_container_defaults section in your master configuration: bind_mounts: - host_path: /tmp container_path: /tmp",
        "e0c475d2-6c79-471e-84ca-e3710a343605": "For example, if user01 submits the Spark application as user02, the system\n                      automatically reverts the namespace back to user01 and runs\n                      the application in the user01 namespace. API/CLI (kubectl) Spark applications run in the user's designated namespace. Users can change the namespace to spark ; Spark applications\n                      run in the spark namespace and become accessible to all\n                      users. If a user changes the namespace in their Spark application, for example user01 changes the namespace to user02, the system accepts the Spark application, but returns an access denied error. Notebook Spark applications run in the user's designated namespace. If a user changes the namespace in their Spark application, for example user01 changes the namespace to user02, the system returns an access denied error. Airflow DAG A Spark application launched through an Airflow DAG automatically runs in\n                      the namespace of the user that deployed the DAG. For example, if user01 deploys a DAG with a Spark application in the\n                      workflow, the Spark application runs in the user01 namespace. Manually triggered DAGs launch in the namespace of the trigger event owner. Scheduled DAGs launch in the namespace of the last user to un-pause the\n                      DAG. Spark History Server In an HPE Ezmeral Unified Analytics Software cluster, one Spark History Server runs in the spark namespace. Users can go\n        to the Spark History Server UI to view a list of all Spark applications that have run.\n        However, users can only view the details of Spark applications that they submit, regardless\n        of the namespace they use (their own namespace or the spark namespace). If a user submits a Spark application in the spark namespace, only that\n        user can view the application details in the Spark History Server UI. For example, if user01 submits a spark application in the spark namespace, user02 cannot access the Spark application details in the Spark\n        History Server UI. Only user01 can view the Spark application details. The system returns an unauthorized message when users try to view application details for\n        Spark applications that were submitted by other users. Setting Security Context for Spark OSS\n        Images The Spark OSS images do not contain the security context required to run\n        Spark applications against volumes in HPE Ezmeral Unified Analytics Software . HPE Ezmeral Unified Analytics Software denies user access to the volume if it cannot authenticate the user,\n        which results in Spark application failures. To add security context to your Spark\n      application, add the following configuration setting in the Spark application\n        YAML: sparkConf:\n    spark.hpe.webhook.security.context.autoconfigure: \"true\" This security context flag sets the pod security context and enables HPE Ezmeral Unified Analytics Software to recognize you as a\n        valid HPE Ezmeral Unified Analytics Software user\n        when you run your Spark applications. When you add the security context flag to the\n        Spark application YAML and run the Spark application, the application automatically runs in\n        your user-designated namespace. If you change the namespace to spark , the\n        Spark application runs in the spark namespace. WARNING Do not\n          set the security context in HPE-Curated Spark images. Setting the security context in\n          HPE-Curated Spark images causes Spark applications to fail. For additional\n        information, see User Isolation and Setting the User Context . On this page Spark History Server Setting Security Context for Spark OSS\n        Images Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "4855050d-d62a-43a5-ac9f-ccbb707963d0": "When configuring your IdP to allow users to SSO to Determined, you will need to specify the location of Determined\u2019s SSO URL and the audience URL. The audience URL should be set to the Determined master\u2019s base URL and the SSO endpoint is at that base URL with a path of /saml/sso. Determined also requires an additional attribute named userName with name format unspecified set to the username of the user attempting SSO (e.g., for Okta, this is the attribute value user.login).",
        "502c5c88-2d0e-43ba-b8a4-8be74428132a": "Each resource pool may specify a task_container_defaults that applies additional defaults on top of the top-level setting (and partition_overrides for Slurm/PBS) for all tasks launched in that resource pool. When applying the defaults, individual fields override prior values, and list fields are appended.",
        "530f9478-54ae-4d50-8541-ce05a93db349": "Viewing Table Information Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Viewing Table Information The topics in this section describe viewing table information. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Managing Tables The topics in this section describe managing tables. Managing Column Families and Columns The topics in this section describe managing column families and columns. Viewing Table Information The topics in this section describe viewing table information. Viewing the List of Tables View the list of tables created on a fabric. Viewing Column Families View column families created on a table. Managing Table Replication The topics in this section describe managing table replication. Administering Access Controls for Tables This topic describes administering access controls for tables. Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Viewing Table Information The topics in this section describe viewing table information. Viewing the List of Tables View the list of tables created on a fabric. Viewing Column Families View column families created on a table. (Topic last modified: 2023-10-23) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "85be390a-3697-4572-b98e-02a94f4d96cd": "fluentd settings.",
        "481d36a6-79a6-4115-9adc-383d0301bcb8": "Next, we need to configure incoming webhooks for our Slack application. In your Slack application\u2019s management page go to the Incoming Webhooks section. Enable the toggle for Activate Incoming Webhooks as shown below. Now that webhooks are enabled we can set up a new webhook integration. Click the Add New Webhook to Workspace button at the bottom of the page. On the next page you will be asked to select the channel that will receive webhook updates. Choose a channel and then press the Allow button and you will be taken back to the Incoming Webhooks page.",
        "255bece7-aca3-47ce-88e5-5bf01f73b749": "Release Date: April 13, 2022 Bug Fixes Resource Pool: Fix a bug that causes the resource pool and resource manager to crash after submitting a command with a non-default priority. We recommend that all users on 0.17.12 and 0.17.13 update to 0.17.14 or later.",
        "3a934b6d-bc06-43f4-bbad-718aeb86ed78": "Importing Applications and Managing the Application Lifecycle Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Installation Provides links to HPE Ezmeral Unified Analytics Software installation and service activation topics. Identity and Access Management Describes identity and access management in HPE Ezmeral Unified Analytics Software . Expanding the Cluster Describes how to add additional user-provided hosts to the management cluster to increase resource capacity and how     to expand the cluster to include the additional user-provided hosts. Shutting Down an HPE Ezmeral Unified Analytics Software Cluster Describes how to gracefully shut down an HPE Ezmeral Unified Analytics Software cluster when you want to perform maintenance or upgrade     tasks. Importing Applications and Managing the Application Lifecycle Describes how to import, manage, and secure applications and frameworks in HPE Ezmeral Unified Analytics Software . Connecting to External S3 Object Stores Describes how to connect HPE Ezmeral Unified Analytics Software to external S3 object storage in AWS, MinIO, and HPE Ezmeral Data Fabric Object Store . Connecting to External HPE Ezmeral Data Fabric Clusters Describes how to connect HPE Ezmeral Unified Analytics Software to an external HPE Ezmeral Data Fabric cluster. Configuring Endpoints Describes the endpoints in HPE Ezmeral Unified Analytics Software and how to configure them. GPU Support Provides information about support for NVIDIA GPU, MIG partitioning, preparing hosts     for GPU-enabled environment, adding hosts and enabling GPU in HPE Ezmeral Unified Analytics Software . Troubleshooting Describes how to identify and debug issues in HPE Ezmeral Unified Analytics Software . Installation Describes how to identify and debug issues during installation. Host (Node) Management Describes how to identify and debug issues for hosts. Metering Describes how to identify and debug issues for metering. Monitoring Describes how to identify and debug issues for monitoring. Logging Describes how to identify and debug issues for logging. Airflow Describes how to identify and debug issues for Airflow. EzPresto Describes how to identify and debug issues for EzPresto . Superset Describes how to identify and debug issues for Superset. Spark Describes how to identify and debug issues for Spark. Importing Applications and Managing the Application Lifecycle Describes how to identify and debug issues while importing applications and managing     the application lifecycle. Security Describes how to identify and debug issues related to security. GPU Describes how to identify and debug issues for GPU. User Interface Describes how to identify and debug issues related to the HPE Ezmeral Unified Analytics Software UI. Support Matrix The tables on this page show the tools and frameworks, HPE Ezmeral Data Fabric ,operating system       versions , and GPU models that are supported for HPE Ezmeral Unified Analytics Software releases. Release Notes This document provides a comprehensive overview of the latest updates and enhancements     in HPE Ezmeral Unified Analytics Software (version     1.3.0), including new features, improvements, bug fixes, and known issues. Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Importing Applications and Managing the Application Lifecycle Describes how to identify and debug issues while importing applications and managing\n    the application lifecycle. Downloading the application chart version fails. If downloading the application chart version fails, verify the chart is present in\n              the chartmuseum repository. Importing applications results in an error. If you get errors while importing applications, Check the error state in the application tile. Check job logs in ezapp-system namespace. Importing applications after fixing the application charts. If you need to import the application after fixing the application chart, follow\n              these steps: Delete the previously imported application. Update the chart version. Re-package the application. Import the re-packaged application. The Open button within the application tile is not working and the endpoint URL is\n            missing from the tile. Verify that values.yaml file includes the ezua section. Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "84b02b04-268d-481f-967c-37bded376c33": "Configuration for commands to run when certain events occur. The value of each option in this section is an array of strings specifying the command and its arguments.",
        "612a5ab3-2241-4d3e-97a6-520ac6dbab48": "The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage.",
        "bd2a6f0d-7190-44c5-a953-c24ee3011340": "The launcher has the following additional requirements on the installation node: Support for an RPM or Debian-based package installer Java 1.8 or greater Sudo is configured to process configuration files present in the /etc/sudoers.d directory Access to the Slurm or PBS command-line interface for the cluster Access to a cluster-wide file system with a consistent path names across the cluster",
        "06065b3b-4038-4985-8b30-af33a10f0957": "The environment section defines properties of the container environment that is used to execute workloads for this experiment. For more information on customizing the trial environment, refer to Customizing Your Environment.",
        "2ffef67c-f70a-409f-b969-89091f6475bb": "Restoring a Volume from Volume Snapshot Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Administering Volume Snapshots Snapshot overview and administering snapshots. Restoring a Volume from Volume Snapshot Describes how to restore a volume from a volume snapshot. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Creating a Standard Volume Procedure to create standard volume. Creating a Mirror Volume Procedure to create mirror volume. Converting Standard Volume to Mirror Volume Editing a Volume Edit accountable entity, volume access for accountable entity and volume hard         quota. Renaming a Volume Rename a volume. Viewing Volume Endpoint Info View volume endpoint information. Viewing Object Endpoint Info to Remotely Access Files as Objects View endpoint information for files in a volume to be able to access the files as         objects when accessed by S3 client. Downloading Volume Endpoint Information Download JSON file containing endpoint information for the selected volume endpoint         information. Deleting a Volume Delete a single volume. Administering Volume Snapshots Snapshot overview and administering snapshots. Schedules for Volume Snapshots Describes schedules for snapshots Creating a Volume Snapshot Create volume snapshot manually via Data Fabric UI . Scheduling Volume Snapshots Assign schedule to volume for creation of volume snapshots. Preserving a Volume Snapshot Preserve a volume snapshot. Restoring a Volume from Volume Snapshot Describes how to restore a volume from a volume snapshot. Deleting a Volume Snapshot Delete a volume snapshot. Data Tiering Conceptual information about data tiering. Mirroring Synopsis of mirrors and mirroring process. Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Restoring a Volume from Volume Snapshot Describes how to restore a volume from a volume snapshot. Prerequisites You must be a fabric user to restore a volume from the volume snapshot. About this task You can restore volume data to a specific point in time with volume snapshots. Follow the steps given below to restore a volume from a volume snapshot. Procedure Log on to the Data Fabric UI . Select the Fabric user option on\n                    the Home page. Click the Table View icon on the Resources card. In the tabular list of\n                    fabrics, click the down arrow for the fabric that contains the volume whose\n                    snapshot is to be restored. Click the volume name seen under Resource Name . Navigate to the Snapshots tab. Select the checkbox for the snapshot from which you wish to restore the\n                    volume. Click the ellipsis seen next to the snapshot to restore from. Click the Restore menu option. Click Restore on the message box that appears. Results The volume is restored from the snapshot. A message informing you about successful\n                restoration of volume is displayed on the Data Fabric UI. The volume snapshot is preserved indefinitely in the .snapshot directory on the volume mount path. Related maprcli Commands To implement the features described on this page, the\n                Data Fabric UI relies on the following maprcli command. The\n                command is provided for general reference. For more information, see maprcli Commands in This Guide . volume snapshot restore (Topic last modified: 2023-11-02) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "9734b08b-6d3f-40a0-96d8-6be9e25c00c9": "The adaptive_asha search method employs multiple calls to the asynchronous successive halving algorithm (ASHA) which is suitable for large-scale experiments with hundreds or thousands of trials.",
        "464c5a63-8d21-44a9-a083-f6e63648d6ab": "This step is similar to Step 7, except that it introduces hyperparameter search and executes full training for each trial. Configure your system the same as Step 7: Confirm that your experiment configuration does not specify resources.slots_per_trial or that it is set to 1. For example: resources: slots_per_trial: 1 Create an experiment without the --test or --local arguments: You might find the --follow, or -f, argument helpful: det experiment create myconfig.yaml my_model_dir -f Diagnose failures: If Step 7 worked but this step does not, check: Check if the error happens when the experiment configuration has searcher.source_trial_id set. One possibility in an actual experiment that does not occur in a --test experiment is the loading of a previous checkpoint. Errors when loading from a checkpoint can be caused by architectural changes, where the new model code is not architecturally compatible with the old model code. Generally, issues in this step are caused by doing training and evaluation continuously. Focus on how that change can cause issues with your code.",
        "1b678035-c296-4050-8123-fec77535de57": "To analyze a single Determined experiment using TensorBoard, use det tensorboard start <experiment-id>: $ det tensorboard start 7 Scheduling TensorBoard (rarely-cute-man) (id: aab49ba5-3357-4145-861c-7e6ff2d702c5)... TensorBoard (rarely-cute-man) was assigned to an agent... Scheduling tensorboard tensorboard (id: c68c9fc9-7eed-475b-a50f-fd78406d7c83)... TensorBoard is running at: http://localhost:8080/proxy/c68c9fc9-7eed-475b-a50f-fd78406d7c83/ disconnecting websocket The Determined master schedules a TensorBoard instance within the cluster. Once the TensorBoard instance is running, The Determined CLI opens the TensorBoard web interface in your local browser. To view information about scheduled and running TensorBoard instances, use: $ det tensorboard list Id | Owner | Description | State | Experiment Id | Trial Ids | Exit Status --------------------------------------+------------+-------------------------------------+------------+-----------------+-------------+-------------- aab49ba5-3357-4145-861c-7e6ff2d702c5 | determined | TensorBoard (rarely-cute-man) | RUNNING | 7 | N/A | N/A",
        "ed37d09f-f7f6-4422-94b5-a46676779b96": "Editing a Storage Policy Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Data Tiering Conceptual information about data tiering. Administering Storage Policies Manage storage policies related to data tiering. Editing a Storage Policy HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Creating a Standard Volume Procedure to create standard volume. Creating a Mirror Volume Procedure to create mirror volume. Converting Standard Volume to Mirror Volume Editing a Volume Edit accountable entity, volume access for accountable entity and volume hard         quota. Renaming a Volume Rename a volume. Viewing Volume Endpoint Info View volume endpoint information. Viewing Object Endpoint Info to Remotely Access Files as Objects View endpoint information for files in a volume to be able to access the files as         objects when accessed by S3 client. Downloading Volume Endpoint Information Download JSON file containing endpoint information for the selected volume endpoint         information. Deleting a Volume Delete a single volume. Administering Volume Snapshots Snapshot overview and administering snapshots. Data Tiering Conceptual information about data tiering. Schedules for Volume Data Tiering Describes schedules for data tiering of volume data Manually Offloading Data to a Cold Tier Recalling Data to the Data Fabric File System Administering Storage Policies Manage storage policies related to data tiering. Creating a Storage Policy Editing a Storage Policy Deleting Storage Policy Delete storage policy. Administering Remote Targets Administering Schedules Introduction to schedules. Mirroring Synopsis of mirrors and mirroring process. Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Editing a Storage Policy About this task You can modify the basic rule to: Add or remove users and/or groups. Change the name of the users and/or groups. Change the number of days since the file was last modified for users and/or\n                    groups. If you switch from a basic rule to an advanced rule, all expressions from the basic\n            rule are carried over to the advanced rule. You can modify an advanced rule using a\n            combination of the following expressions: u Username or user ID, as configured in the OS registry (such as /etc/passwd file, LDAP, etc.), of a specific\n                                        user. Usage: u:<username or user ID> g Group name or group ID, as configured in the OS registry (such as /etc/group file, LDAP, etc.), of a specific\n                                        group. Usage: g:<groupname or group ID> a ( atime ) Time (in seconds or days) since the\n                                files were last accessed. The number of seconds can be specified by\n                                appending s to value and the number of days can be\n                                specified by appending d to the\n                                        value. Usage: \"a:<value>s\" \u2014 specifies atime in seconds \"a:<value>d\" \u2014 specifies atime in days NOTE: If the system time on CLDB and file server nodes\n                                are different, the atime rule for offloading data\n                                may not work as intended.",
        "ab8edbc7-4355-408d-96a9-cdafa150fbe3": "Optional. How aggressively to perform early stopping. There are three modes: aggressive, standard, and conservative; the default is standard. These modes differ in the degree to which early-stopping is used. In aggressive mode, the searcher quickly stops underperforming trials, which enables the searcher to explore more hyperparameter configurations, but at the risk of discarding a configuration too soon. On the other end of the spectrum, conservative mode performs significantly less downsampling, but as a consequence does not explore as many configurations given the same budget. We recommend using either aggressive or standard mode.",
        "6a7ed1c8-d039-41fc-bbb2-7bdb9cf5aa55": "Security Policy Enforcement Process Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Security Policies Add, edit, delete, and manage state of security policies. Security Policy Enforcement Process Describes the steps followed during security policy enforcement on         volumes. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. About Security Policy Domain Describes a security policy domain. Security Policy Implementation Workflow Describes the security policy workflow, in general, and the steps in implementing a         security policy. Security Policy Enforcement Process Describes the steps followed during security policy enforcement on         volumes. Understanding Access Control in a Security Policy The implications of permissions assigned to users and groups in a security         policy. Managing File and Directory ACEs Describes the implications of setting access control expressions on files and             directories. Security Policy Permissions Permissions define which administrative users can create, view, and modify security     policies. Administrators set the permissions on security policies through cluster-level and     security policy-level ACLs. Designating a Fabric as Global Policy Master Designate a fabric in the global namespace as the global policy master. Creating a Security Policy Add a security policy on the global policy master. Viewing a Security Policy View security policy details. Viewing All Security Policies View all security policies on the Data Fabric UI. Editing a Security Policy Make changes to a security policy. Assigning a Security Policy to One or More Volumes Assigning Multiple Security Policies to One or More Volumes Describes how to assign multiple security policies to volumes. Unassigning One or More Security Policies from a Volume Unassign a policy from a volume to which it has been previously assigned. Disabling a Security Policy Describes how to disable a security policy. Enabling a Security Policy Describes how to enable a security policy. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Security Policy Enforcement Process Describes the steps followed during security policy enforcement on\n        volumes. Order of Enforcement Data\n                    Fabric File System enforce security policies\n                hierarchically, starting at the volume level. If the volume-level enforcement mode is set to PolicyAceAndDataAce (default setting), the system evaluates and enforces the ACEs directly applied to data\n                    objects AND the ACEs defined in the security policies applied to data objects. When a user\n                    submits a data-operation request, the system evaluates and enforces the ACEs hierarchically,\n                    starting with the volume in which the data resides. For example, to perform a write operation on a file, the system first evaluates\n                permissions on the volume in which the file resides. If at least one security policy\n                is applied to the volume, the system evaluates the ACEs set in the security policy\n                AND the ACEs or POSIX mode bits directly applied to the volume. Both sets of ACEs\n                must allow the user to access the volume. If one set of ACEs does not permit access\n                to the volume, the system denies the user permission to perform the operation. If\n                both sets of ACEs permit access to the volume, the system checks access permissions\n                on the file.",
        "3233d881-22fd-41b0-a63f-00634c992058": "The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage.",
        "2d4216d9-f068-4a82-89b2-0f4be680bc4d": "Release Date: September 8, 2020 Bug Fixes Deployment: Fix a bug where det-deploy local cluster-up was failing. WebUI: Fix a bug where experiment labels were not displayed on the experiment list page. WebUI: Fix a bug with decoding API responses because of unexpected non-numeric metric values.",
        "97a4a83d-ce1d-42e8-b583-098b9ba6a764": "Specifies the duration in seconds before idle TensorBoard instances are automatically terminated. A TensorBoard instance is considered to be idle if it does not receive any HTTP traffic. The default timeout is 300 (5 minutes).",
        "b6958d16-a557-4a3e-8622-9651b8e7a7c7": "Enabling Metrics in Ray Dashboard Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Feast Provides a brief overview of Feast in HPE Ezmeral Unified Analytics Software . Kubeflow Provides a brief overview of Kubeflow in HPE Ezmeral Unified Analytics Software . HPE Machine Learning Development Environment Provides a brief overview of HPE Machine Learning Development Environment ( HPE MLDE ) in HPE Ezmeral Unified Analytics Software . MLflow Provides a brief overview of MLflow in HPE Ezmeral Unified Analytics Software . Ray Provides a brief overview of Ray in HPE Ezmeral Unified Analytics Software . Connecting to Ray Cluster Describes how to connect to Ray clusters to submit jobs. Using JobSubmissionClient to Submit Ray Jobs Describes how to connect to Ray cluster and submit Ray jobs using JobSubmissionClient . Enabling Metrics in Ray Dashboard Describes how to enable metrics in Ray dashboard. Resource Configuration and Management Describes resource configuration and management for Ray. GPU Support for Ray Describes how to enable GPU, configure the GPU resources, and disable GPU for     Ray. Using whylogs with Ray Describes how to use whylogs with Ray. Ray Best Practices Lists the best practices for Ray. Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Enabling Metrics in Ray Dashboard Describes how to enable metrics in Ray dashboard. Prerequisites Ensure that Ray's head pod has enough resources to run the\n            Grafana server. About this task To enable Metrics view in dashboard, you must install Grafana, configure the data\n                source as centralized Prometheus, and start the Grafana server with the specific\n                configuration file in Ray's head pod. Procedure By default Ray\u2019s metrics are scraped by centralized Prometheus, so specify\n                    Prometheus\u2019 service URL as the data source in /tmp/ray/session_latest/metrics/grafana/provisioning/datasources/default.yml file. For example: apiVersion: 1\n\ndatasources:\n  - name: Prometheus\n    url: http://af-prometheus-kube-prometh-prometheus.prometheus.svc.cluster.local:9090\n    type: prometheus\n    isDefault: true\n    access: proxy Install Grafana in Ray's head pod and navigate to Grafana's home\n                    directory. To install Grafana in Ray\u2019s head pod, follow these steps: Access the shell on the head node. kubectl -n kuberay exec -it <head_pod_name> -- bash Download Grafana. wget https://dl.grafana.com/oss/release/grafana-9.3.6.linux-amd64.tar.gz Go to the Grafana home directory. tar -zxvf grafana-9.3.6.linux-amd64.tar.gz \ncd grafana-9.3.6 Start the Grafana server with the Ray configuration file. ./bin/grafana-server --config /tmp/ray/session_latest/metrics/grafana/grafana.ini web Forward Grafana's default port. kubectl -n kuberay port-forward --address 0.0.0.0 <head_pod_name> 3000:3000 Click the Applications & Frameworks icon on the left\n                    navigation bar. Navigate to the Ray tile under the Data Science tab and click Open . Results Metrics view is enabled in the Ray dashboard. To learn more, see Ray Metrics . Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "0d8cf42a-1e97-4902-bbf8-93d096c683a8": "The Determined Docker images are hosted on Docker Hub. Determined agents need access to Docker Hub for such tasks as building new images for user workloads. If packages, data, or other resources needed by user workloads are hosted on the public Internet, Determined agents need to be able to access them. Note that agents can be configured to use proxies when accessing network resources. For best performance, it is recommended that the Determined master and agents use the same physical network or VPC. When using VPCs on a public cloud provider, you may need to take additional steps to ensure instances in the VPC can access the Internet: On GCP, either the instances must have an external IP address or a GCP Cloud NAT should be configured for the VPC. On AWS, the instances must have a public IP address and a VPC Internet Gateway should be configured for the VPC.",
        "fb284951-e1a4-4a9f-b3af-1c4312ed8a2b": "Specify the port of the APIserver for the fabric to be imported. Click Import . Once the Import operation is finished, ensure that the imported\n            fabric is part of a cluster group and the fabric is listed as part of the global\n            namespace. Complete the following tasks for the new fabric: Registering a Fabric Adding an Activation Key Setting the Billing Model (Topic last modified: 2023-12-08) On this page Considerations for Importing an as-a-Service Fabric Preparing to Import an as-a-Service Fabric Completing the Import Operation by Using the Data Fabric UI \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "f5faacf4-2adb-44f5-a31c-4d488847294f": "Viewing the Software Version Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Viewing the Software Version Describes several ways to identify the core software version for a fabric. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Configuring a Proxy Server for Data Fabric Access to the Internet Describes the procedure to configure an https or http proxy for scenarios where         communication between the internet and Data Fabric must happen over a proxy         server. Creating a Fabric Fabrics make it possible for you to create volumes, buckets, and topics in a cloud or     on-premises deployment. If your organization has multiple departments or multiple use cases to     support, you can create multiple fabrics. This page describes the basic steps to create a new     fabric for any of the supported fabric providers (AWS, Azure, GCP, and on-premises). Importing a Fabric This section provides the steps to import an as-a-service fabric into the global     namespace. Viewing the Fabric Status Describes how to use the Global namespace card. Viewing Fabric Settings Describes how to view and change the fabric settings, which include fabric auditing,     data auditing, and gateway information. Viewing the Fabric Endpoint Describes how to view the endpoint for a fabric on the Data Fabric UI. Viewing the Software Version Describes several ways to identify the core software version for a fabric. Generating S3 Access Keys for the Global Namespace Describes how to obtain an S3       user access key and secret key that can be used to perform operations on S3 resources anywhere       in the global namespace. Setting a Quota for a User Set quota for an individual user. Setting a Quota for a Group Set quota for an individual group. Viewing Fabric-Related Metrics Explains the various fabric-related metrics visible on the overview/data_fabric_ui.html . Setting Default Quotas for Users/Groups Set default values for user and group quotas on a fabric via the Data Fabric UI . Viewing the Fabric Service Status View status of various services running on a fabric. View Capacity Usage by User on Fabric Describes how to check the capacity used by various internal volumes, buckets,             topics , and binary tables created by the user that is logged         in to the Data Fabric UI. SSH Access to a Cloud-Based Fabric Describes how to obtain a fabric-specific .pem file that enables SSH     access to a cloud-based fabric. Deleting a Fabric Delete a remote fabric from the global namespace. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Viewing the Software Version Describes several ways to identify the core software version for a fabric. View the Software Version with User Information To view the core software version: Sign in to the Data Fabric UI . In the upper right corner of the home screen, click the down arrow next to the user\n            name. For example: Viewing the Software Version with Fabric Details To view the core software version: Sign in to the Data Fabric UI , and switch to the Fabric manager view. Click Global namespace . Click the Table view icon to display the resource table with\n            status values.",
        "c2c68fca-fa30-4c8a-9702-1c5902f93881": "determined.pytorch.init*hparams: Optional[Dict] = Noneexp_conf: Optional[Dict[str, Any]] = Nonedistributed: Optional[determined.core._distributed.DistributedContext] = Noneaggregation_frequency: int = 1enable_tensorboard_logging: bool = TrueIterator[determined.pytorch._pytorch_context.PyTorchTrialContext] Creates a PyTorchTrialContext for use with a PyTorchTrial. All trainer.* calls must be within the scope of this context because there are resources started in __enter__ that must be cleaned up in __exit__. Parameters hparams \u2013 (Optional) instance of hyperparameters for the trial exp_conf \u2013 (Optional) for local-training mode. If unset, calling context.get_experiment_config() will fail. distributed \u2013 (Optional) custom distributed training configuration aggregation_frequency \u2013 number of batches before gradients are exchanged in distributed training. This value is configured here because it is used in context.wrap_optimizer. enable_tensorboard_logging \u2013 Configures if upload to tensorboard is enabled",
        "5b3a11dc-0390-45ca-858a-fc40fc997e9a": "Release Date: August 29, 2023 Breaking Changes Remove EstimatorTrial, which has been deprecated since Determined version 0.22.0 (May 2023). Bug Fixes Trials: Fix an issue where trial logs could fail for trials created prior to Determined version 0.17.0. CLI: Fix an issue where template association with workspaces, when listed, was missing. This would prevent templates from being listed for some users and templates on RBAC-enabled clusters.",
        "0db7d9f2-9b69-4e55-9db9-58cef5d0b655": "For more information, see Accessing the HPE Ezmeral Token-Authenticated Internet Repository . RHEL/Rocky/Oracle Enterprise\n                Linux wget --user=<email> --password=<token> -O /tmp/maprgpg.key -q https://package.ezmeral.hpe.com/releases/pub/maprgpg.key && rpm --import /tmp/maprgpg.key\nwget --user=<email> --password=<token> -O /tmp/hpeezdf.pub -q https://package.ezmeral.hpe.com/releases/pub/hpeezdf.pub && rpm --import /tmp/hpeezdf.pub && gpg --import /tmp/hpeezdf.pub Ubuntu wget --user=<email> --password=<token> -O /tmp/maprgpg.key -q https://package.ezmeral.hpe.com/releases/pub/maprgpg.key && sudo apt-key add /tmp/maprgpg.key\nwget --user=<email> --password=<token> -O /tmp/gnugpg.key -q https://package.ezmeral.hpe.com/releases/pub/gnugpg.key && sudo apt-key add /tmp/gnugpg.key For SLES only, you do not have to install the package key because zypper allows package installation with or without the key. To install the client, obtain the Data Fabric packages for\n        your operating system at https://package.ezmeral.hpe.com/ and complete the installation steps described in one of the subsequent\n        topics. Installing the Data Fabric Client on RHEL This section describes how to install the Data Fabric client on Red Hat Enterprise Linux (RHEL). Installing the Data Fabric Client on SLES This section describes how to install the Data Fabric Client on SLES. Installing the Data Fabric Client on Ubuntu This section describes how to install the Data Fabric client on Ubuntu. Setting up the Data Fabric Repository This section describes how to make packages available through the HPE Ezmeral Data Fabric repository. (Topic last modified: 2024-02-02) On this page Basic Steps for Client Installation Preparing to Install a Data Fabric Client \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "ea830181-017e-44e3-89ba-9e86a6f471f0": "The following diagram shows the components and applications in the workflow: Steps Sign in to HPE Ezmeral Unified Analytics Software and perform the following steps: A - Connect Data Sources B \u2013 Select Data Sets and Create a View C - Connect to the Presto Database D - Add the View to Superset and Create a Chart E - Specify Query Conditions to Visualize Results in the Chart F \u2013 Create a Superset Dashboard and Add the Chart (Visualized Data) G \u2013 Monitor Queries IMPORTANT This tutorial demonstrates how to perform a series of tasks in HPE Ezmeral Unified Analytics Software to\n          complete an example workflow. The data and information used in this tutorial is for\n          example purposes only. You must connect Unified Analytics to your own data sources and use the data sets available to you in\n          your data sources. A - Connect Data Sources Connect HPE Ezmeral Unified Analytics Software to\n        external data sources that contain the data sets (tables and views) you want to work with.\n        This tutorial uses MySQL, SQL Server, and Hive as the connected data source examples. To connect a data source: In the left navigation column, select Data Engineering > Data\n              Sources . The Data Sources screen appears. Click Add New Data Source . Complete the steps required to connect to the MySQL, SQL Server, and Hive data sources: Connecting to MySQL In the Add New Data Source screen, click Create\n                        Connection in the MySQL tile. In the drawer that opens, enter the following information in the respective\n                        fields: Name : mysql Connection URL : jdbc:mysql://<ip-address>:<port> Connection User : myaccount Connection Password : moi123 Enable Local Snapshot Table : Select the check box TIP When Enable Local Snapshot Table is selected, the system caches remote\n                            table data to accelerate queries on the tables. The cache is active for\n                            the duration of the configured TTL or until the remote tables in the\n                            data source are altered. Enable Transparent Cache : Select the check box TIP When Enable Transparent Cache is selected, the system caches data\n                            at runtime when queries access remote tables. As the query engine scans\n                            data in remote data sources, the scanned data is cached on the fly.\n                            Results for subsequent queries on the same data are quickly returned\n                            from the cache. The cache lives for the duration of the\n                          session. Click Connect . Upon successful connection, the system returns the\n                          following\n                          message: Successfully added data source \"mysql\". Connecting to SQL Server In the Add New Data Source screen, click Create\n                        Connection in the SQL Server tile. In the drawer that opens, enter the following information in the respective\n                        fields: Name : mssql_ret2 Connection URL :\n                          jdbc:sqlserver:<ip-address>:<port>;database=retailstore Connection User : myaccount Connection Password : moi123 Enable Local Snapshot Table : Select the check box TIP When Enable Local Snapshot Table is selected, the system caches remote\n                            table data to accelerate queries on the tables. The cache is active for\n                            the duration of the configured TTL or until the remote tables in the\n                            data source are altered. Enable Transparent Cache : Select the check box TIP When Enable Transparent Cache is selected, the system caches data\n                            at runtime when queries access remote tables. As the query engine scans\n                            data in remote data sources, the scanned data is cached on the fly.\n                            Results for subsequent queries on the same data are quickly returned\n                            from the cache. The cache lives for the duration of the\n                          session. Click Connect . Upon successful connection, the system returns the\n                          following\n                          message: Successfully added data source \"mssql_ret2\". Connecting to Hive In the Add New Data Source screen, click Create\n                        Connection in the Hive tile. In the drawer that opens, enter the following information in the respective\n                        fields: Name : hiveview Hive Metastore : file Hive Metastore Catalog Dir : file:///data/shared/tmpmetastore In Optional Fields , search for the following fields and add the\n                          specified values: Hive Max Partitions Per Writers : 10000 Hive Temporary Staging Directory Enabled : Unselect Hive Allow Drop Table : Select Enable Local Snapshot Table : Select the check box TIP When Enable Local Snapshot Table is selected, the system caches remote\n                            table data to accelerate queries on the tables. The cache is active for\n                            the duration of the configured TTL or until the remote tables in the\n                            data source are altered. Enable Transparent Cache : Select the check box TIP When Enable Transparent Cache is selected, the system caches data\n                            at runtime when queries access remote tables. As the query engine scans\n                            data in remote data sources, the scanned data is cached on the fly.\n                            Results for subsequent queries on the same data are quickly returned\n                            from the cache.",
        "83d7f651-16cd-43d9-a938-842d74f889f8": "Experiment List: /det/experiments State filter converted from multi-select to single-select. Convert actions from expanded buttons to overflow menu (triple vertical dots). Batch operation logic change to available if the action can be applied to any of the selected experiments Add pagination support that auto turns on when entries extend beyond 10 entries. Experiment Detail: /det/experiments/<id> Implement charting with Plotly with zooming capability. Trial table paginates on the WebUI side in preparation for API pagination in the near future. Convert steps to batches in trials table and metric chart. Update continue trial flow to use batches, epochs or records. Use Monaco editor for the experiment config with YAML syntax highlighting. Add links to source for Checkpoint modal view, allowing users to navigate to the corresponding experiment or trial for the checkpoint. Trial Detail: /det/trials/<id> Add trial information table. Add trial metrics chart. Implement charting with Plotly with zooming capability. Trial info table paginates on the WebUI side in preparation for API pagination in the near future. Add support for batches, records and epochs for experiment config. Convert metric chart to show batches. Convert steps table to batches table. Master Logs: /det/logs, Trial Logs: /det/trials/<id>/logs, Task Logs: /det/<tasktype>/<id>/logs Limit logs to 1000 lines for initial load and load an additional 1000 for each subsequent fetch of older logs. Use new log viewer optimized for efficient rendering. Introduce log line numbers. Add ANSI color support. Add error, warning, and debug visual icons and colors. Add tailing button to enable tailing log behavior. Add scroll to top button to load older logs out Fix back and forth scrolling behavior on log viewer. Cluster: /det/cluster Separate out GPU from CPU resources. Show resource availability and resource count (per type). Render each resource as a donut chart. Navigation Update sidebar navigation for new task and experiment list pages. Add link to new swagger API documentation. Hide pagination controls for tables with less than 10 entries. Bug Fixes Configuration: Do not load the entire experiment configuration when trying to check if an experiment is valid to be archived or unarchived. Configuration: Improve the master to validation hyperparameter configurations when experiments are submitted. Currently, the master checks whether global_batch_size has been specified and if it is numeric. Logs: Fix issue of not detecting newlines in the log messages, particularly Kubernetes log messages. Logs: Add intermediate step to trial log download to alert user that the CLI is the recommended action, especially for large logs. Searchers: Fix a bug in the SHA searcher caused by the promotion of already-exited trials. Security: Apply user authentication to streaming endpoints. Tasks: Allow the master certificate file to be readable even for a non-root task. TensorBoard: Fix issue affecting TensorBoards on AWS in us-east-1 region. TensorBoard: Recursively search for tfevents files in subdirectories, not just the top level log directory. WebUI: Fix scrolling issue that occurs when older logs are loaded, the tailing behavior is enabled, and the view is scrolled up. WebUI: Fix colors used for different states in the cluster resources chart. WebUI: Correct the numbers in the Batches column on the experiment list page. WebUI: Fix cluster and dashboard reporting for disabled slots. WebUI: Fix issue of archive/unarchive not showing up properly under the task actions.",
        "a9994e61-0039-4403-b823-48b4a3c6ae0e": "This document describes how to install, configure, and upgrade a deployment of Determined with dynamic agents on GCP. For an overview of the elastic infrastructure in Determined, visit Elastic Infrastructure.",
        "e5d5ad56-e3a5-415d-b71b-e9d7813925b7": "A list of resource pools. A resource pool is a collection of identical computational resources. You can specify which resource pool a job should be assigned to when the job is submitted. Refer to the documentation on Resource Pools for more information. Defaults to a resource pool with a name default.",
        "5d2666a7-e134-479d-8e0d-9004d6a8d2f6": "CPU resources will be requested for each compute slot. Partitions will be represented as a resource pool with slot type cpu. One node will be allocated per slot.",
        "7d2065a1-ade4-451c-b995-ffd949fc5d75": "It is not the purpose of this section to induce you to infringe any\npatents or other property right claims or to contest validity of any\nsuch claims; this section has the sole purpose of protecting the\nintegrity of the free software distribution system which is\nimplemented by public license practices.  Many people have made\ngenerous contributions to the wide range of software distributed\nthrough that system in reliance on consistent application of that\nsystem; it is up to the author/donor to decide if he or she is willing\nto distribute software through any other system and a licensee cannot\nimpose that choice.\n\n\nThis section is intended to make thoroughly clear what is believed to\nbe a consequence of the rest of this License.\n\n\n  12. If the distribution and/or use of the Library is restricted in\ncertain countries either by patents or by copyrighted interfaces, the\noriginal copyright holder who places the Library under this License may add\nan explicit geographical distribution limitation excluding those countries,\nso that distribution is permitted only in or among countries not thus\nexcluded.  In such case, this License incorporates the limitation as if\nwritten in the body of this License.\n\n\n  13. The Free Software Foundation may publish revised and/or new\nversions of the Lesser General Public License from time to time.\nSuch new versions will be similar in spirit to the present version,\nbut may differ in detail to address new problems or concerns.\n\n\nEach version is given a distinguishing version number.  If the Library\nspecifies a version number of this License which applies to it and\n\"any later version\", you have the option of following the terms and\nconditions either of that version or of any later version published by\nthe Free Software Foundation.  If the Library does not specify a\nlicense version number, you may choose any version ever published by\nthe Free Software Foundation.\n\n\n  14. If you wish to incorporate parts of the Library into other free\nprograms whose distribution conditions are incompatible with these,\nwrite to the author to ask for permission.  For software which is\ncopyrighted by the Free Software Foundation, write to the Free\nSoftware Foundation; we sometimes make exceptions for this.  Our\ndecision will be guided by the two goals of preserving the free status\nof all derivatives of our free software and of promoting the sharing\nand reuse of software generally.\n\n\n                            NO WARRANTY\n\n\n  15. BECAUSE THE LIBRARY IS LICENSED FREE OF CHARGE, THERE IS NO\nWARRANTY FOR THE LIBRARY, TO THE EXTENT PERMITTED BY APPLICABLE LAW.\nEXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR\nOTHER PARTIES PROVIDE THE LIBRARY \"AS IS\" WITHOUT WARRANTY OF ANY\nKIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE\nLIBRARY IS WITH YOU.  SHOULD THE LIBRARY PROVE DEFECTIVE, YOU ASSUME\nTHE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n  16. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN\nWRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY\nAND/OR REDISTRIBUTE THE LIBRARY AS PERMITTED ABOVE, BE LIABLE TO YOU\nFOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR\nCONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE\nLIBRARY (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING\nRENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A\nFAILURE OF THE LIBRARY TO OPERATE WITH ANY OTHER SOFTWARE), EVEN IF\nSUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGES.\n\n\n                     END OF TERMS AND CONDITIONS\n\n\n           How to Apply These Terms to Your New Libraries\n\n\n  If you develop a new library, and you want it to be of the greatest\npossible use to the public, we recommend making it free software that\neveryone can redistribute and change.  You can do so by permitting\nredistribution under these terms (or, alternatively, under the terms of the\nordinary General Public License).\n\n\n  To apply these terms, attach the following notices to the library.  It is\nsafest to attach them to the start of each source file to most effectively\nconvey the exclusion of warranty; and each file should have at least the\n\"copyright\" line and a pointer to where the full notice is found.\n\n\n    <one line to give the library's name and a brief idea of what it does.>\n    Copyright (C) <year>  <name of author>",
        "0277c313-94e8-4fea-a6bc-3332463cd89d": "Release Date: November 30, 2021 New Features WebUI: Add the model registry as a new top-level navigation option, allowing for viewing, editing, and deleting existing models created using the CLI. Add experimental support for Multi-Instance GPUs (MIGs) to agent-based setups, in parity with the experimental support for MIGs in Kubernetes-based setups. Static agents and Kubernetes clusters may be able to use MIG instances for some workloads. Distributed training is not supported, and all MIG instances and nodes within a resource pool must still be homogeneous. Improvements Breaking Change: Model Registry: The names of models in the model registry must now be unique. If multiple models were previously created with the same name in the registry, the names will change. Model Registry CLI: Allow models to be referred to by their now-unique names, not only by ID. Tasks: Historical usage over users now properly accounts for all task types (commands, notebooks, etc.), not just trials. Images: Add environment images for TF 2.7. Agent: The environment.force_pull_image: true option no longer deletes the environment image before re-pulling it. Now, it will only fetch updated layers, which is much less wasteful of network resources and execution time. Bug Fixes Master: Fix a bug where deleting experiments with trial restarts always failed, and then failed to be marked as failed.",
        "94b02fd0-e025-4d89-b21d-dfe6a658974e": "patch_torch_functions (bool, optional, default=None) \u2013 Optional property override. keep_batchnorm_fp32 (bool or str, optional, default=None) \u2013 Optional property override. If passed as a string, must be the string \u201cTrue\u201d or \u201cFalse\u201d. master_weights (bool, optional, default=None) \u2013 Optional property override. loss_scale (float or str, optional, default=None) \u2013 Optional property override. If passed as a string, must be a string representing a number, e.g., \u201c128.0\u201d, or the string \u201cdynamic\u201d. cast_model_outputs (torch.dtype, optional, default=None) \u2013 Option to ensure that the outputs of your model is always cast to a particular type regardless of opt_level. num_losses (int, optional, default=1) \u2013 Option to tell Amp in advance how many losses/backward passes you plan to use. When used in conjunction with the loss_id argument to amp.scale_loss, enables Amp to use a different loss scale per loss/backward pass, which can improve stability. If num_losses is left to 1, Amp will still support multiple losses/backward passes, but use a single global loss scale for all of them. verbosity (int, default=1) \u2013 Set to 0 to suppress Amp-related output. min_loss_scale (float, default=None) \u2013 Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed. If dynamic loss scaling is not used, min_loss_scale is ignored. max_loss_scale (float, default=2.**24) \u2013 Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling. If dynamic loss scaling is not used, max_loss_scale is ignored. Returns Model(s) and optimizer(s) modified according to the opt_level. If optimizers args were lists, the corresponding return value will also be a list. current_train_batchint Current global batch index get_data_configDict[str, Any] Return the data configuration. get_enable_tensorboard_loggingbool Return whether automatic tensorboard logging is enabled get_experiment_idint Return the experiment ID of the current trial. get_global_batch_sizeint Return the global batch size. get_hparamname: strAny Return the current value of the hyperparameter with the given name. get_per_slot_batch_sizeint Return the per-slot batch size. When a model is trained with a single GPU, this is equal to the global batch size. When multi-GPU training is used, this is equal to the global batch size divided by the number of GPUs used to train the model. get_stop_requestedbool Return whether a trial stoppage has been requested. get_tensorboard_pathpathlib.Path Get the path where files for consumption by TensorBoard should be written get_tensorboard_writerAny This function returns an instance of torch.utils.tensorboard.SummaryWriter Trials users who wish to log to TensorBoard can use this writer object. We provide and manage a writer in order to save and upload TensorBoard files automatically on behalf of the user. Usage example: class MyModel(PyTorchTrial): def __init__(self, context): ... self.writer = context.get_tensorboard_writer() def train_batch(self, batch, epoch_idx, batch_idx): self.writer.add_scalar('my_metric', np.random.random(), batch_idx) self.writer.add_image('my_image', torch.ones((3,32,32)), batch_idx) get_trial_idint Return the trial ID of the current trial. is_epoch_endbool Returns true if the current batch is the last batch of the epoch. Not accurate for variable size epochs. is_epoch_startbool Returns true if the current batch is the first batch of the epoch. Not accurate for variable size epochs. set_enable_tensorboard_loggingenable_tensorboard_logging: boolNone Set a flag to indicate whether automatic upload to tensorboard is enabled. set_profiler*args: List[str]**kwargs: AnyNone set_profiler() is a thin wrapper around the native PyTorch profiler, torch-tb-profiler. It overrides the on_trace_ready parameter to the determined tensorboard path, while all other arguments are passed directly into torch.profiler.profile. Stepping the profiler will be handled automatically during the training loop. See the PyTorch profiler plugin for details. Examples: Profiling GPU and CPU activities, skipping batch 1, warming up on batch 2, and profiling batches 3 and 4. self.context.set_profiler( activities=[ torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA, ], schedule=torch.profiler.schedule( wait=1, warmup=1, active=2 ), ) set_stop_requestedstop_requested: boolNone Set a flag to request a trial stoppage. When this flag is set to True, we finish the step, checkpoint, then exit.",
        "d3d35edd-c3b5-4873-8e35-49db6c9cc3b2": "Title Description Transformers The Determined library serves as an alternative to the HuggingFace Trainer Class and provides access to the benefits of using Determined. MMDetection The MMDetection library serves as an alternative to the trainer used by MMDetection and provides access to all of the Determined benefits.",
        "a0db595c-3a59-4ee0-9766-c7626ac21529": "A security policy server in each of the fabrics enforces the policies and\n            manages the security policy metadata in an internal volume named mapr.pbs.base . See Security Policy Implementation Workflow for details on how to apply\n            security policies to fabric volumes on Data Fabric. (Topic last modified: 2024-01-23) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "60b5fe1e-a12c-4a5b-af77-009b4fc11005": "The Determined Docker images are hosted on Docker Hub. Determined agents need access to Docker Hub for such tasks as building new images for user workloads. If packages, data, or other resources needed by user workloads are hosted on the public Internet, Determined agents need to be able to access them. Note that agents can be configured to use proxies when accessing network resources. For best performance, it is recommended that the Determined master and agents use the same physical network or VPC. When using VPCs on a public cloud provider, additional steps might need to be taken to ensure that instances in the VPC can access the Internet: On GCP, the instances need to have an external IP address, or a GCP Cloud NAT should be configured for the VPC. On AWS, the instances need to have a public IP address, and a VPC Internet Gateway should be configured for the VPC. See also: Firewall Rules.",
        "7c4ad272-0d73-4c5e-b9fa-3b658c21de5c": "The port of the Determined master. Defaults to 443 if TLS is enabled and 80 otherwise.",
        "e7cb28f0-14dd-41ee-a666-9d15cd910f13": "Master hostname that containers started by this agent will connect to. Defaults to the value of master_host.",
        "7a653fc6-d162-4444-ba2e-4c450b4dec26": "Docker is a dependency of several Determined system components. For example, every agent node must have Docker installed to run containerized workloads.",
        "070ad750-991c-464b-89cb-fcece72770d9": "These quotas work on tenant\n                volumes as well. You can set hard quota and advisory quota defaults for users and groups. When a user\n                or group is created, the default quota and advisory quota apply unless overridden by\n                specific quotas. Quotas can be specified in mega bytes (MB),  gigabytes (GB),\n                terabytes (TB), petabytes (PB), exabytes (EB), and zettabytes (ZB). User quota is the total space allocated to user on fabric. User hard quota is the total space allocated to user on fabric. Group quota is the total space allocated to group on fabric. Group hard quota is the total space allocated to group on fabric. Fabric reserve limit is the percentage of the total cluster capacity to allocate\n                    for the volumes on the cluster. The size of a disk space quota is expressed in terms of the actual data stored from\n                the user's point of view. Only post-compression data blocks are counted, and\n                snapshot and replica space do not count against quotas. For example, a 10G file that\n                is compressed to 8G and has a replication factor of 3 consumes 24G (3*8G), but\n                charges only 8G to the user or volume's quota. Follow the steps given below to set default quotas for users and/or groups. Procedure Log on to the Data Fabric UI . Select the Fabric manager from the dropdown next to the welcome message on\n                    the Home page. Click In the tabular list of fabrics on the Global namespace card, click the\n                    fabric name for which you wish to set the default user quota and/or group\n                    quota. Click Settings. Click the pencil/edit icon next to the Default quota . On the Edit Fabric Quotas dialog box, enter the values for user quota,\n                    user hard quota, group quota and group hard quota. Change the unit, as\n                    required. Specify the Fabric reserve limit . Click Update. Results The user quota, group quota, and fabric reserve limit\n            specified for the fabric are saved. You can view the fabric default user quota and group\n            quota on the Settings tab for the fabric. You may choose to specify quotas for\n                individual users and/or groups. Such values override the default user quota and\n                group quota for the respective users and/or groups. See Setting a Quota for a User for details. (Topic last modified: 2023-11-06) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "1a7f355f-d7f3-4ebd-8f2d-aac4364cae48": "Specifies how Determined schedules tasks to agents. The scheduler configuration on each resource pool will override the global one. For more on scheduling behavior in Determined, see Scheduling.",
        "d7515345-d7ec-4724-a7c8-beeb64ff8baa": "The Docker image to use for the Determined agents. A valid form is <repository>:<tag>. Defaults to determinedai/determined-agent:<master version>.",
        "a1d11946-6a9c-42c7-a46f-d847c76c7251": "Object Store Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster.",
        "4fc8a469-4ab3-47e6-877c-cc26521dc837": "The following options are generated by Determined and cannot be directly specified by the user. Additional options specified by slurm.sbatch_args will be added to the generated batch file provided that they do not conflict with Determined-controlled settings. Option Description --error --output Captures error/output logs. On job startup failure, a portion of the error log is added to the experiment log to assist in diagnosing the problem. During normal operation, logs are piped directly to the Determined master and these files are not used. --job-name Generated job name of the form det-ai_ followed by the Determined job type (exp, cmd, gc) then an internal HPC launcher unique job id. --partition Partition name as determined by the selected resource pool. --wckey A value identified by the resource_manager.job_project_source configuration. --no-requeue Disable any potential automatic requeue of the job by SLURM. Determined will handle the checkpoint and restart for its experiments.",
        "ddcb6252-ac89-4d31-82ad-c6fac28ae24d": "Optional. The AMI ID of the Determined agent. Defaults to the latest GCP agent image.",
        "ac9a1727-662e-4990-af1b-3677b41b5064": "Each Determined experiment is associated with an experiment seed: an integer ranging from 0 to 231\u20131. The experiment seed can be set using the reproducibility.experiment_seed field of the experiment configuration. If an experiment seed is not explicitly specified, the master will assign one automatically. The experiment seed is used as a source of randomness for any hyperparameter sampling procedures. The experiment seed is also used to generate a trial seed for every trial associated with the experiment. In the Trial interface, the trial seed is accessible within the trial class using self.ctx.get_trial_seed().",
        "758911f9-c3b5-46f2-a80a-77b8fc6bc805": "The AWS secret key to use.",
        "2724bd9f-ab5c-4a29-a3eb-9c5439fe1f94": "Trial logs are shipped to the master and stored in Postgres. If nothing is set, this is the default.",
        "c22451fc-00d5-4e2f-abae-84e2943b7231": "class determined.experimental.client.TrialReferencetrial_id: intsession: determined.common.api._session.Session A TrialReference object is usually obtained from determined.experimental.client.get_trial(). Trial reference class used for querying relevant Checkpoint instances. logsfollow: bool = False*head: Optional[int] = Nonetail: Optional[int] = Nonecontainer_ids: Optional[List[str]] = Nonerank_ids: Optional[List[int]] = Nonestdtypes: Optional[List[str]] = Nonemin_level: Optional[determined.common.experimental.trial.LogLevel] = NoneIterable[str] Return an iterable of log lines from this trial meeting the specified criteria. Parameters follow (bool, optional) \u2013 If the iterable should block waiting for new logs to arrive. Mutually exclusive with head and tail. Defaults to False. head (int, optional) \u2013 When set, only fetches the first head lines. Mutually exclusive with follow and tail. Defaults to None. tail (int, optional) \u2013 When set, only fetches the first head lines. Mutually exclusive with follow and head. Defaults to None. container_ids (List[str], optional) \u2013 When set, only fetch logs from lines from specific containers. Defaults to None. rank_ids (List[int], optional) \u2013 When set, only fetch logs from lines from specific ranks. Defaults to None. stdtypes (List[int], optional) \u2013 When set, only fetch logs from lines from the given stdio outputs. Defaults to None (same as [\"stdout\", \"stderr\"]). min_level \u2013 (LogLevel, optional): When set, defines the minimum log priority for lines that will be returned. Defaults to None (all logs returned). top_checkpointsort_by: Optional[str] = Nonesmaller_is_better: Optional[bool] = Nonedetermined.common.experimental.checkpoint._checkpoint.Checkpoint Return the Checkpoint instance with the best validation metric as defined by the sort_by and smaller_is_better arguments. Parameters sort_by (string, optional) \u2013 The name of the validation metric to order checkpoints by. If this parameter is unset the metric defined in the related experiment configuration searcher field will be used. smaller_is_better (bool, optional) \u2013 Whether to sort the metric above in ascending or descending order. If sort_by is unset, this parameter is ignored. By default, the value of smaller_is_better from the experiment\u2019s configuration is used. select_checkpointlatest: bool = Falsebest: bool = Falseuuid: Optional[str] = Nonesort_by: Optional[str] = Nonesmaller_is_better: Optional[bool] = Nonedetermined.common.experimental.checkpoint._checkpoint.Checkpoint Return the Checkpoint instance with the best validation metric as defined by the sort_by and smaller_is_better arguments. Exactly one of the best, latest, or uuid parameters must be set. Parameters latest (bool, optional) \u2013 Return the most recent checkpoint. best (bool, optional) \u2013 Return the checkpoint with the best validation metric as defined by the sort_by and smaller_is_better arguments. If sort_by and smaller_is_better are not specified, the values from the associated experiment configuration will be used. uuid (string, optional) \u2013 Return the checkpoint for the specified UUID. sort_by (string, optional) \u2013 The name of the validation metric to order checkpoints by. If this parameter is unset the metric defined in the related experiment configuration searcher field will be used. smaller_is_better (bool, optional) \u2013 Whether to sort the metric above in ascending or descending order. If sort_by is unset, this parameter is ignored. By default, the value of smaller_is_better from the experiment\u2019s configuration is used. get_checkpointssort_by: Optional[Union[str, determined.common.experimental.trial.CheckpointSortBy]] = Noneorder_by: Optional[determined.common.experimental.trial.CheckpointOrderBy] = NoneList[determined.common.experimental.checkpoint._checkpoint.Checkpoint] Return a list of Checkpoint instances for the current trial. Either sort_by and order_by are both specified or neither are. Parameters sort_by (string, CheckpointSortBy) \u2013 Which field to sort by. Strings are assumed to be validation metric names. order_by (CheckpointOrderBy) \u2013 Whether to sort in ascending or descending order. stream_metricsgroup: strIterable[determined.common.experimental.metrics.TrialMetrics] Streams metrics for this trial sorted by trial_id, trial_run_id and steps_completed. stream_training_metricsIterable[determined.common.experimental.metrics.TrainingMetrics] @deprecated: Use stream_metrics instead with group set to \u201ctraining\u201d Streams training metrics for this trial sorted by trial_id, trial_run_id and steps_completed. stream_validation_metricsIterable[determined.common.experimental.metrics.ValidationMetrics] @deprecated: Use stream_metrics instead with group set to \u201cvalidation\u201d Streams validation metrics for this trial sorted by trial_id, trial_run_id and steps_completed.",
        "1d892e72-e194-4df8-944b-0f890a5abb17": "Release Date: December 10, 2020 New Features WebUI: Add support for mobile and tablet devices. Check your experiment results on the go! Scheduler: Update the priority scheduler to support specifying priorities and preemption. Improvements Improve the scheduling and scaling behavior of CPU tasks, and allow the maximum number of CPU tasks per agent to be configured via the Configuring the Cluster. Add custom tagging support to AWS dynamic agents. Thank you to sean-adler for contributing this improvement! Support validation_steps in TFKerasTrial\u2019s context.configure_fit(). validation_steps means the same thing in Determined as it does in model.fit(), and has the same limitation (in that it only applies when validation_data is of type tf.data.Dataset). Kubernetes: Support a default user password for Kubernetes deployments. This affects the admin and determined default user accounts. Kubernetes: Release version 0.3.1 of the Determined Helm chart. Bug Fixes Fix a bug in --local --test mode where all GPUs were being passed to the training loop despite the distributed training code paths being disabled. Fix a bug causing active trials that have failed to not be restored properly on a master restart when max_restarts is greater than 0. Allow configurations with a . character in the keys for map fields in the Master Configuration Reference (e.g. task_container_defaults.cpu_pod_spec.metadata.labels). Fix a bug where restoring a large number of experiments after a failure could lead to deadlock. Fix an issue where templates with user-specified bind mounts would merge incorrectly. Thank you to zjorgensenbits for reporting this issue! Deprecated Features The previous version of the priority scheduler is now deprecated. It will remain available as the round_robin scheduler for a limited period of time.",
        "23a0c36a-2c7e-47a4-a039-c22f9ec5d55e": "policy server The service that manages security policies and composite IDs. (Topic last modified: 2023-04-03) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "801d7f07-e7f6-4da9-8d2e-90f377ba1aae": "The file system path on each agent to use. This directory will be mounted to /determined_shared_fs inside the trial container.",
        "a2a53491-c485-47c6-8edb-31c474fba082": "Viewing Object Endpoint Info to Remotely Access Files as Objects Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Viewing Object Endpoint Info to Remotely Access Files as Objects View endpoint information for files in a volume to be able to access the files as         objects when accessed by S3 client. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Creating a Standard Volume Procedure to create standard volume. Creating a Mirror Volume Procedure to create mirror volume. Converting Standard Volume to Mirror Volume Editing a Volume Edit accountable entity, volume access for accountable entity and volume hard         quota. Renaming a Volume Rename a volume. Viewing Volume Endpoint Info View volume endpoint information. Viewing Object Endpoint Info to Remotely Access Files as Objects View endpoint information for files in a volume to be able to access the files as         objects when accessed by S3 client. Downloading Volume Endpoint Information Download JSON file containing endpoint information for the selected volume endpoint         information. Deleting a Volume Delete a single volume. Administering Volume Snapshots Snapshot overview and administering snapshots. Data Tiering Conceptual information about data tiering. Mirroring Synopsis of mirrors and mirroring process. Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Viewing Object Endpoint Info to Remotely Access Files as Objects View endpoint information for files in a volume to be able to access the files as\n        objects when accessed by S3 client. Prerequisites You must be a fabric user to perform this\n            operation. About this task A volume contains one or more files. The files in a volume can be accessed as objects\n                by an S3 client, via the endpoints provided by Data Fabric. You can view the object\n                endpoints from the Data Fabric UI. The object endpoints for files can be used to make API calls in scripts for S3\n                clients to access files in the volume. Follow the steps given below to view object endpoint information for files in a\n                volume. Procedure Log on to the Data Fabric UI . Select Fabric user option from the dropdown on the Home page. Click the Table View icon on the Resources card. In the tabular list of\n                    fabrics, click the down arrow for the fabric that contains the volume. Click the ellipsis under Actions for the required volume. Click the View endpoint option. Results You are able to view the volume endpoint and object endpoint information\n                corresponding to the files on the volume. (Topic last modified: 2024-01-09) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "a76cef56-3450-412a-a594-1b93d13b1e0c": "It provides a shared hierarchical             namespace that is organized like a standard file system. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently\n    stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for\n    performance, reliability, and scalability. (Topic last modified: 2022-05-05) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "c3805fa2-9634-42e9-b48e-b20aadd36251": "Optional. The endpoint to use for S3 clones, e.g., http://127.0.0.1:8080/. If not specified, Amazon S3 will be used.",
        "b6081dca-bf79-4952-939e-3e71ec0abae7": "Experiment Configuration",
        "994912ce-4bf6-4d37-b5b3-a6acf343fb31": "It is only possible to save and restore notebook state on Determined clusters that are configured with a shared filesystem available to all agents. To ensure that your work is saved even if your notebook gets terminated, it is recommended to launch all notebooks with a shared filesystem directory bind-mounted into the notebook container and work on files inside of the bind mounted directory. For example, a user jimmy with a shared filesystem home directory at /shared/home/jimmy could use the following configuration to launch a notebook: $ cat > config.yaml << EOL bind_mounts: - host_path: /shared/home/jimmy container_path: /shared/home/jimmy EOL $ det notebook start --config-file config.yaml By default, launching a cluster by det deploy gcp up, det deploy aws --deployment-type efs, or det deploy aws --deployment-type fsx creates a Network file system that is shared by all the agents and is automatically mounted into Notebook containers at /run/determined/workdir/shared_fs/. To launch a notebook with det deploy local cluster-up, a user can add the --auto-work-dir flag, which mounts the user\u2019s home directory into the task containers by default: $ det deploy local cluster-up --auto-work-dir=\"/shared/home/jimmy\" $ det notebook start Working on a notebook file within the shared bind mounted directory will ensure that your code and Jupyter checkpoints are saved on the shared filesystem rather than an ephemeral container filesystem. If your notebook gets terminated, launching another notebook and loading the previous notebook file will effectively restore the session of your previous notebook. To restore the full notebook state (in addition to code), you can use Jupyter\u2019s File > Revert to Checkpoint functionality. By default, JupyterLab will take a checkpoint every 120 seconds in an .ipynb_checkpoints folder in the same directory as the notebook file. To modify this setting, click on Settings > Advanced Settings Editor and change the value of \"autosaveInternal\" under Document Manager.",
        "823ec62a-335c-4b68-8651-d70907cb1fb0": "The easiest way to use Model Hub Transformers is to start with an existing example Trial. Model Hub Transformers includes thoroughly tested implementations of all core transformers tasks. Model Hub Transformers Trials are infinitely customizable. See the Model Hub Transformers Tutorial to learn how to customize or build a Trial.",
        "dfefeb10-695f-4ede-a138-e09323165950": "developer preview Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster.",
        "ad29d679-f449-4ed9-8770-d782df7c1078": "data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster. The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza.",
        "b4dec272-0d4c-4cc7-a058-e78788d3037f": "Set to\n                      0ms to disable the cache. 1m DURATION Allow Drop Table Allow connector to drop tables. false BOOLEAN Mysql Auto Reconnect When auto reconnect is enabled, presto tries to reconnect to the mysql\n                      server if it finds that connection is down. When it is disabled it will throw\n                      an error without retrying if connection is down. true BOOLEAN Mysql Max Reconnects Number of connection retries. 3 INTEGER Mysql Connection Timeout The time to wait while trying to establish a connection before\n                      terminating the attempt and generating an error. 10 sec DURATION Generic Cache Table Ttl TTL for cache table expiry in minutes. 1440 INTEGER Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "934e8023-80ea-40ae-87b3-05dea709df48": "Deleting a Remote Target Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Data Tiering Conceptual information about data tiering. Administering Remote Targets Deleting a Remote Target Delete a remote target. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Creating a Standard Volume Procedure to create standard volume. Creating a Mirror Volume Procedure to create mirror volume. Converting Standard Volume to Mirror Volume Editing a Volume Edit accountable entity, volume access for accountable entity and volume hard         quota. Renaming a Volume Rename a volume. Viewing Volume Endpoint Info View volume endpoint information. Viewing Object Endpoint Info to Remotely Access Files as Objects View endpoint information for files in a volume to be able to access the files as         objects when accessed by S3 client. Downloading Volume Endpoint Information Download JSON file containing endpoint information for the selected volume endpoint         information. Deleting a Volume Delete a single volume. Administering Volume Snapshots Snapshot overview and administering snapshots. Data Tiering Conceptual information about data tiering. Schedules for Volume Data Tiering Describes schedules for data tiering of volume data Manually Offloading Data to a Cold Tier Recalling Data to the Data Fabric File System Administering Storage Policies Manage storage policies related to data tiering. Administering Remote Targets Creating a Remote Target Create a remote target to offload cold data. Editing Remote Target Credentials Edit credentials for a remote target. Deleting a Remote Target Delete a remote target. Administering Schedules Introduction to schedules. Mirroring Synopsis of mirrors and mirroring process. Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Deleting a Remote Target Delete a remote target. Prerequisites You must be a fabric manager or an infrastructure admin to\n            perform this operation. The remote target to delete must not be associated with a\n                volume. About this task You can delete a remote target that is not in use. Before you delete a remote target,\n                ensure that the data on the remote target is backed up. Follow the steps given below to delete a remote target. Procedure Log on to the Data Fabric UI . Select Fabric manager option from the dropdown on the Home page. Click Fabric Administration seen on the Home page. Select from the fabrics dropdown the fabric for which you wish for which the\n                    remote target has been created. Scroll down to the Remote targets card. NOTE: Alternatively, you can click Global namespace , click the fabric\n                            link in the table view, and navigate to the Remote targets tab\n                            for the fabric. Click the ellipsis under Actions for the remote target whose credentials\n                    you wish to delete. Click Delete . Confirm the deletion. Results The remote target is deleted. The data on the cold tier is\n            inaccessible. The remote target is no longer available for data tiering. (Topic last modified: 2023-11-01) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "32f57a1c-5494-4abd-86ac-308630304bee": "Optional. The number of slots to use for each trial of this experiment. The default value is 1; specifying a value greater than 1 means that multiple GPUs will be used in parallel. Training on multiple GPUs is done using data parallelism. Configuring slots_per_trial to be greater than max_slots is not sensible and will result in an error. Using slots_per_trial to enable data parallel training for PyTorch can alter the behavior of certain models, as described in the PyTorch documentation.",
        "73dc70ff-d2cb-4d1c-8c7e-227c2194f5aa": "Install Determined on Kubernetes Development Guide",
        "89bc1e1f-7226-4d9a-8a3d-b0650fb211df": "Connecting to Ray Cluster Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Feast Provides a brief overview of Feast in HPE Ezmeral Unified Analytics Software . Kubeflow Provides a brief overview of Kubeflow in HPE Ezmeral Unified Analytics Software . HPE Machine Learning Development Environment Provides a brief overview of HPE Machine Learning Development Environment ( HPE MLDE ) in HPE Ezmeral Unified Analytics Software . MLflow Provides a brief overview of MLflow in HPE Ezmeral Unified Analytics Software . Ray Provides a brief overview of Ray in HPE Ezmeral Unified Analytics Software . Connecting to Ray Cluster Describes how to connect to Ray clusters to submit jobs. Using JobSubmissionClient to Submit Ray Jobs Describes how to connect to Ray cluster and submit Ray jobs using JobSubmissionClient . Enabling Metrics in Ray Dashboard Describes how to enable metrics in Ray dashboard. Resource Configuration and Management Describes resource configuration and management for Ray. GPU Support for Ray Describes how to enable GPU, configure the GPU resources, and disable GPU for     Ray. Using whylogs with Ray Describes how to use whylogs with Ray. Ray Best Practices Lists the best practices for Ray. Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Connecting to Ray Cluster Describes how to connect to Ray clusters to submit jobs. NOTE The Ray Client has multithreading and connection issues which impact its reliability and\n        submitting Ray job using Ray Client is an outdated method. Hewlett Packard Enterprise recommends using JobSubmissionClient to submit Ray jobs. For details, see Using JobSubmissionClient to Submit Ray Jobs . To submit jobs using Ray, you can connect to Ray cluster in two different ways: Connecting to Ray in HPE Ezmeral Unified Analytics Software To connect to Ray in HPE Ezmeral Unified Analytics Software ,\n              run: ray.init(address=\"ray://kuberay-head-svc.kuberay:10001\") Connecting to Ray from outside of HPE Ezmeral Unified Analytics Software To connect to Ray cluster from outside of HPE Ezmeral Unified Analytics Software , perform the following steps: Change service type to\n                  NodePort. > kubectl -n kuberay edit service kuberay-head-svc\nspec:\n...\n  type: NodePort\n... Get cluster master IP. > kubectl cluster-info Get client\n                  port. > kubectl -n kuberay describe service kuberay-head-svc\n... \nPort:                     client  10001/TCP\nTargetPort:               10001/TCP\nNodePort:                 client  31536/TCP\nEndpoints:                10.244.1.85:10001 Connect through <K8 Master IP>:<Client\n                  Port> . ray.init(address=\"ray://<K8 Master IP>:31536\") Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "d16f0fc7-f5fc-489d-ba3e-bc897bb2f31c": "If you are looking to enable RBAC on an existing Determined installation, please see the migration guide below. By default, a Determined installation comes with role-based access control disabled. To enable RBAC, set the following option in the master config: security: authz: type: rbac Brand new Determined installations include two user accounts: The admin user has full cluster access by default through the pre-canned ClusterAdmin role. The determined user has no permissions. Both accounts have empty passwords. You are encouraged to set strong passwords or deactivate these accounts for security reasons.",
        "626c8d5f-6a02-4100-a321-ac88541d2a94": "Release Date: May 24, 2022 New Features Web: Themes have been introduced and styles have been adjusted to support various themes. Theme switching is currently limited to dark/light mode and is set first through OS-level preferences, then through browser-level preference. In-app controllers will be coming soon. Add experimental support for recovering live trials on master process restart. Users can restart the master (with updated configuration options or an upgraded software version), and the current running trials will continue running using the original configuration and harness versions. This requires the agent to reconnect within a configurable agent_reconnect_wait period. This is only available for the agent resource manager, and can be enabled for resource pools using the agent_reattach_enabled flag. May only be available for patch-level releases. Web: A trial restart counter has been added to the experiment detail header for single-trial experiments. For multi-trial experiments, trial restart counts are shown in a new Restarts column in the Trials table. Improvements Security: Improved security by requiring admin privileges for the following actions. Reading master config. Enabling or disabling an agent. Enabling or disabling a slot. Logging: Ensure logs for very short tasks are not truncated in Kubernetes. Web: Centralize sidebar options Cluster, Job Queues, and Cluster Logs into Cluster page for a simplified layout. Web: In order to provide a more precise view of resource pools, new fields like accelerator and warm slots have been added. Web: Clicking on resource pool cards will lead to a detail page, which also includes a Stats tab showing average queued time by day. Breaking Changes Security: The following routes and CLI commands now need admin privileges. /config /api/v1/master/config /api/v1/agents/:agent_id/enable /api/v1/agents/:agent_id/disable /agents/:agent_id/slots/:slot_id /api/v1/agents/:agent_id/slots/:slot_id/enable /api/v1/agents/:agent_id/slots/:slot_id/disable det master config det agent enable det agent disable det slot enable det slot disable Logging: The default Fluent Bit version in all deployment modes is now 1.9.3, changed from 1.6. Bug Fixes Web: Fix the user filtering for migrating from Determined 0.17.15 to Determined 0.18.0. API: Fix an issue where the POST /users endpoint always returned an error instead of the user\u2019s information, even when the user was created successfully.",
        "6960ed2d-d338-46be-9b35-20152ed769db": "It may take a few minutes for your DNS\n            settings to propagate. Access the HPE Ezmeral Unified Analytics Software home page by clicking the green bar that reads Open HPE Ezmeral Unified Analytics Software . Note the Product ID in the window. You need the Product ID to activate the HPE Ezmeral Unified Analytics Software service. To activate the HPE Ezmeral Unified Analytics Software service, follow the steps listed in Service Activation and Billing in Connected Environments . (Air-gapped environments only) For a successful Airflow installation, manually set the\n            HTTP proxy or configure Airflow to point to your internal GitHub repository, as\n            described in Airflow DAGs Git Repository . This step is required in an air-gapped\n            environment because Airflow is pre-configured to pull DAGs from an HPE GitHub\n            repository. In air-gapped environments, Airflow cannot access the HPE repository. Run the following command to update the SPIFFE CSI\n            driver: kubectl -n spire set image ds spire-spiffe-csi-driver  spiffe-csi-driver=ghcr.io/spiffe/spiffe-csi-driver:0.2.5 For\n            details, see Host (Node) Management . Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "01066dd7-d390-48ef-aee8-40d696a134e4": "Operating System Support Matrix Jump to main content Get Started Platform Administration Reference Home Reference Provides reference information for the HPE Ezmeral Data Fabric . Operating System Support Matrix The tables on this page show the Linux operating-system versions that are supported for HPE Ezmeral Data Fabric releases. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Reference Provides reference information for the HPE Ezmeral Data Fabric . Release History Describes the currently released versions of the HPE Ezmeral Data Fabric as-a-service platform. Cloud Instance Specifications Compares different aspects of the supported cloud instances of the HPE Ezmeral Data Fabric . Third-Party Storage Solutions Describes global-namespace support for HPE partner storage technologies, including     Scality, WEKA, and VAST. Port Information Describes the ports used by HPE Ezmeral Data Fabric services. maprcli Commands in This Guide Describes how to use maprcli commands provided as reference links in     this guide. Operating System Support Matrix The tables on this page show the Linux operating-system versions that are supported for HPE Ezmeral Data Fabric releases. Doc Site Available as a PDF Provides a link to the downloadable PDF file containing all the information for the     current release. Product Licensing Provides information related to product licensing. Other Resources Provides links to additional resources such as on-demand training, videos, blogs, and     the HPE Ezmeral Data Fabric community. Contact HPE Provides a link to contact HPE Sales or Support. Operating System Support Matrix The tables on this page show the Linux operating-system versions that are supported for HPE Ezmeral Data Fabric releases. Red Hat Enterprise Linux (64-bit) RHEL Version Release 7.6.0 Release 7.5.0 8.8 Yes Yes 8.6 Yes Yes 8.5 Yes Yes 8.4 Yes Yes 8.3 Yes Yes 8.2 Yes Yes 8.1 Yes Yes Rocky Linux (64-bit) Rocky Version Release 7.6.0 Release 7.5.0 8.5 Yes Yes 8.4 Yes Yes Ubuntu (64-bit) Ubuntu Version Release 7.6.0 Release 7.5.0 20.04 Yes Yes 18.04 Yes Yes SLES (64-bit) SLES Version Release 7.6.0 Release 7.5.0 15 SP3 Yes Yes 15 SP2 Yes Yes Oracle Enterprise Linux (OEL) OEL Version Release 7.6.0 Release 7.5.0 8.4 Yes Yes 8.3 Yes Yes 8.2 Yes Yes (Topic last modified: 2024-01-05) On this page Red Hat Enterprise Linux (64-bit) Rocky Linux (64-bit) Ubuntu (64-bit) SLES (64-bit) Oracle Enterprise Linux (OEL) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "08b346ff-0c1c-45bc-a9be-a07513a381b6": "It provides a shared hierarchical             namespace that is organized like a standard file system. data-fabric administrator The \" data-fabric user.\"\n            The user that cluster services run as (typically named mapr or hadoop ) on each node. See data-fabric user . (Topic last modified: 2020-07-13) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "ce245bb6-1bb8-48fb-b47a-e46295de334a": "If you cannot reach the Determined master after installing Determined on Kubernetes, follow these debugging steps: # Get the name of the Helm deployment. helm list # Double check the IP address and port assigned to the Determined master by looking up the master service. kubectl get service determined-master-service-development-<helm deployment name> # Check the status of master deployment. kubectl describe deployment determined-master-deployment-<helm deployment name> # Check the logs of master pod. kubectl logs <determined-master-pod-name>",
        "2599390f-b20c-49ac-9a3f-3070e20c015b": "Determined supports arbitrary training and validation metrics reduction, including during distributed training, by letting you define custom reducers. Custom reducers can be a function or an implementation of the determined.pytorch.MetricReducer interface. See determined.pytorch.PyTorchTrialContext.wrap_reducer() for more information.",
        "eff857ff-060a-496a-a4e0-d8920906c25c": "Use the CLI to listing OAuth clients: det oauth client list",
        "e1a11717-bcb3-4912-9227-8875b918ce19": "Required. The name of the validation metric used to evaluate the performance of a hyperparameter configuration.",
        "ce9e5f0b-22e3-4920-8db9-eeddfa06655c": "You need to implement either the evaluate_batch() or evaluate_full_dataset() method. To load data into the validation loop, define build_validation_data_loader(). To define reducing metrics, define evaluation_reducer(). For example, def evaluate_batch(self, batch: TorchData): images, target = batch output = self.model(images) validation_loss = self.criterion(output, target) return {\"validation_loss\": loss.item()}",
        "f027c65a-e51d-4c22-b8e6-0b09d2596831": "The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage.",
        "87f94444-3447-49c0-bced-80baf4b1a86f": "Binding and unbinding resource pools allows administrators to control resource pool availability within the cluster. Resource pools can be either unbound, meaning they are shared across the entire cluster, or bound to specific workspaces. Experiments, notebooks, TensorBoards, shells, or commands associated with a particular workspace can only use resource pools that are either unbound or bound to a particular workspace. In addition, you can set a bound resource pool as the default compute or auxiliary pool for the workspace. If a user leaves the resource pool configuration option blank for their task, workloads will be sent to the default compute or auxiliary pool. When combined with Role-Based Access Control (RBAC), administrators can restrict compute resources to specific users and groups, enabling resource multi-tenancy for experiments and related artifacts.",
        "85715596-1bba-4f84-872d-3394de9d724f": "An instance needs the following permissions to upload logs to CloudWatch: cloudwatch:PutMetricData Instances will upload their metrics to namespace Determined. An example IAM policy with the appropriate permissions is below. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"cloudwatch:PutMetricData\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }",
        "eda51698-8bf9-4f31-899f-bffae6910d29": "This section provides instructions for installing Determined on WSL using Docker Desktop.",
        "98499020-e4a4-422b-a5b5-45deeff44aad": "The available values are\n                      NONE, SNAPPY, GZIP, LZ4, and ZSTD GZIP STRING possibleValues(NONE, SNAPPY, GZIP, LZ4, ZSTD) Iceberg Catalog Cached Catalog Num The number of Iceberg catalogs to cache, This property is required if the\n                      iceberg.catalog.type is hadoop 10 INTEGER Iceberg Max Partitions Per Writer The Maximum number of partitions handled per writer. 100 INTEGER Iceberg Minimum Assigned Split Weight A decimal value in the range (0, 1] used as a minimum for weights\n                      assigned to each split 0.05 DOUBLE Hive Metastore The type of Hive metastore to use thrift STRING possibleValues(thrift, file, glue) Hive Metastore Catalog Dir Hive file-based metastore catalog directory STRING Hive Metastore Uri Hive metastore URIs (comma separated). STRING Hive Metastore Service Principal The Kerberos principal of the Hive metastore service STRING Hive Metastore Client Principal The Kerberos principal that Presto will use when connecting to the Hive\n                      metastore service. STRING Hive Metastore Client Keytab Hive metastore client keytab location. FILEPATH Hive Hdfs Presto Principal The Kerberos principal that presto will use when connecting to\n                      HDFS STRING Hive Hdfs Presto Keytab HDFS client keytab location FILEPATH Security Config File Config file where rules are defined STRING Security Refresh Period Time after which rules will be refreshed from the file. DURATION Min(1ms) Enable Local Snapshot Table Enables local copy of database table for accelerated query\n                      performance TRUE BOOLEAN Optional Connection Parameters Parameter Description Default Value Data Type Possible Values Iceberg Hadoop Config Resources The path(s) for Hadoop configuration resources. FILEPATH Iceberg Catalog Warehouse The catalog warehouse root path for Iceberg tables. STRING Hive Metastore User Hive file-based metastore username for file access presto STRING Hive Metastore Glue Region AWS region of the Glue Catalog. STRING Hive Metastore Glue Endpoint Url Glue API endpoint URL STRING Hive Metastore Glue Pin Client To Current Region Should the Glue client be pinned to the current EC2 region FALSE BOOLEAN Hive Metastore Glue Max Connections Max number of concurrent connections to Glue 5 INTEGER Min(1) Hive Metastore Glue Max Error Retries Maximum number of error retries for the Glue client 10 INTEGER Min(0) Hive Metastore Glue Default Warehouse Dir Hive Glue metastore default warehouse directory STRING Hive Metastore Glue Catalogid The ID of the Glue Catalog in which the metadata database\n                      resides. STRING Hive Metastore Glue Partitions Segments Number of segments for partitioned Glue tables. 5 INTEGER Min(1), Max(10) Hive Metastore Glue Get Partition Threads Number of threads for parallel partition fetches from Glue. 20 INTEGER Min(1) Hive Metastore Glue Iam Role ARN of an IAM role to assume when connecting to the Glue Catalog. STRING Hive Metastore Glue Aws Access Key AWS access key to use to connect to the Glue Catalog. If specified along\n                      with hive.metastore.glue.aws-secret-key, this parameter takes precedence over\n                      hive.metastore.glue.iam-role. STRING Hive Metastore Glue Aws Secret Key AWS secret key to use to connect to the Glue Catalog. If specified along\n                      with hive.metastore.glue.aws-access-key, this parameter takes precedence over\n                      hive.metastore.glue.iam-role. STRING Hive Metastore Username Username for accessing the Hive metastore STRING Hive Metastore Load Balancing Enabled Enable load balancing between multiple Metastore instances FALSE BOOLEAN Hive Insert Overwrite Immutable Partitions Enabled When enabled, insertion query will overwrite existing partitions when\n                      partitions are immutable.",
        "0a3b3961-43d7-4dd7-924f-4c41d2e06f26": "Acknowlegements Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Acknowlegements Microsoft \u00ae and Windows \u00ae are either registered trademarks or trademarks of Microsoft Corporation in the United States and/or other countries. UNIX \u00ae is a registered trademark of The Open Group. All third-party marks are property of their respective owners. Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "17f93c03-878d-43d7-831a-e5c252065ed1": "The sudo configuration necessary to enable the launcher to perform Slurm/PBS job management on behalf of the requesting Determined user is automatically generated and applied during the startup of the launcher service as specified in the slurm/pbs section of the cluster configuration. Configuration is added to the sudo configuration by the file /etc/sudoers.d/zz_launcher. The configuration is dervied from the following values: The authorized user is configured as resource_manager.user_name (shown below as launcher). The run-as user list is configured to authorize resource_manager.sudo_authorized (shown below as the default value of ALL). A comma-separated list of user/group specifications identifying users for which the launcher can submit/control Slurm/PBS jobs using sudo. The specification !root is automatically appended to this list to prevent privilege elevation. This may be a list of users or groups with exclusions (e.g. %slurmusers,localadmin,!guest ). See the sudoers(5) definition of Runas_List for the full syntax of this value. For Slurm, the authorized commands are the full path to each of the commands sacct, salloc, sbatch, scancel, scontrol, sinfo, squeue, srun. For PBS, the authorized commands are the full path to qsub, qstat, qdel, pbsnodes. The content of a typical /etc/sudoers.d/zz_launcher generated for Slurm is shown below: launcher ALL= (root) NOPASSWD: /bin/chown -R * * launcher ALL= (root) NOPASSWD: /usr/bin/chown -R * * ALL ALL = (root) NOPASSWD: /opt/launcher/bin/user-keytool launcher ALL= (ALL, !root) NOPASSWD:SETENV: /usr/bin/sacct launcher ALL= (ALL, !root) NOPASSWD:SETENV: /usr/bin/salloc launcher ALL= (ALL, !root) NOPASSWD:SETENV: /usr/bin/sbatch launcher ALL= (ALL, !root) NOPASSWD:SETENV: /usr/bin/scancel launcher ALL= (ALL, !root) NOPASSWD:SETENV: /usr/bin/scontrol launcher ALL= (ALL, !root) NOPASSWD:SETENV: /usr/bin/sinfo launcher ALL= (ALL, !root) NOPASSWD:SETENV: /usr/bin/squeue launcher ALL= (ALL, !root) NOPASSWD:SETENV: /usr/bin/srun As noted above, this file is regenerated during the startup of the launcher service. It should not be edited directly and should be configured using the attributes provided in the slurm/pbs section of the cluster configuration.",
        "00ba0376-cec0-4f80-8973-1e148d0891d1": "About Security Policy Domain Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Security Policies Add, edit, delete, and manage state of security policies. About Security Policy Domain Describes a security policy domain. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. About Security Policy Domain Describes a security policy domain. Security Policy Implementation Workflow Describes the security policy workflow, in general, and the steps in implementing a         security policy. Security Policy Enforcement Process Describes the steps followed during security policy enforcement on         volumes. Understanding Access Control in a Security Policy The implications of permissions assigned to users and groups in a security         policy. Managing File and Directory ACEs Describes the implications of setting access control expressions on files and             directories. Security Policy Permissions Permissions define which administrative users can create, view, and modify security     policies. Administrators set the permissions on security policies through cluster-level and     security policy-level ACLs. Designating a Fabric as Global Policy Master Designate a fabric in the global namespace as the global policy master. Creating a Security Policy Add a security policy on the global policy master. Viewing a Security Policy View security policy details. Viewing All Security Policies View all security policies on the Data Fabric UI. Editing a Security Policy Make changes to a security policy. Assigning a Security Policy to One or More Volumes Assigning Multiple Security Policies to One or More Volumes Describes how to assign multiple security policies to volumes. Unassigning One or More Security Policies from a Volume Unassign a policy from a volume to which it has been previously assigned. Disabling a Security Policy Describes how to disable a security policy. Enabling a Security Policy Describes how to enable a security policy. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. About Security Policy Domain Describes a security policy domain. A security policy domain is a group of fabrics that directly or indirectly share data and\n            use the same security policies to control access to the data. A security policy domain\n            consists of one master fabric and zero or more member security policy fabrics that\n            create a global security policy namespace. A global policy master is a prerequisite for the creation of security policies. A global\n            policy master is a fabric on which security policies can be created. You can create and modify security policies only on the fabric that is designated as the\n            global policy master. When you create or update security policies, the policy server\n            updates the mapr.pbs.base volume with the security policy metadata.\n            Subsequently, the security policies are mirrored to other member fabrics in the global\n            namespace. By default, the first fabric or the primary fabric that you create on the global\n            namespace is designated as the global policy master. Hence, it is not required to\n            explicitly assign an alternate global policy master, unless the primary fabric goes\n            down. Each fabric, to which a security policy is applied, operates independently and,\n            therefore, does not require network connectivity to the global policy master to enforce\n            policies.",
        "0d690a01-010c-4b5a-bdcf-496016c0ebbb": "For\n              example: ezua-airgap-util --release <release-number> --lessthan 1mb --copy --dest_path images/ --force\nezua-airgap-util --release <release-number> --lessthan 1mb --copy --dest_path images/ --dest_compress --force To copy multiple images to a local filesystem, run the following command.\n                Provide the destination path where you want to store your\n                files. ezua-airgap-util --release <release-number> <add-on_filters> --copy --dest_path <destination-path> To copy a single image to a local filesystem, execute the following\n                command. Provide the destination path where you want to store your\n                files. ezua-airgap-util --release <release-number> --image <image-name> --copy --dest_path <destination-path> To copy multiple images to a remote container registry, select one of the\n                following options. Provide the destination URL and credentials for your container\n                  registry. Use the --dest_creds <username:password> command line\n                    option: ezua-airgap-util --release <release-number> <add-on-filters> --copy --dest_url <destination-url> --dest_creds <username:password> Alternatively, set environment variable AIRGAP_UTIL_CREDS .\n                    You can set environmental variables using the export command: export AIRGAP_UTIL_CREDS=<username>:<password> To copy a single image to a remote container registry, execute the\n                    following command. Provide the destination URL and credentials for your\n                    container\n                    registry. ezua-airgap-util --release <release-number> --image <image-name> --copy --dest_url <destination-url> --dest_creds <username:password> Air Gap Utility Logging By default, the Air Gap Utility creates a logs/ directory in the present\n        working directory from which you invoked the Air Gap Utility command line. You can change the log directory location as follows: If you pass the --logdir argument in the Air Gap Utility command\n            line, then the Air Gap Utility creates a logs/ directory in the path\n            provided in the --logdir arguement. If you set the AIRGAP_UTIL_LOGDIR environment variable, but do not\n            pass the --logdir argument in the Air Gap Utility command line, then\n            the Air Gap utility creates a logs/ directory in the path set in the AIRGAP_UTIL_LOGDIR environment variable. NOTE The Air Gap Utility does not create log files when commands are run in TTY mode. For\n              example: ezua-airgap-util --release v1.3.0 | grep -i airflow Using Skopeo --options with the Air Gap Utility This section describes how to use Skopeo --options with the Air Gap\n        Utility and provides usage examples. The following examples show the Skopeo --preserve-digests and --retry-times options used with the Air Gap\n        Utility: ezua-airgap-util --release v1.3.0 --image longhornio/livenessprobe:v2.9.0 --copy --dest_path ezua-v1.3.0/ --options=\"--preserve-digests\" zua-airgap-util --release v1.3.0 --image longhornio/livenessprobe:v2.9.0 --copy --dest_path ezua-v1.3.0/ --options=\"--retry-times 5\" You\n        can use multiple Skopeo options with the Air Gap Utility. The following example demonstrates how to use the Skopeo --preserve-digests and --retry-times options\n        together: ezua-airgap-util --release v1.3.0 --image longhornio/livenessprobe:v2.9.0 --copy --dest_path ezua-v1.3.0/ --options=\"--preserve-digests --retry-times 5\" On this page Requirements About the Air Gap Utility Installing the Air Gap Utility Package Using Air Gap Utility Filters Downloading Air Gap Files Air Gap Utility Logging Using Skopeo --options with the Air Gap Utility Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "37b1b1d2-5f33-460c-986b-98edb9c61d69": "To configure the master to use TLS, set the security.tls.cert and security.tls.key options to paths to a TLS certificate file and key file. When TLS is in use, the master will listen on TCP port 8443 by default, rather than 8080. If the master\u2019s certificate is not signed by a well-known CA, then the configured certificate file must contain a full certificate chain that goes all the way to a root certificate.",
        "4cdebab7-ca63-4a13-8932-eb19533c2551": "Determined includes a built-in model registry to manage trained models and their respective versions.",
        "53877d18-67db-4f64-be82-20ff274e40f2": "For every webhook request Determined will generate two headers, X-Determined-AI-Signature and X-Determined-AI-Signature-Timestamp, which can be used to verify each request to a webhook endpoint. The X-Determined-AI-Signature-Timestamp will represent the time at which the request was generated and sent. You can choose to inspect this timestamp and decide whether to discard any requests with a timestamp that is too distant from the current time. The X-Determined-AI-Signature will be a representation of a \u201csigned\u201d request payload. The signed request payload will be generated in the following way: Combine the timestamp in X-Determined-AI-Signature-Timestamp, the comma character \u201c,\u201d and the request body, which will be the entire event payload. Create an HMAC using SHA256 hashing, with the webhook_signing_key and the event payload from previous step. You can then check to make sure the X-Determined-AI-Signature header value and the generated signed payload match. Below is an example of handling a signed payload in Python. import hashlib, hmac, json # User-defined function to authenticate webhook requests def authenticate_webhook_request(request_body, request_headers, webhook_signing_key): timestamp = request_headers[\"X-Determined-AI-Signature-Timestamp\"] signed_payload = request_headers[\"X-Determined-AI-Signature\"] request_body = json.dumps(separators=(\",\", \":\"), obj=json.loads(request_body)) calculated_signed_payload = hmac.new( webhook_signing_key.encode(), f\"{timestamp},{request_body}\".encode(), digestmod=hashlib.sha256, ).hexdigest() return calculated_signed_payload == signed_payload The request body in the function shown above will be the JSON payload from the request. Ensure that the JSON payload does not contain spaces between keys and their values when creating the signed payload. For example \u201c{\u201ckey_one\u201d: \u201cvalue_one\u201d}\u201d will fail authentication, while \u201c{\u201ckey_one\u201d:\u201dvalue_one\u201d}\u201d will yield the correct signed payload value.",
        "b1a5820d-0f03-4e23-ab9e-8552d235d705": "First, specify the ports in the environments -> proxy_ports section of the experiment or task config, for example: environment: proxy_ports: - proxy_port: 8265 proxy_tcp: true Launch your task or experiment normally. Then, use the det CLI to start a tunnel. Running the following command will setup a tunnel proxying localhost:8265 to port 8265 in the task container. python -m determined.cli.tunnel --listener 8265 --auth $DET_MASTER $TASK_ID:8265 where $DET_MASTER is your Determined master address, and $TASK_ID is the task id of the launched task or experiment. You can look up the task id using CLI command det task list. Alternatively, you can use a shortcut which allows to launch the experiment, follow its logs, and run the tunnel all at once: det e create config_file.yaml model_def -f -p 8265",
        "177b005e-42c7-4e50-ae1d-388a84c75fe5": "AD/LDAP Servers Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Installation Provides links to HPE Ezmeral Unified Analytics Software installation and service activation topics. Identity and Access Management Describes identity and access management in HPE Ezmeral Unified Analytics Software . User Isolation Describes user isolation in HPE Ezmeral Unified Analytics Software . User Roles Describes roles that you can assign to users in HPE Ezmeral Unified Analytics Software . AD/LDAP Servers Describes the differences between the internal OpenLDAP server in HPE Ezmeral Unified Analytics Software and external AD/LDAP     servers. Also describes some of the server-related configuration options that you set during     installation. Working with Certs and the Truststore Describes how to provide a truststore with a valid server certificate, including how to     view and locate certs, as well as how to create and validate a truststore for certs. Adding and Removing Users Describes how administrators can add and remove users in HPE Ezmeral Unified Analytics Software . Adding and Removing Users Programmatically Describes how to add and remove users through the Kubernetes API using the EzUserQuery     and EzUserConfig custom resources. Managing Data Access Expanding the Cluster Describes how to add additional user-provided hosts to the management cluster to increase resource capacity and how     to expand the cluster to include the additional user-provided hosts. Shutting Down an HPE Ezmeral Unified Analytics Software Cluster Describes how to gracefully shut down an HPE Ezmeral Unified Analytics Software cluster when you want to perform maintenance or upgrade     tasks. Importing Applications and Managing the Application Lifecycle Describes how to import, manage, and secure applications and frameworks in HPE Ezmeral Unified Analytics Software . Connecting to External S3 Object Stores Describes how to connect HPE Ezmeral Unified Analytics Software to external S3 object storage in AWS, MinIO, and HPE Ezmeral Data Fabric Object Store . Connecting to External HPE Ezmeral Data Fabric Clusters Describes how to connect HPE Ezmeral Unified Analytics Software to an external HPE Ezmeral Data Fabric cluster. Configuring Endpoints Describes the endpoints in HPE Ezmeral Unified Analytics Software and how to configure them. GPU Support Provides information about support for NVIDIA GPU, MIG partitioning, preparing hosts     for GPU-enabled environment, adding hosts and enabling GPU in HPE Ezmeral Unified Analytics Software . Troubleshooting Describes how to identify and debug issues in HPE Ezmeral Unified Analytics Software . Support Matrix The tables on this page show the tools and frameworks, HPE Ezmeral Data Fabric ,operating system       versions , and GPU models that are supported for HPE Ezmeral Unified Analytics Software releases. Release Notes This document provides a comprehensive overview of the latest updates and enhancements     in HPE Ezmeral Unified Analytics Software (version     1.3.0), including new features, improvements, bug fixes, and known issues. Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. AD/LDAP Servers Describes the differences between the internal OpenLDAP server in HPE Ezmeral Unified Analytics Software and external AD/LDAP\n    servers. Also describes some of the server-related configuration options that you set during\n    installation. When you install HPE Ezmeral Unified Analytics Software , the configuration options vary depending on whether you use the\n      internal OpenLDAP server (default) included with HPE Ezmeral Unified Analytics Software or an external AD/LDAP server. After installation, the designated administrator can sign in and grant users permission to\n      access HPE Ezmeral Unified Analytics Software and\n      assign roles. A user management operator running in HPE Ezmeral Unified Analytics Software sets up local resources for users, such as their user\n      profile and workspace, and also enables access. NOTE SSO does not support applications that use AD/LDAP integration to validate credentials\n            presented to an external service. The AD/LDAP server supports access by PLAIN (unsecured) LDAP, LDAPS, or StartTLS.",
        "40a06deb-8be3-4db0-9a49-54aabb77f5e6": "When the number of copies falls below the desired replication factor, but remains equal\n            to or above the minimum replication factor , re-replication occurs\n            after the timeout specified in the cldb.fs.mark.rereplicate.sec parameter. (Topic last modified: 2020-07-13) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "2f75edea-10b1-4d55-8500-80b4471fbc62": "TLS-related configuration settings. enabled: Enable TLS. skip_verify: Skip server certificate verification. certificate: Path to a file containing the cluster\u2019s TLS certificate. Only needed if the certificate is not signed by a well-known CA; cannot be specified if skip_verify is enabled. additional_fluent_outputs: An optional configuration string containing additional Fluent Bit outputs for advanced users to specify logging integrations. See the Fluent Bit documentation for the format and supported logging outputs.",
        "318de1d9-3b4d-46a9-8e8a-c4259cbf9ace": "class determined.core._distributed.DistributedContext*rank: intsize: intlocal_rank: intlocal_size: intcross_rank: intcross_size: intchief_ip: Optional[str] = Nonepub_port: int = 12360pull_port: int = 12376port_offset: int = 0force_tcp: bool = False DistributedContext provides useful methods for effective distributed training. A DistributedContext has the following required args: rank: the index of this worker in the entire job size: the number of workers in the entire job local_rank: the index of this worker on this machine local_size: the number of workers on this machine cross_rank: the index of this machine in the entire job cross_size: the number of machines in the entire job Additionally, any time that cross_size > 1, you must also provide: chief_ip: the ip address to reach the chief worker (where rank==0) DistributedContext has .allgather(), .gather(), and .broadcast() methods, which are easy to use and which can be useful for coordinating work across workers, but it is not a replacement for the allgather/gather/broadcast operations in your particular distributed training framework. classmethod from_horovodhvd: Anychief_ip: Optional[str] = Nonedetermined.core._distributed.DistributedContext Create a DistributedContext using the provided hvd module to determine rank information. Example: import horovod.torch as hvd hvd.init() distributed = DistributedContext.from_horovod(hvd) The IP address for the chief worker is required whenever hvd.cross_size() > 1. The value may be provided using the chief_ip argument or the DET_CHIEF_IP environment variable. classmethod from_deepspeedchief_ip: Optional[str] = Nonedetermined.core._distributed.DistributedContext Create a DistributedContext using the standard deepspeed environment variables to determine rank information. The IP address for the chief worker is required whenever CROSS_SIZE > 1. The value may be provided using the chief_ip argument or the DET_CHIEF_IP environment variable. classmethod from_torch_distributedchief_ip: Optional[str] = Nonedetermined.core._distributed.DistributedContext Create a DistributedContext using the standard torch distributed environment variables to determine rank information. The IP address for the chief worker is required whenever CROSS_SIZE > 1. The value may be provided via the chief_ip argument or the DET_CHIEF_IP environment variable. get_rankint Return the rank of the process in the trial. The rank of a process is a unique ID within the trial. That is, no two processes in the same trial are assigned the same rank. get_local_rankint Return the rank of the process on the agent. The local rank of a process is a unique ID within a given agent and trial; that is, no two processes in the same trial that are executing on the same agent are assigned the same rank. get_sizeint Return the number of slots this trial is running on. get_num_agentsint Return the number of agents this trial is running on. gatherstuff: AnyOptional[List] Gather stuff to the chief. The chief returns a list of all stuff, and workers return None. gather() is not a replacement for the gather functionality of your distributed training framework. gather_localstuff: AnyOptional[List] Gather stuff to the local chief. The local chief returns a list of all stuff, and local workers return None. gather_local() is not a replacement for the gather functionality of your distributed training framework. allgatherstuff: AnyList Gather stuff to the chief and broadcast all of it back to the workers. allgather() is not a replacement for the allgather functionality of your distributed training framework. allgather_localstuff: AnyList Gather stuff to the local chief and broadcast all of it back to the local workers. allgather_local() is not a replacement for the allgather functionality of your distributed training framework. broadcaststuff: AnyAny Every worker gets the stuff sent by the chief. broadcast() is not a replacement for the broadcast functionality of your distributed training framework. broadcast_localstuff: Optional[Any] = NoneAny Every worker gets the stuff sent by the local chief. broadcast_local() is not a replacement for the broadcast functionality of your distributed training framework.",
        "63fbce16-2cbf-49ab-802b-fd46870e12ce": "This works with IAM roles in EC2. FALSE BOOLEAN Hive S3 Encryption Materials Provider Use a custom encryption materials provider for S3 data encryption STRING Hive S3 Multipart Min File Size Minimum file size for an S3 multipart upload 16MB DATASIZE Hive S3 Multipart Min Part Size Minimum part size for an S3 multipart upload 5MB DATASIZE Hive S3 Pin Client To Current Region Pin S3 requests to the same region as the EC2 instance where Presto is\n                      running FALSE BOOLEAN Hive S3 Upload Acl Type Canned ACL type for S3 uploads PRIVATE STRING possibleValues(AUTHENTICATED_READ, AWS_EXEC_READ,\n                      BUCKET_OWNER_FULL_CONTROL, BUCKET_OWNER_READ, LOG_DELIVERY_WRITE, PRIVATE,\n                      PUBLIC_READ, PUBLIC_READ_WRITE) Hive S3 User Agent Prefix The user agent prefix to use for S3 calls STRING Hive S3 Skip Glacier Objects Ignore Glacier objects rather than failing the query. This will skip data\n                      that may be expected to be part of the table or partition FALSE BOOLEAN Hive S3 Sse Enabled Use S3 server-side encryption FALSE BOOLEAN Hive S3 Sse Type The type of key management for S3 server-side encryption S3 STRING possibleValues(S3, KMS) Hive S3 Max Client Retries Maximum number of read attempts to retry 5 INTEGER Min(0) Hive S3 Max Error Retries Maximum number of error retries, set on the S3 client 10 INTEGER Min(0) Hive S3 Max Backoff Time Use exponential backoff starting at 1 second up to this maximum value\n                      when communicating with S3 10.00m DURATION Min(1s) Hive S3 Max Retry Time Maximum time to retry communicating with S3 10.00m DURATION Min(1ms) Hive S3 Connect Timeout The default timeout for creating new connections. 5.00s DURATION Min(1ms) Hive S3 Socket Timeout The default timeout for reading from a connected socket. 5.00s DURATION Min(1ms) Hive S3 Max Connections Sets the maximum number of allowed open HTTP connections 500 INTEGER Min(1) Hive S3 Staging Directory Local staging directory for data written to S3. STRING Hive S3 Aws Access Key Default AWS access key to use. STRING Hive S3 Aws Secret Key Default AWS secret key to use. STRING Hive S3 Endpoint The S3 storage endpoint server. STRING Hive S3 Storage Class The S3 storage class to use when writing the data. STANDARD STRING possibleValues(STANDARD, INTELLIGENT_TIERING) Hive S3 Signer Type Specify a different signer type for S3-compatible storage STRING possibleValues(S3SignerType, AWS3SignerType, AWS4SignerType,\n                      AWSS3V4SignerType, CloudFrontSignerType, QueryStringSignerType) Hive S3 Path Style Access Use path-style access for all requests to the S3-compatible\n                      storage FALSE BOOLEAN Hive S3 Iam Role IAM role to assume STRING Hive S3 Iam Role Session Name AWS STS session name when IAM role to assume to access S3 buckets presto-session STRING Hive S3 Ssl Enabled Use HTTPS to communicate with the S3 API TRUE BOOLEAN Hive S3 Kms Key Id If set, use S3 client-side encryption and use the AWS KMS to store\n                      encryption keys and use the value of this property as the KMS Key ID for newly\n                      created objects STRING Hive S3 Sse Kms Key Id The KMS Key ID to use for S3 server-side encryption with KMS-managed\n                      keys STRING Hive Gcs Json Key File Path JSON key file used to access Google Cloud Storage FILEPATH Hive Gcs Use Access Token Use client-provided OAuth token to access Google Cloud Storage FALSE BOOLEAN Hive Orc Use Column Names Access ORC columns using names from the file FALSE BOOLEAN Hive Orc Max Merge Distance ORC: Maximum size of gap between two reads to merge into a single\n                      read 1MB DATASIZE Hive Orc Max Buffer Size ORC: Maximum size of a single read 8MB DATASIZE Hive Orc Stream Buffer Size ORC: Size of buffer for streaming reads 8MB DATASIZE Hive Orc Max Read Block Size ORC: Soft max size of Presto blocks produced by ORC reader 16MB DATASIZE Hive Rcfile Writer Validate Validate RCFile after write by re-reading the whole file FALSE BOOLEAN Hive Text Max Line Length Maximum line length for text files 100MB DATASIZE Min(1B), Max(1GB) Hive Parquet Use Column Names Access Parquet columns using names from the file FALSE BOOLEAN Hive File Status Cache Tables The tables that have file status cache enabled.",
        "f6088fb5-d5f5-49ed-a123-cff20c9b22b3": "In this section, we\u2019ll define our epoch metric. To follow along, use the model_def_metrics.py script and its accompanying metrics.yaml experiment configuration file. Our script, model_def_metrics.py, is a modification of the model_def.py script. It already reports training and validation metrics to the Determined master and contains a steps_completed variable that is needed to plot metrics on a graph in the WebUI. For a full description of the Core API PyTorch MNIST Tutorial files, visit the Core API User Guide.",
        "5228703e-9531-4684-8e86-77c46beb9a0a": "Jupyter Notebooks are a convenient way to develop and debug machine learning models, visualize the behavior of trained models, or even manage the training lifecycle of a model manually. Determined makes it easy to launch and manage notebooks. Determined Notebooks have the following benefits: Jupyter Notebooks run in containerized environments on the cluster. We can easily manage dependencies using images and virtual environments. The HTTP requests are passed through the master proxy from and to the container. Jupyter Notebooks are automatically terminated if they are idle for a configurable duration to release resources. A notebook instance is considered to be idle if it is not receiving any HTTP traffic and it is not otherwise active (as defined by the notebook_idle_type option in the task configuration). Once a Notebook is terminated, it is not possible to restore the files that are not stored in the persistent directories. You need to ensure that the cluster is configured to mount persistent directories into the container and save files in the persistent directories in the container. See Save and Restore Notebook State for more information. If you open a Notebook tab in JupyterLab, it will automatically open a kernel that will not be shut down automatically so you need to manually terminate the kernels.",
        "8ff5a00e-15ff-4daa-9b2d-d685c9c6ce24": "It provides a shared hierarchical             namespace that is organized like a standard file system. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM\n    users permissions to perform resource operations, such as putting objects in a bucket. You\n    associate access policies with accounts, users, buckets, and objects. (Topic last modified: 2022-01-19) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "6e2058bc-0e7e-4b95-8397-ea2a1cd4a932": "Optional. The maximum number of times we evaluate intermediate results for a trial and terminate poorly performing trials. The default value is 5; only advanced users should consider changing this value.",
        "117d2013-b7f4-48cd-80c5-4805db439e15": "You can use Jupyter Notebooks to conveniently develop and debug machine learning models, visualize the behavior of trained models, and manage the training lifecycle of a model manually. Determined makes launching and managing notebooks easy. Determined Notebooks provide the following benefits: Jupyter Notebooks run in containerized environments on the cluster. This makes it easy to manage dependencies using images and virtual environments. The HTTP requests are passed through the master proxy from and to the container. Jupyter Notebooks can be automatically terminated if they are idle for a configurable duration to release resources. A notebook instance is considered to be idle if it is not receiving any HTTP traffic and it is not otherwise active (as defined by the notebook_idle_type option in the task configuration). To enable this behavior by default, set notebook_timeout option in your master config. To enable it for a particular notebook, set idle_timeout option in the notebook config. After a Notebook is terminated, it is not possible to restore the files that are not stored in the persistent directories. It is important to configure the cluster to mount persistent directories into the container and save files in the persistent directories in the container. See Save and Restore Notebook State for more information. If you open a Notebook tab in JupyterLab, a kernel is automatically opened. This kernel will not be shut down automatically, so you\u2019ll need to manually terminate it. There are two ways to access notebooks in Determined: the CLI and the WebUI. To install the CLI, see Installation.",
        "e4c87a0d-cf11-4f79-a71d-d7cf2b86af7e": "This page describes the basic steps to create a new\n    fabric for any of the supported fabric providers (AWS, Azure, GCP, and on-premises). Before Creating a Fabric Note these considerations: If you are creating your first fabric, see Fabric Deployment Using a Seed Node . You\n            must use the seed node steps to create your first fabric. For all subsequent fabrics you\n            can use the steps on this page. To create a fabric, you must have fabric manager credentials . The Create Fabric button is not displayed\n            for developer and infrastructure admin credentials. Currently, only SSO users can create fabrics. You must obtain a license for and register each new fabric that you create. Always create fabrics one at a time. You cannot create multiple fabrics at the same\n            time. Creating an on-premises fabric requires that you provide host nodes before starting fabric creation. These nodes must meet certain prerequisites. Before creating\n            an on-premises fabric, review Prerequisites for On-Premises Installation . Steps for Creating a Fabric Use the following steps to create a new fabric. Log on to the Data Fabric UI with Fabric Manager credentials . Click Create fabric . The Create fabric form appears. Fill in the configuration parameters for the type of fabric you want to create: AWS Fabric Configuration Parameters Azure Fabric Configuration Parameters GCP Fabric Configuration Parameters On-Premises Fabric Configuration Parameters Click Create . To monitor the progress of fabric creation, check the status bar in the Fabric details dialog box, or click See\n              details . Fabric creation can take 20 minutes or\n              more. If fabric creation fails, you can retry the operation. Click the\n              ellipsis in the Action column, and select Reinitiate . If fabric creation continues to fail, and the\n              failure cannot be resolved manually, contact HPE Support . When the installation status shows Installed , click the\n            ellipsis ( ) in the Action column, and select View endpoints . The URL for the new fabric is displayed, and\n            you can copy the URL to the clipboard. Add your fabric activation key. See Adding an Activation Key . Register the fabric. See Registering a Fabric . Set the billing model. See Setting the Billing Model . (Topic last modified: 2024-01-16) On this page Before Creating a Fabric Steps for Creating a Fabric \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "aec4b0e1-db56-4e0d-b2f8-bef875c6d119": "class determined.searcher.SearcherState Custom Searcher State. Search runners maintain this state that can be used by a SearchMethod to inform event handling. In other words, this state can be taken into account when deciding which operations to return from your event handler. Do not modify SearcherState in your SearchMethod. If your hyperparameter tuning algorithm needs additional state variables, add those variable to your SearchMethod implementation. failures number of failed trials Type Set[uuid.UUID] trial_progress progress of each trial as a number between 0.0 and 1.0 Type Dict[uuid.UUID, float] trials_closed set of completed trials Type Set[uuid.UUID] trials_created set of created trials Type Set[uuid.UUID]",
        "db15338f-dd18-4429-8c1e-f650db65715f": "By default, the Helm chart deploys an instance of Postgres on the same Kubernetes cluster where Determined is deployed. If this is not what you want, you can configure the Helm chart to use an external Postgres database by setting db.hostAddress to the IP address of their database. If db.hostAddress is configured, the Determined Helm chart will not deploy a database.",
        "0b1550d1-91ad-40b5-8fcf-564f365f10f4": "Local checkpoint path: checkpoints/8d45f621-8652-4268-8445-6ae9a735e453 Batch | Checkpoint UUID | Validation Metrics -----------+--------------------------------------+------------------------------------------ 400 | 8d45f621-8652-4268-8445-6ae9a735e453 | { | | \"num_inputs\": 56, | | \"validation_metrics\": { | | \"val_loss\": 0.26509127765893936, | | \"val_categorical_accuracy\": 1 | | } | | } Local checkpoint path: checkpoints/62131ba1-983c-49a8-98ef-36207611d71f Batch | Checkpoint UUID | Validation Metrics -----------+--------------------------------------+------------------------------------------ 1600 | 62131ba1-983c-49a8-98ef-36207611d71f | { | | \"num_inputs\": 50, | | \"validation_metrics\": { | | \"val_loss\": 0.04411194706335664, | | \"val_categorical_accuracy\": 1 | | } | | } Local checkpoint path: checkpoints/a36d2a61-a384-44f7-a84b-8b30b09cb618 Batch | Checkpoint UUID | Validation Metrics -----------+--------------------------------------+------------------------------------------ 400 | a36d2a61-a384-44f7-a84b-8b30b09cb618 | { | | \"num_inputs\": 46, | | \"validation_metrics\": { | | \"val_loss\": 0.07265569269657135, | | \"val_categorical_accuracy\": 1 | | } | | }",
        "ffb1d1d3-3dac-45e9-a459-31843df41c77": "Your system must meet the software and hardware requirements described in the Installation Requirements.",
        "707a59b0-1eb3-40bb-a841-2038afed14d0": "When using TensorFlow models, the load() method returns a compiled model with weights loaded. This will be the same TensorFlow model returned by your build_model() method defined in your trial class specified by the experiment config entrypoint field. The trained model can then be used to make predictions as shown in the following snippet. from determined.experimental import client from determined import keras checkpoint = client.get_experiment(id).top_checkpoint() path = checkpoint.download() model = keras.load_model_from_checkpoint_path(path) predictions = model(samples) TensorFlow checkpoints are saved in either the saved_model or h5 formats and are loaded as trackable objects (see documentation for tf.compat.v1.saved_model.load_v2 for details).",
        "5121138b-ed93-4dbc-b626-18a96bfc0c2e": "15.     Integration.\n\n\nThis Agreement, including any terms contained in your Entitlement, is\nthe entire agreement between you and Oracle relating to its subject\nmatter. It supersedes all prior or contemporaneous oral or written\ncommunications, proposals, representations and warranties and prevails\nover any conflicting or additional terms of any quote, order,\nacknowledgment, or other communication between the parties relating to\nits subject matter during the term of this Agreement. No modification\nof this Agreement will be binding, unless in writing and signed by an\nauthorized representative of each party.\n\n\nFor inquiries please contact: Oracle Corporation, 500 Oracle Parkway,\nRedwood Shores, California 94065, USA. ZLIB License ZLIB license\n\n\nzlib.h -- interface of the 'zlib' general purpose compression library\nversion 1.2.7, May 2nd, 2012\n\n\nCopyright (C) 1995-2012 Jean-loup Gailly and Mark Adler\n\n\nThis software is provided 'as-is', without any express or implied\nwarranty.  In no event will the authors be held liable for any damages\narising from the use of this software.\n\n\nPermission is granted to anyone to use this software for any purpose,\nincluding commercial applications, and to alter it and redistribute it\nfreely, subject to the following restrictions:\n\n\n1. The origin of this software must not be misrepresented; you must not\n   claim that you wrote the original software. If you use this software\n   in a product, an acknowledgment in the product documentation would be\n   appreciated but is not required.\n2. Altered source versions must be plainly marked as such, and must not be\n   misrepresented as being the original software.\n3. This notice may not be removed or altered from any source distribution.\n\n\nJean-loup Gailly        Mark Adler\njloup@gzip.org          madler@alumni.caltech.edu D3.js license (New BSD License) D3.js license (New BSD License)\n\n\nCopyright (c) 2012, Michael Bostock\nAll rights reserved.\n\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n\n* The name Michael Bostock may not be used to endorse or promote products\n  derived from this software without specific prior written permission.\n\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL MICHAEL BOSTOCK BE LIABLE FOR ANY DIRECT,\nINDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\nBUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\nOF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\nEVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Lesser GNU Public License (LGPL) GNU LESSER GENERAL PUBLIC LICENSE\n                       Version 3, 29 June 2007\n\n\n Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n\n\n\n  This version of the GNU Lesser General Public License incorporates\nthe terms and conditions of version 3 of the GNU General Public\nLicense, supplemented by the additional permissions listed below.\n\n\n  0. Additional Definitions.\n\n\n  As used herein, \"this License\" refers to version 3 of the GNU Lesser\nGeneral Public License, and the \"GNU GPL\" refers to version 3 of the GNU\nGeneral Public License.\n\n\n  \"The Library\" refers to a covered work governed by this License,\nother than an Application or a Combined Work as defined below.\n\n\n  An \"Application\" is any work that makes use of an interface provided\nby the Library, but which is not otherwise based on the Library.\nDefining a subclass of a class defined by the Library is deemed a mode\nof using an interface provided by the Library.\n\n\n  A \"Combined Work\" is a work produced by combining or linking an\nApplication with the Library.  The particular version of the Library\nwith which the Combined Work was made is also called the \"Linked\nVersion\".",
        "e74fcee8-a56f-4d51-a591-704a78b4fd7c": "minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation. When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system.",
        "d07b6959-0084-4a1e-83e6-b0a8a5d83bc0": "Argument Description Default Value --cluster-id A string appended to resources to uniquely identify the cluster. required --project-id The project to deploy the cluster in. required",
        "554a6510-910e-48ab-a1a9-010ee2c777e2": "TRUE BOOLEAN Hive Max Partitions Per Writers Maximum number of partitions per writer 100 INTEGER Min(1) Hive Write Validation Threads Number of threads used for verifying data after a write 16 INTEGER Hive Orc Tiny Stripe Threshold ORC: Threshold below which an ORC stripe or file will read in its\n                      entirety 8MB DATASIZE Hive Orc Lazy Read Small Ranges ORC read small disk ranges lazily TRUE BOOLEAN Hive Orc Bloom Filters Enabled ORC: Enable bloom filters for predicate pushdown FALSE BOOLEAN Hive Orc Default Bloom Filter Fpp ORC Bloom filter false positive probability 0.05 DOUBLE Hive Orc Optimized Writer Enabled Experimental: ORC: Enable optimized writer TRUE BOOLEAN Hive Orc Writer Validation Percentage Percentage of ORC files to validate after write by re-reading the whole\n                      file 0 DOUBLE Min(0.0), Max(100.0) Hive Orc Writer Validation Mode Level of detail in ORC validation. Lower levels require more\n                      memory BOTH STRING possibleValues(HASHED, DETAILED, BOTH) Hive Rcfile Optimized Writer Enabled Experimental: RCFile: Enable optimized writer TRUE BOOLEAN Hive Assume Canonical Partition Keys Assume canonical parition keys? FALSE BOOLEAN Hive Parquet Fail On Corrupted Statistics Fail when scanning Parquet files with corrupted statistics TRUE BOOLEAN Hive Parquet Max Read Block Size Parquet: Maximum size of a block to read 16MB DATASIZE Hive Optimize Mismatched Bucket Count Enable optimization to avoid shuffle when bucket count is compatible but\n                      not the same FALSE BOOLEAN Hive Zstd Jni Decompression Enabled use JNI based zstd decompression for reading ORC files FALSE BOOLEAN Hive File Status Cache Size Hive file status cache size 0 LONG Hive File Status Cache Expire Time Hive file status cache : expiry time 0.00s DURATION Hive Per Transaction Metastore Cache Maximum Size Maximum number of metastore data objects in the Hive metastore cache per\n                      transaction 1000 INTEGER Min(1) Hive Metastore Refresh Interval Asynchronously refresh cached metastore data after access if it is older\n                      than this but is not yet expired, allowing subsequent accesses to see fresh\n                      data. 0.00s DURATION Hive Metastore Cache Maximum Size Maximum number of metastore data objects in the Hive metastore\n                      cache 10000 INTEGER Min(1) Hive Metastore Refresh Max Threads Maximum threads used to refresh cached metastore data 100 INTEGER Min(1) Hive Partition Versioning Enabled FALSE BOOLEAN Hive Metastore Impersonation Enabled Should Presto user be impersonated when communicating with Hive\n                      Metastore FALSE BOOLEAN Hive Partition Cache Validation Percentage Percentage of partition cache validation 0 DOUBLE Min(0.0), Max(100.0) Hive Metastore Thrift Client Socks Proxy metastore thrift client socks proxy STRING Hive Metastore Timeout Timeout for Hive metastore requests 10.00s DURATION Hive Dfs Verify Checksum Verify checksum for data consistency TRUE BOOLEAN Hive Metastore Cache Ttl Duration how long cached metastore data should be considered\n                      valid 0.00s DURATION Min(0ms) Hive Metastore Recording Path metastore recording path STRING Hive Replay Metastore Recording replay metastore recording FALSE BOOLEAN Hive Metastore Recoding Duration Metastore recording duration 0.00m DURATION Hive Dfs Require Hadoop Native hadoop native is required? TRUE BOOLEAN Hive Metastore Cache Scope Metastore cache scope ALL STRING possibleValues(ALL, PARTITION) Hive Metastore Authentication Type Hive metastore authentication type. NONE STRING possibleValues(NONE, KERBEROS) Hive Hdfs Authentication Type HDFS authentication type.",
        "edb368ae-46c9-42b8-8579-f068998ce231": "Optional. Specifies the minimum number of slots required for a node to be scheduled during a trial. If gres_supported is set to false, specify slots_per_node in order to utilize more than one GPU per node. It is the user\u2019s responsibility to ensure that slots_per_node GPUs will be available on the nodes selected for the job using other configurations such as targeting a specific resource pool with only slots_per_node GPU nodes or specifying a PBS constraint in the experiment configuration.",
        "0c7c239b-f559-4838-bb6a-5e0de3abdc5e": "Once Determined is installed and Docker is running, you can sign in. Go to http://localhost:8080/. Accept the default username of determined and leave the password empty. Click Sign In.",
        "8d590061-dfe4-4233-89a7-2d6ae10e4a5b": "About this task View a graphical representation of CPU utilization and memory utilization by fabric\n            for the selected time duration. Procedure Log on to the Data Fabric UI . If you are a fabric manager or an infrastructure admin, click  and\n                    check the Fabric utilization card to view the CPU and memory utilization\n                    by fabric. If you are a fabric user, scroll down the Home page to view the System\n                    Resources - CPU and memory utilization card. Select the fabric and the time duration for which you wish to view the\n                            CPU and memory utilization of the fabric. Results For an infrastructure admin or a fabric manager, the CPU\n                and memory utilization is seen as shown in the following image. For\n            a fabric user, you can view the system utilization by the fabric during the selected\n            time duration in a graphical format. The following image shows CPU and memory\n            utilization by the two selected fabrics for the last 6 hours. (Topic last modified: 2023-10-17) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "d5714965-3919-4f85-8fd5-aecb170cb739": "Release Date: December 20, 2022 New Features WebUI: Display total checkpoint size for experiments. WebUI: Add links from forked experiments and continued trials to their parents. API: Add structured fields to task log objects. Cluster: Add support for launcher-provided resource pools. Determined Enterprise Edition now allows for custom resource pools to be defined that submit work to an underlying Slurm/PBS partition on an HPC cluster with different submission options. Cluster: Determined Enterprise Edition now supports the NVIDIA Enroot container platform as an alternative to Apptainer/Singularity/Podman.",
        "d5770ccf-a2d8-4f01-aa8e-1c08740eb459": "GPU Support Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Installation Provides links to HPE Ezmeral Unified Analytics Software installation and service activation topics. Identity and Access Management Describes identity and access management in HPE Ezmeral Unified Analytics Software . Expanding the Cluster Describes how to add additional user-provided hosts to the management cluster to increase resource capacity and how     to expand the cluster to include the additional user-provided hosts. Shutting Down an HPE Ezmeral Unified Analytics Software Cluster Describes how to gracefully shut down an HPE Ezmeral Unified Analytics Software cluster when you want to perform maintenance or upgrade     tasks. Importing Applications and Managing the Application Lifecycle Describes how to import, manage, and secure applications and frameworks in HPE Ezmeral Unified Analytics Software . Connecting to External S3 Object Stores Describes how to connect HPE Ezmeral Unified Analytics Software to external S3 object storage in AWS, MinIO, and HPE Ezmeral Data Fabric Object Store . Connecting to External HPE Ezmeral Data Fabric Clusters Describes how to connect HPE Ezmeral Unified Analytics Software to an external HPE Ezmeral Data Fabric cluster. Configuring Endpoints Describes the endpoints in HPE Ezmeral Unified Analytics Software and how to configure them. GPU Support Provides information about support for NVIDIA GPU, MIG partitioning, preparing hosts     for GPU-enabled environment, adding hosts and enabling GPU in HPE Ezmeral Unified Analytics Software . GPU Resource Management Describes the GPU idle reclaim policy used for GPU resource management. Troubleshooting Describes how to identify and debug issues in HPE Ezmeral Unified Analytics Software . Support Matrix The tables on this page show the tools and frameworks, HPE Ezmeral Data Fabric ,operating system       versions , and GPU models that are supported for HPE Ezmeral Unified Analytics Software releases. Release Notes This document provides a comprehensive overview of the latest updates and enhancements     in HPE Ezmeral Unified Analytics Software (version     1.3.0), including new features, improvements, bug fixes, and known issues. Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. GPU Support Provides information about support for NVIDIA GPU, MIG partitioning, preparing hosts\n    for GPU-enabled environment, adding hosts and enabling GPU in HPE Ezmeral Unified Analytics Software . GPUs provide essential computational power and parallel processing capabilities to accelerate\n      the training and inference processes of deep learning models, reading and processing data\n      frames, processing SQL queries within Spark, and running experiments using Jupyter notebooks\n      integrated with GPUs. The hundreds or thousands of smaller cores working in parallel enable GPUs to process massive\n      amounts of data in a short period of time. HPE Ezmeral Unified Analytics Software supports single-access multi-instance GPU. You can\n      use MIG GPU when there are multiple applications that require GPU acceleration. By using MIG,\n      you can achieve higher resource utilization and cost efficiency. Supported GPU Models To see the GPU models supported by HPE Ezmeral Unified Analytics Software , see GPU Models . MIG Partitioning HPE Ezmeral Unified Analytics Software supports homogenous configuration deployment where\n        the GPU is split into N equal parts with the same amount of memory and CUDA cores. All GPU\n        models on the same Kubernetes cluster must operate in the same MIG mode or in the same\n        configuration mode. HPE Ezmeral Unified Analytics Software does not support any mixed\n        configuration across multiple GPU models. In HPE Ezmeral Unified Analytics Software , GPU partitions are presented as whole devices\n        by using the MIG mechanism. When an application requests one GPU, the application receives a\n        partition. Only one GPU device is visible to the application. To learn more, see CUDA visible devices . During the installation of HPE Ezmeral Unified Analytics Software , you must specify GPU\n        partition size (Whole, Large, Medium, and Small) and request the number of GPU instances\n        required for the workload. You cannot change the GPU partition size later.",
        "d25b0fcc-1293-4773-b099-31d239c96274": "Optional. Instructs Determined to perform an initial validation before any training begins, for each trial. This can be useful to determine a baseline when fine-tuning a model on a new dataset.",
        "9652f977-c660-4a36-b4d8-257225dc05a1": "An example of this is provided in the Configuring Per-Task Pod Specs section of the Customize a Pod guide.",
        "e42a4295-ebf4-403c-9af7-897ba1a508c6": "Using Your Own Open-Source Spark Images Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Spark Provides a brief overview of Apache Spark in HPE Ezmeral Unified Analytics Software . Using Spark Images Describes different types of Spark images supported by HPE Ezmeral Unified Analytics Software . Using HPE-Curated Spark Images Describes how to use HPE-curated Spark images to submit     Spark applications. Using Spark OSS Images Describes how to use Spark Open-Source Software (OSS) images to submit Spark     applications. Using Your Own Open-Source Spark Images Describes how to use your own open-source Spark images to submit Spark     applications. Setting the User Context Describes how to set the user context when using the     Spark OSS     images. List of Spark Images Lists the Spark images distributed by HPE Ezmeral Unified Analytics Software . These         images enables you to run the Spark applications in an air-gapped environment. Creating Spark Applications Describes how to create Spark applications using HPE Ezmeral Unified Analytics Software . Managing Spark Applications Describes how to view and manage Spark applications using HPE Ezmeral Unified Analytics Software . Configuring Memory for Spark Applications Describes how to set memory options for Spark applications. Creating Interactive Sessions Describes how to create interactive sessions in HPE Ezmeral Unified Analytics Software . Submitting Statements Describes how to submit statements in HPE Ezmeral Unified Analytics Software . Managing Interactive Sessions Describes how to view and manage Spark interactive sessions in HPE Ezmeral Unified Analytics Software . Spark History Server Provides an overview of Spark History Server. Using Spark SQL API Describes how to use Spark SQL API in HPE Ezmeral Unified Analytics Software . Enabling GPU Support for Spark Describes NVIDIA spark-rapids accelerator support for Spark, and how     to enable and allocate the GPU resources on Spark. Securely Passing Spark Configuration Values Describes how to pass the sensitive data to Spark configuration using the Kubernetes     Secret. Running Spark Applications in Namespaces Describes how namespaces work with regard to Spark applications in HPE Ezmeral Unified Analytics Software . Using whylogs with Spark Describes how to use whylogs with Spark. Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Using Your Own Open-Source Spark Images Describes how to use your own open-source Spark images to submit Spark\n    applications. Your own open-source Spark images compatible with the Kubernetes version supported on HPE Ezmeral Unified Analytics Software . With the\n      support of bringing your own open-source Spark, you can build Spark with any profile of your\n      choice; however, there will be no support for Data Fabric filesystem, Data Fabric Streams, and\n      any other Data Fabric sources and sinks that require a Data Fabric client. These Spark images\n      will not support Data Fabric-specific security features (data-fabric SASL\n        ( maprsasl )). To use your own open-source Spark images, follow the next steps: Build Spark. See Building Spark . Build Spark images to run in HPE Ezmeral Unified Analytics Software . See Building Images . Choose one of the following: Using the Create Spark Application GUI Using Airflow Using the Create Spark Application GUI To use your own open-source Spark images, choose one of the following option in the GUI: Using Upload YAML Configure your Spark YAML file with the built Spark image of your\n                    choice. image: <base-repository>/<image-name>:<image-tag> To set the logged-in user\u2019s context, add the following configuration in the sparkConf section. spark.hpe.webhook.security.context.autoconfigure: \"true\" To\n                    learn more about user context, see Setting the User Context . Perform the instructions to create a Spark application as described in Creating Spark Applications until you reach the Application\n                      Details step. In the Application Details step, choose the Upload YAML option. Click Select File and, browse and upload the YAML\n                    file. To specify the details for other boxes or options in the Application Details step and to complete creating the\n                    Spark application, see Creating Spark Applications .",
        "f2f31eea-23c8-49d8-9b3e-f0600058756c": "To learn more about descriptions and uses of Notebook images, see Notebook Images Overview . HPE Ezmeral Data Fabric The following table lists the versions of HPE Ezmeral Data Fabric that you\n        can connect HPE Ezmeral Unified Analytics Software to externally: HPE Ezmeral Unified Analytics Software HPE Ezmeral Data Fabric 1.3.0 7.4.0 1 1.2.0 6.2.0, 7.0.0, 7.1.0, 7.2.0, 7.3.0, 7.4.0 1.1.0 6.2.0, 7.0.0, 7.2.0 1.0.0 7.0.0, 7.2.0 1 In HPE Ezmeral Unified Analytics Software 1.3, Hewlett Packard Enterprise recommends connecting externally to HPE Ezmeral Data Fabric 7.4.0. In previous releases, the following HPE Ezmeral Data Fabric versions were tested: 6.2.0, 7.0.0, 7.1.0, 7.2.0,\n        7.3.0. Operating System HPE Ezmeral Unified Analytics Software supports the\n        following operating systems in the versions listed: HPE Ezmeral Unified Analytics Software RHEL Version Rocky Version 1.3.0 8.8 1 8.7 2 1.2.0 8.x 1 8.x 2 1.1.0 8.x 1 8.x 2 1 Only RHEL 8.8 is supported on GPU hosts. 2 There is no GPU support for Rocky 8.x, as NVIDIA does not support the GPU\n        operator running on Rocky 8.x. GPU Models HPE Ezmeral Unified Analytics Software supports the\n        following GPU models: HPE Ezmeral Unified Analytics Software GPU Model 1.3.0 NVIDIA A100 1.2.0 NVIDIA A100 1.1.0 NVIDIA A100 On this page Tools and Frameworks Notebook Images HPE Ezmeral Data Fabric Operating System GPU Models Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "572600c3-71bb-451f-ac4b-fe60494e3d17": "Defines the default Docker registry credentials to use when pulling a custom base Docker image, if needed. If credentials are specified in the experiment config this default value is overridden. Credentials are specified as the following nested fields: username (required) password (required) serveraddress (required) email (optional)",
        "d5ff6f3e-725a-45be-a709-572567d412da": "Release Date: November 20, 2020 Improvements Commands: Support configuring shmSize for commands (e.g., notebooks, shells, TensorBoards) in command configurations. Bug Fixes API: Fix a bug that caused the WebUI\u2019s log viewer to fail to render previous pages of trial logs. WebUI: Fix a bug in opening TensorBoards from the experiment list page via batch selection.",
        "d313246b-cbc4-4545-bef1-2365c531178b": "Optional. When enabled, configures tensor_fusion_threshold and tensor_fusion_cycle_time automatically. Defaults to false.",
        "32455fba-9746-44df-b452-7530313ca89d": "The size (in bytes) of /dev/shm for Determined task containers. Defaults to 4294967296.",
        "10fa2e00-c649-41cd-97dd-8493be41d1e4": "Optional. The priority assigned to this experiment. Only applicable when using the priority scheduler. Experiments with smaller priority values are scheduled before experiments with higher priority values. If using Kubernetes, the opposite is true; experiments with higher priorities are scheduled before those with lower priorities. Refer to Scheduling for more information. When the cluster is deployed with an HPC workload manager, this value is ignored and instead managed by the configured workload manager.",
        "1bc3762a-16ce-4827-b0b4-79a42e5943dd": "Use the CLI to remove OAuth clients: det oauth client remove <client ID>",
        "e2d5cb7b-01a9-44db-9956-c8432695d1db": "The Editor role supersedes the Viewer role, and includes permissions to create, edit, or delete projects, NTSC, and experiments within its scope.",
        "5767aeb2-7ebf-481d-b095-e635b17836c2": "The path to the IdP\u2019s certificate, used to validate assertions.",
        "edfb0b0e-280c-40d8-8b11-7ca15a401ca2": "Environment File Name CPUs determinedai/environments:py-3.8-pytorch-1.12-tf-2.11-cpu-0.24.0 NVIDIA GPUs determinedai/environments:cuda-11.3-pytorch-1.12-tf-2.11-gpu-0.24.0 AMD GPUs determinedai/environments:rocm-5.0-pytorch-1.10-tf-2.7-rocm-0.24.0",
        "5aad57c4-cafc-4081-9fd9-aa4dabe5976c": "You can create one or more folders on the bucket to store\n                objects. Related maprcli Commands To implement the features described on this page, the\n                Data Fabric UI relies on the following maprcli command. The\n                command is provided for general reference. For more information, see maprcli Commands in This Guide . mc mb (Topic last modified: 2023-11-01) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "9de02ac9-3ec8-454b-9157-a0f7c3aebd31": "Once the dependencies are installed, prepare the repository to run devcluster, a tool for running Determined. First, enter the Determined repository and run: ``make all`` Once that has finished, create a new file at ~/.devcluster.yaml and populate it with the following fields: startup_input: \"p\" cwd: /root/determined commands: p: make -C harness build # rebuild Python w: make -C webui build # rebuild WebUI c: make -C docs build # rebuild docs stages: - master: pre: - sh: make -C proto build - sh: make -C master build - sh: make -C tools prep-root config_file: checkpoint_storage: type: \"gcs\" bucket: <name of your bucket> save_experiment_best: 0 save_trial_best: 1 save_trial_latest: 1 db: user: \"postgres\" password: \"postgres\" host: <name of determined db service from `kubectl get services`> port: 5432 name: \"determined\" port: 8081 resource_manager: type: \"kubernetes\" namespace: default max_slots_per_pod: 1 master_service_name: <name of determined master service from `kubectl get services`> log: level: debug root: tools/build You are now ready to build and run the Determined master! From the Determined repo, run devcluster --no-guess-host to build and run the master.",
        "9c6b75d6-f53c-40ff-8ef7-8e5a3e8b3182": "A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage. minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation. When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server.",
        "352582c1-9725-4262-892f-d6381878120e": "Logging Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Installation Provides links to HPE Ezmeral Unified Analytics Software installation and service activation topics. Identity and Access Management Describes identity and access management in HPE Ezmeral Unified Analytics Software . Expanding the Cluster Describes how to add additional user-provided hosts to the management cluster to increase resource capacity and how     to expand the cluster to include the additional user-provided hosts. Shutting Down an HPE Ezmeral Unified Analytics Software Cluster Describes how to gracefully shut down an HPE Ezmeral Unified Analytics Software cluster when you want to perform maintenance or upgrade     tasks. Importing Applications and Managing the Application Lifecycle Describes how to import, manage, and secure applications and frameworks in HPE Ezmeral Unified Analytics Software . Connecting to External S3 Object Stores Describes how to connect HPE Ezmeral Unified Analytics Software to external S3 object storage in AWS, MinIO, and HPE Ezmeral Data Fabric Object Store . Connecting to External HPE Ezmeral Data Fabric Clusters Describes how to connect HPE Ezmeral Unified Analytics Software to an external HPE Ezmeral Data Fabric cluster. Configuring Endpoints Describes the endpoints in HPE Ezmeral Unified Analytics Software and how to configure them. GPU Support Provides information about support for NVIDIA GPU, MIG partitioning, preparing hosts     for GPU-enabled environment, adding hosts and enabling GPU in HPE Ezmeral Unified Analytics Software . Troubleshooting Describes how to identify and debug issues in HPE Ezmeral Unified Analytics Software . Installation Describes how to identify and debug issues during installation. Host (Node) Management Describes how to identify and debug issues for hosts. Metering Describes how to identify and debug issues for metering. Monitoring Describes how to identify and debug issues for monitoring. Logging Describes how to identify and debug issues for logging. Airflow Describes how to identify and debug issues for Airflow. EzPresto Describes how to identify and debug issues for EzPresto . Superset Describes how to identify and debug issues for Superset. Spark Describes how to identify and debug issues for Spark. Importing Applications and Managing the Application Lifecycle Describes how to identify and debug issues while importing applications and managing     the application lifecycle. Security Describes how to identify and debug issues related to security. GPU Describes how to identify and debug issues for GPU. User Interface Describes how to identify and debug issues related to the HPE Ezmeral Unified Analytics Software UI. Support Matrix The tables on this page show the tools and frameworks, HPE Ezmeral Data Fabric ,operating system       versions , and GPU models that are supported for HPE Ezmeral Unified Analytics Software releases. Release Notes This document provides a comprehensive overview of the latest updates and enhancements     in HPE Ezmeral Unified Analytics Software (version     1.3.0), including new features, improvements, bug fixes, and known issues. Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Logging Describes how to identify and debug issues for logging. Log Files Cannot see the log files for certain applications or cannot download the log the\n              files. Verify that the fluentbit pods are running. To get the list of\n                pods in the monitoring namespace,\n                  run: kubectl get pods -n monitoring The log file size exceeds the available disk space and causes the pods to crash. Verify that the logrotate-containerd-logs pods are running. To get\n                the list of pods in the monitoring namespace,\n                  run: kubectl get pods -n monitoring Snapshots Cannot see snapshots every four hours or snapshots are missing. Verify that the ua-application-logging-snapshot-cronjob pod is\n                running. To get the list of pods in the monitoring namespace,\n                  run: kubectl get pods -n monitoring On this page Log Files Snapshots Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "4cbb2711-ce05-4d7f-8587-745e577412dd": "Oracle Connection Parameters Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Accessing Data in External S3 Object Stores Describes how to access data in external object stores from clients, such as Spark and     Kubeflow notebooks. EzPresto Describes the EzPresto SQL query engine     and its featues. Connect Data Sources Provides instructions for connecting HPE Ezmeral Unified Analytics Software to external data sources. Delta Connection Parameters List of Delta connection parameters, descriptions, default values, and supported data     types. Delta Thrift Connection Parameters List of  Delta Thrift connection parameters, descriptions, default values, and     supported data types. Hive Connection Parameters List of Hive connection parameters, descriptions, default values, and supported data     types. Hive Glue Metastore Connection Parameters List of Hive Glue Metastore connection parameters, descriptions, default values, and     supported data types. Hive Thrift Metastore Connection Parameters List of Hive Thrift Metastore connection parameters, descriptions, default values, and     supported data types. Hive Discovery Metastore Connection Parameters Lists Hive discovery metastore connection parameters, parameter descriptions, default     values, and supported data types. MySQL Connection Parameters List of MySQL connection parameters, descriptions, default values, and supported data     types. Oracle Connection Parameters List of Oracle connection parameters, descriptions, default values, and supported data     types. Snowflake Connection Parameters List of  Snowflake connection parameters, descriptions, default values, and supported     data types. SQL Server Connection Parameters List of SQL Server connection parameters, descriptions, default values, and supported     data types. Teradata Connection Parameters List of  Teradata connection parameters, descriptions, default values, and supported     data types. PostgreSQL Connection Parameters List of PostgreSQL connection parameters, descriptions, default values, and supported     data types. Iceberg Connection Parameters List of Iceberg connection parameters, descriptions, default values, and supported data     types. Configuring a Hive Data Source with Kerberos Authentication Describes the required prerequisite steps to complete before you connect HPE Ezmeral Unified Analytics Software to a Hive data source     that uses Kerberos authentication. Connect to CSV and Parquet Data in an External S3 Data Source via Hive Connector Describes how to use the Hive connector with Presto in HPE Ezmeral Unified Analytics Software to connect to CSV and     Parquet data in S3-based external data sources. EzPresto/connect-external-s3-data-source.html Connect to External Applications via JDBC Describes how to connect external applications and BI tools, such as Tableau and     PowerBI, to EzPresto through the EzPresto JDBC endpoint. Using Spark to Query EzPresto Describes how to use Spark to query EzPresto . Connect to EzPresto via Python Client Provides information for connecting to EzPresto from a Python client. Cache Data Describes data caching and provides the steps for caching data in HPE Ezmeral Unified Analytics Software . Submitting Presto Queries from Notebook Describes how to submit Presto queries from the notebook. Airflow Provides an overview of Apache Airflow in HPE Ezmeral Unified Analytics Software . Superset Provides a brief overview of  Superset in HPE Ezmeral Unified Analytics Software . Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Oracle Connection Parameters List of Oracle connection parameters, descriptions, default values, and supported data\n    types. The following tables list the required and optional Oracle connection parameters. Required Connection Parameters Parameter Description Default Value Data Type Connection Url JDBC connection url. null STRING Connection User Specifies the login name of the user for the connection. null STRING Connection Password Specifies the password of the user for the connection. null STRING Enable Local Snapshot Table Enable Caching while querying. true BOOLEAN Optional Connection Parameters Parameter Description Default Value Data Type Case Insensitive Name Matching Match schema and table names case insensitively. false BOOLEAN Case Insensitive Name Matching Cache Ttl Duration for which remote dataset and table names will be cached. Set to\n                      0ms to disable the cache.",
        "4e0c7b2c-51e6-4ea7-82ce-d9f299e5ccb0": "edge node Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster.",
        "a09cbde3-361a-4575-9c42-197b4a9f3e86": "Before setting up a GKE cluster, the user should have Google Cloud SDK and kubectl installed on their local machine.",
        "e71aa193-5221-4449-87dc-3d5e367a9125": "Key for tagging the Determined agent instances. Defaults to managed-by.",
        "27b7ba36-d58c-42d8-8593-d6408c1587a3": "It provides a shared hierarchical\n            namespace that is organized like a standard file system. (Topic last modified: 2023-12-11) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "18e11db9-e30d-4aa5-a74d-8c306fe4facc": "The first two steps of the training loop (forward and backward pass + generate updates) incur the most computational overhead. To reduce this computational overhead, we recommend using the GPU to its maximimum capacity. This can be accomplished by using the largest possible batch size that fits into memory. To achieve this, set the global_batch_size to the largest batch size that fits into a single GPU multiplied by the number of slots. This approach is commonly known as weak scaling. The third step of Determined\u2019s distributed training loop incurs the majority of the communication overhead. Since deep learning models typically perform dense updates, where every model parameter is updated for every training sample, batch_size does not affect how long it takes workers to communicate updates. However, increasing global_batch_size does reduce the required number of passes through the training loop, thus reducing the total communication overhead. Determined optimizes the communication in the third step by using an efficient form of ring all-reduce, which minimizes the amount of communication necessary for all the workers to communicate their updates. Furthermore, Determined reduces the communication overhead by overlapping computation (steps 1 & 2) and communication (step 3) by communicating updates for deeper layers concurrently with computing updates for the shallower layers. Visit Advanced Optimizations for additional optimizations for reducing the communication overhead.",
        "d2800bd7-30a4-465d-bf79-b5d4c89e92db": "\"repository\": \"https://github.com/fkhadra/react-toastify\",\n    \"licenseUrl\": \"https://raw.githubusercontent.com/fkhadra/react-toastify/main/LICENSE\",\n\n-----------------------------------------------------------\ncommons-beanutils\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\ncommons-configuration\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\njoda-time\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\njna\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\ncommons-lang\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nehcache-core\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nannotations\n\nLicense: GNU Lesser Public License\nhttp://www.gnu.org/licenses/lgpl.html\n\n-----------------------------------------------------------\n\nhazelcast\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\njersey-server\n\nLicense: CDDL+GPL License\nhttp://glassfish.java.net/public/CDDL+GPL_1_1.html\n\n-----------------------------------------------------------\n\nlibpam4j\n\nLicense: The MIT license\nhttp://www.opensource.org/licenses/mit-license.php\n\n-----------------------------------------------------------\n\nlombok\n\nLicense: The MIT License\nhttps://projectlombok.org/LICENSE\n\n-----------------------------------------------------------\n\nspring-security-core\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nspring-security-kerberos-core\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nswagger-annotations\n\nCopyright (c) 2009 The Apache Software Foundation.\n\nLicense: Apache License, Version 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0.html\n\n-----------------------------------------------------------\n\nApache Ranger\n\nCopyright 2014-2022 The Apache Software Foundation\n\nLicense: Apache License Version 2.0, January 2004\nhttp://www.apache.org/licenses/LICENSE-2.0\n\n-----------------------------------------------------------\n\nApache NiFi\n\nCopyright 2014-2022 The Apache Software Foundation\n\nLicense: Apache License Version 2.0, January 2004 \nhttp://www.apache.org/licenses/LICENSE-2.0\n\n-----------------------------------------------------------\n\nApache Airflow\n\nCopyright 2016-2021 The Apache Software Foundation\n\nLicense: Apache License  Version 2.0, January 2004  \nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\n=========================================================== Apache License Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n\n   1. Definitions.\n\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.",
        "8a4ef574-3497-48cc-a417-daf366558a4f": "YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for\n            instance is comprised of the following filelets: 64K (primary\n            fid)+(256MB-64KB)+256MB+256MB+256MB. (Topic last modified: 2020-07-13) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "876e06bc-37e8-4c9d-9af0-e0d781b79493": "Monitoring Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Installation Provides links to HPE Ezmeral Unified Analytics Software installation and service activation topics. Identity and Access Management Describes identity and access management in HPE Ezmeral Unified Analytics Software . Expanding the Cluster Describes how to add additional user-provided hosts to the management cluster to increase resource capacity and how     to expand the cluster to include the additional user-provided hosts. Shutting Down an HPE Ezmeral Unified Analytics Software Cluster Describes how to gracefully shut down an HPE Ezmeral Unified Analytics Software cluster when you want to perform maintenance or upgrade     tasks. Importing Applications and Managing the Application Lifecycle Describes how to import, manage, and secure applications and frameworks in HPE Ezmeral Unified Analytics Software . Connecting to External S3 Object Stores Describes how to connect HPE Ezmeral Unified Analytics Software to external S3 object storage in AWS, MinIO, and HPE Ezmeral Data Fabric Object Store . Connecting to External HPE Ezmeral Data Fabric Clusters Describes how to connect HPE Ezmeral Unified Analytics Software to an external HPE Ezmeral Data Fabric cluster. Configuring Endpoints Describes the endpoints in HPE Ezmeral Unified Analytics Software and how to configure them. GPU Support Provides information about support for NVIDIA GPU, MIG partitioning, preparing hosts     for GPU-enabled environment, adding hosts and enabling GPU in HPE Ezmeral Unified Analytics Software . Troubleshooting Describes how to identify and debug issues in HPE Ezmeral Unified Analytics Software . Installation Describes how to identify and debug issues during installation. Host (Node) Management Describes how to identify and debug issues for hosts. Metering Describes how to identify and debug issues for metering. Monitoring Describes how to identify and debug issues for monitoring. Logging Describes how to identify and debug issues for logging. Airflow Describes how to identify and debug issues for Airflow. EzPresto Describes how to identify and debug issues for EzPresto . Superset Describes how to identify and debug issues for Superset. Spark Describes how to identify and debug issues for Spark. Importing Applications and Managing the Application Lifecycle Describes how to identify and debug issues while importing applications and managing     the application lifecycle. Security Describes how to identify and debug issues related to security. GPU Describes how to identify and debug issues for GPU. User Interface Describes how to identify and debug issues related to the HPE Ezmeral Unified Analytics Software UI. Support Matrix The tables on this page show the tools and frameworks, HPE Ezmeral Data Fabric ,operating system       versions , and GPU models that are supported for HPE Ezmeral Unified Analytics Software releases. Release Notes This document provides a comprehensive overview of the latest updates and enhancements     in HPE Ezmeral Unified Analytics Software (version     1.3.0), including new features, improvements, bug fixes, and known issues. Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Monitoring Describes how to identify and debug issues for monitoring. Failure to display alerts and notifications Verify that the ua-monitor-deployment-c797c5f44 pod is up and running. To\n        get the list of pods in the monitoring namespace,\n          run: kubectl get pods -n monitoring Verify that the alertmanager-af-prometheus-kube-prometh-alertmanager-0 pod\n        is up and running. To get the list of pods in the prometheus namespace,\n          run: kubectl get pods -n prometheus Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "d573435f-87d6-4a8f-beb9-dd6e6417c12f": "EzPresto Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Accessing Data in External S3 Object Stores Describes how to access data in external object stores from clients, such as Spark and     Kubeflow notebooks. EzPresto Describes the EzPresto SQL query engine     and its featues. Connect Data Sources Provides instructions for connecting HPE Ezmeral Unified Analytics Software to external data sources. Connect to CSV and Parquet Data in an External S3 Data Source via Hive Connector Describes how to use the Hive connector with Presto in HPE Ezmeral Unified Analytics Software to connect to CSV and     Parquet data in S3-based external data sources. EzPresto/connect-external-s3-data-source.html Connect to External Applications via JDBC Describes how to connect external applications and BI tools, such as Tableau and     PowerBI, to EzPresto through the EzPresto JDBC endpoint. Using Spark to Query EzPresto Describes how to use Spark to query EzPresto . Connect to EzPresto via Python Client Provides information for connecting to EzPresto from a Python client. Cache Data Describes data caching and provides the steps for caching data in HPE Ezmeral Unified Analytics Software . Submitting Presto Queries from Notebook Describes how to submit Presto queries from the notebook. Airflow Provides an overview of Apache Airflow in HPE Ezmeral Unified Analytics Software . Superset Provides a brief overview of  Superset in HPE Ezmeral Unified Analytics Software . Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. EzPresto Describes the EzPresto SQL query engine\n    and its featues. EzPresto in HPE Ezmeral Unified Analytics Software EzPresto is an SQL query engine\n        based on the open-source, Linux foundation multi-parallel processing (MPP) query engine\n        PrestoDB, that is optimized to run federated queries across various data sources. Enterprise\n        BI applications such as Tableau, Power BI, and data processing engines, such as Spark, can\n        leverage EzPresto for rapid query performance\n        and prompt insights through federated data access. You can easily connect EzPresto to multiple\n        types of data sources from the Data Engineering space in HPE Ezmeral Unified Analytics Software by going to Data Engineering > Data Sources . Connections require a JDBC\n        connection URL and user credentials. Data sets available to the connected user display in the Data Catalog, which is accessible\n        by going to Data Engineering > Data Catalog . In the Data Catalog, you\n        select the data sets you want to work with. You can query or cache the selected datasets. When you opt to cache data sets, you can modify the data sets prior to caching them. For\n        example, you can edit table and column names, remove columns, and create new schema. Cached\n        data sets (tables and views) are accessible in the Cached Assets space of HPE Ezmeral Unified Analytics Software . You can access\n        cached assets by going to Data Engineering > Cached Assets . When you opt to query data sets, you can run federated queries (query across data sets in\n        multiple data sources) from the Query Editor. You can access the Query Editor by going to Data Engineering > Query Editor . Querying cached data sets\n        accelerates queries for significant performance gains. You can access the data in connected data sources from Superset and visualize the data that\n        results from complex, federated queries. Superset is accessible in HPE Ezmeral Unified Analytics Software by going to BI Reporting > Dashboards or Applications &\n          Frameworks > Data Engineering tab and clicking Open in the Superset\n        tile. See Superset . You can also monitor the state of queries\n        and query details, including the query plan and resource usage, by going to Administration > EzSQL Cluster Monitoring .",
        "14919cbd-a62f-4866-8c72-d1c526c16fd9": "data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage. minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation.",
        "f2a228ca-ad95-4c3b-b7db-4a3720767b11": "Manually Offloading Data to a Cold Tier Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Data Tiering Conceptual information about data tiering. Manually Offloading Data to a Cold Tier HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Creating a Standard Volume Procedure to create standard volume. Creating a Mirror Volume Procedure to create mirror volume. Converting Standard Volume to Mirror Volume Editing a Volume Edit accountable entity, volume access for accountable entity and volume hard         quota. Renaming a Volume Rename a volume. Viewing Volume Endpoint Info View volume endpoint information. Viewing Object Endpoint Info to Remotely Access Files as Objects View endpoint information for files in a volume to be able to access the files as         objects when accessed by S3 client. Downloading Volume Endpoint Information Download JSON file containing endpoint information for the selected volume endpoint         information. Deleting a Volume Delete a single volume. Administering Volume Snapshots Snapshot overview and administering snapshots. Data Tiering Conceptual information about data tiering. Schedules for Volume Data Tiering Describes schedules for data tiering of volume data Manually Offloading Data to a Cold Tier Recalling Data to the Data Fabric File System Administering Storage Policies Manage storage policies related to data tiering. Administering Remote Targets Administering Schedules Introduction to schedules. Mirroring Synopsis of mirrors and mirroring process. Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Manually Offloading Data to a Cold Tier Prerequisites You must be a fabric user to perform this operation. Data tiering must have been enabled on the volume during the volume creation, to\n                    be able to offload/recall data. To offload data, you must create remote targets. See Creating a Remote Target to add a new remote target. To manage data offloading, you must have created storage policies. See Administering Storage Policies to learn more about managing\n                    storage policies. About this task Data, once offloaded, is purged on the the data fabric cluster to release the\n                disk space. When you delete an entire file, part of a file, or a snapshot,\n                corresponding objects are removed from the tier. Data is offloaded to the tier\n                in the same state, compressed or uncompressed, as was stored in the front-end\n                volume. If data encryption is enabled on the front-end volume (using the dare parameter), data is encrypted during and after\n                offload. At the volume level, data can be offloaded manually by triggering the\n                offload operation. Follow the steps given below to offload data manually from a\n            volume to a cold tier. Procedure Log on to the Data Fabric UI . Select Fabric user on the home page. Click the Table view icon on the Resources card. In the tabular list of\n                    fabrics, click the down arrow for the fabric that contains the volume. Click the ellipsis under Actions for the required volume. Select Offload data . Results The data offload begins to the designated cold\n            tier. (Topic last modified: 2023-11-01) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "6f3239d6-b7cf-40a7-857c-69443a8730c1": "You can tap\n                into cloud-scale capacity for cold data. NOTE: Data Fabric supports tiering for only file and volume data; tiering of tables\n                    and streams is not supported. Creating a Remote Target Create a remote target to offload cold data. Editing Remote Target Credentials Edit credentials for a remote target. Deleting a Remote Target Delete a remote target. (Topic last modified: 2023-07-28) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "8c1055b8-a9ff-4cb1-a947-8f3ef2c7436c": "A security principal is an entity that is performing an action on a resource. Determined supports individual users or user groups as security principals.",
        "2a87cb1d-cbdc-421c-bef3-2d674f6ea6b2": "class determined.keras.load_model_from_checkpoint_pathpath: strtags: Optional[List[str]] = None Loads a checkpoint written by a TFKerasTrial. You should have already downloaded the checkpoint files, likely with Checkpoint.download(). The return type is a TensorFlow AutoTrackable object. Parameters path (string) \u2013 Top level directory to load the checkpoint from. tags (list string, optional) \u2013 Specifies which tags are loaded from the TensorFlow SavedModel. See documentation for tf.compat.v1.saved_model.load_v2.",
        "261ce107-fdfb-4668-896a-e8cf259335a1": "The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage.",
        "f0bd24a5-c179-4c71-b716-0f5b36336984": "Whether to use spot instances. Defaults to false. See Use Spot Instances for more details.",
        "d7bae827-1b17-4991-b802-9ae596c3373c": "To find what kind of anonymous information the WebUI collects, visit Common Configuration Options.",
        "0eae7b6a-79c3-4b4b-8abb-0a84811b8615": "Select Change to private access . In the Data Access dialog, click Proceed or Cancel . If you chose to\n          proceed, the system displays the message: Access changed for the data source:\n              <data-source-name> On this page Granting a Member Access to Data Granting All Members Access to a Data Source (Public Access) Revoking Member Access to Data Revoking Public Access to a Data Source Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "4a7e5845-a9a5-4788-925a-184b1ebe1534": "gateway node Jump to main content Get Started Platform Administration Reference Home Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Glossary Definitions for commonly used terms in MapR Converged Data Platform\n        environments. .snapshot A special directory in the top level of each volume that contains all the snapshots             created or preserved for the volume. access control expression (ACE) A Boolean expression that defines a combination of users, groups, or roles that have             access to an object stored natively such as a directory, file, or HPE Ezmeral Data Fabric Database table. access control list (ACL) A list of permissions attached to an object. An ACL specifies users or system processes that can perform specific actions on an object. access policy An ACL or policy in JSON format that describes user access. Grants accounts and IAM     users permissions to perform resource operations, such as putting objects in a bucket. You     associate access policies with accounts, users, buckets, and objects. administrator A user or users with special privileges to administer the cluster or cluster             resources. Administrative functions can include managing hardware resources, users,             data, services, security, and availability. advisory quota An advisory disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the advisory quota, an alert is sent. air gap Physical isolation between a computer system and unsecured networks. To enhance             security, air-gapped computer systems are disconnected from other systems and             networks. chunk Files in the file system are split into chunks (similar to Hadoop blocks) that are             normally 256 MB by default. Any multiple of 65,536 bytes is a valid chunk size, but             tuning the size correctly is important. Files inherit the chunk size settings of the             directory that contains them, as do subdirectories on which chunk size has not been             explicitly set. Any files written by a Hadoop application, whether via the file APIs or             over NFS, use chunk size specified by the settings for the directory where the file is             written. client node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as an \"edge node.\" Client nodes and edge             nodes are NOT part of a data-fabric cluster. cluster admin The data-fabric             user . compute node A compute node is used to process data using a compute engine (for example, YARN, Hive,             Spark, or Drill). A compute node is by definition a data-fabric cluster node. container The unit of shared storage in a data-fabric cluster. Every container is either a name container or a data             container. container location database (CLDB) A service, running on one or more data-fabric nodes, that maintains the locations of services, containers, and             other cluster information. core The minimum complement of software packages required to construct a data-fabric cluster. These             packages include mapr-core , mapr-core-internal , mapr-cldb , mapr-apiserver , mapr-fileserver , mapr-zookeeper , and others. Note that ecosystem components are not             part of core. data-access gateway A service that acts as a proxy and gateway for translating requests between             lightweight client applications and the data-fabric cluster. data compaction A process that enables users to remove empty or deleted space in the database and             to compact the database to occupy contiguous space. data container One of the two types of containers in a data-fabric cluster. Data containers typically have a             cascaded configuration (master replicates to replica1, replica1 replicates to replica2,             and so on). Every data container is either a master container, an intermediate             container, or a tail container depending on its replication role. data fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. data-fabric administrator The \" data-fabric user.\"             The user that cluster services run as (typically named mapr or hadoop ) on each node. data-fabric gateway A gateway that supports table and stream replication. The data-fabric gateway mediates one-way             communication between a source data-fabric cluster and a destination cluster. The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs.",
        "bbff1acf-83d8-4f61-b94a-0d0616aef010": "The port for the Determined master.",
        "5f0e8cb3-7191-4de4-b5c5-a2e7daff148b": "Click the Table view icon to display the resource table with\n            status values. Click the fabric name to display the fabric details page. The core software version is\n            displayed as the Build Version : More information Release History (Topic last modified: 2023-12-05) On this page View the Software Version with User Information Viewing the Software Version with Fabric Details \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "cfdf7784-15c0-487e-aaee-8331336dd363": "Prerequisites You must be a fabric manager or an infrastructure admin to perform this\n                    operation. One or more fabrics must have been created on or imported into Data Fabric. About this task View the top fabrics that have consumed maximum of the total storage available to the\n                fabric. You can use this data to understand what fabrics are nearing the total storage\n                capacity available to the fabric. Procedure Log on to the Data Fabric UI . Click Fabric\n                        metrics on the Home page. Scroll down to see the Fabric Storage card. Results You are able to see up to five fabrics that have consumed\n            maximum storage capacity out of the available storage capacity available to the\n            individual fabrics. If you have clicked the fabric area on the Fabric Storage card, you are navigated to the Resource page to be able to view the fabric resources. (Topic last modified: 2023-10-16) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "2a89fe76-b57c-455d-9f84-7ab3a9fef60b": "Creating a Local Repository on RHEL, Rocky, or Oracle Linux Jump to main content Get Started Platform Administration Reference Home Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. Creating a Local Repository for an Air-Gapped Installation Describes how to make installation packages available through a local repository for an     air-gapped installation. Creating a Local Repository on RHEL, Rocky, or Oracle Linux Describes how to create and use a local repository for RHEL, Rocky, or Oracle     Linux. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Release Notes These notes contain information about release 7.6.0 of the HPE Ezmeral Data Fabric as-a-service platform. Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. Prerequisites for On-Premises Installation Describes fabric node and user prerequisites for on-premises installation of the HPE Ezmeral Data Fabric . AWS Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Amazon Web Services (AWS). Azure Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Microsoft Azure. GCP Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Google Cloud Platform (GCP). On-Premises Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric that is hosted on-site. Creating a Local Repository for an Air-Gapped Installation Describes how to make installation packages available through a local repository for an     air-gapped installation. Creating a Local Repository on RHEL, Rocky, or Oracle Linux Describes how to create and use a local repository for RHEL, Rocky, or Oracle     Linux. Creating a Local Repository on SLES Describes how to create and use a local repository for SLES. Creating a Local Repository on Ubuntu Describes how to create and use a local repository for Ubuntu. Troubleshooting Seed Node Installation Describes some common issues that can interfere with seed node     installation. Planning Worksheet for Cloud Deployments Print this worksheet, and use it to record configuration information for your cloud     deployment. Help for datafabric_container_setup.sh From the Docker command line, you can access the help text for the datafabric_container_setup.sh script. Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. SSO Using Keycloak Describes how single sign-on (SSO) is implemented by using       Keycloak. Setting Up Clients Summarizes the steps for enabling client communication with the HPE Ezmeral Data Fabric . Upgrade This section contains information that describes how to upgrade the HPE Ezmeral Data Fabric as-a-service platform. User Assistance Describes how to access different resources that can help you learn how to use the HPE Ezmeral Data Fabric . Creating a Local Repository on RHEL, Rocky, or Oracle Linux Describes how to create and use a local repository for RHEL, Rocky, or Oracle\n    Linux. Ensure that you have access to the HPE internet repository so that you can download\n          package files. For more information, see Accessing the HPE Ezmeral Token-Authenticated Internet Repository . On the machine where you will set up the repository, log in as root or\n          use sudo .",
        "d6f601f1-6f22-492b-8dd3-e77466fbdafe": "class determined.pytorch.Reducervalue A Reducer defines a method for reducing (aggregating) evaluation metrics. See evaluation_reducer() for details. AVG SUM MAX MIN",
        "a4defe5c-d97c-4b62-b505-dfaa6051e83e": "Stopping Volume Mirroring Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Mirroring Synopsis of mirrors and mirroring process. Stopping Volume Mirroring Stop mirroring of data that is in progress on a mirror volume. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Creating a Standard Volume Procedure to create standard volume. Creating a Mirror Volume Procedure to create mirror volume. Converting Standard Volume to Mirror Volume Editing a Volume Edit accountable entity, volume access for accountable entity and volume hard         quota. Renaming a Volume Rename a volume. Viewing Volume Endpoint Info View volume endpoint information. Viewing Object Endpoint Info to Remotely Access Files as Objects View endpoint information for files in a volume to be able to access the files as         objects when accessed by S3 client. Downloading Volume Endpoint Information Download JSON file containing endpoint information for the selected volume endpoint         information. Deleting a Volume Delete a single volume. Administering Volume Snapshots Snapshot overview and administering snapshots. Data Tiering Conceptual information about data tiering. Mirroring Synopsis of mirrors and mirroring process. Local Mirroring Remote Mirroring Starting Volume Mirroring Start mirroring of data on a mirror volume. Stopping Volume Mirroring Stop mirroring of data that is in progress on a mirror volume. Scheduling Volume Mirroring Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Stopping Volume Mirroring Stop mirroring of data that is in progress on a mirror volume. Prerequisites Data mirroring must be in progress on the mirror\n            volume. About this task You can stop mirroring of data that is in progress from the associated source volume\n                onto a mirror volume. Follow the steps given below to stop mirroring of data on to a mirror volume. Procedure Log on to the Data Fabric UI . Under the default Fabric user experience , click the Table View icon on\n                    the Resources card. In the tabular list of fabrics, click the down arrow\n                    for the fabric that contains the volume. Click the volume name seen under Resource Name . Navigate to the Mirrors tab. Click the ellipsis under Actions for the volume being mirrored. Click the Stop mirroring option. Results Mirroring of data that is in progress from the source volume\n            onto the mirror volume is stopped.. The status under the Mirroring column on\n            Mirrors tab for the volume changes to Off . (Topic last modified: 2023-10-18) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "b8413b95-206f-43f5-9e2b-e577dec49fe8": "The network interface to use during distributed training. If not set, Determined automatically determines the network interface to use. When training a model with multiple machines, the host network interface used by each machine must have the same interface name across machines. The network interface to use can be determined automatically, but there may be issues if there is an interface name common to all machines but it is not routable between machines. Determined already filters out common interfaces like lo and docker0, but agent machines may have others. If interface detection is not finding the appropriate interface, the dtrain_network_interface option can be used to set it explicitly (e.g., eth11). To learn more about distributed training with Determined, visit the conceptual overview or the intro to implementing distributed training.",
        "9bb1c6f5-c98c-43f6-b3a3-7208e0473857": "Users and/or groups must have been created. About this task View how much storage is used by individual fabric users via the Data Fabric UI . You can determine the fabric storage consumption trends by users on the fabrics that\n                are being monitored via the Data Fabric UI . Viewing fabric storage consumption trends by groups is available if groups have been\n                defined on the Data Fabric UI . The storage consumption by a user/group is aggregated storage size of volumes and\n                topics owned by the user/group. NOTE: Bucket storage consumption is not included as there is no concept of bucket\n                    owner in Data Fabric. You can import a fabric to monitor the fabric usage via the Data Fabric UI . See Importing a Fabric for information importing fabrics. Procedure Log on to the Data Fabric UI . Select the Fabric manager from the\n                    dropdown next to the welcome message on the Home page. Click Fabric metrics on the Home page. Check the Storage use by Users card. Results You are able to view a list of all fabric users and/or groups in the order of the\n                storage utilization. (Topic last modified: 2023-10-28) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "e82bbf3b-b6b7-4e3e-92cc-dc27e8138203": "The current version of Determined provides limited support for reproducibility; unfortunately, the hardware and software stack typically used for deep learning makes perfect reproducibility very challenging. Determined can control and reproduce the following sources of randomness: Hyperparameter sampling decisions. The initial weights for a given hyperparameter configuration. Shuffling of training data in a trial. Dropout or other random layers. Determined currently does not offer support for controlling non-determinism in floating-point operations. Modern deep learning frameworks typically implement training using floating point operations that result in non-deterministic results, particularly on GPUs. If only CPUs are used for training, reproducible results can be achieved, as described in the following sections.",
        "fedf7687-9cf1-41e2-b902-32593171a4f9": "class determined.pytorch.deepspeed.DeepSpeedTrialContext*args: Any**kwargs: Any Bases: determined._trial_context.TrialContext, determined.pytorch._reducer._PyTorchReducerContext Contains runtime information for any Determined workflow that uses the DeepSpeedTrial API. With this class, users can do the following things: Wrap DeepSpeed model engines that contain the model, optimizer, lr_scheduler, etc. This will make sure Determined can automatically provide gradient aggregation, checkpointing and fault tolerance. In contrast to determined.pytorch.PyTorchTrial, the user does not need to wrap optimizer and lr_scheduler as that should all be instead passed to the DeepSpeed initialize function (see https://www.deepspeed.ai/getting-started/#writing-deepspeed-models) when building the model engine. Overwrite a deepspeed config file or dictionary with values from Determined\u2019s experiment config to ensure consistency in batch size and support hyperparameter tuning. Set a custom model parallel configuration that should instantiate a determined.pytorch.deepspeed.ModelParallelUnit dataclass. We automatically set the mpu for data parallel and standard pipeline parallel training. This should only be needed if there is additional model parallelism outside DeepSpeed\u2019s supported methods. Disable data reproducibility checks to allow custom data loaders. Disable automatic gradient aggregation for non-pipeline-parallel training. current_train_batchint Current global batch index disable_auto_grad_accumulationNone Prevent the DeepSpeedTrialController from automatically calling train_batch multiple times to process enough micro batches to meet the per slot batch size. Thus, the user is responsible for manually training on enough micro batches in train_batch to meet the expected per slot batch size. disable_dataset_reproducibility_checksNone disable_dataset_reproducibility_checks() allows you to return an arbitrary DataLoader from build_training_data_loader() or build_validation_data_loader(). Normally you would be required to return a det.pytorch.DataLoader instead, which would guarantee that an appropriate Sampler is used that ensures: When shuffle=True, the shuffle is reproducible. The dataset will start at the right location, even after pausing/continuing. Proper sharding is used during distributed training. However, there can be cases where either reproducibility of the dataset is not needed or where the nature of the dataset can cause the det.pytorch.DataLoader to be unsuitable. In those cases, you can call disable_dataset_reproducibility_checks() and you will be free to return any torch.utils.data.DataLoader you like. Dataset reproducibility will still be possible, but it will be your responsibility. The Sampler classes in determined.pytorch.samplers can help in this regard. classmethod from_configconfig: Dict[str, Any]determined._trial_context.TrialContext Create a context object suitable for debugging outside of Determined. An example for a subclass of PyTorchTrial: config = { ... } context = det.pytorch.PyTorchTrialContext.from_config(config) my_trial = MyPyTorchTrial(context) train_ds = my_trial.build_training_data_loader() for epoch_idx in range(3): for batch_idx, batch in enumerate(train_ds): metrics = my_trial.train_batch(batch, epoch_idx, batch_idx) ... An example for a subclass of TFKerasTrial: config = { ... } context = det.keras.TFKerasTrialContext.from_config(config) my_trial = tf_keras_one_var_model.OneVarTrial(context) model = my_trial.build_model() model.fit(my_trial.build_training_data_loader()) eval_metrics = model.evaluate(my_trial.build_validation_data_loader()) Parameters config \u2013 An experiment config file, in dictionary form. get_data_configDict[str, Any] Return the data configuration. get_enable_tensorboard_loggingbool Return whether automatic tensorboard logging is enabled get_experiment_configDict[str, Any] Return the experiment configuration. get_experiment_idint Return the experiment ID of the current trial. get_hparamname: strAny Return the current value of the hyperparameter with the given name. get_hparamsDict[str, Any] Return a dictionary of hyperparameter names to values. get_stop_requestedbool Return whether a trial stoppage has been requested. get_tensorboard_pathpathlib.Path Get the path where files for consumption by TensorBoard should be written get_tensorboard_writerAny This function returns an instance of torch.utils.tensorboard.SummaryWriter Trials users who wish to log to TensorBoard can use this writer object. We provide and manage a writer in order to save and upload TensorBoard files automatically on behalf of the user.",
        "0cb927a3-13c6-4b46-8965-9c267161d3ff": "Determined only requires you to enable SCIM and set your authentication mode and any necessary credentials.",
        "73b29da6-c64d-46f9-8cd7-93b1cf101e4b": "det command line tool can be installed using pip: pip install determined The command, pip install determined, installs the determined library which includes the Determined command-line interface (CLI).",
        "7017255e-b6a4-4247-87f5-cd0b58743011": "Once you have set up the DNS record, press Enter. Certbot lets you know it has received the certificate and provides the certificate location, key location, and certificate expiration date.",
        "fcf240f7-d77a-44cd-a68e-aaadbcec220e": "You run training code by submitting your code to a cluster and running it as an experiment. To run an experiment, you provide a launcher and specify the launcher in the experiment configuration endpoint field. Launcher options are: legacy bare-Trial-class Determined predefined launchers: Horovod Launcher PyTorch Distributed Launcher DeepSpeed Launcher custom launcher or use one of the Determined predefined launchers a command with arguments, which is run in the container For distributed training, it is good practice to separate the launcher that starts a number of distributed workers from your training script, which typically runs each worker. The distributed training launcher must implement the following logic: Launch all of the workers you want, passing any required peer info, such as rank or chief ip address, to each worker. Monitor workers. If a worker exits with non-zero, the launcher should terminate the remaining workers and exit with non-zero. Exit zero after all workers exit zero. These requirements ensure that distributed training jobs do not hang after a single worker failure.",
        "999212b2-a008-4f13-b970-0c93f4b430d0": "The Viewer role allows a user to see workspaces, projects, notebooks, TensorBoards, shells, commands (NTSC), and experiments, as well as experiment metadata and artifacts within its scope.",
        "2aadfefd-e3dd-4b70-9a92-5a0869fdd31d": "Optional. Like source_trial_id but specifies an arbitrary checkpoint from which to initialize weights. At most one of source_trial_id or source_checkpoint_uuid should be set.",
        "5b057c30-3e7c-41c7-a401-569dbdd511a8": "A mirror schedule specifies how frequently the mirror volume is synchronized\n                with the source volume. In case of a disaster (or any type of data loss on a\n                read-write source volume), the data can be recovered from the mirror volume, but any\n                data written to the source volume since the last successful mirror operation will\n                not be on the mirror volume. Therefore, you should set the mirror schedule such that\n                it meets your RPO (Recovery Point Objective). A tier offload schedule specifies how frequently data in the volume on the\n                fabric is offloaded to the tiered storage. This setting to automatically offload\n                data to the storage tier. Creating a Schedule Add a schedule for data tiering. Editing a Schedule Edit an existing schedule. Scheduling Volume Data Tiering Viewing Schedules View a list of schedules. Deleting a Schedule Delete a schedule. (Topic last modified: 2023-07-28) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "f67832b2-40f5-4ae3-94af-e48114207cd1": "Determined provides APIs for downloading checkpoints and loading them into memory in a Python process. This guide discusses: Querying model checkpoints from trials and experiments. Loading model checkpoints in Python. Storing additional user-defined metadata in a checkpoint. Using the Determined CLI to download checkpoints to disk. The Checkpoint Export API is a subset of the features found in the client module.",
        "caf21b60-bc5b-4549-b7a0-c9092f39a85e": "First Name - Enter the first name of the user. Last Name - Enter the last name of the user. Email ID - Enter the email ID associated with the user. Password - Enter the password for the user. Role - Selecting Administrator assigns the user the administrator role, which\n              gives the user permission to act as an administrator in the HPE Ezmeral Unified Analytics Software UI. If you do\n              not select Administrator, the user is assigned the member role. To remove a user: In the list of users, select the user you want to remove. Click into the Actions column, and click the Delete option. Alternatively,\n          click the Delete button on the screen. The system prompts you to confirm the\n          action. Once you confirm, the user is removed. To edit the role and password for a user: In the list of users, select the user you want to edit. Click into the Actions column, and click the Edit option. In the drawer that opens, change the password and role for the user. Click Update . Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "508845c2-b69e-46cf-87a6-44b8bac87984": "Adding and Removing Users Programmatically Jump to main content HPE Ezmeral Unified Analytics Software 1.3 Documentation Feedback Get Started Describes how to get started with HPE Ezmeral Unified Analytics Software . Administration Provides information about managing applications and clusters in HPE Ezmeral Unified Analytics Software . Installation Provides links to HPE Ezmeral Unified Analytics Software installation and service activation topics. Identity and Access Management Describes identity and access management in HPE Ezmeral Unified Analytics Software . User Isolation Describes user isolation in HPE Ezmeral Unified Analytics Software . User Roles Describes roles that you can assign to users in HPE Ezmeral Unified Analytics Software . AD/LDAP Servers Describes the differences between the internal OpenLDAP server in HPE Ezmeral Unified Analytics Software and external AD/LDAP     servers. Also describes some of the server-related configuration options that you set during     installation. Adding and Removing Users Describes how administrators can add and remove users in HPE Ezmeral Unified Analytics Software . Adding and Removing Users Programmatically Describes how to add and remove users through the Kubernetes API using the EzUserQuery     and EzUserConfig custom resources. Managing Data Access Expanding the Cluster Describes how to add additional user-provided hosts to the management cluster to increase resource capacity and how     to expand the cluster to include the additional user-provided hosts. Shutting Down an HPE Ezmeral Unified Analytics Software Cluster Describes how to gracefully shut down an HPE Ezmeral Unified Analytics Software cluster when you want to perform maintenance or upgrade     tasks. Importing Applications and Managing the Application Lifecycle Describes how to import, manage, and secure applications and frameworks in HPE Ezmeral Unified Analytics Software . Connecting to External S3 Object Stores Describes how to connect HPE Ezmeral Unified Analytics Software to external S3 object storage in AWS, MinIO, and HPE Ezmeral Data Fabric Object Store . Connecting to External HPE Ezmeral Data Fabric Clusters Describes how to connect HPE Ezmeral Unified Analytics Software to an external HPE Ezmeral Data Fabric cluster. Configuring Endpoints Describes the endpoints in HPE Ezmeral Unified Analytics Software and how to configure them. GPU Support Provides information about support for NVIDIA GPU, MIG partitioning, preparing hosts     for GPU-enabled environment, adding hosts and enabling GPU in HPE Ezmeral Unified Analytics Software . Troubleshooting Describes how to identify and debug issues in HPE Ezmeral Unified Analytics Software . Support Matrix The tables on this page show the tools and frameworks, HPE Ezmeral Data Fabric ,operating system       versions , and GPU models that are supported for HPE Ezmeral Unified Analytics Software releases. Release Notes This document provides a comprehensive overview of the latest updates and enhancements     in HPE Ezmeral Unified Analytics Software (version     1.3.0), including new features, improvements, bug fixes, and known issues. Observability Describes observability in HPE Ezmeral Unified Analytics Software . Data Engineering Data engineers can design and build pipelines that transform and transport data into     usable formats for data consumers. Data Analytics Provides a brief overview of data analytics in HPE Ezmeral Unified Analytics Software . Data Science Provides a brief overview of data science in HPE Ezmeral Unified Analytics Software . Notebooks Provides a brief overview of Notebooks in HPE Ezmeral Unified Analytics Software . Glossary Definitions for commonly used terms in HPE Ezmeral Unified Analytics\n        environments. Adding and Removing Users Programmatically Describes how to add and remove users through the Kubernetes API using the EzUserQuery\n    and EzUserConfig custom resources. The user management operator in HPE Ezmeral Unified Analytics Software responds to the\n      EzUserQuery and EzUserConfig custom resources when they are created by a client with the\n      required Kubernetes API permissions. Use the administrative kubectl config that you get when you create the HPE Ezmeral Unified Analytics Software cluster to onboard and manage users programmatically\n      through the Kubernetes API. To onboard a user, complete the following steps: Use the EzUserQuery custom resource to search for the user in the internal or external\n          AD/LDAP directory. The EzUserQuery returns a list of attributes for a user, including the\n          Keycloak ID. The Keycloak ID is required to onboard a user. Use the EzUserConfig custom resource to onboard the user. The following sections describe the custom resources: EzUserQuery Use the EzUserQuery custom resource to query the user AD/LDAP directory. The EzUserQuery properties map directly to the query types of the Keycloak user API.\n        Providing values for the email , firstName , lastName , and/or username properties sets criteria that must match the returned users.",
        "05be0e2d-7dc4-4b05-80d8-4e699ec227ef": "Whether OpenTelemetry is enabled. Defaults to false.",
        "ef00bf8f-36b2-45f4-8590-b56b87a77319": "Release Date: October 29, 2021 New Features Model Registry APIs: Add new APIs to create a model with labels and to update the labels of an existing model. Improvements Breaking Change: Deploy: det deploy now uses cloud images that use the NVIDIA Container Toolkit on agent hosts instead of relying on an older NVIDIA runtime, and custom images should be updated to do the same. Determined will no longer override the default container runtime according to the workload. Breaking Change: Model Registry APIs: Require name in the body rather than the URL for the post_model endpoint. Breaking Change: Model Registry APIs: Use model ID (integer) instead of name (string) as the lookup parameter for the get_model and get_model_versions endpoints. Docs: Switch to the Furo Sphinx theme, which fixes searching in the docs. Bug Fixes Model Registry APIs: Sort models by name, description, and other attributes. Harness: Represent infinite and NaN metric values as strings in JSON. WebUI: Convert infinite and NaN value strings to numeric metrics. WebUI: Report login failures caused by the cluster being unreachable.",
        "a5f34bc4-7ab0-4740-8200-d2205f529847": "Now you can try editing your own script for the purpose of reporting epoch-based metrics to the Determined master. For more tutorials, visit the Tutorials to learn the basics of working with Determined and how to port your existing code to the Determined environment.",
        "c7fced14-af85-4e12-abd1-47f6f844f63b": "Tainting nodes is optional, but you might want to taint nodes to restrict which nodes a pod may be scheduled onto. A taint consists of a taint type, tag, and effect. When using a managed kubernetes cluster (e.g. a GKE, AKS, or EKS cluster), it is possible to specify taints at cluster or nodepool creation using the specified CLIs. Please refer to the set up pages for each managed cluster service for instructions on how to do so. To add taints to an existing resource, it is necessary to use kubectl. Tolerations can be added to Pods by including the tolerations field in the Pod specification.",
        "c8d4b138-3a50-49f6-9736-9611e23a8da1": "Whether Prometheus is enabled. Defaults to false.",
        "9c0d9544-9f60-480b-b264-55c57c9c3473": "If you have not used Determined before, and you want to quickly set up a new training environment, you are at the right place!",
        "e7fb81b6-c0a1-4d0d-a303-c325e5be70da": "This operation takes a few minutes. If this workaound does not resolve the issue, contact HPE Support. Failed Queries If queries fail, go to the Presto UI and view the stack trace for the queries. You can also\n        view the EzPresto log files. You can access the Presto UI from the HPE Ezmeral Unified Analytics Software UI. In the left navigation bar, select Applications & Frameworks . Select the Data Engineering tab. In the EzPresto tile, click on\n          the Endpoint URL. In the Presto UI, select the Failed state. Locate the query and click on the Query ID . Scroll down to the Error Information section to view the stack trace. You can also view the logs in the shared directory. In the left navigation bar, select Data Engineering > Data\n            Sources . On the Data Sources screen, click Browse . Select the following directories in the order shown: shared/ logs/ apps/ app-core/ ezpresto/ Select the log directory for which you want to view EzPresto logs. On this page Insufficient Memory Failed Queries Partners Support Dev-Hub Community Training ALA Privacy Policy Glossary Search Search current doc version",
        "15d4a55d-2541-4e95-82a8-cc0036cf237d": "An int hyperparameter is an integer variable. The minimum and maximum values of the variable are defined by the minval and maxval keys, respectively (inclusive of endpoints). When doing a grid search, the count key must also be specified; this defines the number of points in the grid for this hyperparameter. Grid points are evenly spaced between minval and maxval. See Grid Method for details.",
        "8e64a370-ae9d-4f31-ab0f-1f6fc7466588": "After creating the cluster, kubectl should be used to deploy apps. In order for kubectl to be used with AKS, users need to create or update the cluster kubeconfig. This can be done with the command: az aks get-credentials --resource-group ${AKS_RESOURCE_GROUP} --name ${AKS_CLUSTER_NAME}",
        "8750680a-ab69-4eda-a27a-7d7c98ca6f40": "When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node. YARN resource containers A unit of memory allocated for use by YARN to process each map or reduce         task. ZooKeeper A coordination service for distributed applications. It provides a shared hierarchical             namespace that is organized like a standard file system. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation.",
        "05b9f71d-a0a6-426a-8ff4-5339b81d0f2f": "Creating a Local Repository for an Air-Gapped Installation Jump to main content Get Started Platform Administration Reference Home Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. Creating a Local Repository for an Air-Gapped Installation Describes how to make installation packages available through a local repository for an     air-gapped installation. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Get Started This section describes how you can get started learning about, installing, and using     the HPE Ezmeral Data Fabric . Release Notes These notes contain information about release 7.6.0 of the HPE Ezmeral Data Fabric as-a-service platform. Installation This section contains information about installing the HPE Ezmeral Data Fabric as-a-service platform. Fabric Deployment Using a Seed Node Describes how to install the platform using a seed node and the Create Fabric     interface. Prerequisites for On-Premises Installation Describes fabric node and user prerequisites for on-premises installation of the HPE Ezmeral Data Fabric . AWS Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Amazon Web Services (AWS). Azure Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Microsoft Azure. GCP Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric using Google Cloud Platform (GCP). On-Premises Fabric Configuration Parameters This page describes the configuration values that you need to specify to create a new     fabric that is hosted on-site. Creating a Local Repository for an Air-Gapped Installation Describes how to make installation packages available through a local repository for an     air-gapped installation. Creating a Local Repository on RHEL, Rocky, or Oracle Linux Describes how to create and use a local repository for RHEL, Rocky, or Oracle     Linux. Creating a Local Repository on SLES Describes how to create and use a local repository for SLES. Creating a Local Repository on Ubuntu Describes how to create and use a local repository for Ubuntu. Troubleshooting Seed Node Installation Describes some common issues that can interfere with seed node     installation. Planning Worksheet for Cloud Deployments Print this worksheet, and use it to record configuration information for your cloud     deployment. Help for datafabric_container_setup.sh From the Docker command line, you can access the help text for the datafabric_container_setup.sh script. Service Activation and Billing Describes how to activate and register a new fabric to take advantage of automated     billing. SSO Using Keycloak Describes how single sign-on (SSO) is implemented by using       Keycloak. Setting Up Clients Summarizes the steps for enabling client communication with the HPE Ezmeral Data Fabric . Upgrade This section contains information that describes how to upgrade the HPE Ezmeral Data Fabric as-a-service platform. User Assistance Describes how to access different resources that can help you learn how to use the HPE Ezmeral Data Fabric . Creating a Local Repository for an Air-Gapped Installation Describes how to make installation packages available through a local repository for an\n    air-gapped installation. You can set up a local repository on each node to provide access to installation packages.\n      With this method, nodes do not require internet connectivity. The package manager on each node\n      installs from packages in the local repository. To set up a local repository, nodes need\n      access to a running web server to download the packages. Creating a Local Repository on RHEL, Rocky, or Oracle Linux Describes how to create and use a local repository for RHEL, Rocky, or Oracle     Linux. Creating a Local Repository on SLES Describes how to create and use a local repository for SLES. Creating a Local Repository on Ubuntu Describes how to create and use a local repository for Ubuntu. (Topic last modified: 2023-12-03) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "fce988bc-72df-4ae0-b61b-87aaff4dac40": "Assigning Multiple Security Policies to One or More Volumes Jump to main content Get Started Platform Administration Reference Home Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. Administering Security Policies Add, edit, delete, and manage state of security policies. Assigning Multiple Security Policies to One or More Volumes Describes how to assign multiple security policies to volumes. HPE Ezmeral Data Fabric 7.6 Documentation Search current doc version Administration This section describes how to administer fabric resources in the global namespace of     your HPE Ezmeral Data Fabric as-a-service platform. IPv6 Support in Data Fabric Describes the IPv6 support feature for Data Fabric. Administering Fabrics This section describes fabric operations that you can perform using the Data Fabric     UI. Administering Users and Roles This section describes the operations you can perform related to users, groups, and     roles for the HPE Ezmeral Data Fabric . Administering Buckets Describes the operations you can perform related to buckets for the HPE Ezmeral Data Fabric . Administering Tables Describes the operations you can perform related to tables for HPE Ezmeral Data Fabric . Administering Topics Administer topics for Apache Kafka Wire Protocol with HPE Ezmeral Data Fabric . Administering Volumes Administer volumes on HPE Ezmeral Data Fabric . Auditing Fabric and Fabric Data Auditing in Data Fabric Administering Security Policies Add, edit, delete, and manage state of security policies. About Security Policy Domain Describes a security policy domain. Security Policy Implementation Workflow Describes the security policy workflow, in general, and the steps in implementing a         security policy. Security Policy Enforcement Process Describes the steps followed during security policy enforcement on         volumes. Understanding Access Control in a Security Policy The implications of permissions assigned to users and groups in a security         policy. Managing File and Directory ACEs Describes the implications of setting access control expressions on files and             directories. Security Policy Permissions Permissions define which administrative users can create, view, and modify security     policies. Administrators set the permissions on security policies through cluster-level and     security policy-level ACLs. Designating a Fabric as Global Policy Master Designate a fabric in the global namespace as the global policy master. Creating a Security Policy Add a security policy on the global policy master. Viewing a Security Policy View security policy details. Viewing All Security Policies View all security policies on the Data Fabric UI. Editing a Security Policy Make changes to a security policy. Assigning a Security Policy to One or More Volumes Assigning Multiple Security Policies to One or More Volumes Describes how to assign multiple security policies to volumes. Unassigning One or More Security Policies from a Volume Unassign a policy from a volume to which it has been previously assigned. Disabling a Security Policy Describes how to disable a security policy. Enabling a Security Policy Describes how to enable a security policy. Working with an External NFS Server Associate an external NFS server with Data Fabric to share data across clusters in         the global namespace. Working with External S3 Object Store Administering Alarms Manage alarms via the HPE Ezmeral Data Fabric UI. Monitoring Describes monitoring with OpenTelemetry for HPE Ezmeral Data Fabric . Getting Started with Iceberg Summarizes what you need to know to begin using Iceberg with HPE Ezmeral Data Fabric release 7.6.0. Assigning Multiple Security Policies to One or More Volumes Describes how to assign multiple security policies to volumes. Prerequisites You must be a fabric manager to perform this\n            operation. About this task You can assign multiple security policies to one or more volumes associated with a\n                fabric at one go, via the Data Fabric UI. Follow the steps given below to assign a security policy to one or more volumes. Procedure Log on to the Data Fabric UI . Select Fabric manager from the dropdown on the Home page. Click Security Administration on the Home page. Click View All on the Global policies card. Click Assign Policy . Search for policies in the search bar and select the policies to apply to a\n                    volume or a common set of volumes. Click Select Resources . Select the fabric and one or more volumes to assign the selected policies\n                    to. Click Save . Results The selected security policies are assigned to the\n            selected volumes. Related maprcli Commands To\n                    implement the features described on this page, the Data Fabric UI relies on the\n                    following maprcli command. The command is provided for general\n                    reference. For more information, see maprcli Commands in This Guide . policy\n                            attach (Topic last modified: 2024-01-31) \u00a9Copyright 2024  Hewlett Packard Enterprise Development LP - Partners | Support | Dev-Hub | Community | Training | ALA | Privacy Policy | Glossary",
        "c2a4076b-aa12-4756-a6b9-0fe47b7f20af": "Release Date: June 16, 2020 Retry ConnectionError and ProtocolError types for uploads to Google Cloud Storage. Fix a bug where the CLI was unable to make secure websocket connections to the master. Add the det user rename CLI command for admins to change the username of existing users.",
        "9980f9eb-ac42-4290-8959-c8d070a0de37": "The data-fabric gateway also applies updates from JSON             tables to their secondary indexes and propagates Change Data Capture (CDC)             logs. data-fabric user The user that cluster services run as (typically named mapr or hadoop ) on each node. The data-fabric user, also known as the \" data-fabric admin,\" has full privileges to             administer the cluster. The administrative privilege, with varying levels of control,             can be assigned to other users as well. data node A data node has the function of storing data and always runs FileServer. A data node is             by definition a data-fabric cluster node. desired replication factor The number of copies of a volume that should be maintained by the data-fabric cluster for normal operation. developer preview A label for a feature or collection of features that have usage restrictions.             Developer previews are not tested for production environments, and should be used with             caution. Docker containers The application containers used by Docker software. Docker is a leading proponent of OS virtualization using application containers (\"containerization\"). Domain Relates to Object Store. A domain is a management entity for accounts and users. The     number of users, the amount of disk space, number of buckets in each of the accounts, total     number of accounts, and the number of disabled accounts are all tracked within a domain.     Currently, Object Store only supports the primary domain; you cannot create additional domains.     Administrators can create multiple accounts in the primary domain. domain user Relates to Object Store. A domain user is a cluster security principal authenticated     through AD/LDAP. Domain users only exist in the default account. Domain users can log in to the     Object Store UI with their domain username and password\u200b. Ecosystem Pack (EEP) A selected set of stable, interoperable, and widely used components from the Hadoop             ecosystem that are fully supported on the Data Fabric platform. edge cluster A small-footprint edition of the HPE Ezmeral Data Fabric designed to capture, process,             and analyze IoT data close to the source of the data. edge node A node that runs the mapr-client that can access every cluster node and             is used to access the cluster. Also referred to as a \"client node.\" Client nodes and             edge nodes are NOT part of a data-fabric cluster. fabric A collection of nodes that work together under a unified architecture, along with the             services or technologies running on that architecture. A fabric is similar to a Linux             cluster. Fabrics help you manage your data, making it possible to access, integrate,             model, analyze, and provision your data seamlessly. filelet A filelet, also called an fid, is a 256MB shard of a file. A 1 GB file for             instance is comprised of the following filelets: 64K (primary             fid)+(256MB-64KB)+256MB+256MB+256MB. file system The NFS-mountable, distributed, high-performance HPE Ezmeral Data Fabric data-storage system. gateway node A node on which a mapr-gateway is installed. A gateway node is by             definition a data-fabric cluster node. global namespace (GNS) The data plane that connects HPE Ezmeral Data Fabric deployments.             The global namespace is a mechanism that aggregates disparate and remote data sources             and provides a namespace that encompasses all of your infrastructure and deployments.             Global namespace technology lets you manage globally deployed data as a single resource.             Because of the global namespace, you can view and run multiple fabrics as a single,             logical, and local fabric. The global namespace is designed to span multiple edge nodes,             on-prem data centers, and clouds. heartbeat A signal sent by each FileServer and NFS node every second to provide information to the CLDB about the node's health and resource usage. IAM users Relates to Object Store. An IAM (Identity and Access Management) user represents an     actual user or an application. An administrator creates IAM users in an Object Store account and     assigns access policies to them to control user and application access to resources in the     account. Installer A program that simplifies installation of the HPE Ezmeral Data Fabric. The Installer             guides you through the process of installing a cluster with data-fabric services and             ecosystem components. You can also use the Installer to update a previously installed             cluster with additional nodes, services, and ecosystem components. And you can use the             Installer to upgrade a cluster to a newer core version if the cluster was installed             using the Installer or an Installer Stanza. log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage.",
        "7960fa5a-bd1e-45ee-b58e-710cf4f967f3": "log compaction A process that purges messages previously published to a topic partition,             retaining the latest version. MAST Gateway A gateway that serves as a centralized entry point for all the operations that             need to be performed on tiered storage. minimum replication factor The minimum number of copies of a volume that should be maintained by the data-fabric cluster for normal             operation. When the replication factor falls below this minimum, re-replication occurs             as aggressively as possible to restore the replication level. If any containers in the             CLDB volume fall below the minimum replication factor, writes are disabled until             aggressive re-replication restores the minimum level of replication. mirror A replica of a volume. MOSS MOSS is the acronym for Multithreaded Object Store Server. name container A container in a data-fabric cluster that holds a volume's namespace information and file chunk locations, and             the first 64 KB of each file in the volume. Network File System (NFS) A protocol that allows a user on a client computer to access files over a network as though they were stored locally. node An individual server (physical or virtual machine) in a cluster. NodeManager (NM) A data service that works with the ResourceManager to host the YARN resource             containers that run on each data node. object File and metadata that describes the file. You upload an object into a bucket. You can     then download, open, move, or delete the object. Object Store Object and metadata storage solution built into the HPE Ezmeral Data Fabric . Object Store efficiently     stores data for fast access and leverages the capabilities of the patented HPE Ezmeral Data Fabric file system for     performance, reliability, and scalability. policy server The service that manages security policies and composite IDs. quota A disk capacity limit that can be set for a volume, user, or group. When disk usage exceeds the quota, no more data can be written. replication factor The number of copies of a volume. replication role The replication role of a container determines how that container is replicated to         other storage pools in the cluster. replication role balancer The replication role balancer is a tool that switches the replication roles of containers to ensure that every node has an equal share of of master and replica containers (for name containers) and an equal share of master, intermediate, and tail containers (for data containers). re-replication Re-replication occurs whenever the number of available replica containers drops below the number prescribed by that volume's replication factor. Re-replication may occur for a variety of reasons including replica container corruption, node unavailability, hard disk failure, or an increase in replication factor. ResourceManager (RM) A YARN service that manages cluster resources and schedules             applications. role The service that the node runs in a cluster. You can use a node for one, or a combination             of the following roles: CLDB, JobTracker, WebServer, ResourceManager, Zookeeper,             FileServer, TaskTracker, NFS, and HBase. secret A Kubernetes object that holds sensitive information, such as passwords, tokens,             and keys. Pods that require this sensitive information reference the secret in their pod             definition. Secrets are the method Kubernetes uses to move sensitive data into             pods. secure by default The HPE Ezmeral Data Fabric platform and supported ecosystem components are designed to implement security             unless the user takes specific steps to turn off security options. schedule A group of rules that specify recurring points in time at which certain actions are determined to occur. snapshot A read-only logical image of a volume at a specific point in time. storage pool A unit of storage made up of one or more disks. By default, data-fabric storage pools contain two or three             disks. For high-volume reads and writes, you can create larger storage pools when             initially formatting storage during cluster creation. stripe width The number of disks in a storage pool. super group The group that has administrative access to the data-fabric cluster. super user The user that has administrative access to the data-fabric cluster. tagging Operation of applying a security policy to a resource. ticket In the data-fabric platform, a file that contains keys used to authenticate users and cluster servers.             Tickets are created using the maprlogin or configure.sh utilities and are encrypted to protect their contents.             Different types of tickets are provided for users and services. For example, every user             who wants to access a cluster must have a user ticket, and every node in a cluster must             have a server ticket. volume A tree of files and directories grouped for the purpose of applying a policy or set of         policies to all of them at once. Warden A data-fabric process that             coordinates the starting and stopping of configured services on a node."
    },
    "relevant_docs": {
        "d1380f1e-342e-4b7c-a606-446bbe5186ce": [
            "94a7ba5e-4969-4373-a93d-df835de6e750"
        ],
        "86d73b0a-4409-4292-90ee-bdb979d7cb8c": [
            "94a7ba5e-4969-4373-a93d-df835de6e750"
        ],
        "a3e19e47-b776-4d5a-83e8-50f817b0889f": [
            "f6642fdd-d314-4ae8-86cb-7ba37ab77f7d"
        ],
        "87660fa6-3bec-4009-b517-393901130bc9": [
            "f6642fdd-d314-4ae8-86cb-7ba37ab77f7d"
        ],
        "570b19f6-f7a8-4aa7-b71a-4aa3b3390306": [
            "75895fdb-a1aa-4ff5-9853-50ca24bd5e38"
        ],
        "7b65ebef-6772-47eb-8382-2ed29e9985bc": [
            "75895fdb-a1aa-4ff5-9853-50ca24bd5e38"
        ],
        "19778f85-32f0-4bd0-a833-ae9b0f8ac437": [
            "e14c9567-705f-4187-97a9-835350ee463a"
        ],
        "008531ca-08c8-4b86-be6b-c3b8b3332dc5": [
            "e14c9567-705f-4187-97a9-835350ee463a"
        ],
        "b4221bb6-0d15-48e4-93e8-6f535e71c5e1": [
            "18069236-4588-46a0-b697-66f707cd4fbb"
        ],
        "ece8c69c-e841-4b39-a237-78f86aef0ab4": [
            "18069236-4588-46a0-b697-66f707cd4fbb"
        ],
        "b283a80d-dd3e-4df3-a699-cb781405321e": [
            "3d77431d-dba4-4f6a-be0b-f6e56f2dc62e"
        ],
        "663a87ea-b5f6-4a55-b313-a0960b11d373": [
            "3d77431d-dba4-4f6a-be0b-f6e56f2dc62e"
        ],
        "6c254016-fa58-4734-aaa8-349b6ee6a655": [
            "9c4ac2d7-3018-4e89-b2d0-2f3a43385da7"
        ],
        "8e61ad36-0fdb-4a09-be4c-068ff84b11c6": [
            "9c4ac2d7-3018-4e89-b2d0-2f3a43385da7"
        ],
        "915f0a35-2075-448e-9f11-3649d18bf67b": [
            "ea27a9d3-052d-41a1-893c-bc28f7376e46"
        ],
        "c9ef7f79-ffb5-4fb9-bbe4-71bb6c1d235f": [
            "ea27a9d3-052d-41a1-893c-bc28f7376e46"
        ],
        "07da2a7e-45b0-4ba1-a747-f72e6f64bcde": [
            "d2834df4-0584-4feb-a5d0-3712e6a58532"
        ],
        "64ab02fb-8097-4468-b3a5-76ad263b906e": [
            "d2834df4-0584-4feb-a5d0-3712e6a58532"
        ],
        "cadaf077-7cb6-4d6d-b07a-bb89a7ec687c": [
            "f0a30b5c-0449-4833-997f-5960c5a6027f"
        ],
        "f71033a2-fa5b-4b4b-beb2-ca029b0754fc": [
            "f0a30b5c-0449-4833-997f-5960c5a6027f"
        ],
        "8f907eab-1b3d-45fd-9093-c2db30fc4c50": [
            "17ff5505-676a-4c43-9a69-e3de3dd59cd4"
        ],
        "e5ae023c-965c-475f-b780-5d2ac1322756": [
            "17ff5505-676a-4c43-9a69-e3de3dd59cd4"
        ],
        "0ba37409-75ec-43f3-b3a3-7a8f571b0ea9": [
            "506466d9-bbe2-45e8-828b-d9f51ef233d9"
        ],
        "e82aaf21-01e5-44cd-a028-9ae97781cca8": [
            "506466d9-bbe2-45e8-828b-d9f51ef233d9"
        ],
        "e0f142cc-0503-48d4-ad69-ef2589fdf101": [
            "670717b9-1d1d-4fa0-80b5-96e0ce522bed"
        ],
        "25a3e718-33a3-433f-8f4a-e75f0e377989": [
            "670717b9-1d1d-4fa0-80b5-96e0ce522bed"
        ],
        "ab1e4300-1c05-450a-a00d-1ed0fea57eca": [
            "a26ee83f-a3f8-4382-92df-a58411c810fe"
        ],
        "6c22baaa-d85d-490e-b940-41e41588db9b": [
            "a26ee83f-a3f8-4382-92df-a58411c810fe"
        ],
        "efb4df2a-7f81-4531-b72e-d7ebd80b9f33": [
            "2d41d8d3-3be6-4e91-b9bd-5f2c41868f43"
        ],
        "5512cf76-8908-4883-82c2-0e469fbb25a1": [
            "2d41d8d3-3be6-4e91-b9bd-5f2c41868f43"
        ],
        "2bc451e2-7743-4dc0-98df-292f6f1d2e4b": [
            "81d699b3-666a-47ac-86da-390bbdb16ebe"
        ],
        "e14f901f-08c1-4714-b1a9-41aa129e14c5": [
            "81d699b3-666a-47ac-86da-390bbdb16ebe"
        ],
        "61580c9a-6337-45bc-a9e5-779440ec326f": [
            "1c1e0330-6b22-4646-b430-990b8c41651f"
        ],
        "1579d579-a5d8-46d8-8ae8-25466c7328ae": [
            "1c1e0330-6b22-4646-b430-990b8c41651f"
        ],
        "35d5af25-8ccf-4bbc-92b1-0297c43bb9d9": [
            "b3513020-5035-4a74-b12c-50ac09451a9a"
        ],
        "532b28db-1392-47b9-8102-168b75823eb0": [
            "b3513020-5035-4a74-b12c-50ac09451a9a"
        ],
        "c2d3084a-e8c5-4e9f-a931-eb339b6ea015": [
            "bf272d54-8e7f-4942-9021-42ffd3b4b168"
        ],
        "d2bf479d-ebae-474e-acfb-24b0015b2c61": [
            "bf272d54-8e7f-4942-9021-42ffd3b4b168"
        ],
        "3d9dc61e-7fde-488c-846b-9767dde2907e": [
            "77d62c62-aad5-4169-bef8-18da0dfdcf11"
        ],
        "63b6ef3f-5a41-42ca-bbd6-6a3573f084c9": [
            "77d62c62-aad5-4169-bef8-18da0dfdcf11"
        ],
        "bc961ed0-ffe3-4f47-92e3-02d07724a0f9": [
            "7c687745-5b89-4aa3-8038-32f3a5afc311"
        ],
        "9103ead4-1fb8-435d-a601-8ecc3341ed47": [
            "7c687745-5b89-4aa3-8038-32f3a5afc311"
        ],
        "338a3230-5e1f-44ce-9ff0-b5f6f663671f": [
            "7d89aec4-413e-4042-a056-3ce8419b8c23"
        ],
        "a682a1d5-438f-42f7-a7e3-93f1b86fefe6": [
            "7d89aec4-413e-4042-a056-3ce8419b8c23"
        ],
        "5bffa338-d566-47f1-86cf-a8d71bc1af8d": [
            "341710f1-e7b4-4dba-89ff-8219e7dc0a4f"
        ],
        "ffee9408-4722-494f-9a78-e15be946ab9a": [
            "341710f1-e7b4-4dba-89ff-8219e7dc0a4f"
        ],
        "8baa6086-417c-4de8-8f43-1d51c496fb09": [
            "80e98c90-44c0-41f3-8bae-8dbab35a34d2"
        ],
        "36e2bf56-5227-4177-8a15-b103414490ec": [
            "80e98c90-44c0-41f3-8bae-8dbab35a34d2"
        ],
        "3d5d7305-6273-4532-818a-d62d3a986bad": [
            "8d952cee-7502-4eda-892a-10e63b18537f"
        ],
        "96f2eb2f-c991-4714-a833-7e5bc659ecbe": [
            "8d952cee-7502-4eda-892a-10e63b18537f"
        ],
        "fa867d89-83f9-42cd-996d-1c2b845ce95c": [
            "1518b00a-baa8-483c-9437-f51d9a7263fe"
        ],
        "49624bf4-a015-45e4-80ea-8ac6bbbe93e1": [
            "1518b00a-baa8-483c-9437-f51d9a7263fe"
        ],
        "642f87b5-33bd-418c-ace1-b7574c62d38d": [
            "7a8e19b2-27c2-403a-aa50-c5966afa9af9"
        ],
        "bcffa628-ab6c-42a0-a929-6a08d7057cf7": [
            "7a8e19b2-27c2-403a-aa50-c5966afa9af9"
        ],
        "03f5f4a6-7489-4656-861e-2a78053bc305": [
            "81d5ab64-31f5-41e8-96ce-b42f54490070"
        ],
        "85a0c6be-3002-49a0-884e-d15a367f7247": [
            "81d5ab64-31f5-41e8-96ce-b42f54490070"
        ],
        "1c041fa7-e447-4344-8d5e-426a2dbf0717": [
            "cf2438d9-c669-4de2-b01b-0bbd048255d0"
        ],
        "0b337334-4cce-4eaa-b4a3-87dcef4ad99d": [
            "cf2438d9-c669-4de2-b01b-0bbd048255d0"
        ],
        "e822812e-77fb-41ff-af6b-a9832353da8c": [
            "ee1e8e7e-3a1e-4ae8-a6a9-b96a98ce415b"
        ],
        "45bf325d-8a8d-4205-a827-1eed642e291a": [
            "ee1e8e7e-3a1e-4ae8-a6a9-b96a98ce415b"
        ],
        "4a028393-02ed-4f5e-b3c5-fa6516415191": [
            "8501a294-95f9-4e4c-9619-8391817c1b88"
        ],
        "70380aa8-b847-45e6-80d1-67b2f6b5c715": [
            "8501a294-95f9-4e4c-9619-8391817c1b88"
        ],
        "3942b5d8-06cb-4356-90fc-21cff281c6c7": [
            "674835fc-6600-4c52-aa9d-83c8ab548ffe"
        ],
        "1492797b-a70c-4ad6-bfac-b80bda97b2f8": [
            "674835fc-6600-4c52-aa9d-83c8ab548ffe"
        ],
        "5d6f31d5-3d06-4452-923c-e21e8620ce2a": [
            "1c304f43-82f2-4304-98bb-0857469f2b14"
        ],
        "540fc24b-5285-490c-a8a8-2b2c57c5dc16": [
            "1c304f43-82f2-4304-98bb-0857469f2b14"
        ],
        "3837f3e2-af01-481e-81f6-635af2a51654": [
            "33c0ea29-e6fe-494e-9367-2154942a0d70"
        ],
        "8794dfdb-d985-4911-a472-5ac5bc853b95": [
            "33c0ea29-e6fe-494e-9367-2154942a0d70"
        ],
        "38aa9633-887a-4318-8eff-713434d3a5ec": [
            "e20bd75c-2aa4-423d-be54-27ea6f77ca45"
        ],
        "6e3d81e7-e7b5-4e63-b3d3-293075b1a47e": [
            "e20bd75c-2aa4-423d-be54-27ea6f77ca45"
        ],
        "02118535-555b-4077-8751-8f973d2cf36b": [
            "2b1ebf09-4d45-4eff-bbbf-e9a13f36cf32"
        ],
        "70ed849f-5566-4fb1-90a3-3fecee8f4056": [
            "2b1ebf09-4d45-4eff-bbbf-e9a13f36cf32"
        ],
        "b95053fc-bab4-4924-9b75-18a3e00fb3cd": [
            "85804159-fd19-432f-be9c-46c68b28fe4e"
        ],
        "8a9fb3ae-a9ef-4aa8-bbab-c2068524cd5a": [
            "85804159-fd19-432f-be9c-46c68b28fe4e"
        ],
        "e0292707-4740-44dd-9ed3-8251e1bd234d": [
            "cd3df421-759e-4b17-9bdf-0071ba086acd"
        ],
        "971ef10d-c425-4881-876f-43254a294435": [
            "cd3df421-759e-4b17-9bdf-0071ba086acd"
        ],
        "deb827e7-526d-4656-acb2-7056e8312b21": [
            "6236d16a-1514-4eef-9c54-c303d46f5d8f"
        ],
        "b61bdc36-ae80-4554-841c-2c7af9b87abc": [
            "6236d16a-1514-4eef-9c54-c303d46f5d8f"
        ],
        "b72c4745-3d37-4e26-9ccf-9cc3b8e896ce": [
            "657719e8-580e-4101-a167-4a1ece850592"
        ],
        "f1bb0039-9ff7-49c2-9d64-a803de76202c": [
            "657719e8-580e-4101-a167-4a1ece850592"
        ],
        "875e1393-1e59-4900-9522-79669bf50cb8": [
            "63d6cc12-cad7-428c-b2b2-6e188d5a1720"
        ],
        "9749f926-64ea-482e-815e-394be777afb6": [
            "63d6cc12-cad7-428c-b2b2-6e188d5a1720"
        ],
        "bc146735-0efa-4285-9053-5cdbc65370f3": [
            "19f5fcfe-9da3-4843-8ff8-53a72eaed9cd"
        ],
        "1b2f0338-2f46-4244-8273-bcc542378ac0": [
            "19f5fcfe-9da3-4843-8ff8-53a72eaed9cd"
        ],
        "13e1d320-a8bf-422e-8cef-aedaa073b599": [
            "56212782-f5bf-4da5-b085-34b16470b472"
        ],
        "ce4f5874-8b87-4617-8ed0-939ef07e6765": [
            "56212782-f5bf-4da5-b085-34b16470b472"
        ],
        "614bb638-6ae0-472b-83d0-bb77d8f24462": [
            "8e69b638-0df2-42e8-8379-c38c9799c067"
        ],
        "95a83c33-d007-4e43-b039-8d19acce116e": [
            "8e69b638-0df2-42e8-8379-c38c9799c067"
        ],
        "8d7dab68-2c0e-4fd4-9c8f-908ab355f758": [
            "4b6fca26-f282-4847-941b-46fd92e0dde9"
        ],
        "8f5fd7be-19ea-413d-a5df-c98026cd3939": [
            "4b6fca26-f282-4847-941b-46fd92e0dde9"
        ],
        "8c0d5b1f-910a-459f-93fa-75e05c98b375": [
            "91eeb6f6-59c6-4f2b-9bbc-ade958ff8206"
        ],
        "b7a2e66f-1625-4c5b-be73-c9b1590bd5e6": [
            "91eeb6f6-59c6-4f2b-9bbc-ade958ff8206"
        ],
        "0c39d8b8-f114-44b1-b837-81232ab29d80": [
            "739d2b94-66cf-49ea-b6ea-db5d81a9f58e"
        ],
        "c37d5ba3-5337-4721-93c4-733d74646978": [
            "739d2b94-66cf-49ea-b6ea-db5d81a9f58e"
        ],
        "d3446723-3592-4150-a950-b225a002040b": [
            "48e9fba2-d5a7-493a-bff7-e1a9d8cf1bd8"
        ],
        "84c23a17-58ce-4a5c-b581-4da17a95943f": [
            "48e9fba2-d5a7-493a-bff7-e1a9d8cf1bd8"
        ],
        "d6541e91-5ee7-4ce0-9d75-08f94f188d56": [
            "c473e430-6bf5-4cd6-a295-fd6b0ca98c82"
        ],
        "af22187b-27e5-4fc7-899c-b565e55b925d": [
            "c473e430-6bf5-4cd6-a295-fd6b0ca98c82"
        ],
        "dc1a5313-5892-4414-889d-35bebbcb1f0b": [
            "457c2990-dc76-4b10-8eb1-f11ea2c0b5a5"
        ],
        "1266e30e-3597-4ab1-92b7-862b60908b0c": [
            "457c2990-dc76-4b10-8eb1-f11ea2c0b5a5"
        ],
        "8d1aa15b-f350-497a-baca-e5eadb1866f6": [
            "0889a0cc-0b2d-422c-9260-86625e43c690"
        ],
        "c64f81e7-aeb6-45ec-ab29-730e37240b88": [
            "0889a0cc-0b2d-422c-9260-86625e43c690"
        ],
        "251a76ac-6970-4ab5-8f33-daa4a6ee02d6": [
            "5daaacbb-6ddc-4d70-9e65-206198a0bb2a"
        ],
        "7794943b-1f87-450e-8ec4-8bdbdd540a87": [
            "5daaacbb-6ddc-4d70-9e65-206198a0bb2a"
        ],
        "f7504dea-e5e0-42b1-9f9c-61842ee810f8": [
            "56594034-dd9d-4785-b51c-faab5a528dc0"
        ],
        "9b7e42ce-6b7d-423e-b4b9-c3bebe6520ef": [
            "56594034-dd9d-4785-b51c-faab5a528dc0"
        ],
        "cffba0c4-2727-4ead-ad8a-f62f14ca2e88": [
            "93be7c77-4b51-4565-a247-3d86a5add810"
        ],
        "df48684d-8e72-4c4f-8916-ffcdeaa3755c": [
            "93be7c77-4b51-4565-a247-3d86a5add810"
        ],
        "8b5ed1da-5629-43d7-962c-18b9083de548": [
            "8d304caa-9e33-4ba0-bf34-438809e5114a"
        ],
        "59c2f2a9-560b-4881-b8c8-88d819527071": [
            "8d304caa-9e33-4ba0-bf34-438809e5114a"
        ],
        "4c91c86f-644a-42d4-967d-6639f7dd6164": [
            "5a1a92cb-faf2-4dc6-a021-d4a7e983b4a6"
        ],
        "cabcdd20-ef03-4695-98cb-c1f32fcb4947": [
            "5a1a92cb-faf2-4dc6-a021-d4a7e983b4a6"
        ],
        "3b993edf-9e8e-4228-bc74-f7e8b51e5541": [
            "074593ce-2e20-4579-8e78-e0d07d8d17b7"
        ],
        "04d3e716-3e96-4e63-b026-c645131c0ab9": [
            "074593ce-2e20-4579-8e78-e0d07d8d17b7"
        ],
        "c107977c-e638-4bc3-9391-626cf0a7d402": [
            "07496ed7-46cb-44d8-ad24-f8211eeb98cd"
        ],
        "2c45f0d1-6517-49c5-ab92-1255776d6e18": [
            "07496ed7-46cb-44d8-ad24-f8211eeb98cd"
        ],
        "00941685-bfc2-496e-8d92-d5229e20c6b0": [
            "e55b9a24-cee0-4d02-bc64-a841d961ebfd"
        ],
        "2887dcf2-8264-4119-8c04-6ef22ccba1c5": [
            "e55b9a24-cee0-4d02-bc64-a841d961ebfd"
        ],
        "f33626c8-e230-4540-b090-c1d66e94c1a0": [
            "ec070273-1571-4549-aade-bd150b3690ae"
        ],
        "ec6f0f7b-b3e9-4dab-9ef6-758d674aed60": [
            "ec070273-1571-4549-aade-bd150b3690ae"
        ],
        "65a06c98-7baa-4b07-b6ed-d6129e93906b": [
            "afa2ea98-217d-4ada-acc9-58c4954e52b2"
        ],
        "9062ab4c-369f-446d-be5b-ac0de4d2f934": [
            "afa2ea98-217d-4ada-acc9-58c4954e52b2"
        ],
        "62d14ec5-3298-449e-a895-3068274299d1": [
            "e0c475d2-6c79-471e-84ca-e3710a343605"
        ],
        "7bf695c6-cbc4-40f5-be50-3db6f35c2928": [
            "e0c475d2-6c79-471e-84ca-e3710a343605"
        ],
        "2c056569-f41a-419c-92f6-a3b6a1716463": [
            "4855050d-d62a-43a5-ac9f-ccbb707963d0"
        ],
        "8c4b7651-4b6d-4370-ac54-2290ebc2bf04": [
            "4855050d-d62a-43a5-ac9f-ccbb707963d0"
        ],
        "db4dfa08-6cf1-4978-927e-665705f9713b": [
            "502c5c88-2d0e-43ba-b8a4-8be74428132a"
        ],
        "1c090b37-aa50-4690-8120-cdbd24566a2e": [
            "502c5c88-2d0e-43ba-b8a4-8be74428132a"
        ],
        "845d150c-097a-4afd-8116-e34d251509fe": [
            "530f9478-54ae-4d50-8541-ce05a93db349"
        ],
        "55df4551-3f2a-480b-83cb-ea839a496fd7": [
            "530f9478-54ae-4d50-8541-ce05a93db349"
        ],
        "be1c8ea2-4e13-49d9-be76-3e2148ac8301": [
            "85be390a-3697-4572-b98e-02a94f4d96cd"
        ],
        "92d94e92-39e8-4e4e-ac5e-cb914296fe35": [
            "85be390a-3697-4572-b98e-02a94f4d96cd"
        ],
        "f1a7df6b-708a-4d76-ac6d-e8150b154b21": [
            "481d36a6-79a6-4115-9adc-383d0301bcb8"
        ],
        "575adee2-1e30-42f1-9252-a8a2fa5cfcf5": [
            "481d36a6-79a6-4115-9adc-383d0301bcb8"
        ],
        "a1a4e280-b8f8-4650-887a-3ccc93832de1": [
            "255bece7-aca3-47ce-88e5-5bf01f73b749"
        ],
        "6d2c483f-e831-4a19-b3b1-50afe8a0a515": [
            "255bece7-aca3-47ce-88e5-5bf01f73b749"
        ],
        "ad9977ad-6af9-4332-b41b-fbe111ac914c": [
            "3a934b6d-bc06-43f4-bbad-718aeb86ed78"
        ],
        "8dd627db-fa73-453b-9186-9d93f8735264": [
            "3a934b6d-bc06-43f4-bbad-718aeb86ed78"
        ],
        "a87a20f6-59bc-49c3-86fd-3bbead3119a6": [
            "84b02b04-268d-481f-967c-37bded376c33"
        ],
        "0c3c5626-7d37-4407-bc2b-9f5085b84f6b": [
            "84b02b04-268d-481f-967c-37bded376c33"
        ],
        "33db4107-5458-4fe0-8f0e-447208555da5": [
            "612a5ab3-2241-4d3e-97a6-520ac6dbab48"
        ],
        "f0ff8d8d-8ab6-4489-b2b7-efe287f69612": [
            "612a5ab3-2241-4d3e-97a6-520ac6dbab48"
        ],
        "8d8036b6-d94b-464d-b4cd-96b781f86a9c": [
            "bd2a6f0d-7190-44c5-a953-c24ee3011340"
        ],
        "9ac763a6-2431-409d-9918-fde0b59bfd07": [
            "bd2a6f0d-7190-44c5-a953-c24ee3011340"
        ],
        "62922525-ea88-40e9-bccd-5460233c5bdc": [
            "06065b3b-4038-4985-8b30-af33a10f0957"
        ],
        "bd850e1d-2cf4-459a-abab-4f0f4aca6a61": [
            "06065b3b-4038-4985-8b30-af33a10f0957"
        ],
        "ad8e1efe-4458-46f8-ad31-5365d703a7ea": [
            "2ffef67c-f70a-409f-b969-89091f6475bb"
        ],
        "bcd64ac0-f413-4f44-a321-fa9e866c0661": [
            "2ffef67c-f70a-409f-b969-89091f6475bb"
        ],
        "17e0fc6f-efde-4f58-aef0-03debfecd739": [
            "9734b08b-6d3f-40a0-96d8-6be9e25c00c9"
        ],
        "3cf40bda-7f29-428f-88ba-29ae3349d42f": [
            "9734b08b-6d3f-40a0-96d8-6be9e25c00c9"
        ],
        "f47e14db-8c96-48ca-913b-3b1baf95dcb8": [
            "464c5a63-8d21-44a9-a083-f6e63648d6ab"
        ],
        "d3f134e2-ed81-48c4-8704-10e4ffbc1773": [
            "464c5a63-8d21-44a9-a083-f6e63648d6ab"
        ],
        "02e48dd8-41a9-4adf-9fa1-930fd7d670e0": [
            "1b678035-c296-4050-8123-fec77535de57"
        ],
        "be3c9a80-b654-4ea7-aba1-aa4d80299184": [
            "1b678035-c296-4050-8123-fec77535de57"
        ],
        "bcab10f0-8e86-4171-bca3-c6ed851e0d5e": [
            "ed37d09f-f7f6-4422-94b5-a46676779b96"
        ],
        "d37d4d4c-c2ec-4c52-a8a5-205d4d89a62c": [
            "ed37d09f-f7f6-4422-94b5-a46676779b96"
        ],
        "5ea6938d-c744-427a-81e9-2e79ae00e23c": [
            "ab8edbc7-4355-408d-96a9-cdafa150fbe3"
        ],
        "1b227bc9-add2-474e-860d-a6f59eeb51f9": [
            "ab8edbc7-4355-408d-96a9-cdafa150fbe3"
        ],
        "cb080b9d-ac4f-4bc1-a04f-d6217a32b1d8": [
            "6a7ed1c8-d039-41fc-bbb2-7bdb9cf5aa55"
        ],
        "f9efb1e9-fc21-4baa-998e-aad25916b291": [
            "6a7ed1c8-d039-41fc-bbb2-7bdb9cf5aa55"
        ],
        "e2c1b590-b266-497c-a3c9-7c9cfa6c75ec": [
            "3233d881-22fd-41b0-a63f-00634c992058"
        ],
        "38906cc4-97ec-4a36-ac22-ccacd22334b9": [
            "3233d881-22fd-41b0-a63f-00634c992058"
        ],
        "dd34e3a5-83e7-4486-aafd-3a7d26f289a4": [
            "2d4216d9-f068-4a82-89b2-0f4be680bc4d"
        ],
        "0d0b18f4-9bd6-433a-bd80-3daaa47aa637": [
            "2d4216d9-f068-4a82-89b2-0f4be680bc4d"
        ],
        "7ba65e5f-3e61-4731-85e1-7b1ad216c65d": [
            "97a4a83d-ce1d-42e8-b583-098b9ba6a764"
        ],
        "4ce2dffb-8849-48fd-9460-3508ed1481ec": [
            "97a4a83d-ce1d-42e8-b583-098b9ba6a764"
        ],
        "29a0674a-5a05-4d03-b841-a0fb0f46fa2a": [
            "b6958d16-a557-4a3e-8622-9651b8e7a7c7"
        ],
        "d8ac2800-15a0-4a0d-aa0d-90b64dd4c2ed": [
            "b6958d16-a557-4a3e-8622-9651b8e7a7c7"
        ],
        "58b4029e-e862-45c8-8da1-ef454f693cb1": [
            "0d8cf42a-1e97-4902-bbf8-93d096c683a8"
        ],
        "4b9bb9b4-317c-4795-a180-a55176236f23": [
            "0d8cf42a-1e97-4902-bbf8-93d096c683a8"
        ],
        "65c61499-d066-4eec-98dd-6633aae9a207": [
            "fb284951-e1a4-4a9f-b3af-1c4312ed8a2b"
        ],
        "09cd95dc-f3dd-4c21-a0a9-7161551e8b64": [
            "fb284951-e1a4-4a9f-b3af-1c4312ed8a2b"
        ],
        "368d52c5-ec6e-4f06-8510-c4d7cf8362ba": [
            "f5faacf4-2adb-44f5-a31c-4d488847294f"
        ],
        "7d8016f6-2b01-4f40-952d-40561413db1f": [
            "f5faacf4-2adb-44f5-a31c-4d488847294f"
        ],
        "6ff4fccd-e235-4d5d-a566-aab066957fa2": [
            "c2c68fca-fa30-4c8a-9702-1c5902f93881"
        ],
        "c711e7f6-9052-48ca-926f-db9106266952": [
            "c2c68fca-fa30-4c8a-9702-1c5902f93881"
        ],
        "f9084cdb-8fef-4484-aad2-86b955bc3493": [
            "5b3a11dc-0390-45ca-858a-fc40fc997e9a"
        ],
        "87353a51-03e6-4393-8fca-e77f677e39f3": [
            "5b3a11dc-0390-45ca-858a-fc40fc997e9a"
        ],
        "60d8730f-cb0f-49e3-ae78-3d1525036582": [
            "0db7d9f2-9b69-4e55-9db9-58cef5d0b655"
        ],
        "9053a462-4399-4f41-9e25-3f0439fc7366": [
            "0db7d9f2-9b69-4e55-9db9-58cef5d0b655"
        ],
        "bfed2ed0-5d86-4d62-9317-4f9f70c8caad": [
            "ea830181-017e-44e3-89ba-9e86a6f471f0"
        ],
        "3cc55e2d-786e-458e-9fe7-c0f016f8ced0": [
            "ea830181-017e-44e3-89ba-9e86a6f471f0"
        ],
        "1bed8fab-98c8-4e45-87d7-f15e1b679856": [
            "83d7f651-16cd-43d9-a938-842d74f889f8"
        ],
        "ddf60625-e47a-45a6-851b-ba2e30d3eef1": [
            "83d7f651-16cd-43d9-a938-842d74f889f8"
        ],
        "735d3ba7-e9bc-4c08-9332-917c02a778cc": [
            "a9994e61-0039-4403-b823-48b4a3c6ae0e"
        ],
        "f6fae989-e6f6-4806-addd-7fd30486f2f1": [
            "a9994e61-0039-4403-b823-48b4a3c6ae0e"
        ],
        "f4c3acc3-53bf-4bbd-882b-909305d4dcc8": [
            "e5d5ad56-e3a5-415d-b71b-e9d7813925b7"
        ],
        "635ebcac-be98-4b85-a8b5-360cd40bbf0f": [
            "e5d5ad56-e3a5-415d-b71b-e9d7813925b7"
        ],
        "9ed1cafd-7986-4ffe-9fde-22e28966e942": [
            "5d2666a7-e134-479d-8e0d-9004d6a8d2f6"
        ],
        "0d025ff5-0033-4b35-8f07-52d85f51b5a9": [
            "5d2666a7-e134-479d-8e0d-9004d6a8d2f6"
        ],
        "880c657f-77c2-4ebf-a455-1e373ba793b6": [
            "7d2065a1-ade4-451c-b995-ffd949fc5d75"
        ],
        "f1378d56-bc57-40da-b37e-12b41249111b": [
            "7d2065a1-ade4-451c-b995-ffd949fc5d75"
        ],
        "094977a3-01d5-4487-824a-972dfe151415": [
            "0277c313-94e8-4fea-a6bc-3332463cd89d"
        ],
        "59ed6b75-ba62-438a-9e1a-8d52ab164496": [
            "0277c313-94e8-4fea-a6bc-3332463cd89d"
        ],
        "a85d908e-5800-4ff8-9202-5db6b088c9cc": [
            "94b02fd0-e025-4d89-b21d-dfe6a658974e"
        ],
        "45985897-a3c2-4167-8672-92eca7e778ca": [
            "94b02fd0-e025-4d89-b21d-dfe6a658974e"
        ],
        "641edbf4-d762-4740-9030-2560348e9193": [
            "d3d35edd-c3b5-4873-8e35-49db6c9cc3b2"
        ],
        "a0f90a00-c2bd-42a8-aab7-b2a88c5f0562": [
            "d3d35edd-c3b5-4873-8e35-49db6c9cc3b2"
        ],
        "a72b1384-a98d-4853-b28a-59baed39bf1f": [
            "a0db595c-3a59-4ee0-9766-c7626ac21529"
        ],
        "04302f51-9bf9-421c-9df9-b72d2f26f9cd": [
            "a0db595c-3a59-4ee0-9766-c7626ac21529"
        ],
        "3ecb5941-9bfb-437e-8e9e-1469c7909b2a": [
            "60b5fe1e-a12c-4a5b-af77-009b4fc11005"
        ],
        "bd248b7c-d14a-4151-8061-f5a04372b2c5": [
            "60b5fe1e-a12c-4a5b-af77-009b4fc11005"
        ],
        "0fde5cd0-678b-4b54-97a6-98d65e580520": [
            "7c4ad272-0d73-4c5e-b9fa-3b658c21de5c"
        ],
        "c60a03c5-eeda-4de2-b669-8060807ca384": [
            "7c4ad272-0d73-4c5e-b9fa-3b658c21de5c"
        ],
        "23519186-2c73-4533-8e46-e92578b3b225": [
            "e7cb28f0-14dd-41ee-a666-9d15cd910f13"
        ],
        "5a714d67-f05c-4b8e-8a8b-e64c7fb23eae": [
            "e7cb28f0-14dd-41ee-a666-9d15cd910f13"
        ],
        "95cbbfea-64f5-4aee-b565-95643e9a9421": [
            "7a653fc6-d162-4444-ba2e-4c450b4dec26"
        ],
        "5b348abb-57cd-4671-b7ec-4a1cad4987be": [
            "7a653fc6-d162-4444-ba2e-4c450b4dec26"
        ],
        "26c5cd41-5282-4a1c-a015-00bcbdeef85d": [
            "070ad750-991c-464b-89cb-fcece72770d9"
        ],
        "6b1bc975-92e6-40a2-a3bf-066068055509": [
            "070ad750-991c-464b-89cb-fcece72770d9"
        ],
        "be8c8dfe-c040-4fb9-81fa-a40d4b25d6d7": [
            "1a7f355f-d7f3-4ebd-8f2d-aac4364cae48"
        ],
        "d60e75d6-53da-4bed-a805-62de096eb3ff": [
            "1a7f355f-d7f3-4ebd-8f2d-aac4364cae48"
        ],
        "9ce08b8b-f936-4c0c-8b3f-253e16594761": [
            "d7515345-d7ec-4724-a7c8-beeb64ff8baa"
        ],
        "744917ef-419f-4d37-aa13-1916ea89b2cb": [
            "d7515345-d7ec-4724-a7c8-beeb64ff8baa"
        ],
        "5b2cbdd2-7d89-44b8-ac0f-120c6dac656a": [
            "a1d11946-6a9c-42c7-a46f-d847c76c7251"
        ],
        "e9bff3ce-7760-4c5f-882d-9bff9776babc": [
            "a1d11946-6a9c-42c7-a46f-d847c76c7251"
        ],
        "0544f6aa-ab02-4708-85de-9c806d44aa61": [
            "4fc8a469-4ab3-47e6-877c-cc26521dc837"
        ],
        "9e3fd956-3aad-416c-b3dd-873e4cebb603": [
            "4fc8a469-4ab3-47e6-877c-cc26521dc837"
        ],
        "5c39d981-d298-451e-af3b-5058d33090ea": [
            "ddcb6252-ac89-4d31-82ad-c6fac28ae24d"
        ],
        "c2040d6e-c562-41fc-85a6-936ce793c152": [
            "ddcb6252-ac89-4d31-82ad-c6fac28ae24d"
        ],
        "4aef93ff-112f-4a73-adac-f12bf336be00": [
            "ac9a1727-662e-4990-af1b-3677b41b5064"
        ],
        "4efb1b27-4eb7-4e2d-83e5-857159d2227b": [
            "ac9a1727-662e-4990-af1b-3677b41b5064"
        ],
        "0655cb4a-8717-4621-93b7-1acad840cc98": [
            "758911f9-c3b5-46f2-a80a-77b8fc6bc805"
        ],
        "1586df19-8839-4f46-b368-ef00badedd0f": [
            "758911f9-c3b5-46f2-a80a-77b8fc6bc805"
        ],
        "7f7000a8-1d00-4f0c-bf39-c417541b00e6": [
            "2724bd9f-ab5c-4a29-a3eb-9c5439fe1f94"
        ],
        "9878b8f8-f5dc-4514-835e-da951e815253": [
            "2724bd9f-ab5c-4a29-a3eb-9c5439fe1f94"
        ],
        "526293d4-2977-46f3-ad98-6c8d395dbd71": [
            "c22451fc-00d5-4e2f-abae-84e2943b7231"
        ],
        "6df3b7d8-d2fb-49e4-99bc-3515a9a0a086": [
            "c22451fc-00d5-4e2f-abae-84e2943b7231"
        ],
        "a534d406-c9a1-4bc0-aa9e-8a5156487484": [
            "1d892e72-e194-4df8-944b-0f890a5abb17"
        ],
        "5169979e-f7bc-4564-a969-221d667b3639": [
            "1d892e72-e194-4df8-944b-0f890a5abb17"
        ],
        "2423f6b3-19cf-4979-b732-2099b99cfbd5": [
            "23a0c36a-2c7e-47a4-a039-c22f9ec5d55e"
        ],
        "f0e6fbf2-064c-46ef-b727-13d1862146d2": [
            "23a0c36a-2c7e-47a4-a039-c22f9ec5d55e"
        ],
        "0adbd153-05b5-4059-aa1a-875c98a2887f": [
            "801d7f07-e7f6-4da9-8d2e-90f377ba1aae"
        ],
        "572d4718-b6aa-4ac3-b79f-4b51182e1967": [
            "801d7f07-e7f6-4da9-8d2e-90f377ba1aae"
        ],
        "37078e3b-d7e4-4159-9918-ce80021298ac": [
            "a2a53491-c485-47c6-8edb-31c474fba082"
        ],
        "14d57825-f062-465f-89ca-daacc6c5d9d4": [
            "a2a53491-c485-47c6-8edb-31c474fba082"
        ],
        "aeabed37-6607-4368-9604-0113fccb1e93": [
            "a76cef56-3450-412a-a594-1b93d13b1e0c"
        ],
        "2f0109a3-898c-405f-aef7-26bbf04a43ed": [
            "a76cef56-3450-412a-a594-1b93d13b1e0c"
        ],
        "1a998dc1-a62e-4be6-a555-b8e9f97966b1": [
            "c3805fa2-9634-42e9-b48e-b20aadd36251"
        ],
        "bcda3565-a3e3-45ca-96ac-59615aea6097": [
            "c3805fa2-9634-42e9-b48e-b20aadd36251"
        ],
        "2d8f0fa0-4948-4149-9008-61310abcecf0": [
            "b6081dca-bf79-4952-939e-3e71ec0abae7"
        ],
        "23874358-d062-4f0e-856a-9886c3f40dbd": [
            "b6081dca-bf79-4952-939e-3e71ec0abae7"
        ],
        "209c20a6-3e34-4b65-b075-15f6571a6a2b": [
            "994912ce-4bf6-4d37-b5b3-a6acf343fb31"
        ],
        "a10275ec-702c-4bf5-a629-333828064988": [
            "994912ce-4bf6-4d37-b5b3-a6acf343fb31"
        ],
        "81b8681c-f253-4909-8432-aeae064a5f74": [
            "823ec62a-335c-4b68-8651-d70907cb1fb0"
        ],
        "c81416ae-f0dd-4207-b154-1fb0f9d88c80": [
            "823ec62a-335c-4b68-8651-d70907cb1fb0"
        ],
        "6cb615e6-dbca-461b-9f1b-be8af5765164": [
            "dfefeb10-695f-4ede-a138-e09323165950"
        ],
        "28fb1f44-b85a-45d1-a739-140c2d87b6b0": [
            "dfefeb10-695f-4ede-a138-e09323165950"
        ],
        "c941c1c4-5fd2-43d4-8dfc-d15946d459fc": [
            "ad29d679-f449-4ed9-8770-d782df7c1078"
        ],
        "d21a0b72-404f-4024-aca0-f0f58d693b58": [
            "ad29d679-f449-4ed9-8770-d782df7c1078"
        ],
        "3438cbc3-3f16-4bea-9a0f-4a2ba0d67e49": [
            "b4dec272-0d4c-4cc7-a058-e78788d3037f"
        ],
        "b92252e1-d3c7-4846-98f4-f204fcc879f2": [
            "b4dec272-0d4c-4cc7-a058-e78788d3037f"
        ],
        "ea9754e7-63b2-4cc7-9f2c-a4e0c13236db": [
            "934e8023-80ea-40ae-87b3-05dea709df48"
        ],
        "2d1fd114-b6ff-40a8-8352-38fede04c6e0": [
            "934e8023-80ea-40ae-87b3-05dea709df48"
        ],
        "ccbf780b-dbd6-4da9-b555-f0c1182fe9f5": [
            "32f57a1c-5494-4abd-86ac-308630304bee"
        ],
        "e5b79413-0791-4fc5-a382-d1c4807e9b30": [
            "32f57a1c-5494-4abd-86ac-308630304bee"
        ],
        "8c26c318-d091-47e8-aab4-aab924bf7357": [
            "73dc70ff-d2cb-4d1c-8c7e-227c2194f5aa"
        ],
        "add28ba5-01f2-4ad7-b4de-ea2033bf4644": [
            "73dc70ff-d2cb-4d1c-8c7e-227c2194f5aa"
        ],
        "d05c2027-d36a-4e69-8809-d89c90de13e1": [
            "89bc1e1f-7226-4d9a-8a3d-b0650fb211df"
        ],
        "3fcf217c-c04c-49e0-b1c5-cde882791988": [
            "89bc1e1f-7226-4d9a-8a3d-b0650fb211df"
        ],
        "e7f2a4d9-5999-4999-ad8f-c568ebc3262e": [
            "d16f0fc7-f5fc-489d-ba3e-bc897bb2f31c"
        ],
        "b6842672-68b4-42af-8dc2-c5e9857c08f7": [
            "d16f0fc7-f5fc-489d-ba3e-bc897bb2f31c"
        ],
        "1476d511-0966-475b-8352-9247f9c5357b": [
            "626c8d5f-6a02-4100-a321-ac88541d2a94"
        ],
        "8bc70c1d-c007-49ff-8b90-0b7eeb585f12": [
            "626c8d5f-6a02-4100-a321-ac88541d2a94"
        ],
        "1debdfb9-ab3e-4e84-9323-63115d1d9a98": [
            "6960ed2d-d338-46be-9b35-20152ed769db"
        ],
        "601e14d2-0d26-4255-8cfd-8fc042e33293": [
            "6960ed2d-d338-46be-9b35-20152ed769db"
        ],
        "411dcec7-74a1-466d-a68b-a692a29b7ebc": [
            "01066dd7-d390-48ef-aee8-40d696a134e4"
        ],
        "ed452670-7c8c-4e0b-844c-5a62d8ae8763": [
            "01066dd7-d390-48ef-aee8-40d696a134e4"
        ],
        "bb6389d3-f442-47ee-95e0-b673f622d751": [
            "08b346ff-0c1c-45bc-a9be-a07513a381b6"
        ],
        "0287d221-6dd3-4064-a6c3-de71d17fc262": [
            "08b346ff-0c1c-45bc-a9be-a07513a381b6"
        ],
        "bb3ebc95-6f2f-4169-9654-7c47af58dce4": [
            "ce245bb6-1bb8-48fb-b47a-e46295de334a"
        ],
        "71d07595-0b1f-483b-81f5-0e1a1dfc9811": [
            "ce245bb6-1bb8-48fb-b47a-e46295de334a"
        ],
        "63cf47f5-3d7d-4f79-9a83-21d2761865a6": [
            "2599390f-b20c-49ac-9a3f-3070e20c015b"
        ],
        "45fb27d3-b234-42dc-a576-308396a6fa79": [
            "2599390f-b20c-49ac-9a3f-3070e20c015b"
        ],
        "0501d9f9-390d-457f-873e-b88429b9b869": [
            "eff857ff-060a-496a-a4e0-d8920906c25c"
        ],
        "f0a50587-71ef-4040-90d9-c825a2bddf71": [
            "eff857ff-060a-496a-a4e0-d8920906c25c"
        ],
        "be6a97de-075d-4eed-8f0f-6caa807ce0ae": [
            "e1a11717-bcb3-4912-9227-8875b918ce19"
        ],
        "e4df0231-0072-427c-a837-8e5654eda8fc": [
            "e1a11717-bcb3-4912-9227-8875b918ce19"
        ],
        "b7efbd9e-2616-4d11-90a0-f34ffae375b5": [
            "ce9e5f0b-22e3-4920-8db9-eeddfa06655c"
        ],
        "ba3db739-5ea6-4fb1-b527-0831e3b79182": [
            "ce9e5f0b-22e3-4920-8db9-eeddfa06655c"
        ],
        "df2aab19-03c6-4388-b44f-ab571b0a5cae": [
            "f027c65a-e51d-4c22-b8e6-0b09d2596831"
        ],
        "5e830980-fe0b-4a33-bc4d-57055a290174": [
            "f027c65a-e51d-4c22-b8e6-0b09d2596831"
        ],
        "e7cf90e1-0982-4f1e-a16b-bccff7d558ae": [
            "87f94444-3447-49c0-bced-80baf4b1a86f"
        ],
        "317043fa-15cf-44a9-b41f-7e9ba6d068b3": [
            "87f94444-3447-49c0-bced-80baf4b1a86f"
        ],
        "04132e60-ab65-4f7c-bcbc-acc984284439": [
            "85715596-1bba-4f84-872d-3394de9d724f"
        ],
        "a77c7f7a-fd5d-4f32-9ecd-594a6b79e496": [
            "85715596-1bba-4f84-872d-3394de9d724f"
        ],
        "8273c485-63be-460b-894a-dcecf83b2392": [
            "eda51698-8bf9-4f31-899f-bffae6910d29"
        ],
        "1988dd35-a255-43a6-b53d-1fe84b8f8696": [
            "eda51698-8bf9-4f31-899f-bffae6910d29"
        ],
        "9b903d4e-1fa6-4573-afa7-56b9a5af83d4": [
            "98499020-e4a4-422b-a5b5-45deeff44aad"
        ],
        "a4cf92b0-6a4b-4415-a654-a9a6831b204c": [
            "98499020-e4a4-422b-a5b5-45deeff44aad"
        ],
        "163eaa99-572c-44f6-bfd7-d555b8bc99fe": [
            "0a3b3961-43d7-4dd7-924f-4c41d2e06f26"
        ],
        "2b6490fa-95e4-4676-933d-3f5ecfe95c2d": [
            "0a3b3961-43d7-4dd7-924f-4c41d2e06f26"
        ],
        "90116b47-7dfe-4da6-a520-275965f7f4f3": [
            "17f93c03-878d-43d7-831a-e5c252065ed1"
        ],
        "fae812d5-fc10-430a-b0d8-c3a8c4a6eae6": [
            "17f93c03-878d-43d7-831a-e5c252065ed1"
        ],
        "13dd4fe2-caa9-4eeb-88de-aff7911587d5": [
            "00ba0376-cec0-4f80-8973-1e148d0891d1"
        ],
        "d2c14117-32ff-4e7e-8d18-7f7ee14279de": [
            "00ba0376-cec0-4f80-8973-1e148d0891d1"
        ],
        "7f92649b-64fb-404a-a214-d8399fac23bb": [
            "0d690a01-010c-4b5a-bdcf-496016c0ebbb"
        ],
        "f818b904-ff2c-43c8-8936-0841fb442648": [
            "0d690a01-010c-4b5a-bdcf-496016c0ebbb"
        ],
        "cfba3857-8105-494a-ba57-42fec9a44f02": [
            "37b1b1d2-5f33-460c-986b-98edb9c61d69"
        ],
        "aa68486c-fbb3-49f3-8ea6-5c5001da4933": [
            "37b1b1d2-5f33-460c-986b-98edb9c61d69"
        ],
        "0aeb7c53-abb7-4a43-91aa-5e99ac65a2d9": [
            "4cdebab7-ca63-4a13-8932-eb19533c2551"
        ],
        "24680821-ac44-4281-a65f-6a1c61a81c58": [
            "4cdebab7-ca63-4a13-8932-eb19533c2551"
        ],
        "95593b28-fe29-4b75-8400-9f709242cace": [
            "53877d18-67db-4f64-be82-20ff274e40f2"
        ],
        "2840daa3-ff3e-4742-852d-0e31d83e41ac": [
            "53877d18-67db-4f64-be82-20ff274e40f2"
        ],
        "6590d773-76ab-4807-8b92-7eaeed10f93a": [
            "b1a5820d-0f03-4e23-ab9e-8552d235d705"
        ],
        "ee6d2317-6d1c-4932-ac74-8f308f0d8e9a": [
            "b1a5820d-0f03-4e23-ab9e-8552d235d705"
        ],
        "527e7929-f9d9-4cc7-a4d0-9c625972c45f": [
            "177b005e-42c7-4e50-ae1d-388a84c75fe5"
        ],
        "54612f80-2622-4ae3-a239-6848536e292e": [
            "177b005e-42c7-4e50-ae1d-388a84c75fe5"
        ],
        "461603f7-17b1-4e45-8c20-68fd19517c5e": [
            "40a06deb-8be3-4db0-9a49-54aabb77f5e6"
        ],
        "507b185c-87da-4ef3-9052-5b08329bf952": [
            "40a06deb-8be3-4db0-9a49-54aabb77f5e6"
        ],
        "f8718e95-fe9c-4e77-af15-ac44b22a9bc1": [
            "2f75edea-10b1-4d55-8500-80b4471fbc62"
        ],
        "93eb1ae6-abeb-4b06-8be0-276efd5abede": [
            "2f75edea-10b1-4d55-8500-80b4471fbc62"
        ],
        "e076b910-1722-49db-9fec-5a623151618c": [
            "318de1d9-3b4d-46a9-8e8a-c4259cbf9ace"
        ],
        "3f863240-ff88-4cce-b3fb-a32daf2b7fd9": [
            "318de1d9-3b4d-46a9-8e8a-c4259cbf9ace"
        ],
        "003ed9a4-746d-4686-9ba9-46415f0699ec": [
            "63fbce16-2cbf-49ab-802b-fd46870e12ce"
        ],
        "7695070f-4877-414c-a0fe-d9402b30dd58": [
            "63fbce16-2cbf-49ab-802b-fd46870e12ce"
        ],
        "f56fd3ff-99e5-42a7-8d5d-ff7bf51044d7": [
            "f6088fb5-d5f5-49ed-a123-cff20c9b22b3"
        ],
        "63c28a4f-5d0a-4817-9d5c-d73c01f0311a": [
            "f6088fb5-d5f5-49ed-a123-cff20c9b22b3"
        ],
        "65199081-4704-41e0-8887-75fd9bbfe8d3": [
            "5228703e-9531-4684-8e86-77c46beb9a0a"
        ],
        "7044a773-17cc-4626-b7c5-0430c71b1a3a": [
            "5228703e-9531-4684-8e86-77c46beb9a0a"
        ],
        "ab2314b6-46f1-4ddf-b5f4-ede22f7369b4": [
            "8ff5a00e-15ff-4daa-9b2d-d685c9c6ce24"
        ],
        "78fef877-1a9e-4db5-9529-a305af6ba3cf": [
            "8ff5a00e-15ff-4daa-9b2d-d685c9c6ce24"
        ],
        "77c124af-a4e4-431a-befa-eb93ec14bda5": [
            "6e2058bc-0e7e-4b95-8397-ea2a1cd4a932"
        ],
        "3a4fef42-7418-4773-be87-82f9e6b82a9f": [
            "6e2058bc-0e7e-4b95-8397-ea2a1cd4a932"
        ],
        "f824c7f1-4a4b-44ac-96a0-4626f9c1f11b": [
            "117d2013-b7f4-48cd-80c5-4805db439e15"
        ],
        "96d10fb5-46ea-4f0c-9a1b-0a8977431984": [
            "117d2013-b7f4-48cd-80c5-4805db439e15"
        ],
        "b5592d9e-bda7-4a00-88e4-91cc40af0049": [
            "e4c87a0d-cf11-4f79-a71d-d7cf2b86af7e"
        ],
        "ff518189-de14-4902-ab9c-7901815c3f9a": [
            "e4c87a0d-cf11-4f79-a71d-d7cf2b86af7e"
        ],
        "21a19951-c697-4e14-8116-ec08fec304b4": [
            "aec4b0e1-db56-4e0d-b2f8-bef875c6d119"
        ],
        "26c91efa-03e1-4561-8b19-9dd9366fd85b": [
            "aec4b0e1-db56-4e0d-b2f8-bef875c6d119"
        ],
        "05d017dc-e91b-432d-96ec-fa81005cf263": [
            "db15338f-dd18-4429-8c1e-f650db65715f"
        ],
        "61547668-0d75-4e32-aa6d-d9fa389b7e7d": [
            "db15338f-dd18-4429-8c1e-f650db65715f"
        ],
        "6bddf994-c87d-46f7-bf00-550faf0fe38f": [
            "0b1550d1-91ad-40b5-8fcf-564f365f10f4"
        ],
        "2ec9c900-79b5-43cc-a517-2eb27ea4e2d0": [
            "0b1550d1-91ad-40b5-8fcf-564f365f10f4"
        ],
        "5803ccd7-ee2d-484c-97ee-8d7573d20789": [
            "ffb1d1d3-3dac-45e9-a459-31843df41c77"
        ],
        "089acf19-bc50-460c-8f15-5270248f7455": [
            "ffb1d1d3-3dac-45e9-a459-31843df41c77"
        ],
        "712bcd70-a94a-488e-a216-59b838c5c6d9": [
            "707a59b0-1eb3-40bb-a841-2038afed14d0"
        ],
        "8e79efdb-6fd8-4a76-ac0f-ed77617b83df": [
            "707a59b0-1eb3-40bb-a841-2038afed14d0"
        ],
        "55245fb7-2ff7-4a65-bd2b-4b1f835dea10": [
            "5121138b-ed93-4dbc-b626-18a96bfc0c2e"
        ],
        "491f3f7b-0479-4780-9938-f26f4f05bef6": [
            "5121138b-ed93-4dbc-b626-18a96bfc0c2e"
        ],
        "b18ca0ce-355e-41b2-b904-37eb3c2ded9a": [
            "e74fcee8-a56f-4d51-a591-704a78b4fd7c"
        ],
        "ad6b8719-2005-43e6-babb-38340bd34ae0": [
            "e74fcee8-a56f-4d51-a591-704a78b4fd7c"
        ],
        "f89b6c39-2c0d-4ee9-83c5-cc73e61e3269": [
            "d07b6959-0084-4a1e-83e6-b0a8a5d83bc0"
        ],
        "c217c053-4bb7-404f-8a5a-270b440216e5": [
            "d07b6959-0084-4a1e-83e6-b0a8a5d83bc0"
        ],
        "ac8c34c4-64ad-4f45-9ef1-7e22549173ce": [
            "554a6510-910e-48ab-a1a9-010ee2c777e2"
        ],
        "281dd1c0-c61d-4baf-8212-9ac28c472dbc": [
            "554a6510-910e-48ab-a1a9-010ee2c777e2"
        ],
        "c48edb7d-9ffd-44e4-82c6-333231c9b897": [
            "edb368ae-46c9-42b8-8579-f068998ce231"
        ],
        "d890b24e-11b4-424c-98c3-451c627227e6": [
            "edb368ae-46c9-42b8-8579-f068998ce231"
        ],
        "52fe0d28-cdf9-4932-8291-3d6243eb7706": [
            "0c7c239b-f559-4838-bb6a-5e0de3abdc5e"
        ],
        "6d5052f8-744a-48cb-bca2-217e3d7fee04": [
            "0c7c239b-f559-4838-bb6a-5e0de3abdc5e"
        ],
        "03d9a8fd-c4d9-42cd-8723-dd9ee586ef67": [
            "8d590061-dfe4-4233-89a7-2d6ae10e4a5b"
        ],
        "2c05fd32-b729-48ef-b66f-e5c01d4a91a0": [
            "8d590061-dfe4-4233-89a7-2d6ae10e4a5b"
        ],
        "d7567fa8-5d0b-4fb3-a098-c15a1ca02404": [
            "d5714965-3919-4f85-8fd5-aecb170cb739"
        ],
        "6bf97804-2150-4b60-abf7-66136f077f6d": [
            "d5714965-3919-4f85-8fd5-aecb170cb739"
        ],
        "e2a61bdf-0297-45e6-9cb1-054feac92d4c": [
            "d5770ccf-a2d8-4f01-aa8e-1c08740eb459"
        ],
        "37e35818-409d-48dc-817e-a5d7834d87d9": [
            "d5770ccf-a2d8-4f01-aa8e-1c08740eb459"
        ],
        "382b6cdd-0fc4-4e38-b22b-ce03c897f628": [
            "d25b0fcc-1293-4773-b099-31d239c96274"
        ],
        "3be2046e-2788-4368-9ec4-2f793282b1fd": [
            "d25b0fcc-1293-4773-b099-31d239c96274"
        ],
        "ddde1cbd-86cd-4c0d-ac1a-ce6a7c119828": [
            "9652f977-c660-4a36-b4d8-257225dc05a1"
        ],
        "e30cd312-6f3e-4e21-8d0c-f4e625d77bcc": [
            "9652f977-c660-4a36-b4d8-257225dc05a1"
        ],
        "234fa71b-3388-4298-b497-da61fea05db9": [
            "e42a4295-ebf4-403c-9af7-897ba1a508c6"
        ],
        "f6fe6df3-ea3b-4b57-81c7-d0e72d291936": [
            "e42a4295-ebf4-403c-9af7-897ba1a508c6"
        ],
        "b1e17e0a-5ab5-4fc3-b724-82783bb5c944": [
            "f2f31eea-23c8-49d8-9b3e-f0600058756c"
        ],
        "818a1797-0344-4127-9ad4-4dd33d5de42e": [
            "f2f31eea-23c8-49d8-9b3e-f0600058756c"
        ],
        "050053fa-ebae-4a67-9880-d9a9a53e375d": [
            "572600c3-71bb-451f-ac4b-fe60494e3d17"
        ],
        "1bf91e78-7180-4256-b60e-fb8bd25c6410": [
            "572600c3-71bb-451f-ac4b-fe60494e3d17"
        ],
        "47dcd3b2-5d61-4963-9651-be0528a9d7b7": [
            "d5ff6f3e-725a-45be-a709-572567d412da"
        ],
        "516c4330-b779-45a7-8a03-68a81ffa7427": [
            "d5ff6f3e-725a-45be-a709-572567d412da"
        ],
        "fe31679c-b55a-4762-a141-be80bbe7e68c": [
            "d313246b-cbc4-4545-bef1-2365c531178b"
        ],
        "6cc0dfaf-1254-4e78-9b07-81d2da42e223": [
            "d313246b-cbc4-4545-bef1-2365c531178b"
        ],
        "c916a689-35eb-4d39-89f3-c1a53331aa0a": [
            "32455fba-9746-44df-b452-7530313ca89d"
        ],
        "6dafbdc5-7483-4262-9796-f1b8ae939405": [
            "32455fba-9746-44df-b452-7530313ca89d"
        ],
        "b26003c6-2926-4ab2-ba63-8f71264003cb": [
            "10fa2e00-c649-41cd-97dd-8493be41d1e4"
        ],
        "82898b92-1072-48f0-97b5-f323be53f5a6": [
            "10fa2e00-c649-41cd-97dd-8493be41d1e4"
        ],
        "7ae3978d-afae-472d-84f9-ea67f85249e9": [
            "1bc3762a-16ce-4827-b0b4-79a42e5943dd"
        ],
        "454f4d2e-a11c-49ee-8119-84fcdf384e9a": [
            "1bc3762a-16ce-4827-b0b4-79a42e5943dd"
        ],
        "3fb8b07c-0fc9-41d3-a5a7-40721a0dcd70": [
            "e2d5cb7b-01a9-44db-9956-c8432695d1db"
        ],
        "8f47cd9c-6507-4c34-9c2f-3a9bc29339bc": [
            "e2d5cb7b-01a9-44db-9956-c8432695d1db"
        ],
        "0b366ba0-0167-4904-971c-73e41aafc004": [
            "5767aeb2-7ebf-481d-b095-e635b17836c2"
        ],
        "81e556c0-01c5-4311-abbc-66e1dfd5f325": [
            "5767aeb2-7ebf-481d-b095-e635b17836c2"
        ],
        "c8dcab86-9f47-4619-accc-ed54b7e07b1b": [
            "edfb0b0e-280c-40d8-8b11-7ca15a401ca2"
        ],
        "f6efbac1-3989-457d-a0fa-a17453454a80": [
            "edfb0b0e-280c-40d8-8b11-7ca15a401ca2"
        ],
        "9292b668-17b0-42ea-98aa-5f89c2a7521e": [
            "5aad57c4-cafc-4081-9fd9-aa4dabe5976c"
        ],
        "64ad6641-0877-4e7b-aa9c-25e8dd10752a": [
            "5aad57c4-cafc-4081-9fd9-aa4dabe5976c"
        ],
        "faba35fb-fdc8-4a0c-98d4-e59a6e913a62": [
            "9de02ac9-3ec8-454b-9157-a0f7c3aebd31"
        ],
        "18195a24-015a-46d3-9f20-c7eee8543361": [
            "9de02ac9-3ec8-454b-9157-a0f7c3aebd31"
        ],
        "a638a524-7a5e-4b49-8131-6e7d3e714378": [
            "9c6b75d6-f53c-40ff-8ef7-8e5a3e8b3182"
        ],
        "9538fb36-8ce8-4d3c-9652-400a8a966e40": [
            "9c6b75d6-f53c-40ff-8ef7-8e5a3e8b3182"
        ],
        "f75216d3-a030-46f4-b0f3-9b15d7b34e65": [
            "352582c1-9725-4262-892f-d6381878120e"
        ],
        "c377b66f-2f46-4032-bfcf-2791bb21da62": [
            "352582c1-9725-4262-892f-d6381878120e"
        ],
        "e705282a-f1d0-4620-a60c-dc2af7325dd6": [
            "4cbb2711-ce05-4d7f-8587-745e577412dd"
        ],
        "3cdb5e4b-3393-43b5-87f7-07c1bc9fad09": [
            "4cbb2711-ce05-4d7f-8587-745e577412dd"
        ],
        "7f5ea351-1a4d-4426-87fb-85c5231bb1db": [
            "4e0c7b2c-51e6-4ea7-82ce-d9f299e5ccb0"
        ],
        "9933124e-86eb-445a-9fa4-c8f2f91b6b9e": [
            "4e0c7b2c-51e6-4ea7-82ce-d9f299e5ccb0"
        ],
        "398c1028-83a8-471f-8d8c-fe5a393214df": [
            "a09cbde3-361a-4575-9c42-197b4a9f3e86"
        ],
        "2ff864c8-0501-4035-a80a-4a22976f70b7": [
            "a09cbde3-361a-4575-9c42-197b4a9f3e86"
        ],
        "9280a5c7-234f-4653-918e-2ca5a3c78b0e": [
            "e71aa193-5221-4449-87dc-3d5e367a9125"
        ],
        "aff99e86-87d6-4aba-b23e-8be37b2dccb3": [
            "e71aa193-5221-4449-87dc-3d5e367a9125"
        ],
        "89deaf43-8b45-4129-82fa-e713bd40efa6": [
            "27b7ba36-d58c-42d8-8593-d6408c1587a3"
        ],
        "56ceebde-c856-4436-b048-d81a9a237696": [
            "27b7ba36-d58c-42d8-8593-d6408c1587a3"
        ],
        "2ea9947a-e0fb-4286-9b10-a5ffc32a9342": [
            "18e11db9-e30d-4aa5-a74d-8c306fe4facc"
        ],
        "e1c1bf19-d34d-4f69-8019-c2d0293db398": [
            "18e11db9-e30d-4aa5-a74d-8c306fe4facc"
        ],
        "ee134c66-c90d-4ad0-a60c-749ba5e11179": [
            "d2800bd7-30a4-465d-bf79-b5d4c89e92db"
        ],
        "d5ae3199-4915-4c8e-be8e-16459a30ae96": [
            "d2800bd7-30a4-465d-bf79-b5d4c89e92db"
        ],
        "781fd761-0fa6-4b54-b6b4-3bf0d26f24cf": [
            "8a4ef574-3497-48cc-a417-daf366558a4f"
        ],
        "83b8e911-9d47-47d0-b537-3c3bfe042f26": [
            "8a4ef574-3497-48cc-a417-daf366558a4f"
        ],
        "bf585583-544a-44ba-b7fc-34778527a73c": [
            "876e06bc-37e8-4c9d-9af0-e0d781b79493"
        ],
        "3d2fc287-2d18-4e5a-ac7e-1ea8f987f957": [
            "876e06bc-37e8-4c9d-9af0-e0d781b79493"
        ],
        "a085b76a-0243-40e2-9c94-666346972c00": [
            "d573435f-87d6-4a8f-beb9-dd6e6417c12f"
        ],
        "f6617eff-7c86-408e-9cbc-b325e2172971": [
            "d573435f-87d6-4a8f-beb9-dd6e6417c12f"
        ],
        "0abb20d5-203e-4770-af18-46dee53a19e2": [
            "14919cbd-a62f-4866-8c72-d1c526c16fd9"
        ],
        "eee4c273-2ace-4e33-bf0a-d1bb8e545818": [
            "14919cbd-a62f-4866-8c72-d1c526c16fd9"
        ],
        "d3a84f2c-657c-4725-95c1-79cd488d3cdb": [
            "f2a228ca-ad95-4c3b-b7db-4a3720767b11"
        ],
        "e3e7c89a-501b-456b-9543-ef5f1a1d27f9": [
            "f2a228ca-ad95-4c3b-b7db-4a3720767b11"
        ],
        "f51c2a33-4565-4fdb-9d6c-1cc2cb728db1": [
            "6f3239d6-b7cf-40a7-857c-69443a8730c1"
        ],
        "15b297df-bd29-4f9c-8d8d-fc6e4a1da4b1": [
            "6f3239d6-b7cf-40a7-857c-69443a8730c1"
        ],
        "45d667b7-9b89-4688-bae9-4fa48998e8c9": [
            "8c1055b8-a9ff-4cb1-a947-8f3ef2c7436c"
        ],
        "3d57cdbe-fa78-4ca7-bc28-0d5014ada93b": [
            "8c1055b8-a9ff-4cb1-a947-8f3ef2c7436c"
        ],
        "895fe686-98f0-4dbe-a5d9-34cf8012d299": [
            "2a87cb1d-cbdc-421c-bef3-2d674f6ea6b2"
        ],
        "2a01a260-c6ba-4a58-a0e6-0f9666848f2e": [
            "2a87cb1d-cbdc-421c-bef3-2d674f6ea6b2"
        ],
        "7fdd71f6-a77e-44b8-80ef-a122818fdbf8": [
            "261ce107-fdfb-4668-896a-e8cf259335a1"
        ],
        "8b118cc2-49b3-4c1a-8fb6-c99b793961b1": [
            "261ce107-fdfb-4668-896a-e8cf259335a1"
        ],
        "9963b123-cdd1-408c-a425-4d46aacab3c5": [
            "f0bd24a5-c179-4c71-b716-0f5b36336984"
        ],
        "43947d77-e745-44f8-91d6-8dcf05348655": [
            "f0bd24a5-c179-4c71-b716-0f5b36336984"
        ],
        "50a1f00a-2814-4ba4-b56c-942ef2283456": [
            "d7bae827-1b17-4991-b802-9ae596c3373c"
        ],
        "a58a389b-c837-4c2b-bae8-d71803426316": [
            "d7bae827-1b17-4991-b802-9ae596c3373c"
        ],
        "884e7a12-bd1a-4c95-8da9-269ad5491136": [
            "0eae7b6a-79c3-4b4b-8abb-0a84811b8615"
        ],
        "4aa3acfa-cab4-453e-abda-3ced31662e2b": [
            "0eae7b6a-79c3-4b4b-8abb-0a84811b8615"
        ],
        "115aa5f5-3c1f-4869-8ed7-eb2088bb0bb8": [
            "4a7e5845-a9a5-4788-925a-184b1ebe1534"
        ],
        "9453770b-1869-4daa-9d61-3d562cf66ba8": [
            "4a7e5845-a9a5-4788-925a-184b1ebe1534"
        ],
        "10337334-3290-40f3-9301-1303e3a34eb7": [
            "bbff1acf-83d8-4f61-b94a-0d0616aef010"
        ],
        "5cf6b118-1d08-48ca-b4f2-dd2931028620": [
            "bbff1acf-83d8-4f61-b94a-0d0616aef010"
        ],
        "6ab869c1-85db-4bd5-bb4b-12383805068a": [
            "5f0e8cb3-7191-4de4-b5c5-a2e7daff148b"
        ],
        "0353a183-e3e4-4061-a5fe-87b3b9aecbc5": [
            "5f0e8cb3-7191-4de4-b5c5-a2e7daff148b"
        ],
        "5880ea7a-a2df-4a7d-8241-f75531f16e2e": [
            "cfdf7784-15c0-487e-aaee-8331336dd363"
        ],
        "a23d2408-c078-4835-a5c8-5f7425d6e67c": [
            "cfdf7784-15c0-487e-aaee-8331336dd363"
        ],
        "8ee69c11-f6df-4769-9a95-631be2b3cb03": [
            "2a89fe76-b57c-455d-9f84-7ab3a9fef60b"
        ],
        "5aadb765-cbfa-4a17-9fcb-6477ab4b33ae": [
            "2a89fe76-b57c-455d-9f84-7ab3a9fef60b"
        ],
        "1d3b0b9f-5582-4aec-a745-8cbc5178c6c1": [
            "d6f601f1-6f22-492b-8dd3-e77466fbdafe"
        ],
        "925077c2-2ae9-4bfb-931c-03c96b0ba9fc": [
            "d6f601f1-6f22-492b-8dd3-e77466fbdafe"
        ],
        "e8c9a59d-7a28-4087-a20e-72a238067147": [
            "a4defe5c-d97c-4b62-b505-dfaa6051e83e"
        ],
        "17e9a111-b2c6-43e6-9ce0-1870d69de97f": [
            "a4defe5c-d97c-4b62-b505-dfaa6051e83e"
        ],
        "fcd98428-a325-4e90-88a8-0ae024623c45": [
            "b8413b95-206f-43f5-9e2b-e577dec49fe8"
        ],
        "5f197416-2e12-48f1-a2dc-9c497a0a128a": [
            "b8413b95-206f-43f5-9e2b-e577dec49fe8"
        ],
        "f69abe2a-7386-432f-9ba7-3878fa630835": [
            "9bb1c6f5-c98c-43f6-b3a3-7208e0473857"
        ],
        "be057e2b-605e-46bc-a49f-e724cd49a20f": [
            "9bb1c6f5-c98c-43f6-b3a3-7208e0473857"
        ],
        "2aa59382-86c0-4d3f-a38d-3eca8c38019d": [
            "e82bbf3b-b6b7-4e3e-92cc-dc27e8138203"
        ],
        "b4062ed0-fc8a-4bb1-9d6a-ce2adf7da945": [
            "e82bbf3b-b6b7-4e3e-92cc-dc27e8138203"
        ],
        "603b73ba-7e85-4263-be77-fa16edb84e47": [
            "fedf7687-9cf1-41e2-b902-32593171a4f9"
        ],
        "0048faae-c492-4933-9fd1-7d0436b96c3a": [
            "fedf7687-9cf1-41e2-b902-32593171a4f9"
        ],
        "7551122a-6247-4920-aace-6385c3f7a2ab": [
            "0cb927a3-13c6-4b46-8965-9c267161d3ff"
        ],
        "5350d03e-c5f6-4f0e-9bfb-bbcf7fc8ebad": [
            "0cb927a3-13c6-4b46-8965-9c267161d3ff"
        ],
        "7d6c89de-583b-4c56-a38b-0c1264e49b10": [
            "73b29da6-c64d-46f9-8cd7-93b1cf101e4b"
        ],
        "7d7621ca-3032-4e25-8e12-e7942dca09bd": [
            "73b29da6-c64d-46f9-8cd7-93b1cf101e4b"
        ],
        "eaae6d76-767c-4530-bb50-ccf5b8c95fff": [
            "7017255e-b6a4-4247-87f5-cd0b58743011"
        ],
        "a63409af-3dbc-4332-9fca-2644ad42606d": [
            "7017255e-b6a4-4247-87f5-cd0b58743011"
        ],
        "92330c4b-8d4c-4480-9369-19cc327919f9": [
            "fcf240f7-d77a-44cd-a68e-aaadbcec220e"
        ],
        "a36f1221-111b-463c-a6d3-a2e4cd2e43dd": [
            "fcf240f7-d77a-44cd-a68e-aaadbcec220e"
        ],
        "65810abc-a08d-45d7-8c3a-1615ed04f437": [
            "999212b2-a008-4f13-b970-0c93f4b430d0"
        ],
        "b6bcde95-f097-47bc-ae58-0d69fd4118a8": [
            "999212b2-a008-4f13-b970-0c93f4b430d0"
        ],
        "c227bafd-3359-41c7-aa23-24e4f92218cc": [
            "2aadfefd-e3dd-4b70-9a92-5a0869fdd31d"
        ],
        "31926098-bd9f-4dd0-ade2-09314aba272c": [
            "2aadfefd-e3dd-4b70-9a92-5a0869fdd31d"
        ],
        "53b257eb-9d46-4dfb-89c3-b42deb25aed9": [
            "5b057c30-3e7c-41c7-a401-569dbdd511a8"
        ],
        "e9684185-bef0-4b11-98f0-b955f5c87d5a": [
            "5b057c30-3e7c-41c7-a401-569dbdd511a8"
        ],
        "88dc7259-0b2e-4ff0-b8b1-787dc03c1fef": [
            "f67832b2-40f5-4ae3-94af-e48114207cd1"
        ],
        "cc4bb986-b2d5-41b7-b7ff-803c67b0331b": [
            "f67832b2-40f5-4ae3-94af-e48114207cd1"
        ],
        "88d0a54f-a0cd-407c-a0f2-905acb75b74c": [
            "caf21b60-bc5b-4549-b7a0-c9092f39a85e"
        ],
        "f317be14-6d0c-4555-b617-775b599acff2": [
            "caf21b60-bc5b-4549-b7a0-c9092f39a85e"
        ],
        "645d9ae2-7d27-4a1c-bd7e-6709f326ad07": [
            "508845c2-b69e-46cf-87a6-44b8bac87984"
        ],
        "e57aeb9a-77fe-40ea-a954-b273ca7cf5c1": [
            "508845c2-b69e-46cf-87a6-44b8bac87984"
        ],
        "b4f51d2b-3261-4a79-8b7c-f64f4cd970bf": [
            "05be0e2d-7dc4-4b05-80d8-4e699ec227ef"
        ],
        "066daf4f-6d08-4041-9baf-4b4db3d1d31d": [
            "05be0e2d-7dc4-4b05-80d8-4e699ec227ef"
        ],
        "58172f4f-1f84-459b-9915-8221fd1f2842": [
            "ef00bf8f-36b2-45f4-8590-b56b87a77319"
        ],
        "6f78d5ab-844d-4e96-8a87-a2cd83a38fe7": [
            "ef00bf8f-36b2-45f4-8590-b56b87a77319"
        ],
        "84f006d5-ec63-4f06-8e2b-edf491abecf3": [
            "a5f34bc4-7ab0-4740-8200-d2205f529847"
        ],
        "6029fd16-21c5-4e4c-b385-6c8c00b86073": [
            "a5f34bc4-7ab0-4740-8200-d2205f529847"
        ],
        "93693842-914d-44fd-a271-5d59aad64f34": [
            "c7fced14-af85-4e12-abd1-47f6f844f63b"
        ],
        "e77118c0-8ad4-438d-8d02-90ab855ac936": [
            "c7fced14-af85-4e12-abd1-47f6f844f63b"
        ],
        "91e09e82-3973-48cb-b1f4-72de2ccc5944": [
            "c8d4b138-3a50-49f6-9736-9611e23a8da1"
        ],
        "5f4d8f1f-d6d2-4d7b-9fe6-13c284d6c960": [
            "c8d4b138-3a50-49f6-9736-9611e23a8da1"
        ],
        "6500afc4-7544-4a93-8c8b-6af4a5aa97f4": [
            "9c0d9544-9f60-480b-b264-55c57c9c3473"
        ],
        "f7acdc80-8a5c-4aa2-a198-3c612ccabbfd": [
            "9c0d9544-9f60-480b-b264-55c57c9c3473"
        ],
        "3c098a84-8769-4872-af0e-ef8de129f0b5": [
            "e7fb81b6-c0a1-4d0d-a303-c325e5be70da"
        ],
        "dc8aec40-42b3-493c-91d8-f832bbf75526": [
            "e7fb81b6-c0a1-4d0d-a303-c325e5be70da"
        ],
        "84d549ef-37d9-4325-bac5-2a1f2712f5a6": [
            "15d4a55d-2541-4e95-82a8-cc0036cf237d"
        ],
        "b3da6f58-1e03-41b7-8e90-36260b71a233": [
            "15d4a55d-2541-4e95-82a8-cc0036cf237d"
        ],
        "f9b0efb1-2d84-4b31-aa6b-7f8ce1f02524": [
            "8e64a370-ae9d-4f31-ab0f-1f6fc7466588"
        ],
        "00cc7df8-73a1-480c-929a-796ac7bc43c8": [
            "8e64a370-ae9d-4f31-ab0f-1f6fc7466588"
        ],
        "6b930611-c825-4af7-a9ad-6e5700ee24ad": [
            "8750680a-ab69-4eda-a27a-7d7c98ca6f40"
        ],
        "8f2d7c28-ee11-4963-a918-73b1a243f2d0": [
            "8750680a-ab69-4eda-a27a-7d7c98ca6f40"
        ],
        "23762d6e-3f44-4e86-81f4-ca215fa2b6c6": [
            "05b9f71d-a0a6-426a-8ff4-5339b81d0f2f"
        ],
        "84563ebe-30b9-40a6-ada9-1f16f00e1d3b": [
            "05b9f71d-a0a6-426a-8ff4-5339b81d0f2f"
        ],
        "e223b986-92a0-4f6f-bff5-08d55c297bf0": [
            "fce988bc-72df-4ae0-b61b-87aaff4dac40"
        ],
        "d7505d98-124f-426d-9c9e-8233db7b22e2": [
            "fce988bc-72df-4ae0-b61b-87aaff4dac40"
        ],
        "806b24f7-54d6-4929-b87d-5cc16935d3de": [
            "c2a4076b-aa12-4756-a6b9-0fe47b7f20af"
        ],
        "38491b3d-3faa-44f6-914e-4b31085335c6": [
            "c2a4076b-aa12-4756-a6b9-0fe47b7f20af"
        ],
        "36d1dbbb-a1b4-468f-b847-a5b16a842486": [
            "9980f9eb-ac42-4290-8959-c8d070a0de37"
        ],
        "2b37e814-032a-4c19-9caf-18c88b795e9f": [
            "9980f9eb-ac42-4290-8959-c8d070a0de37"
        ],
        "76258636-b936-4eaf-95fe-0084fa5182b5": [
            "7960fa5a-bd1e-45ee-b58e-710cf4f967f3"
        ],
        "8b3c8a00-ef4c-41be-b1b4-e2241a8299a9": [
            "7960fa5a-bd1e-45ee-b58e-710cf4f967f3"
        ]
    },
    "mode": "text"
}