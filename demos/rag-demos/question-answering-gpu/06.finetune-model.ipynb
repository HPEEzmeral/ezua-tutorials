{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune the Embedding Model on MLDE\n",
    "\n",
    "### Dataset preparation\n",
    "To make our Retrieval Augmented Generation (RAG) Application more effective, we can fine tune our embedding model on our dataset to make it better at retrieving the right chunks when we ask a question. The dataset we need to train it on would be pairs of questions and the chunk it should help retrieve. We have our data in json format, so the first think we need to do is generate questions from it. We're going to use LLamaIndex and OpenAI to generate the questions. We've also included the datasets pre-generated if you want to skip this part.\n",
    "\n",
    "Lets start with getting the content from each of the objects in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-finetuning\n",
      "  Downloading llama_index_finetuning-0.1.4-py3-none-any.whl (26 kB)\n",
      "Collecting llama-index-embeddings-adapter<0.2.0,>=0.1.2\n",
      "  Downloading llama_index_embeddings_adapter-0.1.3-py3-none-any.whl (4.5 kB)\n",
      "Collecting llama-index-postprocessor-cohere-rerank<0.2.0,>=0.1.1\n",
      "  Downloading llama_index_postprocessor_cohere_rerank-0.1.2-py3-none-any.whl (2.7 kB)\n",
      "Collecting sentence-transformers<3.0.0,>=2.3.0\n",
      "  Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.1\n",
      "  Downloading llama_index_llms_openai-0.1.7-py3-none-any.whl (9.3 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.11.post1\n",
      "  Downloading llama_index_core-0.10.14.post1-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting llama-index-llms-gradient<0.2.0,>=0.1.1\n",
      "  Downloading llama_index_llms_gradient-0.1.2-py3-none-any.whl (2.9 kB)\n",
      "Collecting typing-extensions>=4.5.0\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Collecting tenacity<9.0.0,>=8.2.0\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Collecting tiktoken>=0.3.3\n",
      "  Using cached tiktoken-0.6.0-cp310-cp310-macosx_11_0_arm64.whl (949 kB)\n",
      "Collecting dataclasses-json\n",
      "  Using cached dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting requests>=2.31.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.13\n",
      "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.6\n",
      "  Using cached aiohttp-3.9.3-cp310-cp310-macosx_11_0_arm64.whl (387 kB)\n",
      "Collecting PyYAML>=6.0.1\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl (169 kB)\n",
      "Collecting httpx\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Collecting deprecated>=1.2.9.3\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Collecting openai>=1.1.0\n",
      "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 KB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/tylerbritten/Developer/HPE/ezua-tutorials/.direnv/python-3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-finetuning) (1.6.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.66.1\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Collecting typing-inspect>=0.8.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting networkx>=3.0\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting SQLAlchemy[asyncio]>=1.4.49\n",
      "  Downloading SQLAlchemy-2.0.27-cp310-cp310-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=9.0.0\n",
      "  Using cached pillow-10.2.0-cp310-cp310-macosx_11_0_arm64.whl (3.3 MB)\n",
      "Collecting torch<3.0.0,>=2.1.2\n",
      "  Downloading torch-2.2.1-cp310-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gradientai<2.0.0,>=1.6.0\n",
      "  Using cached gradientai-1.7.0-py3-none-any.whl (270 kB)\n",
      "Collecting cohere<5.0,>=4.45\n",
      "  Downloading cohere-4.51-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-macosx_12_0_arm64.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.15.1\n",
      "  Downloading huggingface_hub-0.21.3-py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.2/346.2 KB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Using cached scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl (31.4 MB)\n",
      "Collecting transformers<5.0.0,>=4.32.0\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl (79 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting importlib_metadata<7.0,>=6.0\n",
      "  Using cached importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting backoff<3.0,>=2.0\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting fastavro<2.0,>=1.8\n",
      "  Using cached fastavro-1.9.4.tar.gz (985 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting urllib3<3,>=1.26\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Collecting aenum>=3.1.11\n",
      "  Using cached aenum-3.1.15-py3-none-any.whl (137 kB)\n",
      "Collecting pydantic<2.0.0,>=1.10.5\n",
      "  Using cached pydantic-1.10.14-cp310-cp310-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tylerbritten/Developer/HPE/ezua-tutorials/.direnv/python-3.10/lib/python3.10/site-packages (from gradientai<2.0.0,>=1.6.0->llama-index-llms-gradient<0.2.0,>=0.1.1->llama-index-finetuning) (2.9.0)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/tylerbritten/Developer/HPE/ezua-tutorials/.direnv/python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers<3.0.0,>=2.3.0->llama-index-finetuning) (23.2)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "Collecting certifi\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Collecting idna\n",
      "  Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Collecting anyio\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2023.12.25-cp310-cp310-macosx_11_0_arm64.whl (291 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl (120 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-3.0.3-cp310-cp310-macosx_11_0_universal2.whl (270 kB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.2-cp310-cp310-macosx_11_0_arm64.whl (393 kB)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Using cached tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.21.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/tylerbritten/Developer/HPE/ezua-tutorials/.direnv/python-3.10/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-finetuning) (1.2.0)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tylerbritten/Developer/HPE/ezua-tutorials/.direnv/python-3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->gradientai<2.0.0,>=1.6.0->llama-index-llms-gradient<0.2.0,>=0.1.1->llama-index-finetuning) (1.16.0)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl (18 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Building wheels for collected packages: fastavro\n",
      "  Building wheel for fastavro (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastavro: filename=fastavro-1.9.4-cp310-cp310-macosx_13_0_arm64.whl size=527960 sha256=f8eaa898e01cb30b810b3bc7385e99f1d7033ead82806f1128eaff5de45b1ff7\n",
      "  Stored in directory: /Users/tylerbritten/Library/Caches/pip/wheels/f5/cc/c3/8d2ab49631057f55a6512033e2c58145f81a8c49508f45e839\n",
      "Successfully built fastavro\n",
      "Installing collected packages: pytz, mpmath, dirtyjson, aenum, zipp, wrapt, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, tenacity, sympy, sniffio, safetensors, regex, PyYAML, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, MarkupSafe, joblib, idna, h11, greenlet, fsspec, frozenlist, filelock, fastavro, distro, click, charset-normalizer, certifi, backoff, attrs, async-timeout, yarl, typing-inspect, SQLAlchemy, scipy, requests, pydantic, pandas, nltk, jinja2, importlib_metadata, httpcore, deprecated, anyio, aiosignal, torch, tiktoken, scikit-learn, huggingface-hub, httpx, gradientai, dataclasses-json, aiohttp, tokenizers, openai, llamaindex-py-client, cohere, transformers, llama-index-core, sentence-transformers, llama-index-postprocessor-cohere-rerank, llama-index-llms-openai, llama-index-llms-gradient, llama-index-embeddings-adapter, llama-index-finetuning\n",
      "Successfully installed MarkupSafe-2.1.5 PyYAML-6.0.1 SQLAlchemy-2.0.27 aenum-3.1.15 aiohttp-3.9.3 aiosignal-1.3.1 anyio-4.3.0 async-timeout-4.0.3 attrs-23.2.0 backoff-2.2.1 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 cohere-4.51 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 fastavro-1.9.4 filelock-3.13.1 frozenlist-1.4.1 fsspec-2024.2.0 gradientai-1.7.0 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 huggingface-hub-0.21.3 idna-3.6 importlib_metadata-6.11.0 jinja2-3.1.3 joblib-1.3.2 llama-index-core-0.10.14.post1 llama-index-embeddings-adapter-0.1.3 llama-index-finetuning-0.1.4 llama-index-llms-gradient-0.1.2 llama-index-llms-openai-0.1.7 llama-index-postprocessor-cohere-rerank-0.1.2 llamaindex-py-client-0.1.13 marshmallow-3.21.0 mpmath-1.3.0 multidict-6.0.5 mypy-extensions-1.0.0 networkx-3.2.1 nltk-3.8.1 numpy-1.26.4 openai-1.13.3 pandas-2.2.1 pillow-10.2.0 pydantic-1.10.14 pytz-2024.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 scikit-learn-1.4.1.post1 scipy-1.12.0 sentence-transformers-2.5.1 sniffio-1.3.1 sympy-1.12 tenacity-8.2.3 threadpoolctl-3.3.0 tiktoken-0.6.0 tokenizers-0.15.2 torch-2.2.1 tqdm-4.66.2 transformers-4.38.2 typing-extensions-4.10.0 typing-inspect-0.9.0 tzdata-2024.1 urllib3-2.2.1 wrapt-1.16.0 yarl-1.9.4 zipp-3.17.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/tylerbritten/Developer/HPE/ezua-tutorials/.direnv/python-3.10/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1825\n",
      "['Release Date: September 25, 2023 Breaking Changes Kubernetes: Remove the agent_reattach_enabled config option. Agent reattach is now always enabled. Agent: Take the default value for the --visible-gpus option from the CUDA_VISIBLE_DEVICES or ROCR_VISIBLE_DEVICES environment variables, if defined. New Features SDK: Add the ability to keep track of what experiments use a particular checkpoint or model version for inference. SDK: Add Checkpoint.get_metrics and ModelVersion.get_metrics methods. Kubernetes: Support enabling and disabling agents to prevent Determined from scheduling jobs on specific nodes. Upgrading from a version before this feature to a version after this feature only on Kubernetes will cause queued allocations to be killed on upgrade. Users can pause queued experiments to avoid this. Improvements Enable reporting and display of metrics with floating-point epoch values. API: Allow the reporting of duplicate metrics across multiple report_metrics calls with the same steps_completed, provided they have identical values. SDK: stream_trials_training_metrics() and stream_trials_validation_metrics() are now deprecated. Please use stream_trials_metrics() instead. The corresponding methods of Determined and TrialReference have also been updated similarly. Bug Fixes Checkpoints: Fix an issue where in certain situations duplicate checkpoints with the same UUID would be returned by the WebUI and the CLI. Models: Fix a bug where det model describe and other methods in the CLI and SDK that act on a single model would error if two models had similar names. Workspaces: Fix an issue where notebooks, TensorBoards, shells, and commands would not inherit agent user group and agent user information from their workspace.', 'Release Date: September 11, 2023 Breaking Changes Fluent Bit is no longer used for log shipping and configs associated with Fluent Bit are now no longer in use. Fluent Bit has been replaced with an internal log shipper (the same one that is used for Slurm). Bug Fixes Reduce the time before seeing the first metrics of a new experiment.', 'Release Date: August 29, 2023 Breaking Changes Remove EstimatorTrial, which has been deprecated since Determined version 0.22.0 (May 2023). Bug Fixes Trials: Fix an issue where trial logs could fail for trials created prior to Determined version 0.17.0. CLI: Fix an issue where template association with workspaces, when listed, was missing. This would prevent templates from being listed for some users and templates on RBAC-enabled clusters.', 'Release Date: August 18, 2023 Breaking Changes API: Remove LightningAdapter, which was deprecated in 0.23.1 (June 2023). We recommend that PyTorch Lightning users migrate to the Core API. New Features Environments: Add experimental PyTorch 2.0 images containing PyTorch 2.0.1, Python 3.10.12, and (for the GPU image) CUDA 11.8. Bug Fixes Users: Fix an issue that caused the CLI command det user list to always show “false” in the “remote” column.', 'Release Date: July 31, 2023 Breaking Changes API: The /api/v1/users/setting endpoint no longer accepts storagePath and now accepts a settings array instead of a single setting. New Features Allow non-intersecting dictionaries of metrics to be merged on the same total_batches. This update was rejected before. API: Add a new patch API endpoint /api/v1/master/config that allows the user to make changes to the master config while the cluster is running. Currently, only changing the log config is supported. CLI: Add a new CLI command det master config --log --level <log_level> --color <on/off> that allows the user to change the log level and color settings of the master config while the cluster is still running. det master config can still be used to get the master config. Cluster: Allow binding resource pools to specific workspaces. Bound resource pools can only be used by the workspaces they are bound to. Each workspace can also now have a default compute resource pool and a default auxiliary resource pool configured. Kubernetes: Users may now populate all securityContext fields within the pod spec of the determined-container container except for RunAsUser and RunAsGroup. For those fields, use det user link-with-agent-user instead. WebUI: The experiment list page now has the following new capabilities: Select metrics and hyperparameters as columns. Filter the list on any available column. Specify complex filters. Sort the list on any available column. Display total number of experiments matching the filter. Compare metrics, hyperparameters, and trial details across experiments. Toggle between pagination and infinite scroll. Select preferred table density. Improvements WebUI: Improve performance and stability.']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def extract_single_value_from_json_files(directory, key):\n",
    "    \"\"\"\n",
    "    Reads every JSON file in a directory and extracts a single value from each object based on the specified key.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): The directory path containing JSON files.\n",
    "    - key (str): The key to extract from each object.\n",
    "\n",
    "    Returns:\n",
    "    - values_list (list): A list containing the extracted values from all JSON files.\n",
    "    \"\"\"\n",
    "    values_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                try:\n",
    "                    data = json.load(file)\n",
    "                    for obj in data:\n",
    "                        if key in obj and obj[key] is not None:\n",
    "                            values_list.append(obj[key])\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON file: {file_path}\")\n",
    "    return values_list\n",
    "\n",
    "# Example usage:\n",
    "directory_path = './documents'\n",
    "key_to_extract = 'content'\n",
    "result = extract_single_value_from_json_files(directory_path, key_to_extract)\n",
    "print(len(result))\n",
    "print(result[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we've read in the content into a list. Now we'll want to parse them with a sentence splitter to build nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerbritten/Developer/HPE/ezua-tutorials/.direnv/python-3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 1825/1825 [00:01<00:00, 917.15it/s] \n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "\n",
    "text_list = result\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents,show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Release Date: September 25, 2023 Breaking Changes Kubernetes: Remove the agent_reattach_enabled config option. Agent reattach is now always enabled. Agent: Take the default value for the --visible-gpus option from the CUDA_VISIBLE_DEVICES or ROCR_VISIBLE_DEVICES environment variables, if defined. New Features SDK: Add the ability to keep track of what experiments use a particular checkpoint or model version for inference. SDK: Add Checkpoint.get_metrics and ModelVersion.get_metrics methods. Kubernetes: Support enabling and disabling agents to prevent Determined from scheduling jobs on specific nodes. Upgrading from a version before this feature to a version after this feature only on Kubernetes will cause queued allocations to be killed on upgrade. Users can pause queued experiments to avoid this. Improvements Enable reporting and display of metrics with floating-point epoch values. API: Allow the reporting of duplicate metrics across multiple report_metrics calls with the same steps_completed, provided they have identical values. SDK: stream_trials_training_metrics() and stream_trials_validation_metrics() are now deprecated. Please use stream_trials_metrics() instead. The corresponding methods of Determined and TrialReference have also been updated similarly. Bug Fixes Checkpoints: Fix an issue where in certain situations duplicate checkpoints with the same UUID would be returned by the WebUI and the CLI. Models: Fix a bug where det model describe and other methods in the CLI and SDK that act on a single model would error if two models had similar names. Workspaces: Fix an issue where notebooks, TensorBoards, shells, and commands would not inherit agent user group and agent user information from their workspace.\n",
      "--------\n",
      "{}\n",
      "Release Date: September 11, 2023 Breaking Changes Fluent Bit is no longer used for log shipping and configs associated with Fluent Bit are now no longer in use. Fluent Bit has been replaced with an internal log shipper (the same one that is used for Slurm). Bug Fixes Reduce the time before seeing the first metrics of a new experiment.\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0].metadata)\n",
    "print(nodes[0].text)\n",
    "print(\"--------\")\n",
    "print(nodes[1].metadata)\n",
    "print(nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool,  now we have our text chunked up in a list of nodes. Next thing we're going to do is take a sample of the data. How about 250 each for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 250\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "subset = random.sample(nodes, 500)\n",
    "test, train = subset[:250], subset[250:]\n",
    "\n",
    "print(len(test), len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now we have 250 chunks randomly sampled from our data for training and 250 for validation. Lets use OpenAI gpt-3.5-turbo model to generate questions for these chunks. After that we'll store them in json to use for training. You can skip this part and use the existing json files in the `experiment/` folder instead. If you do decide to run it, replace the existing files in the experiment folder with the ones you generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "train_dataset = generate_qa_embedding_pairs(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\"), nodes=train\n",
    ")\n",
    "test_dataset = generate_qa_embedding_pairs(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\"), nodes=test\n",
    ")\n",
    "\n",
    "train_dataset.save_json(\"demo_dataset.json\")\n",
    "test_dataset.save_json(\"test_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on MLDE\n",
    "\n",
    "Now that we have our data, lets fine tune a model on MLDE. We're going to use `BAAI/bge-m3` but any of the `BAAI` `bge` models should work well enough.\n",
    "We're going to send our experiment to MLDE. Make sure you have the determined client installed (`pip install determined`) and that you're logged in (`det -m <your master url> auth login`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!det -m https://mlde.i006ua.tryezmeral.com:443 e create experiment/const.yaml ./experiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
