[
    {
      "title": "Get Started",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "Start here.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/get-started/",
      "relURI": "/latest/get-started/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "2e5f30201d9cce32b16beaba71bfe5a6"
    },
    {
      "title": "Beginner Tutorial",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Get Started",
      "description": "Learn how to quickly ingest photos, trace their outlines, and output a collage using the transformed data.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/get-started/beginner-tutorial/",
      "relURI": "/latest/get-started/beginner-tutorial/",
      "body": " Before You Start # Install HPE ML Data Management either locally our within the cloud. Install HPE ML Data Management Shell. Join our Slack Community so you can ask any questions you may have! Try out this Glob Tool to learn how to use glob patterns to select files and directories. Context # How HPE ML Data Management Works # HPE ML Data Management is deployed within a Kubernetes cluster to manage and version your data using projects, input repositories, pipelines, datums and output repositories. A project can house many repositories and pipelines, and when a pipeline runs a data transformation job it chunks your inputs into datums for processing.\nThe number of datums is determined by the glob pattern defined in your pipeline specification; if the shape of your glob pattern encompasses all inputs, it will process one datum; if the shape of your glob pattern encompasses each input individually, it will process one datum per file in the input, and so on.\nThe end result of your data transformation should always be saved to /pfs/out. The contents of /pfs/out are automatically made accessible from the pipeline&rsquo;s output repository by the same name. So all files saved to /pfs/out for a pipeline named foo are accessible from the foo output repository.\nPipelines combine to create DAGs, and a DAG can be comprised of just one pipeline. Don&rsquo;t worry if this sounds confusing! We&rsquo;ll walk you through the process step-by-step.\nHow to Interact with HPE ML Data Management # You can interact your HPE ML Data Management cluster using the PachCTL CLI or through Console, a GUI.\nPachCTL is great for users already experienced with using a CLI. Console is great for beginners and helps with visualizing relationships between projects, repos, and pipelines. Tutorial: Image &amp; Video Processing with OpenCV # In this tutorial, we&rsquo;ll walk you through how to use HPE ML Data Management to process images and videos using OpenCV. OpenCV is a popular open-source computer vision library that can be used to perform image processing and video analysis.\nThis DAG has 6 steps with the goal of intaking raw photos and video content, drawing edge-detected traces, and outputting a comparison collage of the original and processed images:\nConvert videos to MP4 format Extract frames from videos Trace the outline of each frame and standalone image Create .gifs from the traced video frames Re-shuffle the content so it is organized by &ldquo;original&rdquo; and &ldquo;traced&rdquo; images Build a comparison collage using a static HTML page 1. Create a Project # By default, when you first start up an instance, the default project is attached to your active context. Create a new project and set the project to your active PachCTL context to avoid having to specify the project name (e.g., --project video-to-frame-traces) in each command.\nTool: CLI Console pachctl create project video-to-frame-traces pachctl config update context --project video-to-frame-traces pachctl list projects üí° If you are using HPE ML Data Management locally, you can view your project in Console and follow along at http://localhost/lineage/video-to-frame-traces\nOpen Console. Select Create Project. Input a project name and description. Select Create. Click on the ellipses next to the project name and select Set Active Project. Copy the command and input it into your IDE terminal. 2. Create an Input Repo # Tool: CLI Console At the top of our DAG, we&rsquo;ll need an input repo that will store our raw videos and images.\npachctl create repo raw_videos_and_images pachctl list repos Open Console. Scroll to the video-to-frame-traces project. Select View Project. Select Create Repo. Input a repo name and description. Select Create. 3. Upload Content # To upload content, you need to specify the repo and branch you&rsquo;d like to upload to (e.g., a master or staging branch). In Console, it automatically defaults to repo@master &mdash; but for PachCTL, you&rsquo;ll need to use the repo@master:filename.ext pattern. By default, your pipeline will trigger any time new data is uploaded to the master branch unless otherwise specified in the pipeline spec at input.pfs.branch or through a branch trigger. For this tutorial, we&rsquo;re going to stick with the default master branch.\nTool: CLI Console At the top of our DAG, we&rsquo;ll need an input repo that will store our raw videos and images.\npachctl put file raw_videos_and_images@master:liberty.png -f https://raw.githubusercontent.com/pachyderm/docs-content/main/images/opencv/liberty.jpg pachctl put file raw_videos_and_images@master:cat-sleeping.MOV -f https://storage.googleapis.com/docs-tutorial-resoruces/cat-sleeping.MOV pachctl put file raw_videos_and_images@master:robot.png -f https://raw.githubusercontent.com/pachyderm/docs-content/main/images/opencv/robot.jpg pachctl put file raw_videos_and_images@master:highway.MOV -f https://storage.googleapis.com/docs-tutorial-resoruces/highway.MOV You can view the contents of your repo by running the following command:\npachctl list files raw_videos_and_images@master Open Console. Scroll to the video-to-frame-traces project. Select View Project. Select the raw_videos_and_images repo. Select the Upload button. Upload your image, video, or directory. Select Create. 4. Create the Video Converter Pipeline # We want to make sure that our DAG can handle videos in multiple formats, so first we&rsquo;ll create a pipeline that will:\nSkip images Skip videos already in the correct format (.mp4) Convert videos to .mp4 format The converted videos will be made available to the next pipeline in the DAG via the video_mp4_converter repo by declaring in the user code to save all converted images to /pfs/out/. This is the standard location for storing output data so that it can be accessed by the next pipeline in the DAG.\nOpen your IDE terminal. Create a new folder for your project called video-to-frame-traces. Copy and paste the following pipeline spec into the terminal to create the file. cat &lt;&lt;EOF &gt; video_mp4_converter.yaml pipeline: name: video_mp4_converter input: pfs: repo: raw_videos_and_images glob: &#34;/*&#34; transform: image: lbliii/video_mp4_converter:1.0.14 cmd: - python3 - /video_mp4_converter.py - --input - /pfs/raw_videos_and_images/ - --output - /pfs/out/ autoscaling: true EOF Create the pipeline by running the following command: pachctl create pipeline -f video_mp4_converter.yaml View: Diagram User Code Output # video_mp4_converter.py import cv2 import pathlib import os import argparse import shutil def video_to_mp4( input, output, fps: int = 0, frame_size: tuple = (), fourcc: str = &#34;XVID&#34; ): print(f&#34;Converting video: {input}&#34;) vidcap = cv2.VideoCapture(input) if not fps: fps = round(vidcap.get(cv2.CAP_PROP_FPS)) success, arr = vidcap.read() if not frame_size: height, width, _ = arr.shape frame_size = width, height writer = cv2.VideoWriter( output, apiPreference=0, fourcc=cv2.VideoWriter_fourcc(*fourcc), fps=fps, frameSize=frame_size, ) frame_count = 0 while success: frame_count += 1 writer.write(arr) success, arr = vidcap.read() writer.release() vidcap.release() print(f&#34;Converted {frame_count} frames&#34;) def process_video_files(input_path, output_path): for root, _, files in os.walk(input_path): for file in files: file_path = os.path.join(root, file) # Skip non-video files if not file_path.lower().endswith((&#39;.mp4&#39;, &#39;.avi&#39;, &#39;.mov&#39;)): print(f&#34;Skipping: {file_path}&#34;) return # Convert video files to MP4 if file_path.lower().endswith((&#39;.avi&#39;, &#39;.mov&#39;)): base_file = os.path.splitext(file)[0] out_path = os.path.join(output_path, base_file + &#34;.mp4&#34;) video_to_mp4(file_path, output=out_path) print(f&#34;Converted: {file} to {out_path}&#34;) else: # Copy existing MP4 files out_path = os.path.join(output_path, file) print(f&#34;Copying: {file} to {out_path}&#34;) shutil.copy(file_path, out_path) def main(): parser = argparse.ArgumentParser( prog=&#39;convert_to_mp4.py&#39;, description=&#39;Convert non-MP4 videos to MP4&#39; ) parser.add_argument(&#39;-i&#39;, &#39;--input&#39;, required=True, help=&#39;Input video directory&#39;) parser.add_argument(&#39;-o&#39;, &#39;--output&#39;, required=True, help=&#39;Output video directory&#39;) args = parser.parse_args() if not os.path.exists(args.input): print(&#34;Input directory does not exist.&#34;) return if not os.path.exists(args.output): os.makedirs(args.output) print(f&#34;Input: {args.input}&#34;) print(f&#34;Output: {args.output}&#34;) print(&#34;======================&#34;) process_video_files(args.input, args.output) if __name__ == &#34;__main__&#34;: main() You can view the output commit by running one of following commands:\npachctl list commits video_mp4_converter pachctl list commits video_mp4_converter@master PROJECT REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION video-to-frame-traces video_mp4_converter master 30dadf5cfdf447d2b8ed45644653a509 51 minutes ago 14.05MiB AUTO You can view the output files by running one of the following commands:\npachctl list files video_mp4_converter@master pachctl list files video_mp4_converter@&lt;commit-id&gt; NAME TYPE SIZE /cat-sleeping.mp4 file 7.535MiB /highway.mp4 file 6.517MiB üìñ Every pipeline, at minimum, needs a name, an input, and a transform. The input is the data that the pipeline will process, and the transform is the user code that will process the data. transform.image is the Docker image available in a container registry (Docker Hub) that will be used to run the user code. transform.cmd is the command that will be run inside the Docker container; it is the entrypoint for the user code to be executed against the input data.\n5. Create the Image Flattener Pipeline # Next, we&rsquo;ll create a pipeline that will flatten the videos into individual .png image frames. Like the previous pipeline, the user code outputs the frames to /pfs/out so that the next pipeline in the DAG can access them in the image_flattener repo.\ncat &lt;&lt;EOF &gt; image_flattener.yaml pipeline: name: image_flattener input: pfs: repo: video_mp4_converter glob: &#34;/*&#34; transform: image: lbliii/image_flattener:1.0.0 cmd: - python3 - /image_flattener.py - --input - /pfs/video_mp4_converter - --output - /pfs/out/ autoscaling: true EOF pachctl create pipeline -f image_flattener.yaml View: Diagram User Code Output # image_flattener.py import cv2 import os import argparse import pathlib def flatten_video(vid_path, out_path, base_file): os.makedirs(out_path, exist_ok=True) current_frame = 0 video = cv2.VideoCapture(vid_path) while True: ret, frame = video.read() if ret: name = os.path.join(out_path, f&#39;{base_file}-frame-{current_frame:010d}.jpg&#39;) cv2.imwrite(name, frame) current_frame += 1 else: break video.release() def process_video_files(input_path, output_path): for root, _, files in os.walk(input_path): for file in files: file_path = os.path.join(root, file) if pathlib.Path(file_path).suffix.lower() == &#39;.mp4&#39;: base_file = os.path.splitext(file)[0] out_path = os.path.join(output_path, base_file) print(f&#34;Converting: {file}&#34;) flatten_video(file_path, out_path, base_file) else: print(f&#34;Skipping: {file_path}&#34;) def main(): parser = argparse.ArgumentParser( prog=&#39;flatten_to_images.py&#39;, description=&#39;Flatten Video to Images&#39; ) parser.add_argument(&#39;-i&#39;, &#39;--input&#39;, required=True, help=&#39;Input video directory&#39;) parser.add_argument(&#39;-o&#39;, &#39;--output&#39;, required=True, help=&#39;Output image directory&#39;) args = parser.parse_args() if not os.path.exists(args.input): print(&#34;Input directory does not exist.&#34;) return if not os.path.exists(args.output): os.makedirs(args.output) print(f&#34;Input: {args.input}&#34;) print(f&#34;Output: {args.output}&#34;) print(&#34;======================&#34;) process_video_files(args.input, args.output) if __name__ == &#34;__main__&#34;: main() pachctl list files image_flattener@master 6. Create the Image Tracing Pipeline # Up until this point, we&rsquo;ve used a simple single input from the Pachyderm file system (input.pfs) and a basic glob pattern (/*) to specify shape of our datums. This particular pattern treats each top-level file and directory as a single datum. However, in this pipeline, we have some special requirements:\nWe want to process only the raw images from the raw_videos_and_images repo We want to process all of the flattened video frame images from the image_flattener pipeline To achieve this, we&rsquo;re going to need to use a union input (input.union) to combine the two inputs into a single input for the pipeline.\nFor the raw_videos_and_images input, we can use a more powerful glob pattern to ensure that only image files are processed (/*.{png,jpg,jpeg}) For the image_flattener input, we can use the same glob pattern as before (/*) to ensure that each video&rsquo;s collection of frames is processed together Notice how we also update the transform.cmd to accommodate having two inputs.\ncat &lt;&lt;EOF &gt; image_tracer.yaml pipeline: name: image_tracer description: A pipeline that performs image edge detection by using the OpenCV library. input: union: - pfs: repo: raw_videos_and_images glob: &#34;/*.{png,jpg,jpeg}&#34; - pfs: repo: image_flattener glob: &#34;/*&#34; transform: image: lbliii/image_tracer:1.0.8 cmd: - python3 - /image_tracer.py - --input - /pfs/raw_videos_and_images - /pfs/image_flattener - --output - /pfs/out/ autoscaling: true EOF pachctl create pipeline -f image_tracer.yaml View: Diagram User Code Output # image_tracer.py import cv2 import numpy as np from matplotlib import pyplot as plt import os import argparse def make_edges(image, input_path, output_path): try: img = cv2.imread(image) if img is None: print(f&#34;Error reading image: {image}&#34;) return tail = os.path.split(image)[1] subdirectory = os.path.relpath(os.path.dirname(image), input_path) output_subdir = os.path.join(output_path, subdirectory) os.makedirs(output_subdir, exist_ok=True) edges = cv2.Canny(img, 100, 200) output_filename = os.path.splitext(tail)[0] + &#39;_edges.png&#39; output_filepath = os.path.join(output_subdir, output_filename) plt.imsave(output_filepath, edges, cmap=&#39;gray&#39;) print(f&#34;Processed: {image} -&gt; {output_filepath}&#34;) except Exception as e: print(f&#34;Error processing image: {image}&#34;) print(f&#34;Error details: {e}&#34;) def process_image_files(input_path, output_path): for dirpath, dirs, files in os.walk(input_path): total = len(files) current = 0 for file in files: current += 1 print(f&#34;Processing {file}, #{current} of {total}&#34;) make_edges(os.path.join(dirpath, file), input_path, output_path) def main(): parser = argparse.ArgumentParser( prog=&#39;image_tracer.py&#39;, description=&#39;Trace Images&#39; ) parser.add_argument(&#39;-i&#39;, &#39;--input&#39;, nargs=&#39;+&#39;, required=True, help=&#39;Input image directory&#39;) parser.add_argument(&#39;-o&#39;, &#39;--output&#39;, required=True, help=&#39;Output image directory&#39;) args = parser.parse_args() print(f&#34;Input: {args.input}&#34;) print(f&#34;Output: {args.output}&#34;) print(&#34;======================&#34;) for input_path in args.input: if not os.path.exists(input_path): print(f&#34;Input directory does not exist: {input_path}&#34;) else: print(f&#34;Processing images in: {input_path}&#34;) process_image_files(input_path, args.output) print(&#34;Done.&#34;) return if __name__ == &#34;__main__&#34;: main() pachctl list files image_tracer@master üìñ Since this pipeline is converting videos to video frames, it may take a few minutes to complete.\n7. Create the Gif Pipeline # Next, we&rsquo;ll create a pipeline that will create two gifs:\nA gif of the original video&rsquo;s flattened frames (from the image_flattener output repo) A gif of the video&rsquo;s traced frames (from the image_tracer output repo) To make a gif of both the original video frames and the traced frames, we&rsquo;re going to again need to use a union input so that we can process the image_flattener and image_tracer output repos.\nNotice that the glob pattern has changed; here, we want to treat each directory in an input as a single datum, so we use the glob pattern /*/. This is because we&rsquo;ve declared in the user code to store the video frames in a directory with the same name as the video file.\ncat &lt;&lt;EOF &gt; movie_gifer.yaml pipeline: name: movie_gifer description: A pipeline that converts frames into a gif using the OpenCV library. input: union: - pfs: repo: image_flattener glob: &#34;/*/&#34; - pfs: repo: image_tracer glob: &#34;/*/&#34; transform: image: lbliii/movie_gifer:1.0.5 cmd: - python3 - /movie_gifer.py - --input - /pfs/image_flattener - /pfs/image_tracer - --output - /pfs/out/ autoscaling: true EOF pachctl create pipeline -f movie_gifer.yaml View: Diagram User Code Output # movie_gifer.py import cv2 import os import argparse import imageio def make_gifs(directory, input_path, output_path): try: # Create output folder if it doesn&#39;t exist relative_dir = os.path.relpath(directory, input_path) output_folder = os.path.join(output_path, relative_dir) os.makedirs(output_folder, exist_ok=True) # Get all the images in the provided directory images = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(&#39;.jpg&#39;) or f.endswith(&#39;.png&#39;)] # Sort the pngs so they are in order images.sort() # Create the output filename tail = os.path.split(directory)[1] base_filename = os.path.splitext(tail)[0] if &#39;tracer&#39; in input_path: suffix = &#39;edges&#39; else: suffix = &#39;original&#39; output_filename = f&#39;{base_filename}_{suffix}.gif&#39; output_filepath = os.path.join(output_folder, output_filename) # Create the gif gif_images = [cv2.imread(i)[:, :, ::-1] for i in images] # Convert BGR to RGB imageio.mimsave(output_filepath, gif_images, duration=0.1) print(f&#34;Processed: {directory} -&gt; {output_filepath}&#34;) except Exception as e: print(f&#34;Error processing directory: {directory}&#34;) print(f&#34;Error details: {e}&#34;) def process_image_directories(input_path, output_path): # For each directory of images, make a gif for dirpath, dirs, files in os.walk(input_path): total = len(dirs) current = 0 for dir in dirs: current += 1 print(f&#34;Processing {dir}, #{current} of {total}&#34;) make_gifs(os.path.join(dirpath, dir), input_path, output_path) def main(): parser = argparse.ArgumentParser( prog=&#39;movie_gifer.py&#39;, description=&#39;Convert images to gifs&#39; ) parser.add_argument(&#39;-i&#39;, &#39;--input&#39;, nargs=&#39;+&#39;, required=True, help=&#39;Input image directory&#39;) parser.add_argument(&#39;-o&#39;, &#39;--output&#39;, required=True, help=&#39;Output image directory&#39;) args = parser.parse_args() print(f&#34;Input: {args.input}&#34;) print(f&#34;Output: {args.output}&#34;) print(&#34;======================&#34;) for input_path in args.input: if not os.path.exists(input_path): print(f&#34;Input directory does not exist: {input_path}&#34;) else: print(f&#34;Processing images in: {input_path}&#34;) process_image_directories(input_path, args.output) print(&#34;Done.&#34;) if __name__ == &#34;__main__&#34;: main() pachctl list files movie_gifer@master üìñ Since this pipeline is converting video frames to gifs, it may take a few minutes to complete.\n8. Create the Content Shuffler Pipeline # We have everything we need to make the comparison collage, but before we do that we need to re-shuffle the content so that the original images and gifs are in one directory (originals) and the traced images and gifs are in another directory (edges). This will help us more easily process the data via our user code for the collage. This is a common step you will encounter while using HPE ML Data Management referred to as a shuffle pipeline.\ncat &lt;&lt;EOF &gt; content_shuffler.yaml pipeline: name: content_shuffler description: A pipeline that collapses our inputs into one datum for the collager. input: union: - pfs: repo: movie_gifer glob: &#34;/&#34; - pfs: repo: raw_videos_and_images glob: &#34;/*.{png,jpg,jpeg}&#34; - pfs: repo: image_tracer glob: &#34;/*.{png,jpg,jpeg}&#34; transform: image: lbliii/content_shuffler:1.0.0 cmd: - python3 - /content_shuffler.py - --input - /pfs/movie_gifer - /pfs/raw_videos_and_images - /pfs/image_tracer - --output - /pfs/out/ autoscaling: true EOF pachctl create pipeline -f content_shuffler.yaml View: Diagram User Code Output # content_shuffler.py import argparse import os import shutil def shuffle_content(input_path, output_path): # create an originals and edges directory in the output path originals_output_path = f&#34;{output_path}/originals&#34; if not os.path.exists(originals_output_path): os.makedirs(originals_output_path) edges_output_path = f&#34;{output_path}/edges&#34; if not os.path.exists(edges_output_path): os.makedirs(edges_output_path) for dirpath, dirs, files in os.walk(input_path): for file in files: if &#34;frame&#34; and &#34;edges&#34; not in file: # Copy the original image to the originals directory shutil.copy(f&#34;{dirpath}/{file}&#34;, originals_output_path) elif &#34;frame&#34; not in file and &#34;edges&#34; in file: # Copy the images and gifs to the edges directory shutil.copy(f&#34;{dirpath}/{file}&#34;, edges_output_path) def main(): parser = argparse.ArgumentParser( prog=&#39;content_collager.py&#39;, description=&#39;Convert images and gifs into a collage&#39; ) parser.add_argument(&#39;-i&#39;, &#39;--input&#39;, nargs=&#39;+&#39;, required=True, help=&#39;Input image directory&#39;) parser.add_argument(&#39;-o&#39;, &#39;--output&#39;, required=True, help=&#39;Output image directory&#39;) args = parser.parse_args() print(f&#34;Input: {args.input} \\nOutput: {args.output}\\n&#34;) total_inputs = len(args.input) current_input = 0 for input_path in args.input: try: current_input += 1 print(f&#34;{input_path}; {current_input}/{total_inputs}&#34;) shuffle_content(input_path, args.output) except Exception as e: print(f&#34;Exception: {e}&#34;) if __name__ == &#34;__main__&#34;: main() pachctl list files content_shuffler@master 9. Create the Content Collager Pipeline # Finally, we&rsquo;ll create a pipeline that produces a static html page for viewing the original and traced content side-by-side.\ncat &lt;&lt;EOF &gt; content_collager.yaml pipeline: name: content_collager description: A pipeline that creates a static HTML collage. input: pfs: glob: &#34;/&#34; repo: content_shuffler transform: image: lbliii/content_collager:1.0.64 cmd: - python3 - /content_collager.py - --input - /pfs/content_shuffler - --output - /pfs/out/ autoscaling: true EOF pachctl create pipeline -f content_collager.yaml View: Diagram User Code Output # content_collager.py import os import argparse from bs4 import BeautifulSoup import shutil html_template = &#34;&#34;&#34; &lt;html&gt; &lt;head&gt; &lt;title&gt;Content Collage&lt;/title&gt; &lt;style&gt; #collage-container { display: flex; } #originals, #edges { flex: 1; padding: 20px; border: 1px solid #ccc; display: flex; flex-direction: column; align-items: center; gap: 10px; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Content collage&lt;/h1&gt; &lt;div id=&#34;collage-container&#34;&gt; &lt;div id=&#34;originals&#34;&gt; &lt;h2&gt;Original&lt;/h2&gt; &lt;/div&gt; &lt;div id=&#34;edges&#34;&gt; &lt;h2&gt;Edges&lt;/h2&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; &#34;&#34;&#34; def create_html_page(output_path): index_path = os.path.join(output_path, &#34;collage&#34;, &#34;index.html&#34;) if not os.path.exists(os.path.join(output_path, &#34;collage&#34;)): os.makedirs(os.path.join(output_path, &#34;collage&#34;)) if not os.path.exists(index_path): with open(index_path, &#34;w&#34;) as f: f.write(html_template) def append_image_to_html_page(output_path, image_path): index_path = os.path.join(output_path, &#34;collage&#34;, &#34;index.html&#34;) with open(index_path, &#34;r&#34;) as f: html_content = f.read() soup = BeautifulSoup(html_content, &#34;html.parser&#34;) # if the image path has the word &#34;originals&#34; in it, add it to the originals div if &#34;edges&#34; in image_path: originals_div = soup.find(&#34;div&#34;, id=&#34;edges&#34;) if originals_div: img_tag = soup.new_tag(&#34;img&#34;, src=image_path, width=&#34;300&#34;, style=&#34;display: block;&#34;) originals_div.append(img_tag) with open(index_path, &#34;w&#34;) as f: f.write(str(soup)) # otherwise, add it to the collage div else: collage_div = soup.find(&#34;div&#34;, id=&#34;originals&#34;) if collage_div: img_tag = soup.new_tag(&#34;img&#34;, src=image_path, width=&#34;300&#34;, style=&#34;display: block;&#34;) collage_div.append(img_tag) with open(index_path, &#34;w&#34;) as f: f.write(str(soup)) def process_content(input_path, output_path): try: # Create the HTML page create_html_page(output_path) # Create the output directory /collage/static static_output_path = os.path.join(output_path, &#34;collage&#34;, &#34;static&#34;) if not os.path.exists(static_output_path): os.makedirs(static_output_path) for dirpath, _, files in os.walk(input_path): sorted_files = sorted(files) for file in sorted_files: print(f&#34;Copying {file} to {static_output_path}&#34;) shutil.copy(os.path.join(dirpath, file), os.path.join(static_output_path, file)) append_image_to_html_page(output_path, f&#34;./static/{file}&#34;) except Exception as e: print(f&#34;Exception: {e}&#34;) def main(): parser = argparse.ArgumentParser( prog=&#39;content_collager.py&#39;, description=&#39;Convert images and gifs into a collage&#39; ) parser.add_argument(&#39;-i&#39;, &#39;--input&#39;, nargs=&#39;+&#39;, required=True, help=&#39;Input image directory&#39;) parser.add_argument(&#39;-o&#39;, &#39;--output&#39;, required=True, help=&#39;Output image directory&#39;) args = parser.parse_args() print(f&#34;Input: {args.input} \\nOutput: {args.output}\\n&#34;) try: process_content(args.input[0], args.output) except Exception as e: print(f&#34;Exception: {e}&#34;) if __name__ == &#34;__main__&#34;: main() Navigate to your project in Console. Click on the content_collager pipeline‚Äôs output directory. Click Inspect Commit. Check the collage directory. Select the Download button. Unzip and open the index.html file in your browser to see the collage. pachctl list files content_collager@master Exploring Resources, Data, &amp; Logs # Congratulations! You&rsquo;ve successfully created a DAG of pipelines that process video files into a collage. However, we&rsquo;ve only just scratched the surface of what you can do with HPE ML Data Management. Now that you have a working pipeline, try out some of these commands to explore all of the details associated with the DAG.\nTips: List Inspect Troubleshoot You can quickly take an account of all the resources you&rsquo;ve created by listing them in the terminal.\npachctl list projects pachctl list repos pachctl list pipelines pachctl list commits pachctl list jobs --pipeline content_collager pachctl list files content_collager@master You can inspect resources to get key details from within the terminal. This is a fast and easy way to validate resource creation and config.\npachctl inspect project video-to-frame-traces pachctl inspect repo content_collager pachctl inspect commit content_collager@&lt;commit-id&gt; pachctl inspect pipeline content_collager pachctl inspect files content_collager@master Let&rsquo;s say you&rsquo;ve uploaded corrupted data or realized that your pipeline&rsquo;s glob pattern/user code is flawed and you want to stop job processing and take a look at your logs. There&rsquo;s a number of commands you can run to get to the bottom of the issue.\npachctl stop pipeline content_collager pachctl stop job content_collager@&lt;job-id&gt; pachctl logs --pipeline content_collager pachctl debug dump debug_dump.tar.gz Once you&rsquo;ve updated your pipeline spec/user code, you&rsquo;ll want to reprocess the data for one of your pipelines. Here&rsquo;s how you can do that:\npachctl update pipeline -f content_collager.yaml --reprocess For a comprehensive list of operations, check out the Build DAGs section of the documentation or browse the Command Library.\nBonus Exercise # How would you update the glob pattern in the video converter pipeline spec (video_mp4_converter.yaml) to only process video files in the raw_videos_and_images repo? That would enable you to reduce the complexity of the user code in def process_video_files and make the pipeline more efficient. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "tutorials"
      ],
      "id": "a219978dbbcf8f025593aff1ebc84a77"
    },
    {
      "title": "First-Time Setup",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Get Started",
      "description": "",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/get-started/first-time-setup/",
      "relURI": "/latest/get-started/first-time-setup/",
      "body": "HPE ML Data Management can be deployed in Kubernetes using a wide variety of container orchestrators, but to get you set up for the very first time, we recommend using Docker Desktop. This installation method is very fast and will provide you with everything you need to start the Beginner Tutorial.\nBefore You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:\nOpen a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each of the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; All installation steps after 1. Install Docker Desktop must be run through the WSL terminal (Ubuntu) and not in Powershell.\nYou are now ready to continue to Step 1.\nYou can optionally install Homebrew to easily install tools like Helm. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; 1. Install Docker Desktop # Install Docker Desktop for your machine. Navigate to Settings for Mac, Windows, or Linux. Adjust your resources (~4 CPUs and ~12GB Memory) Enable Kubernetes Select Apply &amp; Restart. 2. Install Pachctl CLI # Operating System: MacOs &amp; Windows Debian Linux brew tap pachyderm/tap &amp;&amp; brew install pachyderm/tap/pachctl@2.7 curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_amd64.deb &amp;&amp; sudo dpkg -i /tmp/pachctl.deb AMD\ncurl -L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_linux_amd64.tar.gz | sudo tar -xzv --strip-components=1 -C /usr/local/bin ARM\ncurl-L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_linux_arm64.tar.gz | sudo sudo tar -xzv --strip-components=1 -C /usr/local/bin 3. Install &amp; Configure Helm # Install Helm: brew install helm Add the Pachyderm repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install PachD: üí° Open your browser and check http://localhost before installing. If any other tools are using the same port as HPE ML Data Management, add the following argument to the below command: --set proxy.service.httpPort=8080\nVersion: Community Edition Enterprise For Docker Desktop:\nhelm install pachyderm pachyderm/pachyderm \\ --set deployTarget=LOCAL \\ --set proxy.enabled=true \\ --set proxy.service.type=LoadBalancer \\ --set proxy.host=localhost For Minikube:\nhelm install pachyderm pachyderm/pachyderm \\ --set deployTarget=LOCAL \\ --set proxy.enabled=true \\ --set proxy.service.type=LoadBalancer \\ --set proxy.host=localhost Are you using an Enterprise trial key? If so, you can set up Enterprise Pachyderm locally by storing your trial key in a license.txt file and passing it into the following Helm command:\nhelm install pachyderm pachyderm/pachyderm \\ --set deployTarget=LOCAL \\ --set pachd.enterpriseLicenseKey=&#34;$(cat license.txt)&#34; \\ --set proxy.enabled=true \\ --set proxy.service.type=LoadBalancer \\ --set proxy.host=localhost This unlocks Enterprise features but also requires user authentication to access Console. A mock user is created by default to get you started, with the username: admin and password: password.\nThis may take several minutes to complete.\n4. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 5. Connect to Cluster # pachctl connect http://localhost:80 ‚ö†Ô∏è If you set the httpPort to a new value, such as 8080, use that value in the command. pachctl connect http://localhost:8080\nOptionally open your browser and navigate to the Console UI.\nüí° You can check your Pachyderm version and connection to pachd at any time with the following command:\npachctl version COMPONENT VERSION pachctl 2.7.3 pachd 2.7.3 ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "820f8709482fa5cee161363bde0c6d11"
    },
    {
      "title": "Connect to Existing Instance",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Get Started",
      "description": "Learn how to connect to your organization's existing instance.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/get-started/connect-to-existing/",
      "relURI": "/latest/get-started/connect-to-existing/",
      "body": " Before You Start # This guide assumes you have already installed HPE ML Data Management. You should know the URL of your organization&rsquo;s HPE ML Data Management instance, located in your Helm Chart at proxy.host. How to Log in to a Cluster via IdP # Open a terminal. Connect to your organization&rsquo;s instance. Method: HTTP HTTPS (TLS) pachctl connect http://pachyderm.&lt;your-proxy.host-value&gt; pachctl connect https://&lt;your-proxy.host-value&gt; Input the following command: pachctl auth login Select the connector you wish to use. Provide your credentials. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9e689eb2c259978dbd89dcfc1055bdb1"
    },
    {
      "title": "Language Clients",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Get Started",
      "description": "Learn about our available language clients.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/get-started/clients/",
      "relURI": "/latest/get-started/clients/",
      "body": "pachctl is the command-line tool you use to interact with a HPE ML Data Management cluster in your terminal. However, external applications might need to interact with HPE ML Data Management directly through our APIs.\nIn this case, HPE ML Data Management offers language specific SDKs in Go and Python.\nGo Client # The HPE ML Data Management team officially supports the Go client. It implements most of the functionalities provided with the pachctl CLI tool.\nGenerate And Serve The godocs Locally # Golang&rsquo;s package (godoc), installed by default by the Go installer, can generate the Go client&rsquo;s documentation from the go code.\nTo generate the docs:\nSet your GOPATH:\nexport PATH=$(go env GOPATH)/bin:$PATH In HPE ML Data Management&rsquo;s root directory, start the godocs server:\ngo run golang.org/x/tools/cmd/godoc -http=:6060 -goroot=&#34;&lt;your go root directory - for example: /Users/yourusername/pachyderm&gt;&#34; See https://pkg.go.dev/golang.org/x/tools/cmd/godoc for the complete list of flags available.\nIn your favorite browser, run localhost:6060/pkg/\n‚ö†Ô∏è A compatible version of gRPC is needed when using the Go client. You can identify the compatible version by searching for the version number next to replace google.golang.org/grpc =&gt; google.golang.org/grpc in https://github.com/pachyderm/pachyderm/blob/master/go.mod then:\ngo get google.golang.org/grpc cd $GOPATH/src/google.golang.org/grpc git checkout v1.29.1 Running Go Examples # The HPE ML Data Management godocs reference (see generation instructions above) provides examples of how you can use the Go client API. You need to have a running HPE ML Data Management cluster to run these examples.\nMake sure that you use your pachd_address in client.NewFromAddress(&quot;&lt;your-pachd-address&gt;:30650&quot;). For example, if you are testing on minikube, run minikube ip to get this information.\nSee the OpenCV Example in Go for more information.\nPython Clients # Pachyderm-SDK (New) # The Python client pachyderm-sdk is the new Python client for HPE ML Data Management and is officially supported by the HPE ML Data Management team.\nReference Docs Client Initialization Starter Project Python-Pachyderm (Old) # ‚ö†Ô∏è The Python client python-pachyderm will be deprecated in 9 months as of 08/10/2023.\n‚ÑπÔ∏è Use python-pachyderm v7.3 with HPE ML Data Management 2.7.x.\nYou will find all you need to get you started or dive into the details of the available modules and functions in the API documentation, namely:\nThe installation instructions and links to PyPI. A quick &ldquo;Hello World&rdquo; example to jumpstart your understanding of the API. Links to python-pachyderm main Github repository with a list of useful examples. As well as the entire reference API. Node Client # Our Javascript client node-pachyderm has been deprecated.\nOther languages # HPE ML Data Management uses a simple protocol buffer API. Protobufs support other languages, any of which can be used to programmatically use HPE ML Data Management. We have not built clients for them yet. It is an easy way to contribute to HPE ML Data Management if you are looking to get involved.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "sdk",
        "sdks",
        "golang",
        "python",
        "javascript",
        "developers",
        "client",
        "python-pachyderm"
      ],
      "id": "6ae5b5f0cd79fdc7db272499c2269f23"
    },
    {
      "title": "Learn",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "Learn about how our platform works.",
      "date": "January 30, 2023",
      "uri": "https://mldm.pachyderm.com/latest/learn/",
      "relURI": "/latest/learn/",
      "body": "HPE ML Data Management is a data science platform that provides data-driven pipelines with version control and autoscaling. It is container-native, allowing developers to use the languages and libraries that are best suited to their needs, and runs across all major cloud providers and on-premises installations.\nThe platform is built on Kubernetes and integrates with standard tools for CI/CD, logging, authentication, and data APIs, making it scalable and incredibly flexible. HPE ML Data Management‚Äôs data-driven pipelines allow you to automatically trigger data processing based on changes in your data, and the platform‚Äôs autoscaling capabilities ensure that resource utilization is optimized, maximizing developer efficiency.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "35086b30a659a4efdfd10f56bdc9440f"
    },
    {
      "title": "Key Features",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Learn",
      "description": "Learn about the key features and benefits of our powerful data processing platform.",
      "date": "January 30, 2023",
      "uri": "https://mldm.pachyderm.com/latest/learn/key-features/",
      "relURI": "/latest/learn/key-features/",
      "body": " Key Features and Benefits # The following are the key features of HPE ML Data Management that make it a powerful data processing platform.\nData-driven Pipelines # Automatically trigger pipelines based on changes in the data. Orchestrate batch or real-time data pipelines. Only process dependent changes in the data. Reproducibility and data lineage across all pipelines. Version Control # Track every change to your data automatically. Works with any file type. Supports collaboration through a git-like structure of commits. Autoscaling and Deduplication # Autoscale jobs based on resource demand. Automatically parallelize large data sets. Automatically deduplicate data across repositories. Flexibility and Infrastructure Agnosticism # Use existing cloud or on-premises infrastructure. Process any data type, size, or scale in batch or real-time pipelines. Container-native architecture allows for developer autonomy. Integrates with existing tools and services, including CI/CD, logging, authentication, and data APIs. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "79bf63b0b065c446cad46c138b62fd3c"
    },
    {
      "title": "Target Audience",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Learn",
      "description": "Discover if our platform is the right solution for your large-scale data processing and analysis needs.",
      "date": "January 30, 2023",
      "uri": "https://mldm.pachyderm.com/latest/learn/target-audience/",
      "relURI": "/latest/learn/target-audience/",
      "body": " Target Audience # HPE ML Data Management is designed for data engineers and data scientists who are managing and processing large amounts of data in a scalable and efficient manner. HPE ML Data Management is ideal for organizations working with big data and require robust, version-controlled, reproducible, and distributed data pipelines.\nIt is particularly useful for large unstructured data processing jobs, such as dataset curation for computer vision, speech recognition, video analytics, NLP, and many others.\nNon-Target Audience # HPE ML Data Management is not intended for users who do not require large-scale data processing and analysis. For instance, data scientists who are just starting with a small project may not need HPE ML Data Management&rsquo;s distributed system. Additionally, users with limited experience with containerization, cloud computing, and distributed systems may find it challenging to use HPE ML Data Management effectively.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "8aed82b2a12515cc7188bcf7b6ecae04"
    },
    {
      "title": "Basic Concepts",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Learn",
      "description": "Discover how our platform provides a secure, scalable, and version-controlled solution for storing and processing large amounts of data through its most basic concepts.",
      "date": "January 30, 2023",
      "uri": "https://mldm.pachyderm.com/latest/learn/basic-concepts/",
      "relURI": "/latest/learn/basic-concepts/",
      "body": " HPE ML Data Management File System # The HPE ML Data Management File System (PFS) is the backbone of the HPE ML Data Management data platform, providing a secure, scalable, and efficient way to store and manage large amounts of data. It is a version-controlled data management system that enables users to store any type of data in any format and scale, from a single file to a directory of files. The PFS is built on top of Postgres and S3, ensuring that your data is secure, consistent, and easily accessible. With PFS, users can version their data and work collaboratively with their teams, using branches and commits to manage and track changes over time.\nRepositories (Repo) # HPE ML Data Management repositories are version controlled, meaning that they keep track of changes to the data stored within them. Each repository can contain any type of data, including individual files or directories of files, and can handle data of any scale.\nLearn more about Input Repositories and Output Repositories.\nBranches # Branches in HPE ML Data Management are similar to branches in Git. They are pointers to commits that move along a growing chain of commits. This allows you to work with different versions of your data within the same repository.\nLearn more about Branches\nCommits # A commit in HPE ML Data Management is created automatically whenever data is added to or deleted from a repository. Each commit preserves the state of all files in the repository at the time of the commit, similar to a snapshot. Each commit is uniquely identifiable by a UUID and is immutable, meaning that the source data can never change.\nLearn more about Commits\nHPE ML Data Management Pipeline System # The HPE ML Data Management Pipeline System (PPS) is a core component of the HPE ML Data Management platform, designed to run robust data pipelines in a scalable and reproducible manner. With PPS, you can define, execute, and monitor complex data transformations using code that is run in Docker containers. The output of each pipeline is version-controlled in a HPE ML Data Management data repository, providing a complete, auditable history of all processing steps. In this way, PPS provides a flexible, data-driven solution for managing your data processing needs, while keeping data and processing results secure, reproducible, and scalable.\nLearn more about the PPS\nPipelines # HPE ML Data Management pipelines are used to transform data from HPE ML Data Management repositories. The output data is versioned in a HPE ML Data Management data repository, and the code for the transformation is run in Docker containers. Pipelines are triggered by new commits to a branch, making them data-driven.\nLearn more about Pipelines\nJobs # A job in HPE ML Data Management is the execution of a pipeline with a new commit. The data is distributed and parallelized computation is performed across a cluster. Each job is uniquely identified, making it possible to reproduce the results of a specific job.\nLearn more about Jobs\nDatum # A datum in HPE ML Data Management is a unit of computation for a job. It is used to distribute the processing workloads and to define how data can be split for parallel processing.\nLearn more about Datums\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "eea3afced4fcb5aee77f4e3edeaf00c1"
    },
    {
      "title": "Intro to Data Versioning",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Learn",
      "description": "Learn how to interact with versioned data, including creating and managing data repositories, creating and navigating commits, and branching to manage the evolution of data.",
      "date": "January 30, 2023",
      "uri": "https://mldm.pachyderm.com/latest/learn/intro-data-versioning/",
      "relURI": "/latest/learn/intro-data-versioning/",
      "body": " Introduction to Data Versioning # On this page we want to give a brief overview of how to use and interact with versioned data inside HPE ML Data Management. Collectively, this is often referred to as the HPE ML Data Management File System (PFS).\nRepositories # Data versioning in HPE ML Data Management starts with creating a data repository. HPE ML Data Management data repos are similar to Git repositories in that they provide a place to track changes made to a set of files.\nUsing the HPE ML Data Management CLI (pachctl) we would create a repository called data with the create repo command.\npachctl create repo data Once a repo is created, data can be added, deleted, or updated to a branch and all changes are versioned with commits.\nCommits # In HPE ML Data Management, commits are made to branches of a repo. For example, in the following session if we add a file to our data repository, that file will be captured in a commit.\n$ pachctl put file data@master -f my_file.bin $ pachctl list commit images@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master 6806cce 4 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE /my_file.bin file 57.27KiB If we then delete that file, it is removed from the active state of the branch, but the commit still exists.\n$ pachctl delete file data@master:/my_file.bin $ pachctl list commit data@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master ff1867a 3 seconds ago 0B USER data master 6806cce 20 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE Then if we add the file back, we&rsquo;ll see a third commit.\n$ pachctl create file data@master:/my_file.bin $ pachctl list commit data@master REPO BRANCH COMMIT FINISHED SIZE ORIGIN data master 0ec029b 20 seconds ago 57.27KiB USER data master ff1867a 3 seconds ago 0B USER data master 6806cce 20 seconds ago 57.27KiB USER $ pachctl list file data@master NAME TYPE SIZE /my_file.bin file 57.27KiB Visualizing the commit history for the master branch looks like the following.\n%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; tag: &#34;master&#34; Branches are a critical for tracking commits. The branch functions as a pointer to the most recent commit to the branch. For instance, when we create a new commit on the master branch (pachctl put file data@master -f my_new_file), we would create a new commit and our master branch would point at it.\n%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;master&#34; As we&rsquo;ve already seen, we can reference the HEAD of the branch, with the syntax, data@master.\n%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; type:HIGHLIGHT tag: &#34;HEAD&#34; Navigating Commits # Here we&rsquo;ll introduce the basics of how to navigate commits. Navigating these commits is an important aspect of working with PFS, and allows you to easily manage the history and evolution of your data.\nOne useful feature for navigating commits in PFS is the ability to refer to a previous commit using ancestry syntax. This syntax allows you to specify a commit relative to the current one, making it easy to compare and manipulate different versions of your data.\nThis makes it simple to switch between different versions of your data, and to perform operations like diffing, branching, and merging.\n%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master^&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; type:HIGHLIGHT commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; To refer to the commit 2 before the HEAD:\n%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master^^&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; type:HIGHLIGHT commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; Similarly, we can abbreviate this with the following syntax:\n%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master^2&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; type:HIGHLIGHT commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; We can reference the commits in numerical order using .n, where n is the commit number.\n%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master.1&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; type:HIGHLIGHT commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; %%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;data@master.-1&#39;}} }%% gitGraph commit id:&#34;6806cce&#34; commit id:&#34;ff1867a&#34; commit id:&#34;0ec029b&#34; type:HIGHLIGHT commit id:&#34;b69b3e3&#34; tag: &#34;HEAD&#34; Branches # In HPE ML Data Management, branches are used to track changes in a repository. You can think of a branch as a tag on a specific commit. Branches are associated with a particular commit and are updated as new commits are made (moving the HEAD of that branch to its most recent commit). This also means that at any time, you can change the commit that a branch is associated with, affecting branch history.\nHere&rsquo;s an example of a repo with three branches, each with its own history of commits:\n%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit commit branch v1.0 commit commit commit branch v1.1 commit commit commit tag:&#34;v1.1:HEAD&#34; checkout v1.0 commit tag:&#34;v1.0:HEAD&#34; checkout master commit tag:&#34;master:HEAD&#34; &ldquo;Merging&rdquo; Branches # The concept of merging binary data from different commits is complex. Ultimately, there are too many edge cases to do it reliably for every type of binary data, because computing a diff between two commits is ultimately meaningless unless you know how to compare the data. For example, we know that text files can be compared line-by-line or a bitmap image pixel by pixel, but how would we compute a diff for, say, binary model files?\nAdditionally, the output of a merge is usually a master copy, the official set of files desired. We rarely combine multiple pieces of image data to make one image, and if we are, we have usually created a technique for doing so. In the end, some files will be deleted, some updated, and some added.\nInstead, merging data, means creating a new commit with the desired combination of files and pointing our branch at that commit. In order to maintain a proper history, we would also want to make sure that the parent of that commit is relevant to what we want as well.\nFor example, in this situation, we have created a branch, dev, based on the 1-2833cd3 commit. We have committed multiple times to the dev branch, but nothing to master.\n%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit id:&#34;0-96e9b89&#34; commit id:&#34;1-2833cd3&#34; tag:&#34;master:HEAD&#34; branch dev commit id:&#34;2-25a8daf&#34; commit id:&#34;3-6413afc&#34; commit id:&#34;4-41a750b&#34; tag:&#34;dev:HEAD&#34; In this case it is simple to simply move the master branch to follow the most recent commit on dev, 4-41a750b.\npachctl create branch data@master --head 41a750b\nWhich would look like this:\n%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit id:&#34;0-96e9b89&#34; commit id:&#34;1-2833cd3&#34; branch dev commit id:&#34;2-25a8daf&#34; commit id:&#34;3-6413afc&#34; commit id:&#34;4-41a750b&#34; tag:&#34;master:HEAD, dev:HEAD&#34; Or from the history perspective of the respective branches:\n%%{init: { &#39;logLevel&#39;: &#39;debug&#39;, &#39;theme&#39;: &#39;base&#39;, &#39;gitGraph&#39;: {&#39;showBranches&#39;: true, &#39;showCommitLabel&#39;:true,&#39;mainBranchName&#39;: &#39;master&#39;}} }%% gitGraph commit id:&#34;0-96e9b89&#34; commit id:&#34;1-2833cd3&#34; branch dev commit id:&#34;2-25a8daf&#34; commit id:&#34;3-6413afc&#34; checkout dev commit tag:&#34;dev:HEAD&#34; id:&#34;4-41a750b&#34; checkout master commit id:&#34;2-25a8daf &#34; commit id:&#34;3-6413afc &#34; commit tag:&#34;master:HEAD&#34; id:&#34;4-41a750b &#34; Branches are useful for many reasons, but in HPE ML Data Management they also form the foundation of the pipeline system. New commits on branches can be used to trigger pipelines to run, resulting in one of the key differentiators, data-driven pipelines.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "b8fb7c346bc31748da8be17677c38a15"
    },
    {
      "title": "Intro to Pipelines",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Learn",
      "description": "Learn about the Pipeline System and how to define pipelines in YAML for data transformation and processing, including datums, jobs, and advanced glob patterns.",
      "date": "January 30, 2023",
      "uri": "https://mldm.pachyderm.com/latest/learn/intro-pipelines/",
      "relURI": "/latest/learn/intro-pipelines/",
      "body": " Introduction to Pipelines # The HPE ML Data Management Pipeline System (PPS) is a powerful tool for automating data transformations. With PPS, pipelines can be automatically triggered whenever input data changes, meaning that data transformations happen automatically in response to changes in your data, without the need for manual intervention.\nPipelines in HPE ML Data Management are defined by a pipeline specification and run on Kubernetes. The output of a pipeline is stored in a versioned data repository, which allows you to reproduce any transformation that occurs in HPE ML Data Management.\nPipelines can be combined into a computational DAG (directed acyclic graph), with each pipeline being triggered when an upstream commit is finished. This allows you to build complex workflows that can process large amounts of data efficiently and with minimal manual intervention.\nPipeline Specification # This is a HPE ML Data Management pipeline definition in YAML. It describes a pipeline called transform that takes data from the data repository and transforms it using a Python script my_transform_code.py.\npipeline: name: transform input: pfs: repo: data glob: &#34;/*&#34; transform: image: my-transform-image:v1.0 cmd: - python - &#34;/my_transform_code.py&#34; - &#34;--input&#34; - &#34;/pfs/data/&#34; - &#34;--output&#34; - &#34;/pfs/out/&#34; Here&rsquo;s a breakdown of the different sections of the pipeline definition:\npipeline specifies the name of the pipeline (in this case, it&rsquo;s transform). This name will also be used as the name for the output data repository. input specifies the input for the pipeline. In this case, the input is taken from the data repository in HPE ML Data Management. glob is used to specify how the files from the repository map to datums for processing. In this case, /* is used to specify all files in the repository can be processed individually. transform specifies the code and image to use for processing the input data. The image field specifies the Docker image to use for the pipeline. In this example, the image is named my-transform-image with a tag of v1.0. The cmd field specifies the command to run inside the container. In this example, the command is python /my_transform_code.py, which runs a Python script named my_transform_code.py. The script is passed the --input flag pointing to the input data directory, and the --output flag pointing to the output data directory. /pfs/data/ and /pfs/out/ are directories created by HPE ML Data Management. The input directory will contain an individual datum when the job is running, and anything put into the output directory will be committed to the output repositories when the job is complete. So, in summary, this pipeline definition defines a pipeline called transform that takes all files in the data repository, runs a Python script to transform them, and outputs the results to the out repository.\nDatums and Jobs # Pipelines can distribute work across a cluster to parallelize computation. Each time data is committed to a HPE ML Data Management repository, a job is created for each pipeline with that repo as an input to process the data.\nTo determine how to distribute data and computational work, datums are used. A datum is an indivisible unit of data required by the pipeline, defined according to the pipeline spec. The datums will be distributed across the cluster to be processed by workers.\n‚ÑπÔ∏è Only one job per pipeline will be created per commit, but there may be many datums per job.\nFor example, say you have a bunch of images that you want to normalize to a single size. You could iterate through each image and use opencv to change the size of it. No image depends on any other image, so this task can be parallelized by treating each image as an individual unit of work, a datum.\nNext, let‚Äôs say you want to create a collage from those images. Now, we need to consider all of the images together to combine them. In this case, the collection of images would be a single datum, since they are all required for the process.\nHPE ML Data Management input specifications can handle both of these situations with the glob section of the Pipeline Specification.\nBasic Glob Patterns # In this section we&rsquo;ll introduce glob patterns and datums in a couple of examples.\nIn the basic glob pattern example below, the input glob pattern is /*. This pattern matches each image at the top level of the images@master branch as an individual unit of work.\npipeline: name: resize description: A pipeline that resizes an image. input: pfs: glob: /* repo: images transform: cmd: - python - resize.py - --input - /pfs/images/* - --output - /pfs/out/ image: pachyderm/opencv When the pipeline is executed, it retrieves the datums defined in the input specification. For each datum, the worker downloads the necessary files into the Docker container at the start of its execution, and then performs the transform. Once the execution is complete, the output for each execution is combined into a commit and written to the output data repository.\nIn this example, the input glob pattern is /. This pattern matches everything at the top level of the images@master branch as an individual unit of work.\npipeline: name: collage description: A pipeline that creates a collage for a collection of images. input: pfs: glob: / repo: images transform: cmd: - python - collage.py - --input - /pfs/images/* - --output - /pfs/out/ image: pachyderm/opencv When this pipeline runs, it retrieves a single datum from the input specification. The job runs the single datum, downloading all the files from the images@master into the Docker container, and performs the transform. The result is then committed to the output data repository.\nAdvanced Glob Patterns # Datums can also be created from advanced operations, such as Join, Cross, Group, Union, and others to combine glob patterns from multiple data repositories. This allows us to create complex datum definitions, enabling sophisticated data processing pipelines.\nPipeline Communication (Advanced) # A much more detailed look at how HPE ML Data Management actually triggers pipelines is shown in the sequence diagram below. This is a much more advanced level of detail, but knowing how the different pieces of the platform interact can be useful.\nBefore we look at the diagram, it may be helpful to provide a brief recap of the main participants involved:\nUser: The user is the person interacting with HPE ML Data Management, typically through the command line interface (CLI) or one of the client libraries. PFS (HPE ML Data Management File System): PFS is the underlying file system that stores all of the data in HPE ML Data Management. It provides version control and lineage tracking for all data inside it. PPS (HPE ML Data Management Pipeline System): PPS is how code gets applied to the data in HPE ML Data Management. It manages the computational graph, which describes the dependencies between different steps of the data processing pipeline. Worker: Workers are Kubernetes pods that executes the jobs defined by PPS. Each worker runs a container image that contains the code for a specific pipeline. The worker will iterate through the datums it is given and apply user code to it. sequenceDiagram participant User participant PPS participant PFS participant Worker User-&gt;&gt;PFS: pachctl create repo foo activate PFS Note over PFS: create branch foo@master deactivate PFS User-&gt;&gt;PPS: pachctl create pipeline bar activate PPS PPS-&gt;&gt;PFS: create branch bar@master &lt;br&gt; (provenant on foo@master) PPS-&gt;&gt;Worker: create pipeline worker master Worker-&gt;&gt;PFS: subscribe to bar@master &lt;br&gt; (because it&#39;s subvenant on foo@master) deactivate PPS User-&gt;&gt;PFS: pachctl put file -f foo@master data.txt activate PFS Note over PFS: start commit PFS-&gt;&gt;PFS: propagate commit &lt;br&gt; (start downstream commits) Note over PFS: copy data.txt to open commit Note over PFS: finish commit PFS--&gt;&gt;Worker: subscribed commit returns deactivate PFS Note over Worker: Pipeline Triggered activate Worker Worker-&gt;&gt;PPS: Create job Worker-&gt;&gt;PFS: request datums for commit PFS--&gt;&gt;Worker: Datum list loop Each datum PFS-&gt;&gt;Worker: download datum Note over Worker: Process datum with user code Worker-&gt;&gt;PFS: copy data to open output commit end Worker-&gt;&gt;PFS: Finish commit Worker-&gt;&gt;PPS: Finish job deactivate Worker This diagram illustrates the data flow and interaction between the user, the HPE ML Data Management Pipeline System (PPS), the HPE ML Data Management File System (PFS), and a worker node when creating and running a HPE ML Data Management pipeline. Note, this is simplified for the single worker case. The multi-worker and autoscaling mechanisms are more complex.\nThe sequence of events begins with the user creating a PFS repo called foo and a PPS pipeline called bar with the foo repo as its input. When the pipeline is created, PPS creates a branch called bar@master, which is provenant on the foo@master branch in PFS. A worker pod is then created in the Kubernetes cluster by PPS, which subscribes to the bar@master branch.\nWhen the user puts a file named data.txt into the foo@master branch, PFS starts a new commit and propagates the commit, opening downstream commits for anything impacted. The worker receives the subscribed commit and when it finishes, triggers the pipeline.\nThe triggered pipeline creates a job for the pipeline, requesting datums for the output commit. For each datum, the worker downloads the data, processes it with the user&rsquo;s code, and writes the output to an open output commit in PFS. Once all datums have been processed, the worker finishes the output commit and the job is marked as complete.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "27cdf186dbecf140c9dcdd80c54ae2b6"
    },
    {
      "title": "Intro to Console",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Learn",
      "description": "Learn how to perform various actions in the Console UI.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/console-guide/",
      "relURI": "/latest/learn/console-guide/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "865523605e5f190c993eab643f945c9c"
    },
    {
      "title": "View Project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Intro to Console",
      "description": "Learn how to view a project in the console UI.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/console-guide/view-project/",
      "relURI": "/latest/learn/console-guide/view-project/",
      "body": " How to View a Project in Console # Authenticate to HPE ML Data Management or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Your project is displayed as a DAG (Directed Acyclic Graph) by default.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9a3ccd8dc1e8a2c353f626f0f6b8fe72"
    },
    {
      "title": "View List",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Intro to Console",
      "description": "Learn how to view a list of Repos or Pipelines in the console UI.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/console-guide/view-list/",
      "relURI": "/latest/learn/console-guide/view-list/",
      "body": " How to View a List of Resources in Console # Authenticate to HPE ML Data Management or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select View List. The DAG view is converted into a List view, organized by resource types. Select Repositories to view a list of repositories. Select Pipelines to view a list of pipelines. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "5615fd14b0afc2dd920efe604c663e93"
    },
    {
      "title": "View Pipelines",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Intro to Console",
      "description": "Learn how to view pipeline details in the console UI.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/console-guide/view-pipeline/",
      "relURI": "/latest/learn/console-guide/view-pipeline/",
      "body": " How to View Pipeline Details in Console # Authenticate to HPE ML Data Management or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select a Pipeline. Scroll and tab through the side panel to review the pipeline&rsquo;s details. Job Overview: Contains details like the number of datums processed, success status, and runtime. Pipeline Info: Contains details like the pipeline&rsquo;s description, number of tries allowed, output branch, and output repos. Pipeline Spec: Contains the pipeline spec which can be copied or downloaded as json/yaml. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e5c2a6b449563f8849124e4da617a28b"
    },
    {
      "title": "View Jobs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Intro to Console",
      "description": "Learn how to view job details in the console UI.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/console-guide/view-jobs/",
      "relURI": "/latest/learn/console-guide/view-jobs/",
      "body": " How to View Jobs From a Pipeline in Console # Authenticate to HPE ML Data Management or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select Jobs. Scroll through the list of jobs. Select See Details. Select Read Logs. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "1eaf4f10ef715976d544ad2021459dbd"
    },
    {
      "title": "View Outputs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Intro to Console",
      "description": "Learn how to view output files in the Console UI.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/console-guide/view-outputs/",
      "relURI": "/latest/learn/console-guide/view-outputs/",
      "body": " How to View Output Files in Console # Authenticate to HPE ML Data Management or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select an Output. Select View Files. Select See Files. Perform one of the following: Preview Download ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "67435ae0c6a4b4307af2542f56e989a1"
    },
    {
      "title": "View Inputs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Intro to Console",
      "description": "Learn how to view input files in the Console UI.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/console-guide/view-inputs/",
      "relURI": "/latest/learn/console-guide/view-inputs/",
      "body": " How to View Input Files in Console # Authenticate to HPE ML Data Management or access Console via Localhost. Scroll through the project list to find a project you want to view. Select View Project. Select an Input Repo. Select View Files. Perform one of the following: Preview Download ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9a098a8b5fc3df3002159ad52375fe4d"
    },
    {
      "title": "Developer Workflow",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Learn",
      "description": "Learn how to manage and process data in your CI workflow.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/developer-workflow/",
      "relURI": "/latest/learn/developer-workflow/",
      "body": "In general, the developer workflow for HPE ML Data Management involves adding data to versioned data repositories, creating pipelines to read from those repositories, executing the pipeline&rsquo;s code, and writing the pipeline&rsquo;s output to other data repositories. Both the data and pipeline can be iterated on independently with HPE ML Data Management handling the code execution according to the pipeline specfication. The workflow steps are shown below.\nData Workflow # Adding data to HPE ML Data Management is the first step towards building data-driven pipelines. There are multiple ways to add data to a HPE ML Data Management repository:\nBy using the pachctl put file command By using a special type of pipeline, such as a spout or cron By using one of the HPE ML Data Management&rsquo;s language clients By using a compatible S3 client For more information, see Load Your Data Into HPE ML Data Management.\nPipeline Workflow # The fundamental concepts of HPE ML Data Management are very powerful, but the manual build steps mentioned in the pipeline workflow can become cumbersome during rapid-iteration development cycles. We&rsquo;ve created a few helpful developer workflows and tools to automate steps that are error-prone or repetitive:\nThe push images flag or --push-images is a optional flag that can be passed to the create or update pipeline command. This option is most useful when you need to customize your Docker image or are iterating on the Docker image and code together, since it tags and pushes the image before updating the pipeline. CI/CD Integration provides a way to incorporate HPE ML Data Management functions into the CI process. This is most useful when working with a complex project or for code collaboration. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "1286528d0ba6108b5b29bf5aaf9f6b55"
    },
    {
      "title": "CI/CD Integration",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Developer Workflow",
      "description": "Learn how to integrate into your overall CI/CD workflows.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/developer-workflow/ci-cd-integration/",
      "relURI": "/latest/learn/developer-workflow/ci-cd-integration/",
      "body": "HPE ML Data Management is a powerful system for providing data provenance and scalable processing to data scientists and engineers. You can make it even more powerful by integrating it with your existing continuous integration and continuous deployment (CI/CD) workflows and systems. If you are just starting to use HPE ML Data Management and not setting up automation for your HPE ML Data Management build processes, see Working with Pipelines.\nThe following diagram demonstrates automated HPE ML Data Management development workflow with CI:\nAlthough initial CI setup might require extra effort on your side, in the long run, it brings significant benefits to your team, including the following:\nSimplified workflow for data scientists. Data scientists do not need to be aware of the complexity of the underlying containerized infrastructure. They can follow an established Git process, and the CI platform takes care of the Docker build and push process behind the scenes.\nYour CI platform can run additional unit tests against the submitted code before creating the build.\nFlexibility in tagging Docker images, such as specifying a custom name and tag or using the commit SHA for tagging.\nCI Workflow # The CI workflow includes the following steps:\nA new commit triggers a Git hook.\nTypically, HPE ML Data Management users store the following artifacts in a Git repository:\nA Dockerfile that you use to build local images. A pipeline.json specification file that you can use in a Makefile to create local builds, as well as in the CI/CD workflows. The code that performs data transformations. A commit hook in Git for your repository triggers the CI/CD process. It uses the information in your pipeline specification for subsequent steps.\nBuild an image.\nYour CI process automatically starts the build of a Docker container image based on your code and the Dockerfile.\nPush the image tagged with commit ID to an image registry.\nYour CI process pushes a Docker image created in Step 2 to your preferred image registry. When a data scientist submits their code to Git, a CI process uses the Dockerfile in the repository to build, tag with a Git commit SHA, and push the container to your image registry.\nUpdate the pipeline spec with the tagged image.\nIn this step, your CI/CD infrastructure uses your updated pipeline.json specification and fills in the Git commit SHA for the version of the image that must be used in this pipeline. Then, it runs the pachctl update pipeline command to push the updated pipeline specification to HPE ML Data Management. After that, HPE ML Data Management pulls a new image from the registry automatically. When the production pipeline is updated with the pipeline.json file that has the correct image tag in it, HPE ML Data Management restarts all pods for this pipeline with the new image automatically.\nGitHub Actions # GitHub actions are a convenient way to kick off workflows and perform integration. These can be used to:\nManually trigger a pipeline build, or Automatically build a pipeline from a commit or pull request. In our example, we show how to use the HPE ML Data Management GitHub Action to incorporate HPE ML Data Management functions to run on a Pull Request or at other points during development.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "workflows"
      ],
      "id": "d82fdeaabe0648b66353ff5fef0d0f36"
    },
    {
      "title": "Create a Machine Learning Workflow",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Developer Workflow",
      "description": "Learn how to integrate into your Machine Learning workflows.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/developer-workflow/create-ml-workflow/",
      "relURI": "/latest/learn/developer-workflow/create-ml-workflow/",
      "body": "Because HPE ML Data Management is a language and framework agnostic and platform, and because it easily distributes analysis over large data sets, data scientists can use any tooling for creating machine learning workflows. Even if that tooling is not familiar to the rest of an engineering organization, data scientists can autonomously develop and deploy scalable solutions by using containers. Moreover, HPE ML Data Management‚Äôs pipeline logic paired with data versioning make any results reproducible for debugging purposes or during the development of improvements to a model.\nFor maximum leverage of HPE ML Data Management&rsquo;s built functionality, HPE ML Data Management recommends that you combine model training processes, persisted models, and model utilization processes, such as making inferences or generating results, into a single HPE ML Data Management pipeline Directed Acyclic Graph (DAG).\nSuch a pipeline enables you to achieve the following goals:\nKeep a rigorous historical record of which models were used on what data to produce which results. Automatically update online ML models when training data or parameterization changes. Easily revert to other versions of an ML model when a new model does not produce an expected result or when bad data is introduced into a training data set. The following diagram demonstrates an ML pipeline:\nYou can update the training dataset at any time to automatically train a new persisted model. Also, you can use any language or framework, including Apache Spark‚Ñ¢, Tensorflow‚Ñ¢, scikit-learn‚Ñ¢, or other, and output any format of persisted model, such as pickle, XML, POJO, or other. Regardless of the framework, HPE ML Data Management versions the model so that you can track the data that was used to train each model.\nHPE ML Data Management processes new data coming into the input repository with the updated model. Also, you can recompute old predictions with the updated model, or test new models on previously input and versioned data. This feature enables you to avoid manual updates to historical results or swapping ML models in production.\nFor examples of ML workflows in HPE ML Data Management see Machine Learning Examples.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "workflows"
      ],
      "id": "c1f92a52541a4583d61d12dc76543389"
    },
    {
      "title": "The Push Images Flag",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Developer Workflow",
      "description": "Learn how to use the --push-images flag to accelerate pipeline development speed.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/developer-workflow/push-images-flag/",
      "relURI": "/latest/learn/developer-workflow/push-images-flag/",
      "body": "The --push-images flag is one way to improve development speed when working with pipelines.\nThe --push-images flag performs the following steps after you have built your image:\nIn your local registry, generates a unique tag for the image named after the transform.image field of your pipeline spec. üí° You must build your image with your username as a prefix (example: pachyderm/example-joins-inner-outer) &ndash; This name must match the one declared in the transform.image field of your pipeline spec.\nPushes the Docker image, with the tag, to your registry Updates the image tag in the pipeline spec json (on the fly) to match the new image Submits the updated pipeline to the HPE ML Data Management cluster The usage of the flag is shown below:\npachctl update pipeline -f &lt;pipeline name&gt; --push-images --registry &lt;registry&gt; --username &lt;registry user&gt; ‚ÑπÔ∏è For more details on the --push-images flag, see Update a Pipeline.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pachctl"
      ],
      "id": "ebbf8b44d3f592b646f88d7619e412dc"
    },
    {
      "title": "Working with Pipelines",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Developer Workflow",
      "description": "Learn about the steps involved in building, testing, and deploying data-transformation pipelines.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/developer-workflow/working-with-pipelines/",
      "relURI": "/latest/learn/developer-workflow/working-with-pipelines/",
      "body": "A typical HPE ML Data Management workflow involves multiple iterations of experimenting with your code and pipeline specs.\nIn general, there are five steps to working with a pipeline. The stages can be summarized in the image below.\nWe will walk through each of the stages in detail.\nStep 1: Write Your Analysis Code # Because HPE ML Data Management is completely language-agnostic, the code that is used to process data in HPE ML Data Management can be written in any language and can use any libraries of choice. Whether your code is as simple as a bash command or as complicated as a TensorFlow neural network, it needs to be built with all the required dependencies into a container that can run anywhere, including inside of HPE ML Data Management. See Examples.\nYour code does not have to import any special HPE ML Data Management functionality or libraries. However, it must meet the following requirements:\nRead files from a local file system. HPE ML Data Management automatically mounts each input data repository as /pfs/&lt;repo_name&gt; in the running containers of your Docker image. Therefore, the code that you write needs to read input data from this directory, similar to any other file system.\nBecause HPE ML Data Management automatically spreads data across parallel containers, your analysis code does not have to deal with data sharding or parallelization. For example, if you have four containers that run your Python code, HPE ML Data Management automatically supplies 1/4 of the input data to /pfs/&lt;repo_name&gt; in each running container. These workload balancing settings can be adjusted as needed through HPE ML Data Management tunable parameters in the pipeline specification.\nWrite files into a local file system, such as saving results. Your code must write to the /pfs/out directory that HPE ML Data Management mounts in all of your running containers. Similar to reading data, your code does not have to manage parallelization or sharding.\nStep 2: Build Your Docker Image # When you create a HPE ML Data Management pipeline, you need to specify a Docker image that includes the code or binary that you want to run. Therefore, every time you modify your code, you need to build a new Docker image, push it to your image registry, and update the image tag in the pipeline spec. This section describes one way of building Docker images, but if you have your own routine, feel free to apply it.\nTo build an image, you need to create a Dockerfile. However, do not use the CMD field in your Dockerfile to specify the commands that you want to run. Instead, you add them in the cmd field in your pipeline specification. HPE ML Data Management runs these commands inside the container during the job execution rather than relying on Docker to run them. The reason is that HPE ML Data Management cannot execute your code immediately when your container starts, so it runs a shim process in your container instead, and then, it calls your pipeline specification&rsquo;s cmd from there.\n‚ÑπÔ∏è The Dockerfile example below is provided for your reference only. Your Dockerfile might look completely different.\nTo build a Docker image, complete the following steps:\nIf you do not have a registry, create one with a preferred provider. If you decide to use DockerHub, follow the Docker Hub Quickstart to create a repository for your project.\nCreate a Dockerfile for your project. See the OpenCV example.\nBuild a new image from the Dockerfile by specifying a tag:\ndocker build -t &lt;image&gt;:&lt;tag&gt; . For more information about building Docker images, see Docker documentation.\nStep 3: Push Your Docker Image to a Registry # Once your image is built and tagged, you need to upload the image into a public or private image registry, such as DockerHub.\nAlternatively, you can use the HPE ML Data Management&rsquo;s built-in functionality to tag, and push images by running the pachctl update pipeline command with the --push-images flag. For more information, see Update a pipeline.\nLog in to an image registry.\nIf you use DockerHub, run:\ndocker login --username=&lt;dockerhub-username&gt; --password=&lt;dockerhub-password&gt; &lt;dockerhub-fqdn&gt; Push your image to your image registry.\nIf you use DockerHub, run:\ndocker push &lt;image&gt;:tag ‚ÑπÔ∏è Pipelines require a unique tag to ensure the appropriate image is pulled. If a floating tag, such as latest, is used, the Kubernetes cluster may become out of sync with the Docker registry, concluding it already has the latest image.\nStep 4: Create/Edit the Pipeline Config # HPE ML Data Management&rsquo;s pipeline specification files store the configuration information about the Docker image and code that HPE ML Data Management should run, the input repo(s) of the pipeline, parallelism settings, GPU usage etc&hellip; Pipeline specifications are stored in JSON or YAML format.\nA standard pipeline specification must include the following parameters:\nname transform input ‚ÑπÔ∏è Some special types of pipelines, such as a spout pipeline, do not require you to specify all of these parameters. Spout pipelines, for example, do not have input repos.\nCheck our reference pipeline specification page, for a list of all available fields in a pipeline specification file.\nYou can store your pipeline specifications locally or in a remote location, such as a GitHub repository.\nA simple pipeline specification file in JSON would look like the example below. The pipeline takes its data from the input repo data, runs worker containers with the defined image &lt;image&gt;:&lt;tag&gt; and command, then outputs the resulting processed data in the my-pipeline output repo. During a job execution, each worker sees and reads from the local file system /pfs/data containing only matched data from the glob expression, and writes its output to /pfs/out with standard file system functions; HPE ML Data Management handles the rest.\n# my-pipeline.json { &#34;pipeline&#34;: { &#34;name&#34;: &#34;my-pipeline&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;&lt;image&gt;:&lt;tag&gt;&#34;, &#34;cmd&#34;: [&#34;command&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } } } Step 5: Deploy/Update the Pipeline # As soon as you create a pipeline, HPE ML Data Management spins up one or more Kubernetes pods in which the pipeline code runs. By default, after the pipeline finishes running, the pods continue to run while waiting for the new data to be committed into the HPE ML Data Management input repository. You can configure this parameter, as well as many others, in the pipeline specification.\nCreate a HPE ML Data Management pipeline from the spec:\npachctl create pipeline -f my-pipeline.json You can specify a local file or a file stored in a remote location, such as a GitHub repository. For example, https://raw.githubusercontent.com/pachyderm/pachyderm/2.7.x/examples/opencv/edges.json.\nIf your pipeline specification changes, you can update the pipeline by running\npachctl update pipeline -f my-pipeline.json ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines"
      ],
      "id": "f8ca7229c8c0803757afa3dad597e19a"
    },
    {
      "title": "Diagrams",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Learn",
      "description": "View diagrams to learn about architecture and workflows.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/diagrams/",
      "relURI": "/latest/learn/diagrams/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "aeca865387492859715a7c3594a6152a"
    },
    {
      "title": "High-Level Architecture Diagram",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Diagrams",
      "description": "View a high-level architecture diagram of the product..",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/diagrams/architecture/",
      "relURI": "/latest/learn/diagrams/architecture/",
      "body": " ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f906a14a344d6a12e59ba050f47612ab"
    },
    {
      "title": "Glossary",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Learn",
      "description": "",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/",
      "relURI": "/latest/learn/glossary/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9cb763a828820e85df5e862196b3e628"
    },
    {
      "title": "Ancestry Syntax",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of Ancestry Syntax, which is used to reference the history of commits and branches in a repository.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/ancestry-syntax/",
      "relURI": "/latest/learn/glossary/ancestry-syntax/",
      "body": " About # Ancestry syntax in HPE ML Data Management is used to reference the history of commits and branches in a HPE ML Data Management input repository. Ancestry syntax is similar to Git syntax, and it allows users to navigate the history of commits and branches using special characters like ^ and ..\nThe ^ character is used to reference a commit or branch parent, where commit^ refers to the parent of the commit, and branch^&quot; refers to the parent of the branch head. Multiple ^ characters can be used to reference earlier ancestors, for example, commit^^ refers to the grandparent of the commit.\nThe . character is used to reference a specific commit in the history of a branch. For example, branch.1 refers to the first commit on the branch, branch.2 refers to the second commit, and so on.\nAncestry syntax allows users to access historical versions of data stored in HPE ML Data Management, which can be useful for tasks like debugging, testing, and auditing. However, it&rsquo;s important to note that resolving ancestry syntax can be computationally intensive, especially for long chains of commits, so it&rsquo;s best to use this feature judiciously.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "b7c8dce2cee0037d63d61f8f7c3dd99a"
    },
    {
      "title": "Branch",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a branch, which is a pointer to a commit that moves along with new commits as they are submitted.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/branch/",
      "relURI": "/latest/learn/glossary/branch/",
      "body": " About # A HPE ML Data Management branch is a pointer to a commit that moves along with new commits. By default, HPE ML Data Management does not create any branches when you create a repository. Most users create a master branch to initiate the first commit.\nBranches allow collaboration between teams of data scientists. However, the master branch is sufficient for most users.\nEach branch stores information about provenance, including input and output branches. HPE ML Data Management pipelines trigger a job when changes are detected in the HEAD of a branch.\nYou can create additional branches with pachctl create branch and view branches with pachctl list branch. Deleting a branch doesn&rsquo;t delete its commits, and all branches require a head commit.\nExample # pachctl list branch images # BRANCH HEAD # master c32879ae0e6f4b629a43429b7ec10ccc ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "concepts",
        "pachctl",
        "data-operations"
      ],
      "id": "4ed02e97407f4a5aea6db2c7342977dc"
    },
    {
      "title": "Commit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a commit, which is an atomic operation that snapshots and preserves the state of files/directories within a repository.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/commit/",
      "relURI": "/latest/learn/glossary/commit/",
      "body": " About # In HPE ML Data Management, commits snapshot and preserve the state of files and directories in a repository at a point in time. Unlike Git, HPE ML Data Management commits are centralized and transactional. You can create a commit with pachctl start commit and save it with pachctl finish commit. Once the commit is closed its contents are immutable. Commits may be chained together to represent a sequence of states.\nAll commits have an alphanumeric ID, and you can reference a commit with &lt;repo&gt;@&lt;commitID&gt;. Each commit has an origin that indicates why it was produced (USER or AUTO).\nGlobal Commits # A commit with global scope (global commit) represents the set of all provenance-dependent commits sharing the same ID.\nSub-Commits # A commit with a more focused scope (sub-commit) represents the &ldquo;Git-like&rdquo; record of one commit in a single branch of a repository‚Äôs file system.\nActions # List Commits Squash Commit Delete Commit Track Downstream Provenance ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "concepts",
        "pachctl",
        "data-operations"
      ],
      "id": "f756a62f86d8c042a7ad2db5a7ea8536"
    },
    {
      "title": "Commit Set",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a commit set, which is an immutable set of all the commits that resulted from a modification to the system.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/commit-set/",
      "relURI": "/latest/learn/glossary/commit-set/",
      "body": " About # A Commit Set is an immutable set of all the commits that resulted from a modification to the system. The commits within a commit set share a name (i.e. Global ID). This naming scheme enables you to reference data related to a commit anywhere in their provenance graph by simply naming it.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "concepts",
        "pachctl",
        "data-operations"
      ],
      "id": "1547a4ac132021229fe7268de174ba81"
    },
    {
      "title": "Cron",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a cron",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/cron/",
      "relURI": "/latest/learn/glossary/cron/",
      "body": " About # A cron, short for &ldquo;chronograph,&rdquo; is a time-based job scheduler that allows users to schedule and automate the execution of recurring tasks or commands at specific intervals. These tasks, referred to as cron jobs, are typically scripts or commands that perform specific actions.\nHPE ML Data Management uses the concept of crons in two ways: a cron pipeline spec, and a cron trigger.\nCron Pipelines vs Cron Triggers # Cron Pipelines trigger on their specified cron interval, plus each time a new input data is added. This enables you to create pipelines that trigger jobs at least once on a regular schedule. This could be useful if you periodically make changes to your user code, but have no reason to commit more data. When you do commit more data, the Cron Pipeline still triggers as a normal pipeline would.\nCron Triggers enable you to set up a scheduled reoccurring event on a repo branch that evaluates and fires the trigger. When a Cron Trigger fires, but no new data has been added, there are no new downstream commits or jobs.\n",
      "beta": "<no value>",
      "hidden": "false",
      "categories": [],
      "tags": [
        "concepts",
        "triggers",
        "data-operations",
        "pipeline"
      ],
      "id": "0816f4628b73e76339ab3113062d000d"
    },
    {
      "title": "DAG",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about DAGs, the Directed Acyclic Graphs that define the order in which pipelines are executed and how data flows between them.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/dag/",
      "relURI": "/latest/learn/glossary/dag/",
      "body": " About # In HPE ML Data Management, a Directed Acyclic Graph (DAG) is a collection of pipelines connected by data dependencies. The DAG defines the order in which pipelines are executed and how data flows between them.\nEach pipeline in a DAG processes data from its input repositories and produces output data that can be used as input by downstream pipelines. The input repositories of a pipeline can be the output repositories of other pipelines, allowing data to flow through the DAG.\nTo create a DAG in HPE ML Data Management, you create multiple pipeline specifications and define the dependencies between them. You can define dependencies between pipelines using the input parameter in the pipeline specification. For example, if you have two pipelines named A and B, and B depends on the output of A, you would set the input parameter of B to the name of the output repository of A.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "4334b267b45042fb9a5f6e390f454dd5"
    },
    {
      "title": "Data Parallelism",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of data parallelism.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/data-parallelism/",
      "relURI": "/latest/learn/glossary/data-parallelism/",
      "body": " About # Data parallelism refers to a parallel computing technique where a large dataset is partitioned and processed in parallel across multiple computing resources within a directed acyclic graph (DAG) or pipeline. In data parallelism, each task in the DAG/pipeline operates on a different subset of the dataset in parallel, allowing for efficient processing of large amounts of data. The results of each task are then combined to produce the final output. Data parallelism is often used in machine learning and deep learning pipelines where large datasets need to be processed in parallel using multiple computing resources. By distributing the data across different nodes, data parallelism can help reduce the overall processing time and improve the performance of the pipeline.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "concepts",
        "parallelism"
      ],
      "id": "35bcaf30aa8f1a56d3ccfab0c0c62379"
    },
    {
      "title": "Datum",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about datums, the smallest indivisible unit of computation within a job.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/datum/",
      "relURI": "/latest/learn/glossary/datum/",
      "body": " About # A datum is the smallest indivisible unit of computation within a job. Datums are used to:\nDivide your input data Distribute processing workloads A datum&rsquo;s scope can be as large as all of your data at once, a directory, a file, or a combination of multiple inputs. The shape and quantity of your datums is determined by a glob pattern defined in your pipeline specification.\nA job can have one, many, or no datums. Each datum is processed independently with a single execution of the user code on one of the pipeline worker pods. The individual output files produced by all of your datums are combined to create the final output commit.\nIf a job is successfully executed but has no matching files to transform, it is considered a zero-datum job.\nDatum Processing States # When a pipeline runs, it processes your datums. Some of them get processed successfully and some might be skipped or even fail. Generally, processed datums fall into either successful or failure state category.\nSuccessful States # State Description Success The datum has been successfully processed in this job. Skipped The datum has been successfully processed in a previous job, has not changed since then, and therefore, it was skipped in the current job. Failure States # State Description Failed The datum failed to be processed. Any failed datum in a job fails the whole job. Recovered The datum failed, but was recovered by the user&rsquo;s error handling code. Although the datum is marked as recovered, HPE ML Data Management does not process it in the downstream pipelines. A recovered datum does not fail the whole job. Just like failed datums, recovered datums are retried on the next run of the pipeline. You can view the information about datum processing states in the output of the pachctl list job &lt;jobID&gt; command:\n‚ÑπÔ∏è Datums that failed are still included in the total, but not shown in the progress indicator.\nRestarts # A job restarts when an internal error (not the user code) occurs while processing a job. These occurrences are counted in the RESTART column.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "bd6fcaef603671919b8d57b6b7c72dc1"
    },
    {
      "title": "Deferred Processing",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of deferred processing, which allows you to commit data more frequently than you process it.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/deferred-processing/",
      "relURI": "/latest/learn/glossary/deferred-processing/",
      "body": " About # Deferred Processing is a technique that allows you to commit data more frequently than you process it. By default, HPE ML Data Management automatically processes any newly committed data added to its input branch. For example, you may want to commit data hourly, but retrain your ML model daily.\nActions: # Defer processing via a staging branch Process specific commits Set branch triggers Set a custom output branch ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "data-operations",
        "datums",
        "branch triggers",
        "trigger"
      ],
      "id": "ad3de9874edff14abb08139c76e80aa5"
    },
    {
      "title": "Distributed Computing",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of distributed computing, which allows you to split your jobs across multiple workers.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/distributed-computing/",
      "relURI": "/latest/learn/glossary/distributed-computing/",
      "body": " About # Distributed computing is a technique that allows you to split your jobs across multiple HPE ML Data Management workers via the Parallelism PPS attribute. Leveraging distributed computing enables you to build production-scale pipelines with adjustable resources to optimize throughput.\nFor each job, all the datums are queued up and then distributed across the available workers. When a worker finishes processing its datum, it grabs a new datum from the queue until all the datums complete processing. If a worker pod crashes, its datums are redistributed to other workers for maximum fault tolerance.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "data-operations",
        "pipelines"
      ],
      "id": "79b655b3b884c4ede9260365f21cc47f"
    },
    {
      "title": "File",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a file, which is a Unix filesystem object that stores data.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/file/",
      "relURI": "/latest/learn/glossary/file/",
      "body": " About # A file is a Unix filesystem object, which is a directory or file, that stores data. Unlike source code version-control systems that are most suitable for storing plain text files, you can store any type of file in HPE ML Data Management, including binary files.\nOften, data scientists operate with comma-separated values (CSV), JavaScript Object Notation (JSON), images, and other plain text and binary file formats. HPE ML Data Management supports all file sizes and formats and applies storage optimization techniques, such as deduplication, in the background.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "concepts",
        "pachctl",
        "data-operations"
      ],
      "id": "4d0aaef70efcce491779da99cf0ddfe9"
    },
    {
      "title": "Glob Pattern",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a glob pattern, which is a string of characters that specifies a set of filenames or paths in a file system.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/glob-pattern/",
      "relURI": "/latest/learn/glossary/glob-pattern/",
      "body": " About # A glob pattern is a string of characters that specifies a set of filenames or paths in a file system. The term &ldquo;glob&rdquo; is short for &ldquo;global,&rdquo; and refers to the fact that a glob pattern can match multiple filenames or paths at once. For HPE ML Data Management, you can use glob patterns to define the shape of your datums against your inputs, which are spread across HPE ML Data Management workers for distributing computing.\nExamples # Glob Pattern Datum created / HPE ML Data Management denotes the whole repository as a single datum and sends all input data to a single worker node to be processed together. /* HPE ML Data Management defines each top-level files / directories in the input repo, as a separate datum. For example, if you have a repository with ten files and no directory structure, HPE ML Data Management identifies each file as a single datum and processes them independently. /*/* HPE ML Data Management processes each file / directory in each subdirectories as a separate datum. /** HPE ML Data Management processes each file in all directories and subdirectories as a separate datum. Glob patterns can also use other special characters, such as the question mark (?) to match a single character, or brackets ([...]) to match a set of characters.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "concepts",
        "pachctl",
        "datums",
        "pipelines",
        "data-operations"
      ],
      "id": "52e71eea4c735902d712227ab90f4d06"
    },
    {
      "title": "Global Identifier",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a global identifier, which is a unique identifier for a DAG.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/globalid/",
      "relURI": "/latest/learn/glossary/globalid/",
      "body": " About # Global Identifiers provide a simple way to follow the provenance of a DAG. Commits and jobs sharing the same Global ID represent a logically-related set of objects.\nWhen a new commit is made, HPE ML Data Management creates an associated commit ID; all resulting downstream commits and jobs in your DAG will then share that same ID (the Global Identifier).\nThe following diagram illustrates the global commit and its various components: Actions # List all global commits &amp; jobs List all sub-commits associated with a global ID Track provenance downstream, live Delete a Branch Head ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "data-operations",
        "pipelines"
      ],
      "id": "96d1d7ae145c981764d357e8b6dc061b"
    },
    {
      "title": "History",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of history (version control), which is a record of the changes made to data over time.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/history/",
      "relURI": "/latest/learn/glossary/history/",
      "body": " About # History in HPE ML Data Management is a record of the changes made to data over time, stored as a series of immutable snapshots (commits) that can be accessed using ancestry syntax and branch pointers. Each commit has a parentage structure, where new commits inherit content from their parents, creating a chain of commits that represents the full history of changes to the data.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "6b9fed4e22d89b300c768a1b43dbb257"
    },
    {
      "title": "Input Repository",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of an input repository, which is a location where data resides that is used as input for a pipeline.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/input-repo/",
      "relURI": "/latest/learn/glossary/input-repo/",
      "body": " About # In HPE ML Data Management, an input repository is a location where data resides that is used as input for a HPE ML Data Management pipeline. To define an input repository, you need to fill out the input attribute in pipeline&rsquo;s specification file.\nThere are several ways to structure the content of your input repos, such as:\nCross Group PFS Join Once you have defined an input repository, you can use it as the input source for a HPE ML Data Management pipeline. The pipeline will automatically subscribe to the branch of the input repository and process any new data that is added to the branch according to the pipeline configuration.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "bad5b841213d357f531a38fb0eb29048"
    },
    {
      "title": "Job",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a Job, which is a unit of work that is created by a pipeline.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/job/",
      "relURI": "/latest/learn/glossary/job/",
      "body": " About # A job is an execution of a pipeline triggered by new data detected in an input repository.\nWhen a commit is made to the input repository of a pipeline, jobs are created for all downstream pipelines in a directed acyclic graph (DAG), but they do not run until the prior pipelines they depend on produce their output. Each job runs the user&rsquo;s code against the current commit in a repository at a specified branch and then submits the results to the output repository of the pipeline as a single output commit.\nEach job has a unique alphanumeric identifier (ID) that users can reference in the &lt;pipeline&gt;@&lt;jobID&gt; format. Jobs have the following states:\nSate Description CREATED An input commit exists, but the job has not been started by a worker yet. STARTING The worker has allocated resources for the job (that is, the job counts towards parallelism), but it is still waiting on the inputs to be ready. UNRUNNABLE The job could not be run, because one or more of its inputs is the result of a failed or unrunnable job. As a simple example, say that pipelines Y and Z both depend on the output from pipeline X. If pipeline X fails, both pipeline Y and Z will pass from STARTING to UNRUNNABLE to signify that they had to be cancelled because of upstream failures. RUNNING The worker is processing datums. EGRESS The worker has completed all the datums and is uploading the output to the egress endpoint. FINISHING After all of the datum processing and egress (if any) is done, the job transitions to a finishing state where all of the post-processing tasks such as compaction are performed. FAILURE The worker encountered too many errors when processing a datum. KILLED The job timed out, or a user called StopJob SUCCESS None of the bad stuff happened. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "concepts",
        "pachctl",
        "data-operations",
        "pipelines"
      ],
      "id": "6b09bd4c4235f71e1c021582c19cc072"
    },
    {
      "title": "NLP",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of NLP, which is a subfield of machine learning that focuses on teaching machines to understand and generate human language.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/nlp/",
      "relURI": "/latest/learn/glossary/nlp/",
      "body": "NLP (Natural Language Processing) is a subfield of machine learning that focuses on teaching machines to understand and generate human language. It involves developing algorithms and models that can process, analyze, and generate natural language data, such as text, speech, and other forms of communication.\nNLP has numerous applications in various industries, such as chatbots, voice assistants, machine translation, sentiment analysis, and text classification, among others. Some common techniques used in NLP include text preprocessing, feature extraction, language modeling, sequence-to-sequence models, attention-based models, and transformer-based models like BERT and GPT.\nNLP models and algorithms often require large amounts of labeled data for training, and they can be computationally intensive. However, recent advancements in deep learning have led to significant improvements in NLP models, allowing them to achieve state-of-the-art performance on various natural language tasks.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "53ba5f5ccfa69ab3b5c2af1b5f1e74e8"
    },
    {
      "title": "Output Repository",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of an output repo, which is a repository where the results of a pipeline's processing are stored after being transformed by the provided user code.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/output-repo/",
      "relURI": "/latest/learn/glossary/output-repo/",
      "body": " About # In HPE ML Data Management, an output repo is a repository where the results of a pipeline&rsquo;s processing are stored after being transformed by the provided user code. Every pipeline automatically creates an output repository with the same name as the pipeline.\nWhen a pipeline runs, it creates a new commit in the output repository with the results of the processing. The commit contains a set of files that represent the output of the pipeline. Each commit in the output repository corresponds to a job that was run to generate that output.\nAn output repository can be created or deleted using a pachctl CLI command or the HPE ML Data Management API.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "fb5be24ab78172bedd92706793a70fdb"
    },
    {
      "title": "Pachyderm Worker",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a Pachyderm worker.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/pachyderm-worker/",
      "relURI": "/latest/learn/glossary/pachyderm-worker/",
      "body": " About # HPE ML Data Management workers are kubernetes pods that run the docker image (your user code) specified in the pipeline specification. When you create a pipeline, HPE ML Data Management spins up workers that continuously run in the cluster, waiting for new data to process.\nEach datum goes through the following processing phases inside a HPE ML Data Management worker pod:\nPhase Description Downloading The HPE ML Data Management worker pod downloads the datum contents into HPE ML Data Management. Processing The HPE ML Data Management worker pod runs the contents of the datum against your code. Uploading The HPE ML Data Management worker pod uploads the results of processing into an output repository. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "ea831ec701d968d85a722e0d79cae3f2"
    },
    {
      "title": "Pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a pipeline, which is a primitive responsible for reading data from a specified source, transforming it according to the pipeline specification, and writing the result to an output repo.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/pipeline/",
      "relURI": "/latest/learn/glossary/pipeline/",
      "body": " About # A pipeline is a HPE ML Data Management primitive responsible for reading data from a specified source, such as a HPE ML Data Management repo, transforming it according to the pipeline specification, and writing the result to an output repo.\nPipelines subscribe to a branch in one or more input repositories, and every time the branch has a new commit, the pipeline executes a job that runs user code to completion and writes the results to a commit in the output repository.\nPipelines are defined declaratively using a JSON or YAML file (the pipeline specification), which must include the name, input, and transform parameters at a minimum. Pipelines can be chained together to create a directed acyclic graph (DAG).\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "b7b649c7999782fe6d3c572ae334c605"
    },
    {
      "title": "Pipeline Inputs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a pipeline input, which is the source of the data that the pipeline reads and processes.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/pipeline-inputs/",
      "relURI": "/latest/learn/glossary/pipeline-inputs/",
      "body": " About # In HPE ML Data Management, pipeline inputs are defined as the source of the data that the pipeline reads and processes. The input for a pipeline can be a HPE ML Data Management repository (input repo) or an external data source, such as a file in a cloud storage service.\nTo define a pipeline input, you need to specify the source of the data and how the data is organized. This is done in the input section of the pipeline specification file, which is a YAML or JSON file that defines the configuration of the pipeline.\nInput Types # The input section can contain one or more input sources, each specified as a separate block.\nPFS Cron Egress (DB) Egress (Storage) Service Spout S3 ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f9319cd5e230648fe8adca9f5ca56760"
    },
    {
      "title": "Pipeline Specification",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a pipeline specification, which is a declarative configuration file used to define the behavior of a pipeline.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/pipeline-specification/",
      "relURI": "/latest/learn/glossary/pipeline-specification/",
      "body": " About # A pipeline specification is a declarative configuration file used to define the behavior of a HPE ML Data Management pipeline. It is typically written in YAML or JSON format and contains information about the pipeline&rsquo;s input sources, output destinations, Docker image (user code), command, and other metadata.\nIn addition to simply transforming your data, you can also achieve more advanced techniques though the pipeline specification, such as:\nDeferred professing Distributed computing ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "94c3b7d2c086dad50ba4a1e35b3ae5ec"
    },
    {
      "title": "Project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of a project, which is a workspace collection of repositories and pipelines.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/project/",
      "relURI": "/latest/learn/glossary/project/",
      "body": " About # In HPE ML Data Management, Projects are a scoped boundary full of input repositories, pipelines, and the output repos associated with pipelines. With Enterprise and Authentication enabled, Projects also offer RBAC authorization controls for users and teams. This means that users can only access the data within a project if they have been granted access to that project.\nRelated Operations # Create a Project Set a Project as Current Add a Project Resource Grant Project Access Delete a Project ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f0af5d70922a1f38a36f00db4d35a206"
    },
    {
      "title": "Provenance",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of provenance, which is the recorded data lineage that tracks the dependencies and relationships between datasets.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/provenance/",
      "relURI": "/latest/learn/glossary/provenance/",
      "body": " About # Provenance in HPE ML Data Management refers to the tracking of the dependencies and relationships between datasets, as well as the ability to go back in time and see the state of a dataset or repository at a particular moment. HPE ML Data Management models both commit provenance and branch provenance to represent the dependencies between data in the pipeline.\nCommit Provenance # Commit provenance refers to the relationship between commits in different repositories. If a commit in a repository is derived from a commit in another repository, the derived commit is provenant on the source commit. Capturing this relationship supports queries regarding how data in a commit was derived.\nBranch Provenance # Branch provenance represents a more general relationship between data. It asserts that future commits in the downstream branch will be derived from the head commit of the upstream branch.\nTraversing Provenance # HPE ML Data Management automatically maintains a complete audit trail, allowing all results to be fully reproducible. To track the direct provenance of commits and learn where the data in the repository originates, you can use the pachctl inspect command to view provenance information, including the origin kind, direct provenance, and size of the data.\nHPE ML Data Management&rsquo;s DAG structure makes it easy to traverse the provenance and subvenance in any commit. All related steps in a DAG share the same global identifier, making it possible to run pachctl list commit &lt;commitID&gt; to get the full list of all the branches with commits created due to provenance relationships.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "concepts",
        "pachctl",
        "data-operations"
      ],
      "id": "73874675e81c52f17e9f66003f2578c3"
    },
    {
      "title": "Task Parallelism",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of task parallelism.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/task-parallelism/",
      "relURI": "/latest/learn/glossary/task-parallelism/",
      "body": " About # Task parallelism refers to a parallel computing technique where multiple tasks within a directed acyclic graph (DAG) or pipeline are executed simultaneously on different computing resources. In task parallelism, the focus is on executing different tasks in parallel rather than parallelizing a single task. This means that each task in the DAG/pipeline is executed independently of other tasks, allowing for efficient use of resources and faster completion of the overall DAG/pipeline. Task parallelism is often used in data processing pipelines or workflows where tasks can be executed in parallel without any dependency on each other.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "concepts",
        "parallelism"
      ],
      "id": "c6445c7c4185ec6b2d65e0bde6344873"
    },
    {
      "title": "User Code",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Glossary",
      "description": "Learn about the concept of User Code, which is custom code that users write to process their data in pipelines.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/learn/glossary/user-code/",
      "relURI": "/latest/learn/glossary/user-code/",
      "body": " About # In HPE ML Data Management, user code refers to the custom code that users write to process their data in pipelines. User code can be written in any language and can use any libraries or frameworks.\nHPE ML Data Management allows users to define their user code as a Docker image, which can be pushed to a registry and referenced using the Transform attribute of the pipeline&rsquo;s specification. The user code image contains the necessary dependencies and configuration for the code to run in HPE ML Data Management&rsquo;s distributed computing environment.\nUser code can be defined for each pipeline stage in HPE ML Data Management, allowing users to chain together multiple processing steps and build complex data pipelines. HPE ML Data Management also provides a Python library for building pipelines, which simplifies the process of defining user code and specifying pipeline stages.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "71605dff83a19d917130495bdd554348"
    },
    {
      "title": "Set Up",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "Set up locally, in the cloud, or on-premises.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/",
      "relURI": "/latest/set-up/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a8e1f5d22c5a60458f74ded48660be1f"
    },
    {
      "title": "Cloud Deploy",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Set Up",
      "description": "Learn how to deploy using your preferred cloud provider.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/cloud-deploy/",
      "relURI": "/latest/set-up/cloud-deploy/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "aws",
        "azure",
        "gcp",
        "cloud-deploy"
      ],
      "id": "b035fa07efcc627cf6181c2b6ce9b9fb"
    },
    {
      "title": "AWS Deployment",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Cloud Deploy",
      "description": "Learn how to deploy to the cloud with AWS.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/cloud-deploy/aws/",
      "relURI": "/latest/set-up/cloud-deploy/aws/",
      "body": " Before You Start # This guide assumes that you have already tried HPE ML Data Management locally and have all of the following installed:\nKubectl Pachctl Helm AWS CLI Eksctl 1. Create an EKS Cluster # Use the eksctl tool to deploy an EKS Cluster: eksctl create cluster --name pachyderm-cluster --region &lt;region&gt; -profile &lt;your named profile&gt; Verify deployment: kubectl get all 2. Create an S3 Bucket # Run the following command: aws s3api create-bucket --bucket ${BUCKET_NAME} --region ${AWS_REGION} Verify. aws s3 ls 3. Enable Persistent Volumes Creation # Create an IAM OIDC provider for your cluster. Install the Amazon EBS Container Storage Interface (CSI) driver on your cluster. Create a gp3 storage class manifest file (e.g., gp3-storageclass.yaml) kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp3 annotations: storageclass.kubernetes.io/is-default-class: &#34;true&#34; provisioner: kubernetes.io/aws-ebs parameters: type: gp3 fsType: ext4 Set gp3 to your default storage class. kubectl apply -f gp3-storageclass.yaml Verify that it has been set as your default. kubectl get storageclass 4. Set up an RDS PostgreSQL Instance # By default, HPE ML Data Management runs with a bundled version of PostgreSQL. For production environments, it is strongly recommended that you disable the bundled version and use an RDS PostgreSQL instance.\n‚ö†Ô∏è Aurora Serverless PostgreSQL is not supported.\nIn the RDS console, create a database in the region matching your HPE ML Data Management cluster. Choose the PostgreSQL engine. Select a PostgreSQL version &gt;= 13.3. Configure your DB instance as follows: SETTING Recommended value DB instance identifier Fill in with a unique name across all of your DB instances in the current region. Master username Choose your Admin username. Master password Choose your Admin password. DB instance class The standard default should work. You can change the instance type later on to optimize your performances and costs. Storage type and Allocated storage If you select io1, keep the 100 GiB default size. Read more information on Storage for RDS on Amazon&rsquo;s website. Storage autoscaling If your workload is cyclical or unpredictable, enable storage autoscaling to allow RDS to scale up your storage when needed. Standby instance We highly recommend creating a standby instance for production environments. VPC Select the VPC of your Kubernetes cluster. Attention: After a database is created, you can&rsquo;t change its VPC. Read more on VPCs and RDS on Amazon documentation. Subnet group Pick a Subnet group or Create a new one. Read more about DB Subnet Groups on Amazon documentation. Public access Set the Public access to No for production environments. VPC security group Create a new VPC security group and open the postgreSQL port or use an existing one. Password authentication or Password and IAM database authentication Choose one or the other. Database name In the Database options section, enter HPE ML Data Management&rsquo;s Database name (We are using HPE ML Data Managementin this example.) and click Create database to create your PostgreSQL service. Your instance is running. Warning: If you do not specify a database name, Amazon RDS does not create a database. üìñ Standalone Clusters\nIf you are deploying a standalone cluster, you must create a second database named dex in your RDS instance for HPE ML Data Management&rsquo;s authentication service. Read more about dex on PostgreSQL in Dex&rsquo;s documentation.\nMulti-cluster setups use Enterprise Server to handle authentication, so you do not need to create a dex database.\nCreate a new user account and grant it full CRUD permissions to both HPE ML Data Managementand (when applicable) dex databases. Read about managing PostgreSQL users and roles in this blog. HPE ML Data Management will use the same username to connect to HPE ML Data Managementas well as to dex. 5. Create a Values.yaml # Version: Community Edition Enterprise global: postgresql: postgresqlAuthType: &#34;scram-sha-256&#34; # use &#34;md5&#34; if using postgresql &lt; 14 postgresqlUsername: &#34;username&#34; postgresqlPassword: &#34;password&#34; # The name of the database should be HPE ML Data Management&#39;s (&#34;pachyderm&#34; in the example above), not &#34;dex&#34; # See also # postgresqlExistingSecretName: &#34;&lt;yoursecretname&gt;&#34; postgresqlDatabase: &#34;databasename&#34; # The postgresql database host to connect to. Defaults to postgres service in subchart postgresqlHost: &#34;RDS CNAME&#34; # The postgresql database port to connect to. Defaults to postgres server in subchart postgresqlPort: &#34;5432&#34; postgresql: # turns off the install of the bundled postgres. # If not using the built in Postgres, you must specify a Postgresql # database server to connect to in global.postgresql enabled: false deployTarget: &#34;AMAZON&#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: amazon: bucket: &#34;bucket_name&#34; # this is an example access key ID taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # this is an example secret access key taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; region: &#34;us-east-2&#34; externalService: enabled: true console: enabled: true global: postgresql: postgresqlAuthType: &#34;scram-sha-256&#34; # use &#34;md5&#34; if using postgresql &lt; 14 postgresqlUsername: &#34;username&#34; postgresqlPassword: &#34;password&#34; # The name of the database should be HPE ML Data Management&#39;s (&#34;pachyderm&#34; in the example above), not &#34;dex&#34; # See also # postgresqlExistingSecretName: &#34;&lt;yoursecretname&gt;&#34; postgresqlDatabase: &#34;databasename&#34; # The postgresql database host to connect to. Defaults to postgres service in subchart postgresqlHost: &#34;RDS CNAME&#34; # The postgresql database port to connect to. Defaults to postgres server in subchart postgresqlPort: &#34;5432&#34; postgresql: # turns off the install of the bundled postgres. # If not using the built in Postgres, you must specify a Postgresql # database server to connect to in global.postgresql enabled: false deployTarget: &#34;AMAZON&#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: amazon: bucket: &#34;bucket_name&#34; # this is an example access key ID taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # this is an example secret access key taken from https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html (AWS Credentials) secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; region: &#34;us-east-2&#34; # Enterprise key enterpriseLicenseKey: &#34;YOUR_ENTERPRISE_TOKEN&#34; console: enabled: true 6. Configure Helm # Run the following to add the HPE ML Data Management repo to Helm:\nhelm repo add pachyderm https://helm.pachyderm.com helm repo update helm install pachyderm pachyderm/pachyderm -f my_pachyderm_values.yaml 7. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 8. Connect to Cluster # You&rsquo;ll need your organization&rsquo;s cluster URL (proxy.host) value to connect.\nRun the following command to get your cluster URL: kubectl get services | grep pachyderm-proxy | awk &#39;{print $4}&#39; Connect to your cluster:\nMethod: HTTP HTTPS (TLS) pachctl connect http://pachyderm.&lt;your-proxy.host-value&gt; pachctl connect https://pachyderm.&lt;your-proxy.host-value&gt; ‚ÑπÔ∏è If the connection commands did not work together, run each separately.\nOptionally open your browser and navigate to the Console UI.\nüí° You can check your HPE ML Data Management version and connection to pachd at any time with the following command:\npachctl version COMPONENT VERSION pachctl 2.7.3 pachd 2.7.3 ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "aws",
        "cloud-deploy"
      ],
      "id": "3fcb8d7339dafe916b49b116e2814dfd"
    },
    {
      "title": "Azure Deployment",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Cloud Deploy",
      "description": "Learn how to deploy to the cloud with Azure.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/cloud-deploy/azure/",
      "relURI": "/latest/set-up/cloud-deploy/azure/",
      "body": " Before You Start # This guide assumes that you have already tried HPE ML Data Management locally and have all of the following installed:\nKubectl Pachctl Helm Azure CLI. 1. Create an AKS Cluster # You can deploy Kubernetes on Azure by following the official Azure Kubernetes Service documentation, use the quickstart walkthrough, or follow the steps in this section.\nAt a minimum, you will need to specify the parameters below:\nVariable Description RESOURCE_GROUP A unique name for the resource group where HPE ML Data Management is deployed. For example, pach-resource-group. LOCATION An Azure availability zone where AKS is available. For example, centralus. NODE_SIZE The size of the Kubernetes virtual machine (VM) instances. To avoid performance issues, HPE ML Data Management recommends that you set this value to at least Standard_DS4_v2 which gives you 8 CPUs, 28 Gib of Memory, 56 Gib SSD.\nIn any case, use VMs that support premium storage. See Azure VM sizes for details around which sizes support Premium storage. CLUSTER_NAME A unique name for the HPE ML Data Management cluster. For example, pach-aks-cluster. You can choose to follow the guided steps in Azure Service Portal&rsquo;s Kubernetes Services or use Azure CLI.\nLog in to Azure:\naz login This command opens a browser window. Log in with your Azure credentials. Resources can now be provisioned on the Azure subscription linked to your account.\nCreate an Azure resource group or retrieve an existing group.\naz group create --name ${RESOURCE_GROUP} --location ${LOCATION} Example:\naz group create --name test-group --location centralus System Response:\n{ &#34;id&#34;: &#34;/subscriptions/6c9f2e1e-0eba-4421-b4cc-172f959ee110/resourceGroups/pach-resource-group&#34;, &#34;location&#34;: &#34;centralus&#34;, &#34;managedBy&#34;: null, &#34;name&#34;: &#34;pach-resource-group&#34;, &#34;properties&#34;: { &#34;provisioningState&#34;: &#34;Succeeded&#34; }, &#34;tags&#34;: null, &#34;type&#34;: null } Create an AKS cluster in the resource group/location:\nFor more configuration options: Find the list of all available flags of the az aks create command.\naz aks create --resource-group ${RESOURCE_GROUP} --name ${CLUSTER_NAME} --node-vm-size ${NODE_SIZE} --node-count &lt;node_pool_count&gt; --location ${LOCATION} Example:\naz aks create --resource-group test-group --name test-cluster --generate-ssh-keys --node-vm-size Standard_DS4_v2 --location centralus Confirm the version of the Kubernetes server by running kubectl version.\n‚ÑπÔ∏è &ldquo;See Also:&rdquo; - Azure Virtual Machine sizes\nOnce your Kubernetes cluster is up, and your infrastructure configured, you are ready to prepare for the installation of HPE ML Data Management. Some of the steps below will require you to keep updating the values.yaml started during the setup of the recommended infrastructure:\n2. Create a Storage Container # HPE ML Data Management needs an Azure Storage Container (Object store) to store your data.\nTo access your data, HPE ML Data Management uses a Storage Account with permissioned access to your desired container. You can either use an existing account or create a new one in your default subscription, then use the JSON key associated with the account and pass it on to HPE ML Data Management.\nSet up the following variables:\nSTORAGE_ACCOUNT: The name of the storage account where you store your data. CONTAINER_NAME: The name of the Azure blob container where you store your data. Create an Azure storage account:\naz storage account create \\ --resource-group=&#34;${RESOURCE_GROUP}&#34; \\ --location=&#34;${LOCATION}&#34; \\ --sku=Premium_LRS \\ --name=&#34;${STORAGE_ACCOUNT}&#34; \\ --kind=BlockBlobStorage System response:\n{ &#34;accessTier&#34;: null, &#34;creationTime&#34;: &#34;2019-06-20T16:05:55.616832+00:00&#34;, &#34;customDomain&#34;: null, &#34;enableAzureFilesAadIntegration&#34;: null, &#34;enableHttpsTrafficOnly&#34;: false, &#34;encryption&#34;: { &#34;keySource&#34;: &#34;Microsoft.Storage&#34;, &#34;keyVaultProperties&#34;: null, &#34;services&#34;: { &#34;blob&#34;: { &#34;enabled&#34;: true, ... Make sure that you set Stock Keeping Unit (SKU) to Premium_LRS and the kind parameter is set to BlockBlobStorage. This configuration results in a storage that uses SSDs rather than standard Hard Disk Drives (HDD). If you set this parameter to an HDD-based storage option, your HPE ML Data Management cluster will be too slow and might malfunction.\nVerify that your storage account has been successfully created:\naz storage account list Obtain the key for the storage account (STORAGE_ACCOUNT) and the resource group to be used to deploy HPE ML Data Management:\nSTORAGE_KEY=&#34;$(az storage account keys list \\ --account-name=&#34;${STORAGE_ACCOUNT}&#34; \\ --resource-group=&#34;${RESOURCE_GROUP}&#34; \\ --output=json \\ | jq &#39;.[0].value&#39; -r )&#34; ‚ÑπÔ∏è Find the generated key in the Storage accounts &gt; Access keys section in the Azure Portal or by running the following command az storage account keys list --account-name=${STORAGE_ACCOUNT}.\nCreate a new storage container within your storage account:\naz storage container create --name ${CONTAINER_NAME} \\ --account-name ${STORAGE_ACCOUNT} \\ --account-key &#34;${STORAGE_KEY}&#34; 3. Create a Values.yaml # Version: Community Edition Enterprise deployTarget: &#34;MICROSOFT&#34; proxy: enabled: true service: type: LoadBalancer pachd: storage: microsoft: # storage container name container: &#34;blah&#34; # storage account name id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # storage account key secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; externalService: enabled: true console: enabled: true deployTarget: &#34;MICROSOFT&#34; proxy: enabled: true service: type: LoadBalancer host: &lt;insert-external-ip-address-or-dns-name&gt; pachd: storage: microsoft: # storage container name container: &#34;blah&#34; # storage account name id: &#34;AKIAIOSFODNN7EXAMPLE&#34; # storage account key secret: &#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&#34; # Enterprise key enterpriseLicenseKey: &#34;YOUR_ENTERPRISE_TOKEN&#34; console: enabled: true 4. Configure Helm # Run the following to add the HPE ML Data Management repo to Helm:\nhelm repo add pachyderm https://helm.pachyderm.com helm repo update helm install pachyderm pachyderm/pachyderm -f my_pachyderm_values.yaml 5. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 6. Connect to Cluster # You&rsquo;ll need your organization&rsquo;s cluster URL (proxy.host) value to connect.\nRun the following command to get your cluster URL: kubectl get services | grep pachyderm-proxy | awk &#39;{print $4}&#39; Connect to your cluster:\nMethod: HTTP HTTPS (TLS) pachctl connect http://pachyderm.&lt;your-proxy.host-value&gt; pachctl connect https://pachyderm.&lt;your-proxy.host-value&gt; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "azure",
        "cloud-deploy"
      ],
      "id": "7d15897c35f27c281e3f4dfef2d93582"
    },
    {
      "title": "GCP Deployment",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Cloud Deploy",
      "description": "Learn how to deploy to the cloud with GCP.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/cloud-deploy/gcp/",
      "relURI": "/latest/set-up/cloud-deploy/gcp/",
      "body": " Before You Start # This guide assumes that:\nYou have already tried HPE ML Data Management locally and have some familiarity with Kubectl, Helm, Google Cloud SDK and jq You have access to a Google Cloud account linked to an active billing account (gcloud alpha billing accounts list) Configure Variables # Configure the following variables. PROJECT_NAME=&#34;pachyderm-0001&#34; SQL_ADMIN_PASSWORD=&#34;batteryhorsestaple&#34; BILLING_ACCOUNT_ID=&#34;000000-000000-000000&#34; # see `gcloud alpha billing accounts list` # This group of variables can be changed, but are sane defaults GCP_REGION=&#34;us-central1&#34; GCP_ZONE=&#34;us-central1-a&#34; K8S_NAMESPACE=&#34;default&#34; CLUSTER_MACHINE_TYPE=&#34;n2-standard-2&#34; SQL_CPU=&#34;2&#34; SQL_MEM=&#34;7680MB&#34; LOGGING=&#34;SYSTEM&#34; PROJECT_ID=$(echo &#34;pach-$(openssl rand -base64 32 | tr -dc &#39;a-z0-9-&#39; | fold -w 2-26 | head -n 1)&#34;) ## optional name generator, useful if you are creating multiple clusters or testing. adjective=(&#34;happy&#34; &#34;silly&#34; &#34;brave&#34; &#34;witty&#34; &#34;elegant&#34; &#34;fierce&#34; &#34;gentle&#34; &#34;clever&#34; &#34;vibrant&#34; &#34;charming&#34;) color=(&#34;red&#34; &#34;blue&#34; &#34;green&#34; &#34;yellow&#34; &#34;purple&#34; &#34;orange&#34; &#34;pink&#34; &#34;black&#34; &#34;white&#34; &#34;gray&#34;) animal=(&#34;cat&#34; &#34;dog&#34; &#34;elephant&#34; &#34;lion&#34; &#34;tiger&#34; &#34;panda&#34; &#34;giraffe&#34; &#34;zebra&#34; &#34;monkey&#34; &#34;bear&#34;) NAME=&#34;${adjective[$RANDOM % ${#adjective[@]}]}-${color[$RANDOM % ${#color[@]}]}-${animal[$RANDOM % ${#animal[@]}]}&#34; # The following variables probably shouldn&#39;t be changed CLUSTER_NAME=&#34;${NAME}-gke&#34; BUCKET_NAME=&#34;${NAME}-gcs&#34; LOKI_BUCKET_NAME=&#34;${NAME}-logs-gcs&#34; CLOUDSQL_INSTANCE_NAME=&#34;${NAME}-sql&#34; GSA_NAME=&#34;${NAME}-gsa&#34; LOKI_GSA_NAME=&#34;${NAME}-loki-gsa&#34; STATIC_IP_NAME=&#34;${NAME}-ip&#34; ROLE1=&#34;roles/cloudsql.client&#34; ROLE2=&#34;roles/storage.admin&#34; ROLE3=&#34;roles/storage.objectCreator&#34; SERVICE_ACCOUNT=&#34;${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com&#34; LOKI_SERVICE_ACCOUNT=&#34;${LOKI_GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com&#34; PACH_WI=&#34;serviceAccount:${PROJECT_ID}.svc.id.goog[${K8S_NAMESPACE}/pachyderm]&#34; SIDECAR_WI=&#34;serviceAccount:${PROJECT_ID}.svc.id.goog[${K8S_NAMESPACE}/pachyderm-worker]&#34; CLOUDSQLAUTHPROXY_WI=&#34;serviceAccount:${PROJECT_ID}.svc.id.goog[${K8S_NAMESPACE}/k8s-cloudsql-auth-proxy]&#34; Save to an .env file. Source them by inputting source .env into the terminal before starting the installation guide. The following steps use a template to create a GKE cluster, a Cloud SQL instance, and a static IP address. The template also creates a service account for HPE ML Data Management and Loki, and grants the service account the necessary permissions to access the Cloud SQL instance and storage buckets. You do not have to use this template, but it&rsquo;s a good outline for understanding how to create your own set up.\n1. Create a New Project # Create a new project (e.g.,pachyderm-quickstart-project). You can pre-define the project ID using between 6-30 characters, starting with a lowercase letter. This ID will be used to set up the cluster and will be referenced throughout this guide.\ngcloud projects create ${PROJECT_ID} --name=${PROJECT_NAME} --set-as-default gcloud alpha billing projects link ${PROJECT_ID} --billing-account=${BILLING_ACCOUNT_ID} Enable the following APIs:\ngcloud services enable container.googleapis.com gcloud services enable sqladmin.googleapis.com gcloud services enable compute.googleapis.com 2. Create a Static IP Address # Create the static IP Address: gcloud compute addresses create ${STATIC_IP_NAME} --region=${GCP_REGION} Get the static IP address: STATIC_IP_ADDR=$(gcloud compute addresses describe ${STATIC_IP_NAME} --region=${GCP_REGION} --format=&#34;json&#34; --flatten=&#34;address&#34; | jq -r &#39;.[]&#39;) 3. Create a GKE Cluster # Create a GKE cluster with the following command:\ngcloud container clusters create ${CLUSTER_NAME} \\ --region=${GCP_REGION} \\ --machine-type=${CLUSTER_MACHINE_TYPE} \\ --workload-pool=${PROJECT_ID}.svc.id.goog \\ --enable-ip-alias \\ --create-subnetwork=&#34;&#34; \\ --logging=${LOGGING} \\ --enable-dataplane-v2 \\ --enable-shielded-nodes \\ --release-channel=&#34;regular&#34; \\ --workload-metadata=&#34;GKE_METADATA&#34; \\ --enable-autorepair \\ --enable-autoupgrade \\ --disk-type=&#34;pd-ssd&#34; \\ --image-type=&#34;COS_CONTAINERD&#34; Grant your user account the privileges needed for the helm install to work properly:\n# By default, GKE clusters have RBAC enabled. To allow the &#39;helm install&#39; to give the &#39;pachyderm&#39; service account # the requisite privileges via clusterrolebindings, you will need to grant *your user account* the privileges # needed to create those clusterrolebindings. # # Note that this command is simple and concise, but gives your user account more privileges than necessary. See # https://docs.pachyderm.io/en/latest/deploy-manage/deploy/rbac/ for the complete list of privileges that the # Pachydermserviceaccount needs. kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account) Connect to the cluster:\ngcloud container clusters get-credentials ${CLUSTER_NAME} --region=${GCP_REGION} 4. Create Storage Buckets # gsutil mb -l ${GCP_REGION} gs://${BUCKET_NAME} gsutil mb -l ${GCP_REGION} gs://${LOKI_BUCKET_NAME} 5. Create a Cloud SQL Instance # Create a Cloud SQL instance with the following command: gcloud sql instances create ${CLOUDSQL_INSTANCE_NAME} \\ --database-version=POSTGRES_14 \\ --cpu=${SQL_CPU} \\ --memory=${SQL_MEM} \\ --zone=${GCP_ZONE} \\ --availability-type=ZONAL \\ --storage-size=50GB \\ --storage-type=SSD \\ --storage-auto-increase \\ --root-password=${SQL_ADMIN_PASSWORD} Create a databases for HPE ML Data Management and Dex: gcloud sql databases create pachyderm -i ${CLOUDSQL_INSTANCE_NAME} gcloud sql databases create dex -i ${CLOUDSQL_INSTANCE_NAME} Get the Cloud SQL connection name: CLOUDSQL_CONNECTION_NAME=$(gcloud sql instances describe ${CLOUDSQL_INSTANCE_NAME} --format=json | jq .&#34;connectionName&#34;) 6. Create Service Accounts # Create a service account for HPE ML Data Management and Loki.\ngcloud iam service-accounts create ${GSA_NAME} gcloud projects add-iam-policy-binding ${PROJECT_ID} \\ --member=&#34;serviceAccount:${SERVICE_ACCOUNT}&#34; \\ --role=&#34;${ROLE1}&#34; gcloud projects add-iam-policy-binding ${PROJECT_ID} \\ --member=&#34;serviceAccount:${SERVICE_ACCOUNT}&#34; \\ --role=&#34;${ROLE2}&#34; gcloud projects add-iam-policy-binding ${PROJECT_ID} \\ --member=serviceAccount:${SERVICE_ACCOUNT} \\ --role=&#34;${ROLE3}&#34; gcloud iam service-accounts add-iam-policy-binding ${SERVICE_ACCOUNT} \\ --role roles/iam.workloadIdentityUser \\ --member &#34;${PACH_WI}&#34; gcloud iam service-accounts add-iam-policy-binding ${SERVICE_ACCOUNT} \\ --role roles/iam.workloadIdentityUser \\ --member &#34;${SIDECAR_WI}&#34; gcloud iam service-accounts add-iam-policy-binding ${SERVICE_ACCOUNT} \\ --role roles/iam.workloadIdentityUser \\ --member &#34;${CLOUDSQLAUTHPROXY_WI}&#34; gcloud iam service-accounts create ${LOKI_GSA_NAME} gcloud iam service-accounts keys create &#34;${LOKI_GSA_NAME}-key.json&#34; --iam-account=&#34;${LOKI_SERVICE_ACCOUNT}&#34; gcloud projects add-iam-policy-binding ${PROJECT_ID} \\ --member=&#34;serviceAccount:${LOKI_SERVICE_ACCOUNT}&#34; \\ --role=&#34;${ROLE2}&#34; gcloud projects add-iam-policy-binding ${PROJECT_ID} \\ --member=&#34;serviceAccount:${LOKI_SERVICE_ACCOUNT}&#34; \\ --role=&#34;${ROLE3}&#34; 7. Create a Loki Secret # kubectl create secret generic loki-service-account --from-file=&#34;${LOKI_GSA_NAME}-key.json&#34; 8. Build a Helm Values File # Create a values.yaml file, inserting the variables we&rsquo;ve created in the previous steps: cat &lt;&lt;EOF &gt; ${NAME}.values.yaml deployTarget: &#34;GOOGLE&#34; pachd: enabled: true externalService: enabled: false image: tag: &#34;2.7.3&#34; lokiDeploy: true lokiLogging: true storage: google: bucket: &#34;${BUCKET_NAME}&#34; serviceAccount: additionalAnnotations: iam.gke.io/gcp-service-account: &#34;${SERVICE_ACCOUNT}&#34; create: true name: &#34;pachyderm&#34; worker: serviceAccount: additionalAnnotations: iam.gke.io/gcp-service-account: &#34;${SERVICE_ACCOUNT}&#34; create: true name: &#34;pachyderm-worker&#34; cloudsqlAuthProxy: connectionName: ${CLOUDSQL_CONNECTION_NAME} serviceAccount: &#34;${SERVICE_ACCOUNT}&#34; enabled: true resources: requests: memory: &#34;500Mi&#34; cpu: &#34;250m&#34; postgresql: enabled: false global: postgresql: postgresqlAuthType: &#34;scram-sha-256&#34; postgresqlHost: &#34;cloudsql-auth-proxy.${K8S_NAMESPACE}.svc.cluster.local.&#34; postgresqlPort: &#34;5432&#34; postgresqlSSL: &#34;disable&#34; postgresqlUsername: &#34;postgres&#34; postgresqlPassword: &#34;${SQL_ADMIN_PASSWORD}&#34; loki-stack: loki: env: - name: GOOGLE_APPLICATION_CREDENTIALS value: /etc/secrets/${LOKI_GSA_NAME}-key.json extraVolumes: - name: loki-service-account secret: secretName: loki-service-account extraVolumeMounts: - name: loki-service-account mountPath: /etc/secrets config: schema_config: configs: - from: 1989-11-09 object_store: gcs store: boltdb-shipper schema: v11 index: period: 24h prefix: loki_index_ chunks: prefix: loki_chunks_ storage_config: boltdb_shipper: active_index_directory: /data/loki/boltdb-shipper-active cache_location: /data/loki/boltdb-shipper-cache cache_ttl: 24h shared_store: gcs gcs: bucket_name: &#34;${LOKI_BUCKET_NAME}&#34; grafana: enabled: false proxy: enabled: true service: type: LoadBalancer loadBalancerIP: ${STATIC_IP_ADDR} httpPort: 80 httpsPort: 443 tls: enabled: false EOF Install using the following command: helm repo add pachyderm https://helm.pachyderm.com helm repo update helm install pachyderm -f ./${NAME}.values.yaml pachyderm/pachyderm 9. Connect to Cluster # You&rsquo;ll need your organization&rsquo;s cluster URL (proxy.host) value to connect.\nRun the following command to get your cluster URL: kubectl get services | grep pachyderm-proxy | awk &#39;{print $4}&#39; Connect to your cluster:\nMethod: HTTP HTTPS (TLS) pachctl connect grpc://&lt;your-proxy.host-value&gt;:80 pachctl connect grpcs://&lt;your-proxy.host-value&gt;:443 You can optionally run port-forward to connect to console in your dashboard at http://localhost:4000/.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "gcp",
        "cloud-deploy"
      ],
      "id": "bded9f53cbb08228d8a1d99143a57fa7"
    },
    {
      "title": "Console Setup",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Cloud Deploy",
      "description": "Learn how to deploy the Console UI from the cloud (AWS, GCP, Azure).",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/cloud-deploy/console/",
      "relURI": "/latest/set-up/cloud-deploy/console/",
      "body": " Before You Start # You must have HPE ML Data Management installed using one of the following guides:\nAWS GCP Azure Deploy # Set up your Proxy and DNS and point your browser to: http://&lt;external-IP-address-or-domain-name&gt;:80 or, https://&lt;external-IP-address-or-domain-name&gt;:443 if TLS is enabled Set up your IDP during deployment. ‚ÑπÔ∏è You can use the mock user (username:admin, password: password) to login to Console when authentication is enabled but no Identity provider was wired (Enterprise).\nConfigure your Identity Provider As Part of Helm: To configure your Identity Provider as a part of helm install, see examples for the oidc.upstreamIDPs value in the helm chart values specification and read our IDP Configuration page for a better understanding of each field. Manually via Values.yaml: You can manually update your values.yaml with oidc.mockIDP = false. Connect. Method: HTTP HTTPS (TLS) pachctl connect http://pachyderm.&lt;your-proxy.host-value&gt; pachctl connect https://pachyderm.&lt;your-proxy.host-value&gt; You are all set! You should land on the Projects page of Console.\nEnterprise + Helm # When Enterprise is enabled through Helm, Auth is automatically activated. This means that you do not need to run pachctl auth activate; a pachyderm-auth Kubernetes secret is created which contains a rootToken key. Use {{&quot;kubectl get secret pachyderm-auth -o go-template='{{.data.rootToken | base64decode }}'&quot;}} to retrieve it and save it where you see fit.\nConsiderations # If you run pachctl auth activate, the secret is not updated. Instead, the rootToken is printed in your STDOUT for you to save; the same behavior applies if you activate enterprise manually (pachctl license activate) and then activate authentication (pachctl auth activate). You can set the helm value pachd.activateAuth to false to prevent the automatic bootstrap of auth on the cluster. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "console",
        "cloud-deploy"
      ],
      "id": "897a46ff86d8896528daf1e857a8eeff"
    },
    {
      "title": "Set Up AWS Secret Manager",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Cloud Deploy",
      "description": "Learn how to securely manage and centralize your secrets.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/cloud-deploy/aws-secret-manager/",
      "relURI": "/latest/set-up/cloud-deploy/aws-secret-manager/",
      "body": "For production environments, we highly recommend securing and centralizing the storage and management of your secrets (database access, root token, enterprise key, etc&hellip;) in AWS Secrets Manager, then allow your EKS cluster to retrieve those secrets using fine-grained IAM policies.\nThis section will walk you through the steps to enable your EKS cluster to retrieve secrets from AWS Secrets Manager.\nBefore You Start # Make sure you have completed the steps found in the AWS deployment instructions before proceeding.\n1. Install The AWS Secrets and Configuration Provider (ASCP) # To retrieve your secrets through your workloads running on your cluster, you will first need to install:\nA Secrets Store CSI driver AWS Secrets Manager and Config Provider ‚ö†Ô∏è The ASCP works with Amazon Elastic Kubernetes Service (Amazon EKS) 1.17+.\nInstall the Secrets Store CSI Driver # Deploy the Secrets Store CSI driver by following the installation steps.\nüí° Make sure to enable the Sync as Kubernetes Secret feature explicitly by setting the helm parameter syncSecret.enabled to true.\n‚ÑπÔ∏è helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts helm install csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver --namespace kube-system --set syncSecret.enabled=true Install the AWS Provider # AWS provider for the Secrets Store CSI Driver allows you to make secrets stored in Secrets Manager appear as files mounted in Kubernetes pods.\n‚ÑπÔ∏è kubectl apply -f https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml 2. Store HPE ML Data Management&rsquo;s Secrets in Secrets Manager # In your Secret Manager Console, click on Store a new secret, select the Other type of Secret (for generic secrets), provide the following Key/Value pairs, then choose a secret name.\nSecret Key Description Value root_token Root clusterAdmin of your cluster Any postgresql_password Password to your database Any OAUTH_CLIENT_SECRET Oauth client secret for Console Required if you set an Enterprise key Any enterprise_license_key Your enterprise license Your enterprise License key pachd_oauth_client_secret Oauth client secret for pachd Any enterprise_secret Needed if you connect to an enterprise server Any Create your secret, then retrieve its arn. It will be needed in the next phase.\n3. Grant Your EKS Cluster Access To Your Secrets Manager # Your cluster has an OpenID Connect issuer URL associated with it. To use IAM roles for service accounts, an IAM OIDC provider must exist for your cluster.\nCreate an IAM OIDC Provider # Before granting your EKS pods the proper permissions to access your secrets, you need to create an IAM OIDC provider for your cluster or retrieve the arn of your provider if you already have one created.\nFollow the steps in AWS user guide\nExample # eksctl utils associate-iam-oidc-provider --cluster=&#34;&lt;cluster-name&gt;&#34; Create An IAM Policy That Grants Read Access To Your Secret # Create a new Policy from your IAM Console Select the JSON tab. Copy/Paste the following text in the JSON tab { &#34;Version&#34;: &#34;2012-10-17&#34;, &#34;Statement&#34;: [ { &#34;Effect&#34;: &#34;Allow&#34;, &#34;Action&#34;: [ &#34;secretsmanager:GetSecretValue&#34;, &#34;secretsmanager:DescribeSecret&#34;, ], &#34;Resource&#34;: [ &lt;!-- Copy the arn of your secret HERE - see example below&gt; &#34;arn:aws:secretsmanager:&lt;region&gt;:&lt;account&gt;„äôÔ∏è&lt;your secret name&gt;&#34; ] } ] } This policy limits the access to the secrets that your EKS cluster needs to access.\nAttach Your Policy To An IAM Role and The Role To Your Service Account # Create an IAM role and attach the IAM policy that you specified to it. The role is associated with a Kubernetes service account created in the namespace that you specify (your cluster&rsquo;s) and annotated with eks.amazonaws.com/role-arn:arn:aws:iam::111122223333:role/my-role-name.\nExample # eksctl create iamserviceaccount \\ --name &#34;&lt;my-service-account&gt;&#34; \\ --cluster &#34;&lt;my-cluster&gt;&#34; \\ --attach-policy-arn \\ &#34;&lt;Copy the arn of your policy HERE&gt;&#34; \\ --approve \\ --override-existing-serviceaccounts 4. Mount Your Secrets In Your EKS Cluster # To show secrets in EKS as though they are files on the filesystem, you need to create a SecretProviderClass YAML file that contains information about your secrets as well as information on how to display them in the EKS pod. Use the file provided below and run kubectl apply -f yoursecretclass.yaml.\nThe SecretProviderClass must be in the same namespace as the EKS cluster.\nmetadata: # Insert your secret name name: pach-secrets spec: provider: aws parameters: objects: | - objectName: &#34;pach-secrets&#34; objectType: &#34;secretsmanager&#34; jmesPath: - path: root_token objectAlias: root-token - path: postgresql_password objectAlias: postgresql-password - path: OAUTH_CLIENT_SECRET objectAlias: OAUTH_CLIENT_SECRET - path: enterprise_license_key objectAlias: enterprise-license-key - path: pachd_oauth_client_secret\tobjectAlias: pachd-oauth-client-secret - path: enterprise_secret objectAlias: enterprise-secret secretObjects: - data: - key: root-token objectName: root-token secretName: root-token type: Opaque - data: - key: postgresql-password objectName: postgresql-password secretName: postgresql-password type: Opaque - data: - key: OAUTH_CLIENT_SECRET objectName: OAUTH_CLIENT_SECRET secretName: console-oauth-client-secret type: Opaque - data: - key: enterprise-license-key objectName: enterprise-license-key secretName: enterprise-license-key type: Opaque - data: - key: pachd-oauth-client-secret\tobjectName: pachd-oauth-client-secret\tsecretName: pachd-oauth-client-secret type: Opaque - data: - key: enterprise-secret objectName: enterprise-secret secretName: enterprise-secret type: Opaque 5. Create A Syncer Pod # Once your secret class is configured, a pod needs to request the class to trigger the CSI driver and retrieve the secrets in Kubernetes. Update the file below with your serviceAccountName and secretProviderClass before you run a kubectl apply -f syncerpod.yaml\napiVersion: v1 kind: Pod metadata: name: secret-syncer spec: containers: - name: secret-syncer image: k8s.gcr.io/pause volumeMounts: - name: secrets-store-inline mountPath: &#34;/mnt/secrets-store&#34; readOnly: true terminationGracePeriodSeconds: 3 serviceAccountName: &#34;&lt;Insert your service account name&gt;&#34; volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: # Insert the name of your Secret Provider secretProviderClass: &#34;pach-secrets&#34; Run a quick kubectl get all to check on your new pod.\n6. Update Secrets In HPE ML Data Management Values.YAML # Finally, using the secretName(s) of your SecretProviderClass above, update HPE ML Data Management&rsquo;s values.YAML with the list of secrets you will be needing.\nChoose the ones that apply to your use case.\ndeployTarget: LOCAL global: postgresql: postgresqlExistingSecretName: postgresql-password postgresqlExistingSecretKey: postgresql-password console: enabled: true config: oauthClientSecretSecretName: console-oauth-client-secret pachd: rootTokenSecretName: root-token enterpriseSecretSecretName: enterprise-secret oauthClientSecretSecretName: pachd-oauth-client-secret\tenterpriseLicenseKeySecretName: enterprise-license-key activateEnterprise: true Your Secrets Manager is now configured to provide credential values to your cluster.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "aws",
        "secrets"
      ],
      "id": "63f7c9a532908c0792559c6fb4395fd3"
    },
    {
      "title": "Local Deploy",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Set Up",
      "description": "Learn how to install locally using your favorite container solution.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/local-deploy/",
      "relURI": "/latest/set-up/local-deploy/",
      "body": " What is a Local Installation? # A local installation means that you will allocate resources from your local machine (e.g., your laptop) to spin up a Kubernetes cluster to run HPE ML Data Management. This installation method is not for a production setup, but is great for personal use, testing, and product exploration.\nWhich Guide Should I Use? # Both the Docker Desktop and Minikube installation guides support MacOS, Windows, and Linux. If this is your first time using Kubernetes, try Docker Desktop &mdash; if you are experienced with Kubernetes, you can deploy using a variety of solutions not listed here (KinD, Rancher Desktop, Podman, etc.).\nüí° Binary Files (Advanced Users)\nYou can download the latest binary files from GitHub for a direct installation of pachctl and the mount-server.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "deployment"
      ],
      "id": "229f733b178c19e8c7421278e65c6228"
    },
    {
      "title": "Docker Desktop",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Local Deploy",
      "description": "Learn how to install locally with Docker Desktop.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/local-deploy/docker/",
      "relURI": "/latest/set-up/local-deploy/docker/",
      "body": " Before You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:\nOpen a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each of the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; All installation steps after 1. Install Docker Desktop must be run through the WSL terminal (Ubuntu) and not in Powershell.\nYou are now ready to continue to Step 1.\nYou can optionally install Homebrew to easily install tools like Helm. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; 1. Install Docker Desktop # Install Docker Desktop for your machine. Navigate to Settings for Mac, Windows, or Linux. Adjust your resources (~4 CPUs and ~12GB Memory) Enable Kubernetes Select Apply &amp; Restart. 2. Install Pachctl CLI # Operating System: MacOs &amp; Windows Debian Linux brew tap pachyderm/tap &amp;&amp; brew install pachyderm/tap/pachctl@2.7 curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_amd64.deb &amp;&amp; sudo dpkg -i /tmp/pachctl.deb AMD\ncurl -L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_linux_amd64.tar.gz | sudo tar -xzv --strip-components=1 -C /usr/local/bin ARM\ncurl-L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_linux_arm64.tar.gz | sudo sudo tar -xzv --strip-components=1 -C /usr/local/bin 3. Install &amp; Configure Helm # Install Helm: brew install helm Add the Pachyderm repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install PachD: üí° Open your browser and check http://localhost before installing. If any other tools are using the same port as HPE ML Data Management, add the following argument to the below command: --set proxy.service.httpPort=8080\nVersion: Community Edition Enterprise helm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Are you using an Enterprise trial key? If so, you can set up Enterprise Pachyderm locally by storing your trial key in a license.txt file and passing it into the following Helm command:\nhelm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer --set pachd.enterpriseLicenseKey=$(cat license.txt) --set ingress.host=localhost A mock user is created by default to get you started, with the username: admin and password: password.\nThis may take several minutes to complete.\n4. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 5. Connect to Cluster # pachctl connect http://localhost:80 ‚ö†Ô∏è If you set the httpPort to a new value, such as 8080, use that value in the command. pachctl connect http://localhost:8080\nOptionally open your browser and navigate to the Console UI.\nüí° You can check your Pachyderm version and connection to pachd at any time with the following command:\npachctl version COMPONENT VERSION pachctl 2.7.3 pachd 2.7.3 ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "docker",
        "linux",
        "mac",
        "windows",
        "getting-started",
        "local-deploy"
      ],
      "id": "c74b7de66e798dcfa9d3d02bafd85133"
    },
    {
      "title": "Minikube",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Local Deploy",
      "description": "Learn how to install locally with Minikube.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/local-deploy/minikube/",
      "relURI": "/latest/set-up/local-deploy/minikube/",
      "body": "Minikube is a tool that quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. It&rsquo;s a great solution for trying out HPE ML Data Management locally.\nBefore You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; You must have Docker Desktop installed with Kubernetes enabled. You must have Docker Desktop installed with Kubernetes enabled. You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:\nOpen a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; All installation steps after this point must be run through the WSL terminal (Ubuntu) and not in Powershell.\nYou are now ready to continue to Step 1.\nYou can optionally install Homebrew to easily install tools like Docker, Minikube, and Helm. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; You must have Docker Desktop installed. 1. Install Docker # brew install docker See the official Docker getting started guide for the most up-to-date installation steps.\n2. Install &amp; Start Minikube # Install # brew install minikube See the official Minikube getting started guide for the most up-to-date installation steps.\nStart # Launch Docker Desktop. Start Minikube: minikube start 3. Install Pachctl CLI # Operating System: MacOs &amp; Windows Debian Linux brew tap pachyderm/tap &amp;&amp; brew install pachyderm/tap/pachctl@2.7 curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_amd64.deb &amp;&amp; sudo dpkg -i /tmp/pachctl.deb AMD\ncurl -o /tmp/pachctl.tar.gz -L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_linux_amd64.tar.gz &amp;&amp; tar -xvf /tmp/pachctl.tar.gz -C /tmp &amp;&amp; sudo cp /tmp/pachctl_2.7.3_linux_amd64/pachctl /usr/local/bin ARM\ncurl -o /tmp/pachctl.tar.gz -L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_linux_arm64.tar.gz &amp;&amp; tar -xvf /tmp/pachctl.tar.gz -C /tmp &amp;&amp; sudo cp /tmp/pachctl_2.7.3_linux_arm64/pachctl /usr/local/bin 4. Install &amp; Configure Helm # Install Helm: brew install helm Add the HPE ML Data Management repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install Pachyderm: Version: Community Edition Enterprise helm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Are you using an Enterprise trial key? If so, you can set up Enterprise HPE ML Data Management locally by storing your trial key in a license.txt file and passing it into the following Helm command:\nhelm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer --set pachd.enterpriseLicenseKey=$(cat license.txt) --set ingress.host=localhost This unlocks Enterprise features but also requires user authentication to access Console. A mock user is created by default to get you started, with the username: admin and password: password.\nThis may take several minutes to complete.\n5. Verify Installation # Run the following command in a new terminal to check the status of your pods: kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE default console-6b9bb8766d-f2gm4 1/1 Running 0 41m default etcd-0 1/1 Running 0 41m default pachd-76896d6b5d-kmfvw 1/1 Running 0 41m default pachd-loki-0 1/1 Running 0 41m default pachd-promtail-rm5ss 1/1 Running 0 41m default pachyderm-kube-event-tail-b9b554fb6-dpcsr 1/1 Running 0 41m default pg-bouncer-5c9494c678-z57qh 1/1 Running 0 41m default postgres-0 1/1 Running 0 41m kube-system coredns-6d4b75cb6d-jnl5f 1/1 Running 3 (42m ago) 97d kube-system etcd-minikube 1/1 Running 4 (42m ago) 97d kube-system kube-apiserver-minikube 1/1 Running 3 (42m ago) 97d kube-system kube-controller-manager-minikube 1/1 Running 4 (42m ago) 97d kube-system kube-proxy-bngzv 1/1 Running 3 (42m ago) 97d kube-system kube-scheduler-minikube 1/1 Running 3 (42m ago) 97d kube-system storage-provisioner 1/1 Running 5 (42m ago) 97d kubernetes-dashboard dashboard-metrics-scraper-78dbd9dbf5-swttf 1/1 Running 3 (42m ago) 97d kubernetes-dashboard kubernetes-dashboard-5fd5574d9f-c7ptx 1/1 Running 4 (42m ago) 97d Re-run this command after a few minutes if pachd is not ready. 6. Connect to Cluster # Run minikube tunnel to start a tunnel. In a separate terminal, get the external IP address of the pachyderm-proxy service: kubectl get all service/pachyderm-proxy LoadBalancer 10.110.148.228 127.0.0.1 80:32024/TCP Run the following command to connect to your cluster: pachctl connect 127.0.0.1:80 Optionally open your browser and navigate to the Console UI.\nüí° You can check your HPE ML Data Management version and connection to pachd at any time with the following command:\npachctl version COMPONENT VERSION pachctl 2.7.3 pachd 2.7.3 ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "minikube",
        "getting-started",
        "local-deploy",
        "linux",
        "mac",
        "windows"
      ],
      "id": "91b4aedda821f2202d0649c99fd9ec6b"
    },
    {
      "title": "On-Prem Deploy",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Set Up",
      "description": "Learn how to install on your premises.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/on-prem/",
      "relURI": "/latest/set-up/on-prem/",
      "body": " Before you start # Before you can deploy HPE ML Data Management, you will need to perform the following actions:\nInstall kubectl Install Helm Deploy Kubernetes on-premises. Deploy two Kubernetes persistent volumes for HPE ML Data Management metadata storage. Deploy an on-premises object store using a storage provider like MinIO, EMC&rsquo;s ECS, or SwiftStack to provide S3-compatible access to your data storage. Install PachCTL and PachCTL Auto-completion. How to Deploy HPE ML Data Management On-Premises # 1. Install HPE ML Data Management via Helm # helm repo add pachyderm https://helm.pachyderm.com helm repo update 2. Configure Helm Values # View and copy a full helm chart from GitHub or ArtifactHub for reference when configuring your Helm values file. You can quickly explore the options for different sections of the Helm chart from our Helm series documentation.\nAdd Storage classes to Helm Values # Update your Helm values file to include the storage classes you are going to use:\netcd: storageClass: MyStorageClass size: 10Gi postgresql: persistence: storageClass: MyStorageClass size: 10Gi Size &amp; Configure Object Store # Determine the endpoint of your object store, for example minio-server:9000. Choose a unique name for the bucket you will dedicate to HPE ML Data Management. Create a new access key ID and secret key for HPE ML Data Management to use when accessing the object store. Update the HPE ML Data Management Helm values file with the endpoint, bucket name, access key ID, and secret key. pachd: storage: backend: minio minio: endpoint: minio-server:9000 bucket: pachyderm-bucket id: pachyderm-access-key secret: pachyderm-secret-key secure: false Configure Authentication &amp; Authorization # To set up Authentication, you must use the Enterprise version of HPE ML Data Management and provide a valid license key.\nWe recommend that you create a secret and provide it on the Helm chart as the value to the attribute pachd.enterpriseLicenseKeySecretName. Once deployed, Pachyderm stores your provided Enterprise license as the platform secret pachyderm-license in the key enterprise-license-key.\n‚ÑπÔ∏è After deploying HPE ML Data Management, you can log in as the root user and begin to add users to certain resource types such as Projects and Repos.\npachctl auth set project &lt;project-name&gt; &lt;role-name&gt; user:&lt;username@email.com&gt; For more information on user permissions, see the Authorization section.\n3. Deploy # helm install pachyderm -f values.yaml pachyderm/pachyderm --version &lt;your_chart_version&gt; üí° You can update your Helm values file using the following command:\nhelm upgrade pachyderm pachyderm/pachyderm -f values.yml ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "deployment"
      ],
      "id": "848da01f077dfda9db8d1d34e3d59b03"
    },
    {
      "title": "Pachctl",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Set Up",
      "description": "Learn how to install PachCTL.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/pachctl/",
      "relURI": "/latest/set-up/pachctl/",
      "body": " Operating System: MacOs, Windows, &amp; Darwin Debian Linux brew tap pachyderm/tap &amp;&amp; brew install pachyderm/tap/pachctl@2.7 curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_amd64.deb &amp;&amp; sudo dpkg -i /tmp/pachctl.deb AMD\ncurl -L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_linux_amd64.tar.gz | sudo tar -xzv --strip-components=1 -C /usr/local/bin ARM\ncurl-L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_linux_arm64.tar.gz | sudo sudo tar -xzv --strip-components=1 -C /usr/local/bin ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "deployment"
      ],
      "id": "aa262acfc681cc1f1e4437ea727e6b93"
    },
    {
      "title": "Pachctl Auto-completion",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Set Up",
      "description": "Learn how to install our auto-completion helper tool (it's great for learning PachCTL commands).",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/pachctl-autocomplete/",
      "relURI": "/latest/set-up/pachctl-autocomplete/",
      "body": "HPE ML Data Management auto-completion allows you to automatically finish partially typed commands by pressing TAB.\nBefore You Start # You must already have PachCTL installed How to Install Auto-Completion # Command Shell: Zsh Bash Verify that bash-completion is installed on your machine. For example, if you have installed bash completion by using Homebrew, type:\nbrew info bash-completion This command returns information about the directory in which bash-completion and bash completion scripts are installed. For example, /usr/local/etc/bash_completion.d/. You need to specify the path to bash_completion.d as the path to which install pachctl autocompletion. Also, the output of the info command might have a suggestion to include the path to bash-completion into your ~/.bash_profile file.\nInstall pachctl autocompletion:\npachctl completion bash --install --path &lt;path/to/bash-completion&gt; For example, if you specify the path to bash-completion as /usr/local/etc/bash_completion.d/pachctl, your system response looks like this:\nSystem response:\nBash completions installed in /usr/local/etc/bash_completion.d/pachctl, you must restart bash to enable completions. Restart your terminal.\npachctl autocomplete should now be enabled in your system.\nInstall zsh-completions:\nbrew install zsh-completions Install pachctl autocompletion:\nüí° You&rsquo;ll need to install this in the same directory your zsh-completions are installed in. You can run the following to find the correct path:\necho $fpath pachctl completion bash --install --path /opt/homebrew/share/zsh-completions/_pachctl Restart your terminal; pachctl autocomplete should now be enabled in your system.\nIf you run into warnings in your terminal related to zsh compinit: insecure directories, you can run the following to fix it:\nchmod go-w /opt/homebrew/share Testing # You can perform the following tests to verify that pachctl autocompletion is working:\nOpen a new terminal.\nInput the following:\npachctl v Hit TAB. You should see the following output:\nvalidate -- Validate the specification of a Pachyderm resource. version -- Print Pachyderm version information. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "deployment"
      ],
      "id": "8d1212be4f05bf5e44d640aaf9bd7afc"
    },
    {
      "title": "Authentication & IdP Connectors",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Set Up",
      "description": "Learn how to enable users to access a cluster using their preferred identity provider.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/connectors/",
      "relURI": "/latest/set-up/connectors/",
      "body": "HPE ML Data Management has an embedded Open ID Connect based on Dex, allowing for vendor-neutral authentication using your existing credentials from various back-ends. You can enable users to authenticate to a HPE ML Data Management cluster using their favorite Identity Providers by following the articles in this section.\nWhen you enable authentication, you gain access to HPE ML Data Management&rsquo;s authorization features. You can use HPE ML Data Management&rsquo;s Role-Based Access Control (RBAC) model to configure authorization for your users and assign roles that grant certain permissions for interacting with HPE ML Data Management&rsquo;s resources.\nUseful Auth PachCTL Commands # Command Description pachctl auth login Logs in to the cluster pachctl auth logout Logs out of the cluster pachctl auth whoami Returns the current user‚Äôs username pachctl auth get-groups Returns the current user‚Äôs groups pachctl auth get-config Returns the instance‚Äôs current auth configuration pachctl auth get cluster Returns the role bindings for the cluster pachctl auth get project &lt;project-name&gt; Returns the role bindings for a project pachctl auth get repo &lt;repo-name&gt; Returns the role bindings for a repo ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "de5368dbc3d87216edeb8c36527bc41d"
    },
    {
      "title": "MockIDP",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Authentication & IdP Connectors",
      "description": "Learn how to authenticate with MockIDP for testing and development purposes.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/connectors/mockidp/",
      "relURI": "/latest/set-up/connectors/mockidp/",
      "body": "MockIDP is used for testing and development so that you can run HPE ML Data Management and experiment with permissions locally without having to set up an external identity provider.\nBefore You Start # You must have Enterprise enabled on your cluster How to Activate MockIDP # The following steps assume that you are deploying locally and wish to have Console enabled.\nObtain your current user-input helm values:\nhelm get values pachyderm &gt; values.yaml Add Console configuration:\nconsole: disableTelemetry: true config: oauthRedirectURI: http://localhost/oauth/callback/?inline=true oauthClientSecret: &#39;123&#39; Add OIDC configuration:\noidc: issuerURI: &#34;http://pachd:30658/dex&#34; userAccessibleOauthIssuerHost: http://localhost A minimal values.yaml file should look like this:\ndeployTarget: LOCAL pachd: enterpriseLicenseKeySecretName: pachyderm-enterprise-key proxy: enabled: true host: localhost service: type: LoadBalancer console: disableTelemetry: true config: oauthRedirectURI: http://localhost/oauth/callback/?inline=true oauthClientSecret: &#39;123&#39; oidc: issuerURI: &#34;http://pachd:30658/dex&#34; userAccessibleOauthIssuerHost: http://localhost Upgrade your cluster:\nhelm upgrade pachyderm pachyderm/pachyderm -f values.yaml Connect:\npachctl connect grpc://localhost:80 Log in via the browser at localhost or via the CLI:\npachctl auth login user: admin password: password Verify that you are logged in:\npachctl auth whoami You are &#34;user:kilgore@kilgore.trout&#34; session expires: 21 Sep 23 18:07 UTC List your projects to view your permissions/access level:\npachctl list projects ACTIVE PROJECT ACCESS_LEVEL CREATED DESCRIPTION default [clusterAdmin projectWriter] 3 days ago - * batch-inference-1 [clusterAdmin projectOwner] About an hour ago - You can now test and develop locally with MockIDP and Console enabled.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "identity-providers",
        "permissions",
        "management",
        "mockidp"
      ],
      "id": "f71089065af9a7468b431d05fa62e46c"
    },
    {
      "title": "Auth0",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Authentication & IdP Connectors",
      "description": "Learn how to enable authentication through Auth0 as an Identity Provider.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/connectors/auth0/",
      "relURI": "/latest/set-up/connectors/auth0/",
      "body": " Before You Start # You must have the following enabled on your cluster: Enterprise TLS You must have an Auth0 account You should know the value of your proxy.host setting in your Helm values.yaml file How to Enable Auth0 as an IdP # ‚ö†Ô∏è The following guide does not cover how to create user pools in Auth0; once initially configured, any user with a Gmail account can log in to your HPE ML Data Management instance. Refer to the official Auth0 documentation on how to manage users.\n1. Create an App on Auth0 # Log in to your Auth0 account. In Applications, click Create Application. Type the name of your application, such as HPE ML Data Management. In the application type, select Regular Web Application. Click Create. Go to the application settings. Scroll down to Application URIs. In the Allowed Callback URLs, add the HPE ML Data Management callback link in the following format: https://&lt;your.proxy.host.value&gt;/dex/callback Scroll down to Show Advanced Settings. Select Grant Types. Verify that Authorization Code and Refresh Token are selected. 2. Define Helm Config # The following steps add the OIDC section to your Helm chart. When an upstream IdP is successfully added to the list, HPE ML Data Management&rsquo;s default MockIdP is disabled automatically. You can add multiple IdPs to upstreamIDPs.\nNavigate to your values.yamls file or obtain your current Helm values.yaml overrides: helm get values pachyderm &gt; values.yaml Add the following section: Syntax: json yaml { &#34;oidc&#34;: { &#34;upstreamIDPs&#34;: [ { &#34;type&#34;: &#34;oidc&#34;, &#34;id&#34;: &#34;auth0&#34;, &#34;name&#34;: &#34;Auth0&#34;, &#34;config&#34;: { &#34;issuer&#34;: &#34;https://&lt;auth0.app.domain.url&gt;/&#34;, &#34;clientID&#34;: &#34;FbTzaVdABC9TbX07pXqxHwofuEOux004&#34;, &#34;clientSecret&#34;: &#34;1kbxtx22DLLMNOrjJgV-RaaUsmTzGoQ3h4UEeQ2hmduP1qPLK5yTOsrmwwVNXP9U&#34;, &#34;redirectURI&#34;: &#34;https://&lt;proxy.host.value.com&gt;/dex/callback&#34;, &#34;insecureEnableGroups&#34;: true, &#34;insecureSkipEmailVerified&#34;: true, &#34;insecureSkipIssuerCallbackDomainCheck&#34;: false } } ] } } oidc: upstreamIDPs: - type: oidc id: auth0 name: Auth0 config: issuer: https://&lt;auth0.app.domain.url&gt;/ clientID: FbTzaVdFCB9TbX07pXqxBwofuEOux004 clientSecret: 1kbxtx22DLGSULrjJgV-TaaUsmTzGoQ3h4UZeQ2hmduP1qPLK5yTOsrmwwVNXP9U redirectURI: https://&lt;proxy.host.value.com&gt;/dex/callback insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: false ‚ÑπÔ∏è Note that HPE ML Data Management&rsquo;s YAML format is a simplified version of Dex&rsquo;s sample config.\nUpdate the following attributes: Field Description issuer The Auth0 App&rsquo;s domain URL, found under Settings &gt; Basic Information; must have https:// and a trailing slash /. clientID The Auth0 App&rsquo;s client ID, found under Settings &gt; Basic Information. clientSecret The Auth0 App&rsquo;s client secret, found under Settings &gt; Basic Information. redirectURI A combination of your proxy host value and /dex/callback. For example, https://console.pachdemo.com/dex/callback. Save your changes and upgrade your cluster: helm upgrade pachyderm pachyderm/pachyderm -f values.yaml üí° Alternatively, you can create a secret containing your dex connectors (Key: upstream-idps) and reference its name in the field oidc.upstreamIDPsSecretName.\n3. Login # The users registered with your IdP are now ready to Log in to HPE ML Data Management\nTroubleshooting # PachD CrashLoopBackOff # If you encounter a CrashLoopBackOff error after running the kubectl get pods command, it&rsquo;s likely that one of the following needs to be fixed:\nyour issuer value is incorrect (it must have https:// and a trailing slash /). you have an unexpected field such as version in the config section oidc.updstreamIDPs entry. Example Error in PachD Pod logs\nYou can obtain your pod logs by running: kubectl logs &lt;pachd-pod-name&gt; &gt; logs.txt\ncreate connector with ID: &#34;auth0&#34;: unable to open connector: failed to get provider: oidc: issuer did not match the issuer returned by provider, expected &#34;https://&lt;auth0.app.domain.url&gt;/&#34; got &#34;https://&lt;auth0.app.domain.url&gt;&#34; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "identity-providers",
        "permissions",
        "management",
        "integrations"
      ],
      "id": "a6cfda4e2b4f9799c0cc0f1c92ded0e5"
    },
    {
      "title": "Okta",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Authentication & IdP Connectors",
      "description": "Learn how to authenticate with Okta.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/connectors/okta/",
      "relURI": "/latest/set-up/connectors/okta/",
      "body": " Before You Start # You must have the following enabled on your cluster: Enterprise TLS You must have an Okta account You should know the value of your proxy.host setting in your Helm values.yaml file How to Enable Okta as an IdP # 1. Create an App on Okta # Log in to Okta. Navigate to Applications &gt; Applications. Select Create App Integration. Choose the OIDC - OpenID Connect sign-in method. Choose the Web Application application type. Click Next. Name the application, such as HPE ML Data Management. Navigate to General Settings &gt; Grant Type and check the following: Authorization Code Refresh Token Navigate to Sign-in Redirect URIs and input the following: https://&lt;your.proxy.host.value&gt;/dex/callback Navigate to Assignments and select your preferred Controlled Access policy. Click Save. 2. Define Helm Config # The following steps add the OIDC section to your Helm chart. When an upstream IdP is successfully added to the list, HPE ML Data Management&rsquo;s default MockIdP is disabled automatically. You can add multiple IdPs to upstreamIDPs.\nNavigate to your values.yamls file or obtain your current Helm values.yaml overrides: helm get values pachyderm &gt; values.yaml Add the following section: Syntax: json yaml { &#34;oidc&#34;: { &#34;upstreamIDPs&#34;: [ { &#34;type&#34;: &#34;oidc&#34;, &#34;id&#34;: &#34;okta&#34;, &#34;name&#34;: &#34;Okta&#34;, &#34;config&#34;: { &#34;issuer&#34;: &#34;https://trial-1839456.okta.com/&#34;, &#34;clientID&#34;: &#34;0oa74mh2scJf29qOD697&#34;, &#34;clientSecret&#34;: &#34;VNwbzOBltNcaotD2CU5iRyTuqOPpwLR-RC16ai7wakta95W00p7X5HYkEgS_5UWH&#34;, &#34;redirectURI&#34;: &#34;https://&lt;proxy.host.value.com&gt;/dex/callback&#34;, &#34;insecureEnableGroups&#34;: true, &#34;insecureSkipEmailVerified&#34;: true, &#34;insecureSkipIssuerCallbackDomainCheck&#34;: false } } ] } } oidc: upstreamIDPs: - type: oidc id: okta name: Okta config: issuer: https://trial-1839456.okta.com/ clientID: 0oa74mh2scJf29qOD697 clientSecret: VNwbzOBltNcaotD2CU5iRyTuqOPpwLR-RC16ai7wakta95W00p7X5HYkEgS_5UWH redirectURI: https://&lt;proxy.host.value.com&gt;/dex/callback insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: false ‚ÑπÔ∏è Note that HPE ML Data Management&rsquo;s YAML format is a simplified version of Dex&rsquo;s sample config.\nUpdate the following attributes: Field Description issuer The Okta App&rsquo;s domain URL, found under Sign On &gt; OpenID Connect ID Token; must have https://. clientID The Okta App&rsquo;s client ID, found under General &gt; Client Credentials. clientSecret The Okta App&rsquo;s client secret, found under General &gt; Client Secrets. redirectURI A combination of your proxy host value and /dex/callback. For example, https://console.pachdemo.com/dex/callback. Save your changes and upgrade your cluster: helm upgrade pachyderm pachyderm/pachyderm -f values.yaml üí° Alternatively, you can create a secret containing your dex connectors (Key: upstream-idps) and reference its name in the field oidc.upstreamIDPsSecretName.\n3. Login # The users registered with your IdP are now ready to Log in to HPE ML Data Management\nTroubleshooting # PachD CrashLoopBackOff # If you encounter a CrashLoopBackOff error after running the kubectl get pods command, it&rsquo;s likely that one of the following needs to be fixed:\nyour issuer value is incorrect (sometimes it needs a trailing slash /, it should match exactly what you see in Okta). you have an unexpected field such as version in the config section oidc.updstreamIDPs entry. Example Error in PachD Pod logs\nYou can obtain your pod logs by running: kubectl logs &lt;pachd-pod-name&gt; &gt; logs.txt\ncreate connector with ID: &#34;okta&#34;: unable to open connector: failed to get provider: oidc: issuer did not match the issuer returned by provider, expected &#34;https://trial-1839456.okta.com/&#34; got &#34;https://trial-1839456.okta.com&#34; Okta QR Code / Login Link Doesn&rsquo;t Work # You may need to download the Okta Verify app on your mobile device and scan the QR code through the app log in.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "identity-providers",
        "permissions",
        "management",
        "integrations"
      ],
      "id": "1683a7cf49d1bfd290616d206f0a0462"
    },
    {
      "title": "Authorization (RBAC)",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Set Up",
      "description": "Learn how to set up and manage Role-Based Access Control (RBAC).",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/authorization/",
      "relURI": "/latest/set-up/authorization/",
      "body": "You can use HPE ML Data Management&rsquo;s Role-Based Access Control (RBAC) model to configure authorization for your users. Users can be assigned roles that grant certain permissions for interacting with HPE ML Data Management&rsquo;s resources.\nAuthorization Model # Users Types # User Type Prefix Description clusterAdmin user IdP User user Any user or group of users authenticated by your Identity Provider to access HPE ML Data Management. Robot User robot A Service account used for third party applications/systems integrating with HPE ML Data Management APIs/Clients. Pipeline User pipeline An internal Service Account used for Pipelines when interacting with HPE ML Data Management resources. All Cluster Users A general subject that represents everyone who has logged in to a cluster. Resource Types # Resource Type Description Cluster A set of nodes for running containerized applications. Containers allow users to run repeatable and standardized code. Project A project is a container of 1 or more DAGs that allows for users to organize their repos. Projects allow multiple teams to work in a cluster. Repo A repository is where data is stored and contains both files and folders. Repos tracks all changes to the data and creates a history of data changes. Role Types # Role Type Description Cluster Roles Granted at the cluster level. Project Roles Granted at the project level. Repo Roles Granted at the repo level or at the cluster level. License Expiration # When an Enterprise License expires, a HPE ML Data Management cluster with enabled User Access Management goes into an admin-only state. In this state, only ClusterAdmins have access to the data stored in HPE ML Data Management. This safety measure keeps sensitive data protected, even when an enterprise subscription becomes stale. To return the cluster to its previous state, run pachctl license activate and submit your new code.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "720160f18747e0daa7e5ad86359852d1"
    },
    {
      "title": "Access Control (RBAC) Roles & Permissions",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Authorization (RBAC)",
      "description": "Learn how to manage access to resources using roles.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/authorization/permissions/",
      "relURI": "/latest/set-up/authorization/permissions/",
      "body": "This page describes how HPE ML Data Management&rsquo;s access control system works and how you can use it to manage access in HPE ML Data Management. Use RBAC to grant granular access to specific HPE ML Data Management resources.\nHow RBAC Works # Role-based Access Control works by managing access for users (human or robot) through assigned roles. Roles contain a set of granular permissions (create, read, update, delete) for a given resource. In HPE ML Data Management, resources include clusters, projects, and repositories.\nA user can have many roles, and some roles encompass the permissions of other roles. For example, if you have a clusterAdmin, all other permissions belonging to more restricted roles are included.\nüí° You can use the command pachctl auth roles-for-permission &lt;permission&gt; to look up which roles provide a given permission.\nAdmin Roles # clusterAdmin # The clusterAdmin role includes all of the previous permissions, plus the following:\nPermission CLUSTER_MODIFY_BINDINGS CLUSTER_GET_BINDINGS CLUSTER_AUTH_ACTIVATE CLUSTER_AUTH_DEACTIVATE CLUSTER_AUTH_GET_CONFIG CLUSTER_AUTH_SET_CONFIG CLUSTER_AUTH_MODIFY_GROUP_MEMBERS CLUSTER_AUTH_GET_GROUPS CLUSTER_AUTH_GET_GROUP_USERS CLUSTER_AUTH_EXTRACT_TOKENS CLUSTER_AUTH_RESTORE_TOKEN CLUSTER_AUTH_ROTATE_ROOT_TOKEN CLUSTER_AUTH_DELETE_EXPIRED_TOKENS CLUSTER_AUTH_GET_PERMISSIONS_FOR_PRINCIPAL CLUSTER_AUTH_REVOKE_USER_TOKENS CLUSTER_ENTERPRISE_ACTIVATE CLUSTER_ENTERPRISE_HEARTBEAT CLUSTER_ENTERPRISE_GET_CODE CLUSTER_ENTERPRISE_DEACTIVATE CLUSTER_DELETE_ALL CLUSTER_ENTERPRISE_PAUSE oidcAppAdmin # Permission CLUSTER_IDENTITY_DELETE_OIDC_CLIENT CLUSTER_IDENTITY_CREATE_OIDC_CLIENT CLUSTER_IDENTITY_UPDATE_OIDC_CLIENT CLUSTER_IDENTITY_LIST_OIDC_CLIENTS CLUSTER_IDENTITY_GET_OIDC_CLIENT idpAdmin # Permission CLUSTER_IDENTITY_CREATE_IDP CLUSTER_IDENTITY_UPDATE_IDP CLUSTER_IDENTITY_LIST_IDPS CLUSTER_IDENTITY_GET_IDP CLUSTER_IDENTITY_DELETE_IDP secretAdmin # Permission CLUSTER_CREATE_SECRET CLUSTER_LIST_SECRETS SECRET_INSPECT SECRET_DELETE identityAdmin # Permission CLUSTER_IDENTITY_SET_CONFIG CLUSTER_IDENTITY_GET_CONFIG licenseAdmin # Permission CLUSTER_LICENSE_ACTIVATE CLUSTER_LICENSE_GET_CODE CLUSTER_LICENSE_ADD_CLUSTER CLUSTER_LICENSE_UPDATE_CLUSTER CLUSTER_LICENSE_DELETE_CLUSTER CLUSTER_LICENSE_LIST_CLUSTERS Project Roles # All users have the PROJECT_LIST_REPO and PROJECT_CREATE_REPO permissions by default.\nüí° You can view your access level by running the command pachctl list project and checking the ACCESS_LEVEL column.\nprojectViewer # Permission PROJECT_LIST_REPO projectWriter # The projectWriter role includes all of the projectViewer permissions, plus the following:\nPermission PROJECT_CREATE_REPO projectOwner # Permission PROJECT_DELETE PROJECT_MODIFY_BINDINGS projectCreator # Permission PROJECT_CREATE Repo Roles # repoReader # Permission REPO_READ REPO_INSPECT_COMMIT REPO_LIST_COMMIT REPO_LIST_BRANCH REPO_LIST_FILE REPO_INSPECT_FILE REPO_ADD_PIPELINE_READER REPO_REMOVE_PIPELINE_READER PIPELINE_LIST_JOB repoWriter # The repoWriter role includes all of the repoReader permissions, plus the following:\nPermission REPO_WRITE REPO_DELETE_COMMIT REPO_CREATE_BRANCH REPO_DELETE_BRANCH REPO_ADD_PIPELINE_WRITER repoOwner # The repoOwner role includes all of the repoWriter and repoReader permissions, plus the following:\nPermission REPO_MODIFY_BINDINGS REPO_DELETE Misc Roles # debugger # Permission CLUSTER_DEBUG_DUMP CLUSTER_GET_PACHD_LOGS robotUser # Permission CLUSTER_AUTH_GET_ROBOT_TOKEN pachdLogReader # Permission CLUSTER_GET_PACHD_LOGS ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "permissions",
        "management",
        "roles",
        "RBAC"
      ],
      "id": "32aaad4798955907b0d660c5339dd5e1"
    },
    {
      "title": "Manage RBAC via Console",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Authorization (RBAC)",
      "description": "Learn how to grant and modify roles on given resources for a user.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/authorization/add-roles-console/",
      "relURI": "/latest/set-up/authorization/add-roles-console/",
      "body": " Before You Start # You must have an active Enterprise key You must have TLS enabled on your cluster You must have an Authentication Provider (IdP) set up Auth0 Okta Review the Roles &amp; Permissions. Review the User Types Confirm you have the right role(s) to grant a user access to a given resource (e.g., you have the projectOwner role on a given project you wish to add other users to) Cluster-level admin roles are not currently implementable via Console How to Assign Roles to a User # On a Project # Roles granted at the Project level are inherited by all repositories within that project. If you grant a user repoReader on a project, they will have repoReader on all repositories within that project and that role will not be removable on the repo level.\nLog in to the HPE ML Data Management Console. Scroll to a project you wish to add a user to. Select the ellipsis icon &gt; Edit Project Roles Select a User Type from the dropdown: user: an individual by name or email address; requires that user&rsquo;s email address be registered or available to your IdP (e.g., either explicitly listed or allowed via your email domain) group: a group of users; requires that your IdP supports groups tied to an email address robot: a service account allClusterUsers: all users on the cluster If not allClusterUsers, provide a name or email address. Select a Role from the dropdown. projectViewer: Can view the project and see a list of its repositories. projectWriter: projectViewer permissions + can also create repositories. projectOwner: projectWriter permissions + can also delete repositories and modify role bindings. repoReader: Can read every repository in the project. repoWriter repoReader permissions + can also push to every repository in the project. repoOwner repoWriter permissions + can also delete repositories and modify role bindings. Select Add. Select Done. üí° You can add more roles to a user by selecting the plus icon and remove them by selecting the X icon . On a Repository # Roles granted at the Repository level are not inherited by other repositories within that project. This is useful if you want to grant a user repoReader on a single repository within a project, but not on all repositories within that project.\nLog in to the HPE ML Data Management Console. Select a View Project on the project containing the repository you wish to add a user to. Select the repo (either from the DAG view or the List view). Select Set Roles. Select a User Type from the dropdown. If not allClusterUsers, provide a name or email address. Select a Role from the dropdown. Select Add. Select Done. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "permissions",
        "management",
        "roles",
        "rbac"
      ],
      "id": "35f3f41ef18fbabae3f9d08a3f299d93"
    },
    {
      "title": "Add Roles to User via PachCTL",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Authorization (RBAC)",
      "description": "Learn how to grant and modify permissions on given resources for a user.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/authorization/add-user-roles/",
      "relURI": "/latest/set-up/authorization/add-user-roles/",
      "body": " Before You Start # You must have an active Enterprise key You must have TLS enabled on your cluster You must have an Authentication Provider (IdP) set up Auth0 Okta Review the Access Control (RBAC) Roles &amp; Permissions. Confirm you have the right role(s) to grant a user access to a given resource (e.g., you have the projectOwner role on a given project you wish to add other users to) üí° You can check your current roles and permissions on a given project by running the following:\npachctl auth check project &lt;project-name&gt; Roles: [projectOwner] Permissions: [REPO_READ REPO_INSPECT_COMMIT REPO_LIST_COMMIT REPO_LIST_BRANCH REPO_LIST_FILE REPO_INSPECT_FILE REPO_ADD_PIPELINE_READER REPO_REMOVE_PIPELINE_READER PIPELINE_LIST_JOB REPO_WRITE REPO_DELETE_COMMIT REPO_CREATE_BRANCH REPO_DELETE_BRANCH REPO_ADD_PIPELINE_WRITER REPO_MODIFY_BINDINGS REPO_DELETE PROJECT_LIST_REPO PROJECT_CREATE_REPO PROJECT_DELETE PROJECT_MODIFY_BINDINGS] How to Assign Roles to a User # As Root Admin # This guide assumes resources (projects, repositories) have already been created in your cluster.\n‚ÑπÔ∏è You can skip steps 2 and 3 if you are using the MockIdP connector and just want to explore/practice, as you are already logged in as the admin user. Even though you can assign permissions to new users in MockIdP, you cannot log in as them.\nOpen your terminal. Connect as the root user using the following command: pachctl auth use-auth-token Input your root token. If you did not initially set a pachd.rootToken or pachd.rootTokenSecretName in your Helm values.yaml configuration, the root token is autogenerated as a Kubernetes secret named pachyderm-auth that can be decoded and read using the following command: kubectl get secret pachyderm-auth -o jsonpath=&#34;{.data.root-token}&#34; | base64 --decode Verify you are connected as the root user by running the following command: pachctl auth whoami You are &#34;pach:root&#34; Run one of the following commands to assign a role: Resource Type: Project Repo Other All pachctl auth set project &lt;project-name&gt; &lt;role-name&gt; user:&lt;username@email.com&gt; pachctl auth set repo &lt;repo-name&gt; &lt;role-name&gt; user:&lt;username@email.com&gt; pachctl auth set enterprise clusterAdmin user:&lt;email&gt; pachctl auth set &lt;resource&gt; &lt;resource-name&gt; [role1,role2 | none ] &lt;prefix:subject&gt; Admin Roles Project Roles Repo Roles Misc Roles clusterAdmin projectViewer repoReader debugger oidcAppAdmin projectWriter repoWriter robotUser idpAdmin projectOwner repoOwner pachdLogReader secretAdmin projectCreator identityAdmin licenseAdmin Confirm access by running the following command: Resource Type: Project Repo pachctl auth get project &lt;project-name&gt; user:lawrence.lane@hpe.com: [projectOwner] pachctl auth get repo &lt;repo-name&gt; user:lawrence.lane@hpe.com: [repoOwner] You can also use these steps to update a users permissions.\nAs Project Owner # Open your terminal. Log in. pachctl auth login Add a user and assign their role to a project that you own. Resource Type: Project Repo Other All pachctl auth set project &lt;project-name&gt; &lt;role-name&gt; user:&lt;username@email.com&gt; pachctl auth set repo &lt;repo-name&gt; &lt;role-name&gt; user:&lt;username@email.com&gt; pachctl auth set enterprise clusterAdmin user:&lt;email&gt; pachctl auth set &lt;resource&gt; &lt;resource-name&gt; [role1,role2 | none ] &lt;prefix:subject&gt; Confirm access by running the following command: Resource Type: Project Repo pachctl auth get project &lt;project-name&gt; user:lawrence.lane@hpe.com: [projectOwner] pachctl auth get repo &lt;repo-name&gt; user:lawrence.lane@hpe.com: [repoOwner] ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "permissions",
        "management"
      ],
      "id": "e720b830c1a8e39a3c5559efc301d534"
    },
    {
      "title": "Add Roles to Group via PachCTL",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Authorization (RBAC)",
      "description": "Learn how to grant and modify permissions on given resources for a group of users.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/authorization/add-group-roles/",
      "relURI": "/latest/set-up/authorization/add-group-roles/",
      "body": " Before You Start # You must have an active Enterprise key You must have an Authentication Provider (IdP) set up that supports groups Auth0 Okta Review the Access Control (RBAC) Roles &amp; Permissions. Confirm you have the right role(s) to grant a user access to a given resource (e.g., you have the projectOwner role on a given project you wish to add other users to) How to Assign Roles to a Group # This guide uses Auth0 and assumes resources (projects, repositories) have already been created in your cluster.\nEnable group management in your IdP of choice . Update your connector config to include the appropriate attributes. Syntax: JSON YAML { &#34;type&#34;: &#34;oidc&#34;, &#34;id&#34;: &#34;auth0&#34;, &#34;name&#34;: &#34;Auth0&#34;, &#34;version&#34;: 1, &#34;config&#34;:{ &#34;issuer&#34;: &#34;https://dev-k34x5yjn.us.auth0.com/&#34;, &#34;clientID&#34;: &#34;hegmOc5rTotLPu5ByRDXOvBAzgs3wuw5&#34;, &#34;clientSecret&#34;: &#34;7xk8O71Uhp5T-bJp_aP2Squwlh4zZTJs65URPma-2UT7n1iigDaMUD9ArhUR-2aL&#34;, &#34;redirectURI&#34;: &#34;http(s)://&lt;insert-external-ip-or-dns-name&gt;/dex/callback&#34;, &#34;scopes&#34;: [&#34;groups&#34;, &#34;email&#34;, &#34;profile&#34;], &#34;claimMapping&#34;:{ &#34;groups&#34;: &#34;http://pachyderm.com/groups&#34; }, &#34;insecureEnableGroups&#34;: true } } type: oidc id: auth0 name: Auth0 version: 1 config: issuer: https://dev-k34x5yjn.us.auth0.com/ clientID: hegmOc5rTotLPu5ByRDXOvBAzgs3wuw5 clientSecret: 7xk8O71Uhp5T-bJp_aP2Squwlh4zZTJs65URPma-2UT7n1iigDaMUD9ArhUR-2aL redirectURI: http(s)://&lt;insert-external-ip-or-dns-name&gt;/dex/callback scopes: - groups - email - profile claimMapping: groups: http://pachyderm.com/groups insecureEnableGroups: true Update the config by running the following command: pachctl idp update-connector &lt;connector-id&gt; --version 2 Grant the group roles by running the following command: pachctl auth set &lt;resource-type&gt; &lt;resource-name&gt; &lt;role-name&gt; group:&lt;group-name&gt; Confirm the group&rsquo;s roles were updated for the given resource: Resource Type: Project Repo pachctl auth get project &lt;project-name&gt; pachctl auth get repo &lt;repo-name&gt; üí° The command pachctl auth get-groups lists the groups that have been defined on your cluster.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "permissions",
        "management"
      ],
      "id": "f3f5e56e64ae22b64d445c191a594a0c"
    },
    {
      "title": "Connection",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Set Up",
      "description": "Learn how to connect to your organization's publicly exposed cluster.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/cluster-pachctl-connect/",
      "relURI": "/latest/set-up/cluster-pachctl-connect/",
      "body": "If you are exposing your cluster publicly (e.g. via a load balancer), you can connect to it using the pachctl connect command. This command will configure your local pachctl to connect to your cluster.\nOpen a terminal. Retrieve the external IP address of your load balancer or your domain name. kubectl get services | grep pachyderm-proxy | awk &#39;{print $4}&#39; Update the context of your cluster using the IP address. pachctl connect https://pachyderm.&lt;your-proxy.host-value&gt; Verify you are using the correct context. pachctl config get active-context Verify you can connect to your cluster by printing the PachD version. pachctl version ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "load balancer",
        "tcp",
        "pachctl connect",
        "ip"
      ],
      "id": "312fc900fdf8b42c0a36b019e856fe5e"
    },
    {
      "title": "Environment Variables",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Set Up",
      "description": "Learn how to configure environment variables.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/environment-variables/",
      "relURI": "/latest/set-up/environment-variables/",
      "body": "You can define environment variables that handle required configuration. In HPE ML Data Management, you can define the following types of environment variables:\npachd variables: Used for your HPE ML Data Management daemon container.\nHPE ML Data Management worker variables: Used by the Kubernetes pods that run your pipeline code.\nüí° You can reference environment variables in your code. For example, if your code writes data to an external system and you want to know the current job ID, you can use the PACH_JOB_ID environment variable to refer to the current job ID.\npachd Environment Variables # You can find the list of pachd environment variables in the pachd manifest by running the following command:\nkubectl get deploy pachd -o yaml The following tables list all the pachd environment variables.\nGlobal Configuration # Environment Variable Default Value Description ETCD_SERVICE_HOST N/A The host on which the etcd service runs. ETCD_SERVICE_PORT N/A The etcd port number. PPS_WORKER_GRPC_PORT 80 The GRPs port number. PORT 650 The pachd port number. HTTP_PORT 652 The HTTP port number. PEER_PORT 653 The port for pachd-to-pachd communication. NAMESPACE default The namespace in which HPE ML Data Management is deployed. PachD Configuration # Environment Variable Default Value Description NUM_SHARDS 32 The max number of pachd pods that can run in a single cluster. STORAGE_BACKEND &quot;&quot; The storage backend defined for the HPE ML Data Management cluster. STORAGE_HOST_PATH &quot;&quot; The host path to storage. KUBERNETES_PORT_443_TCP_ADDR none An IP address that Kubernetes exports automatically for your code to communicate with the Kubernetes API. Read access only. Most variables that have use the PORT_ADDRESS_TCP_ADDR pattern are Kubernetes environment variables. For more information,\nsee Kubernetes environment variables. METRICS true Defines whether anonymous HPE ML Data Management metrics are being collected or not. BLOCK_CACHE_BYTES 1G The size of the block cache in pachd. WORKER_IMAGE &quot;&quot; The base Docker image that is used to run your pipeline. WORKER_SIDECAR_IMAGE &quot;&quot; The pachd image that is used as a worker sidecar. WORKER_IMAGE_PULL_POLICY IfNotPresent The pull policy that defines how Docker images are pulled. You can set a Kubernetes image pull policy as needed. LOG_LEVEL info Verbosity of the log output. If you want to disable logging, set this variable to 0. Viable Options debug info error\nFor more information, see Go logrus log levels. IAM_ROLE &quot;&quot; The role that defines permissions for HPE ML Data Management in AWS. IMAGE_PULL_SECRET &quot;&quot; The Kubernetes secret for image pull credentials. EXPOSE_OBJECT_API false Controls access to internal HPE ML Data Management API. WORKER_USES_ROOT true Controls root access in the worker container. S3GATEWAY_PORT 600 The S3 gateway port number DISABLE_COMMIT_PROGRESS_COUNTER false A feature flag that disables commit propagation progress counter. If you have a large DAG, setting this parameter to true might help improve etcd performance. You only need to set this parameter on the pachd pod. HPE ML Data Management passes this parameter to worker containers automatically. Storage Configuration # Environment Variable Default Value Description STORAGE_MEMORY_THRESHOLD N/A Defines the storage memory threshold. STORAGE_SHARD_THRESHOLD N/A Defines the storage shard threshold. Pipeline Worker Environment Variables # HPE ML Data Management defines many environment variables for each HPE ML Data Management worker that runs your pipeline code. You can print the list of environment variables into your HPE ML Data Management logs by including the env command into your pipeline specification. For example, if you have an images repository, you can configure your pipeline specification like this:\n{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;env&#34; }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/&#34;, &#34;repo&#34;: &#34;images&#34; } }, &#34;transform&#34;: { &#34;cmd&#34;: [&#34;sh&#34; ], &#34;stdin&#34;: [&#34;env&#34;], &#34;image&#34;: &#34;ubuntu:14.04&#34; } } Run this pipeline and, upon completion, you can view the log with variables by running the following command:\npachctl logs --pipeline=env PPS_WORKER_IP=172.17.0.7 DASH_PORT_8081_TCP_PROTO=tcp PACHD_PORT_600_TCP_PORT=600 KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 ... You should see a lengthy list of variables. Many of them define internal networking parameters that most probably you will not need to use.\nMost users find the following environment variables particularly useful:\nEnvironment Variable Description AWS_ACCESS_KEY_ID The ID that contains your AWS access key; requires pfs.s3: true or s3_out:true in your pipeline spec. AWS_SECRET_ACCESS_KEY The name of the secret which contains your AWS access key; requires pfs.s3: true or s3_out:true in your pipeline spec. PACH_JOB_ID The ID of the current job. For example, PACH_JOB_ID=8991d6e811554b2a8eccaff10ebfb341. PACH_DATUM_ID The ID of the current Datum. PACH_DATUM_&lt;input.name&gt;_JOIN_ON Exposes the join_on match to the pipeline&rsquo;s job. PACH_DATUM_&lt;input.name&gt;_GROUP_BY Expose the group_by match to the pipeline&rsquo;s job. PACH_OUTPUT_COMMIT_ID The ID of the commit in the output repo for the current job. For example, PACH_OUTPUT_COMMIT_ID=a974991ad44d4d37ba5cf33b9ff77394. PPS_NAMESPACE The PPS namespace. For example, PPS_NAMESPACE=default. PPS_SPEC_COMMIT The hash of the pipeline specification commit.\nThis value is tied to the pipeline version. Therefore, jobs that use the same version of the same pipeline have the same spec commit. For example, PPS_SPEC_COMMIT=3596627865b24c4caea9565fcde29e7d. PPS_POD_NAME The name of the pipeline pod. For example, pipeline-env-v1-zbwm2. PPS_PIPELINE_NAME The name of the pipeline that this pod runs. For example, env. PIPELINE_SERVICE_PORT_PROMETHEUS_METRICS The port that you can use to exposed metrics to Prometheus from within your pipeline. The default value is 9090. HOME The path to the home directory. The default value is /root &lt;input-repo&gt;=&lt;path/to/input/repo&gt; The path to the filesystem that is defined in the input in your pipeline specification. HPE ML Data Management defines such a variable for each input. The path is defined by the glob pattern in the spec. For example, if you have an input images and a glob pattern of /, HPE ML Data Management defines the images=/pfs/images variable. If you have a glob pattern of /*, HPE ML Data Management matches the files in the images repository and, therefore, the path is images=/pfs/images/liberty.png. input_COMMIT The ID of the commit that is used for the input. For example, images_COMMIT=fa765b5454e3475f902eadebf83eac34. S3_ENDPOINT A HPE ML Data Management S3 gateway sidecar container endpoint. If you have an S3 enabled pipeline, this parameter specifies a URL that you can use to access the pipeline&rsquo;s repositories state when a particular job was run. The URL has the following format: http://&lt;job-ID&gt;-s3:600. An example of accessing the data by using AWS CLI looks like this: `echo foo_data In addition to these environment variables, Kubernetes injects others for Services that run inside the cluster. These variables enable you to connect to those outside services, which can be powerful but might also result in processing being retried multiple times.\nFor example, if your code writes a row to a database, that row might be written multiple times because of retries. Interaction with outside services must be idempotent to prevent unexpected behavior. Furthermore, one of the running services that your code can connect to is HPE ML Data Management itself. This is generally not recommended as very little of the HPE ML Data Management API is idempotent, but in some specific cases it can be a viable approach.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "configuration"
      ],
      "id": "8119651404fd1e6256dec24063317254"
    },
    {
      "title": "Kubernetes RBAC",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Set Up",
      "description": "Learn how our platform supports Kubernetes' Role-Base Access Controls (RBAC).",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/rbac/",
      "relURI": "/latest/set-up/rbac/",
      "body": "HPE ML Data Management has support for Kubernetes Role-Based Access Controls (RBAC), which is a default part of all HPE ML Data Management deployments. In most use cases, HPE ML Data Management sets all the RBAC permissions automatically. However, if you are deploying HPE ML Data Management on a cluster that your company owns, security policies might not allow certain RBAC permissions by default. Therefore, you need to contact your Kubernetes administrator and provide the following list of required permissions:\nRules: []rbacv1.PolicyRule{{ APIGroups: []string{&#34;&#34;}, Verbs: []string{&#34;get&#34;, &#34;list&#34;, &#34;watch&#34;}, Resources: []string{&#34;nodes&#34;, &#34;pods&#34;, &#34;pods/log&#34;, &#34;endpoints&#34;}, }, { APIGroups: []string{&#34;&#34;}, Verbs: []string{&#34;get&#34;, &#34;list&#34;, &#34;watch&#34;, &#34;create&#34;, &#34;update&#34;, &#34;delete&#34;}, Resources: []string{&#34;replicationcontrollers&#34;, &#34;services&#34;}, }, { APIGroups: []string{&#34;&#34;}, Verbs: []string{&#34;get&#34;, &#34;list&#34;, &#34;watch&#34;, &#34;create&#34;, &#34;update&#34;, &#34;delete&#34;}, Resources: []string{&#34;secrets&#34;}, ResourceNames: []string{client.StorageSecretName}, }}, The following table explains how HPE ML Data Management uses those permissions:\nPermission Description Access to nodes Used for metrics reporting, disabling should not affect HPE ML Data Management&rsquo;s operation. Access to pods, replica controllers, and services HPE ML Data Management uses this permission to monitor the created pipelines. The permissions related to replicationcontrollers and services are used in the setup and deletion of pipelines. Each pipeline has its own RC and service in addition to the pods. Access to secrets Required to give various kinds of credentials to pipelines, including storage credentials to access S3 or other object storage backends, Docker credentials to pull from a private registry, and others. RBAC and DNS # In older Kubernetes versions, kube-dns did not work properly with RBAC. To check if your cluster is affected by this issue, run:\nkubectl get all --namespace=kube-system System response:\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 1 1 1 0 3m NAME DESIRED CURRENT READY AGE rs/kube-dns-86f6f55dd5 1 1 0 3m NAME READY STATUS RESTARTS AGE po/kube-addon-manager-oryx 1/1 Running 0 3m po/kube-dns-86f6f55dd5-xksnb 2/3 Running 4 3m po/kubernetes-console-bzjjh 1/1 Running 0 3m po/storage-provisioner 1/1 Running 0 3m NAME DESIRED CURRENT READY AGE rc/kubernetes-console 1 1 1 3m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP 3m svc/kubernetes-console NodePort 10.97.194.16 &lt;none&gt; 80:30000/TCP 3m In the output above, po/kubernetes-console-bzjjh has only two out of three pods ready and has restarted four times. To fix this issue, run:\nkubectl -n kube-system create sa kube-dns kubectl -n kube-system patch deploy/kube-dns -p &#39;{&#34;spec&#34;: {&#34;template&#34;: {&#34;spec&#34;: {&#34;serviceAccountName&#34;: &#34;kube-dns&#34;}}}}&#39; These commands enforce kube-dns to use the appropriate ServiceAccount. Kubernetes has created the ServiceAccount, but does not use it until you run the above commands.\nResolving RBAC Permissions on GKE # When you deploy HPE ML Data Management on GKE, you might see the following error:\nError from server (Forbidden): error when creating &#34;STDIN&#34;: clusterroles.rbac.authorization.k8s.io &#34;pachyderm&#34; is forbidden: attempt to grant extra privileges: To fix this issue, run the following command and redeploy HPE ML Data Management:\nkubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account) ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "configuration",
        "permissions"
      ],
      "id": "ac0da25ee07e99a2e6408675b146789b"
    },
    {
      "title": "Import a Kubernetes Context",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Set Up",
      "description": "Learn how to import and embed a Kubernetes Context into a Pachyderm context.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/import-kubernetes-context/",
      "relURI": "/latest/set-up/import-kubernetes-context/",
      "body": "After you have deployed HPE ML Data Management, the Pachyderm context is not created. Therefore, you need to manually create a new HPE ML Data Management context with the embedded current Kubernetes context and activate that context.\nTo import a Kubernetes context, complete the following steps:\nVerify that the cluster was successfully deployed:\nkubectl get pods You should see a pod for pachd running (alongside etcd, pg-bouncer or postgres, console, depending on your installation).\nSystem Response:\nNAME READY STATUS RESTARTS AGE console-6c989c8d56-ftxk7 1/1 Running 0 3d18h etcd-0 1/1 Running 0 3d18h pachd-f9fd5b6fc-8d774 1/1 Running 0 3d18h pg-bouncer-794d8f68f-sjbbh 1/1 Running 0 3d18h Create a new HPE ML Data Management context with the embedded Kubernetes context:\npachctl config import-kube &lt;new-pachyderm-context-name&gt; -k `kubectl config current-context` Verify that the context was successfully created and view the context parameters:\nExample:\npachctl config get context &lt;new-pachyderm-context-name&gt; System Response:\n{ &#34;source&#34;: &#34;IMPORTED&#34;, &#34;cluster_name&#34;: &#34;minikube&#34;, &#34;auth_info&#34;: &#34;minikube&#34;, &#34;namespace&#34;: &#34;default&#34; } Activate the new HPE ML Data Management context:\npachctl config set active-context &lt;new-pachyderm-context-name&gt; Verify that the new context has been activated:\npachctl config get active-context ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "kubernetes"
      ],
      "id": "e11660f446346f0da2fe5f877ee844c8"
    },
    {
      "title": "Log Aggregation (Loki)",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Set Up",
      "description": "Learn how to enable log aggregation with Loki through the Promtail agent service.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/loki/",
      "relURI": "/latest/set-up/loki/",
      "body": " Shipping logs to Loki # Loki retrieves logs from pods in Kubernetes through an agent service called Promtail. Promtail runs on each node and sends logs from Kubernetes pods to the Loki API Server, tagging each log entry with information about the pod that produced it.\nYou need to configure Promtail for your environment to ship logs to your Loki instance. If you are running multiple nodes, then you will need to install and configure Promtail for each node shipping logs to Loki.\nFetching logs # While installing Loki will enable the collection of logs, commands such as pachctl logs will not fetch logs directly from Loki until the LOKI_LOGGING environment variable on the pachd container is true.\nThis is controlled by the helm value pachd.lokiLogging, which can be set by adding the following to your values.yaml file:\npachd: lokiLogging: true HPE ML Data Management reads logs from the Loki API Server with a particular set of tags. The URI at which HPE ML Data Management reads from the Loki API Server is determined by the LOKI_SERVICE_HOST and LOKI_SERVICE_PORT environment values automatically added by Loki Kubernetes service.\nIf Loki is deployed after the pachd container, the pachd container will need to be redeployed to receive these connection parameters.\n‚ÑπÔ∏è If you are not running Promtail on the node where your HPE ML Data Management pods are located, you will be unable to get logs for pipelines running on that node via pachctl logs -p pipelineName.\nDefault Loki Bundle # Per default, HPE ML Data Management ships with an embedded version of Loki that can be deployed by adding the lokiDeploy: true next to the existing lokiLogging: true.\npachd: lokiDeploy: true lokiLogging: true In such case, add the following section to your value.yaml:\nloki-stack: loki: persistence: enabled: true accessModes: - ReadWriteOnce size: 5Gi storageClassName: standard annotations: {} grafana: enabled: true ‚ÑπÔ∏è Grafana Users:\nTo use Grafana, deploy with loki-stack.grafana.enabled: true.\nTo access Grafana, run port-forward with kubectl port-forward svc/pachyderm-grafana 4001:80. Change the port 4001 to what suits you best.\nLogin to localhost:4001 with the username admin, and the password found with running kubectl get secret pachyderm-grafana -o jsonpath=&quot;{.data.admin-password}&quot; | base64 -d. If enterprise is activated, you will be able to inspect containers logs in your console.\nUsing Loki in Another Namespace # Instead of deploying a local loki instance in your pachyderm namespace, you can configure pachyderm to use a loki running in another namespace. To do so, you must set lokiHost and lokiPort. You should also set lokiDeploy: false to prevent the chart from deploying a local loki instance.:\npachd: lokiDeploy: false lokiHost: &#34;&lt;loki-namespace&gt;.&lt;loki-service-name&gt;.svc.cluster.local.&#34; lokiPort: 3100 References # Loki Documentation - https://grafana.com/docs/loki/latest/ Promtail Documentation - https://grafana.com/docs/loki/latest/clients/promtail/ Operating Loki - https://grafana.com/docs/loki/latest/operations/ ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "loki",
        "logs"
      ],
      "id": "84e7dad185e80cab0e38a30e8774e911"
    },
    {
      "title": "Non-Default Namespaces",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Set Up",
      "description": "Learn how to deploy to a non-default namespace for easier admin management.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/namespaces/",
      "relURI": "/latest/set-up/namespaces/",
      "body": "Often, production deploys of HPE ML Data Management involve deploying HPE ML Data Management to a non-default namespace. This helps administrators of the cluster more easily manage HPE ML Data Management components alongside other things that might be running inside of Kubernetes (DataDog, TensorFlow Serving, etc.).\nTo deploy HPE ML Data Management to a non-default namespace, you need to add the -n or --namespace flag when deploying. If the namespace does not already exist, you can have Helm create it with --create-namespace.\nhelm install &lt;args&gt; --namespace pachyderm --create-namespace To talk to your HPE ML Data Management cluster:\nYou can either modify an existing pachctl context\npachctl config update context --namespace pachyderm or import one from Kubernetes:\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "deployment"
      ],
      "id": "1a08388ab76465eeb2d2d3009bf6dd07"
    },
    {
      "title": "Enterprise Edition",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Set Up",
      "description": "Learn about the unique features and settings specific to Enterprise edition.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/enterprise/",
      "relURI": "/latest/set-up/enterprise/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "bb80d701aae63fa0c0f8ad5fb5c7339d"
    },
    {
      "title": "Activate Enterprise via Helm",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Enterprise Edition",
      "description": "Learn how to deploy the Enterprise edition using Helm.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/enterprise/activate-via-helm/",
      "relURI": "/latest/set-up/enterprise/activate-via-helm/",
      "body": " Before You Start # You must have a HPE ML Data Management Enterprise License Key. You must have pachctl and HPE ML Data Management installed. You must have the HPE ML Data Management Helm repo downloaded. How to Activate Enterprise HPE ML Data Management via Helm # Activation Method: License License Secret Open your Helm values.yml file. Find the the pachd.enterpriseLicenseKey attribute. Input your enterprise key. Upgrade your cluster by running the following command: helm upgrade pachyderm pachyderm/pachyderm -f values.yml Once deployed, HPE ML Data Management stores your provided Enterprise license as the platform secret pachyderm-license in the key enterprise-license-key.\nCreate a secret for your Enterprise license. kubectl create secret generic pachyderm-enterprise-key \\ --from-literal=enterprise-license-key=&#39;&lt;replace-with-key&gt;&#39; \\ --dry-run=client -o json | jq &#39;del(.metadata.resourceVersion)&#39; &gt; pachyderm-enterprise-key.json Or { &#34;kind&#34;: &#34;Secret&#34;, &#34;apiVersion&#34;: &#34;v1&#34;, &#34;metadata&#34;: { &#34;name&#34;: &#34;pachyderm-enterprise-key&#34;, &#34;creationTimestamp&#34;: null }, &#34;data&#34;: { &#34;enterprise-license-key&#34;: &#34;&lt;replace-with-key&gt;&#34; } } Upload the secret to your cluster. pachctl create secret -f pachyderm-enterprise-key.json Obtain your current user-input helm values: helm get values pachyderm &gt; values.yaml Find the the pachd.enterpriseLicenseKeySecretName attribute. Input your license&rsquo;s secret name found in meta.name of pachyderm-enterprise-key.json (e.g., pachyderm-enterprise-key-secret). deployTarget: LOCAL proxy: enabled: true host: localhost service: type: LoadBalancer pachd: enterpriseLicenseKeySecretName: &#34;pachyderm-enterprise-key&#34; Upgrade your cluster by running the following command: helm upgrade pachyderm pachyderm/pachyderm -f values.yml Enterprise automatically enables authentication. You can log in using the following command:\npachctl auth login username: admin password: password However, to use Console, you must set up an IdP.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "enterprise",
        "setup",
        "activate",
        "enterprise key",
        "license"
      ],
      "id": "3273926180b46e83b252b8a9984775db"
    },
    {
      "title": "Activate Enterprise via PachCTL",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Enterprise Edition",
      "description": "Learn how to deploy the Enterprise edition using the PachCTL CLI for an existing cluster.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/enterprise/activate-via-pachctl/",
      "relURI": "/latest/set-up/enterprise/activate-via-pachctl/",
      "body": " Before You Start # You must have a HPE ML Data Management Enterprise License Key. You must have pachctl and HPE ML Data Management installed. You must have the HPE ML Data Management Helm repo downloaded. How to Activate Enterprise HPE ML Data Management via Pachctl # Open your terminal. Input the following command: echo &lt;your-activation-token&gt; | pachctl license activate Verify the status of the enterprise activation: pachctl enterprise get-state # ACTIVE You have unlocked HPE ML Data Management&rsquo;s enterprise features.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "enterprise"
      ],
      "id": "6707755a1b7f6a2a430b7463472c1ef4"
    },
    {
      "title": "Features Overview",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Enterprise Edition",
      "description": "Learn about the main features unique Enterprise edition.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/enterprise/overview/",
      "relURI": "/latest/set-up/enterprise/overview/",
      "body": "Enterprise helps you scale and manage HPE ML Data Management data pipelines by removing all scaling limits and providing you with additional features not available in the Community Edition.\n‚ÑπÔ∏è Want to try Enterprise, or simply have a few questions? Get in touch with us at sales@pachyderm.io or on our Slack.\nAdditional Features # Authentication: Authenticate against your favorite OIDC providers. Role-Based Access Control (RBAC): Use RBAC on pachyderm resources (clusters, projects, repos), silo data, and prevent unintended changes on production pipelines. Enterprise Server: Simplify licensing and Identity Provider management by using one Enterprise server to register many HPE ML Data Management clusters. Additionally, you have access to a PachCTL command that pauses (pachctl enterprise pause) and unpauses (pachctl enterprise unpause) your cluster for a backup and restore. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "enterprise"
      ],
      "id": "d7166163c885e822369a8a3bb7200ac1"
    },
    {
      "title": "Enterprise Server (ES)",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Set Up",
      "description": "Learn how to spin up and manage a Enterprise server.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/enterprise-server/",
      "relURI": "/latest/set-up/enterprise-server/",
      "body": "You can manage your enterprise licensing and identity provider (IdP) integrations through the Enterprise Server. A Enterprise Server can have multiple HPE ML Data Management clustered registered to it.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "1cfaaa86d75657099df42c3ea8c17d96"
    },
    {
      "title": "Activate ES for Multi-Cluster",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Enterprise Server (ES)",
      "description": "Learn how to set up a Enterprise server as a standalone cluster within a multi-cluster deployment.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/enterprise-server/multi-cluster-setup/",
      "relURI": "/latest/set-up/enterprise-server/multi-cluster-setup/",
      "body": "This guide deploys Enterprise Server as a standalone cluster within a multi-cluster deployment.\nBefore You Start # There are a few minor differences to note when deploying an Enterprise Server when compared to a standard HPE ML Data Management cluster:\nNo deployment target is necessary in your Helm chart since there is no object store The Enterprise Server cluster contains the dex database Each registered cluster requires its own PostgresSQL pachyderm database How to Activate Enterprise for Multi-Cluster # Create a separate Kubernetes namespace dedicated to your enterprise server: kubectl create namespace enterprise-server kubectl config set-context --current --namespace=enterprise-server Create a Helm chart enterprise-server-values.yml file for your enterprise server (see Helm Chart Reference Guide). Deploy the Enterprise Server cluster: helm install enterprise-server pachyderm/pachyderm --f enterprise-server-values.yml Verify deployment: kubectl get all --namespace enterprise-server Reference Diagram # The following diagram gives you a quick overview of an organization with multiple HPE ML Data Management clusters behind a single Enterprise Server. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "enterprise",
        "deployment",
        "helm"
      ],
      "id": "6388671ea77ef498042715b890e1756b"
    },
    {
      "title": "Activate ES for Single-Cluster",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Enterprise Server (ES)",
      "description": "Learn how to set up a Enterprise server for a single-cluster environment embedded in pachd.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/enterprise-server/single-cluster-setup/",
      "relURI": "/latest/set-up/enterprise-server/single-cluster-setup/",
      "body": "You can register an existing single-cluster HPE ML Data Management instance to the embedded Enterprise Server that comes included with pachd using the steps in this guide. Doing so enables you to also activate authentication and set up IdP connectors.\nBefore You Start # You must have an Enterprise license key You must have an active HPE ML Data Management cluster How to Activate Enterprise Server # Open your terminal. Activate Enterprise Server: echo &lt;enterprise-license-key-value&gt; | pachctl license activate Activate Authentication: pachctl auth activate --enterprise Set up your Identity Provider (IdP). ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "enterprise",
        "deployment",
        "helm"
      ],
      "id": "5e7d39f1eaf917dab44dab081d9087a3"
    },
    {
      "title": "Register a Cluster via Helm",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Enterprise Server (ES)",
      "description": "Learn how to register a pachd cluster to your Enterprise Server using Helm.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/enterprise-server/register-cluster-via-helm/",
      "relURI": "/latest/set-up/enterprise-server/register-cluster-via-helm/",
      "body": " Before You Start # You must have an Enterprise license key You must have the HPE ML Data Management Helm repo downloaded. How to Register a Cluster # Open your Helm values.yml file. Update the pachd section with the following attributes: pachd: activateEnterpriseMember: true enterpriseServerAddress: &#34;grpc://&lt;ENTERPRISE_SERVER_ADDRESS&gt;&#34; enterpriseCallbackAddress: &#34;grpc://&lt;PACHD_ADDRESS&gt;&#34; enterpriseServerToken: &#34;&lt;ENTERPRISE-SERVER-TOKEN&gt;&#34; # the same root token of the enterprise cluster # Alternatively, use a secret enterpriseServerTokenSecretName: &#34;&lt;Name of you secret containing enterpriseServerToken&gt;&#34; Upgrade the cluster: helm upgrade pachyderm pachyderm/pachyderm -f values.yml ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "enterprise",
        "deployment",
        "helm"
      ],
      "id": "ac73ff7b76f3f3cc08e66c02c1003698"
    },
    {
      "title": "Register a Cluster via PachCTL",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Enterprise Server (ES)",
      "description": "Learn how to register a pachd cluster to your Enterprise Server using PachCTL.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/enterprise-server/register-cluster-via-pachctl/",
      "relURI": "/latest/set-up/enterprise-server/register-cluster-via-pachctl/",
      "body": " Before You Start # You must have an Enterprise license key You must have an active HPE ML Data Management cluster You must have the HPE ML Data Management Helm repo downloaded. How to Register a Cluster # Open your terminal. Run the following command: pachctl enterprise register --id &lt;my-pachd-config-name&gt; --enterprise-server-address &lt;pach-enterprise-IP&gt;:650 --pachd-address &lt;pachd-IP&gt;:650 Attribute Description --id the name of the context pointing to your cluster in ~/.pachyderm/config.json. --enterprise-server-address the host and port where pachd can reach the enterprise server. --pachd-address the host and port where the enterprise server can reach pachd. This may be internal to the kubernetes cluster, or over the internet. View all registered clusters with your enterprise server: pachctl license list-clusters # Using enterprise context: my-enterprise-context-name # id: john # address: ae1ba915f8b5b477c98cd26c67d7563b-66539067.us-west-2.elb.amazonaws.com:650 # version: 2.0.0 # auth_enabled: true # last_heartbeat: 2021-05-21 18:37:36.072156 +0000 UTC # --- # id: doe # address: 34.71.247.191:650 # version: 2.0.0 # auth_enabled: true # last_heartbeat: 2021-05-21 18:43:42.157027 +0000 UTC # --- Activate Authentication: pachctl auth activate --enterprise Set up your Identity Provider (IdP). ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "enterprise",
        "deployment",
        "pachctl"
      ],
      "id": "af64fc131f776823d70598a26b65a6f4"
    },
    {
      "title": "Server Management",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Enterprise Server (ES)",
      "description": "Learn how to manage your Enterprise server.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/enterprise-server/manage/",
      "relURI": "/latest/set-up/enterprise-server/manage/",
      "body": " Contexts # The enterprise server has a separate context in the pachctl config file (~/.pachyderm/config.json).\nPachctl has an active pachd context (the cluster it is binded to), and separately an active enterprise context.\nTo check the active enterprise context, run:\npachctl config get active-enterprise-context ‚ö†Ô∏è In a single-cluster deployment, the active enterprise context will be the same as the enterprise context. The pachctl license and pachctl idp commands run against the enterprise context. pachctl auth commands accept an --enterprise flag to run against the enterprise context. Configuring IDPs # To configure IDP integrations, use pachctl idp create-connector as documented in the HPE ML Data Management Integration with Identity Providers page.\nManage your Enterprise Server # Add Users As Administrators # By default, only the root token (Root User) can administer the Enterprise Server. Run the following command to add more ClusterAdmin to your Enterprise Server:\npachctl auth set enterprise clusterAdmin user:&lt;email&gt; List All Registered Clusters # pachctl license list-clusters The output includes the pachd version, whether auth is enabled, and the last heartbeat:\nid: pach-2 address: 34.71.247.191:650 version: 2.0.0 auth_enabled: true last_heartbeat: 2021-05-21 18:43:42.157027 +0000 UTC Synchronize all available contexts in your ~/.pachyderm/config.json file # In the case where the enterprise server of your organization has multiple pachd instances, you can use the following command to ‚Äúdiscover‚Äù other pachd instances. It will automatically update your ~/.pachyderm/config.json file with all the contexts you can connect to.\npachctl enterprise sync-contexts Update The Enterprise License # To apply a new license and have it picked up by all clusters, run:\npachctl license activate --no-register Unregister A Cluster # To unregister a given cluster from your Enterprise Server, run:\npachctl license delete-cluster --id &lt;cluster id&gt; Undeploy # To undeploy a Cluster registered with an Enterprise Server: Unregister the cluster as mentioned above (pachctl license delete-cluster) Then, undeploy it: helm uninstall ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "enterprise",
        "management"
      ],
      "id": "25ffa62ef1b670b8888377fba9489481"
    },
    {
      "title": "Server Setup",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Enterprise Server (ES)",
      "description": "Learn how to set up a Enterprise server.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/enterprise-server/setup/",
      "relURI": "/latest/set-up/enterprise-server/setup/",
      "body": " ‚ÑπÔ∏è For POCs and smaller organizations with one single HPE ML Data Management cluster, the Enterprise Server services can be run embedded in pachd. A separate deployment is not necessary. An organization with a single HPE ML Data Management cluster can run the Enterprise Server services embedded within pachd.\nThe setup of an Enterprise Server requires to:\nDeploy it. Activate your Enterprise Key and enable Auth. Register your newly created or existing HPE ML Data Management clusters with your enterprise server. Optional: Enable Auth on each cluster. 1. Deploy An Enterprise Server # Deploying and configuring an enterprise server can be done in one of two ways:\nProvide all licensing and authentication configurations as a part of the Helm deployment. Use pachctl commands to set up licensing and authentication. As Part Of A Regular HPE ML Data Management Helm Deployment # Update your values.yaml with your enterprise license key and auth configurations (for an example on localhost, see the example values.yaml here) or check our minimal example below to your values.yaml.\n‚ö†Ô∏è If a pachyderm cluster will also be installed in the same kubernetes cluster, they should be installed in different namespaces: kubectl create namespace enterprise helm install ... --set enterpriseServer.enabled=true --namespace enterprise This command deploys postgres, etcd and a deployment and service called pach-enterprise. pach-enterprise uses the same docker image and pachd binary, but it listens on a different set of ports (31650, 31657, 31658) to avoid conflicts with pachd.\nCheck the state of your deployment by running: kubectl get all --namespace enterprise System Response\nNAME READY STATUS RESTARTS AGE pod/etcd-5fd7c675b6-46kz7 1/1 Running 0 113m pod/pach-enterprise-6dc9cb8f66-rs44t 1/1 Running 0 105m pod/postgres-6bfd7bfc47-9mz28 1/1 Running 0 113m values.yaml for a standalone Enterprise Server as part of a multi-cluster deployment # Deploying a standalone enterprise server requires setting the helm parameter enterpriseServer.enabled to true and the pachd.enabled to false.\nenterpriseServer: enabled: true pachd: enabled: false enterpriseLicenseKey: &#34;&lt;ENTERPRISE-LICENSE-KEY&gt;&#34; # Alternatively, you can pass your license in a secret enterpriseLicenseKeySecretName: &#34;&lt;enterprise License key secret name&gt;&#34; oauthClientID: &#34;pachd&#34; oauthRedirectURI: &#34;http://&lt;PACHD-IP&gt;:30657/authorization-code/callback&#34; ## if a secret name is not provided in `oauthClientSecretSecretName`, a secret containing `oauthClientSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-auth` under the key `auth-config&#39; oauthClientSecret: &#34;&#34; oauthClientSecretSecretName: &#34;&#34; ## if a secret name is not provided in `enterpriseSecretSecretName`, a secret containing `enterpriseSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-enterprise` under the key `enterprise-secret&#39; enterpriseSecretSecretName: &#34;&#34; enterpriseSecret: &#34;&#34; activateAuth: true ## if a secret name is not provided in `rootTokenSecretName`, a secret containing `rootToken` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-auth` under the key `rootToken&#39; rootTokenSecretName: &#34;&#34; rootToken: &#34;&#34; externalService: enabled: true oidc: issuerURI: &#34;http://&lt;PACHD-IP&gt;:30658/&#34; ## userAccessibleOauthIssuerHost is necessary in localhost settings or anytime the registered Issuer address isn&#39;t accessible outside the cluster # userAccessibleOauthIssuerHost: &#34;localhost:30658&#34; ## if `mockIDP` is set to true, `pachd.upstreamIDPs` will be ignored in favor of a testing placeholder IDP with username/password: admin/password mockIDP: false ## to set up upstream IDPs, set pachd.mockIDP to false, ## and populate the pachd.upstreamIDPs with an array of Dex Connector configurations. ## See the example below or https://dexidp.io/docs/connectors/ upstreamIDPs: - id: idpConnector jsonConfig: &gt;- { &#34;issuer&#34;: &#34;&lt;ISSUER&gt;&#34;, &#34;clientID&#34;: &#34;&lt;CLIENT-ID&gt;&#34;, &#34;clientSecret&#34;: &#34;&lt;CLIENT-SECRET&gt;&#34;, &#34;redirectURI&#34;: &#34;http://&lt;PACHD-IP&gt;:30658/callback&#34;, &#34;insecureEnableGroups&#34;: true, &#34;insecureSkipEmailVerified&#34;: true, &#34;insecureSkipIssuerCallbackDomainCheck&#34;: true, &#34;forwardedLoginParams&#34;: [&#34;login_hint&#34;] } name: idpConnector type: oidc values.yaml for an embedded single-cluster deployment # pachd: enterpriseLicenseKey: &#34;&lt;ENTERPRISE-LICENSE-KEY&gt;&#34; # Alternatively, you can pass your license in a secret enterpriseLicenseKeySecretName: &#34;&lt;enterprise License key secret name&gt;&#34; oauthClientID: &#34;pachd&#34; oauthRedirectURI: &#34;http://&lt;PACHD-IP&gt;:30657/authorization-code/callback&#34; ## if a secret name is not provided in `oauthClientSecretSecretName`, a secret containing `oauthClientSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-auth` under the key `auth-config&#39; oauthClientSecret: &#34;&#34; oauthClientSecretSecretName: &#34;&#34; ## if a secret name is not provided in `enterpriseSecretSecretName`, a secret containing `enterpriseSecret` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-enterprise` under the key `enterprise-secret&#39; enterpriseSecretSecretName: &#34;&#34; enterpriseSecret: &#34;&#34; activateAuth: true ## if a secret name is not provided in `rootTokenSecretName`, a secret containing `rootToken` (or a randomly generated value if empty) will be created on install and stored in the k8s secret &#39;pachyderm-auth` under the key `rootToken&#39; rootTokenSecretName: &#34;&#34; rootToken: &#34;&#34; externalService: enabled: true oidc: issuerURI: &#34;http://&lt;PACHD-IP&gt;:30658/&#34; ## userAccessibleOauthIssuerHost is necessary in localhost settings or anytime the registered Issuer address isn&#39;t accessible outside the cluster # userAccessibleOauthIssuerHost: &#34;localhost:30658&#34; ## if `mockIDP` is set to true, `pachd.upstreamIDPs` will be ignored in favor of a testing placeholder IDP with username/password: admin/password mockIDP: false ## to set up upstream IDPs, set pachd.mockIDP to false, ## and populate the pachd.upstreamIDPs with an array of Dex Connector configurations. ## See the example below or https://dexidp.io/docs/connectors/ upstreamIDPs: - id: idpConnector jsonConfig: &gt;- { &#34;issuer&#34;: &#34;&lt;ISSUER&gt;&#34;, &#34;clientID&#34;: &#34;&lt;CLIENT-ID&gt;&#34;, &#34;clientSecret&#34;: &#34;&lt;CLIENT-SECRET&gt;&#34;, &#34;redirectURI&#34;: &#34;http://&lt;PACHD-IP&gt;:30658/callback&#34;, &#34;insecureEnableGroups&#34;: true, &#34;insecureSkipEmailVerified&#34;: true, &#34;insecureSkipIssuerCallbackDomainCheck&#34;: true, &#34;forwardedLoginParams&#34;: [&#34;login_hint&#34;] } name: idpConnector type: oidc This results in a single pachd pod, with authentication enabled, and an IDP integration configured.\n‚ÑπÔ∏è Update the following values as follows:\nPACHD-IP: The address of HPE ML Data Management&rsquo;s IP. Retrieve HPE ML Data Management external IP address if necessary. ISSUER, CLIENT-ID, CLIENT-SECRET.\nCheck the list of all available helm values at your disposal in our reference documentation or on Github.\n‚ö†Ô∏è When enterprise is enabled through Helm, auth is automatically activated (i.e., you do not need to run pachctl auth activate) and a pachyderm-auth k8s secret is created containing a rootToken key. Use {{&quot;kubectl get secret pachyderm-auth -o go-template='{{.data.rootToken | base64decode }}'&quot;}} to retrieve it and save it where you see fit. However, this secret is only used when configuring through helm:\nIf you run pachctl auth activate, the secret is not updated. Instead, the rootToken is printed in your STDOUT for you to save. Set the helm value pachd.activateAuth to false to prevent the automatic bootstrap of auth on the cluster. On An Existing HPE ML Data Management Cluster # To enable the Enterprise Server on an existing cluster:\nActivate your enterprise key and authentication then proceed to configuring IDP integrations. 2. Activate Enterprise Licensing And Enable Authentication # Use your enterprise key to activate your enterprise server: echo &lt;your-activation-token&gt; | pachctl license activate Then enable Authentication at the Enterprise Server level: pachctl auth activate --enterprise ‚ö†Ô∏è Enabling Auth will return a root token for the enterprise server. This is separate from the root tokens for each pachd (cluster). They should all be stored securely.\nOnce the enterprise server is deployed, deploy your cluster(s) and register it(them) with the enterprise server.\n3. Register Your Cluster With The Enterprise Server # Similarly to the enterprise server, we can configure our pachyderm clusters to leverage Helm for licensing and authentication in one of two flavors:\nProvide enterprise registration information as a part of the Helm deployment of a cluster. Register a cluster with the Enterprise Server using pachctl commands. Register Clusters With Helm # Add the enterprise server&rsquo;s root token, and network addresses to the values.yaml of each cluster you plan to deploy and register, for the cluster and enterprise server to communicate (for an example on localhost, see the example values.yaml here), or insert our minimal example below to your values.yaml.\nvalues.yaml with activation of an enterprise license and authentication # pachd: activateEnterpriseMember: true enterpriseServerAddress: &#34;grpc://&lt;ENTERPRISE_SERVER_ADDRESS&gt;&#34; enterpriseCallbackAddress: &#34;grpc://&lt;PACHD_ADDRESS&gt;&#34; enterpriseServerToken: &#34;&lt;ENTERPRISE-SERVER-TOKEN&gt;&#34; # the same root token of the enterprise cluster # Alternatively, use a secret enterpriseServerTokenSecretName: &#34;&lt;Name of you secret containing enterpriseServerToken&gt;&#34; ‚ö†Ô∏è When setting your enterprise server info as part of the Helm deployment of a cluster, auth is automatically activated unless the helm value pachd.activateAuth was intentionally set to false. (i.e., you can skip step 4).\nIn this case, a pachyderm-auth k8s secret is automatically created containing an entry for your rootToken in the key rootToken. Use the following to retrieve it and save it where you see fit:\n{{&#34;kubectl get secret pachyderm-auth -o go-template=&#39;{{.data.rootToken | base64decode }}&#39;&#34;}} Register Clusters With pachctl # Run this command for each of the clusters you wish to register using pachctl:\npachctl enterprise register --id &lt;my-pachd-config-name&gt; --enterprise-server-address &lt;pach-enterprise-IP&gt;:650 --pachd-address &lt;pachd-IP&gt;:650 --id is the name of the context pointing to your cluster in ~/.pachyderm/config.json.\n--enterprise-server-address is the host and port where pachd can reach the enterprise server. In production, the enterprise server may be exposed on the internet.\n--pachd-address is the host and port where the enterprise server can reach pachd. This may be internal to the kubernetes cluster, or over the internet.\nDisplay the list of all registered clusters with your enterprise server:\npachctl license list-clusters Using enterprise context: my-enterprise-context-name id: john address: ae1ba915f8b5b477c98cd26c67d7563b-66539067.us-west-2.elb.amazonaws.com:650 version: 2.0.0 auth_enabled: true last_heartbeat: 2021-05-21 18:37:36.072156 +0000 UTC --- id: doe address: 34.71.247.191:650 version: 2.0.0 auth_enabled: true last_heartbeat: 2021-05-21 18:43:42.157027 +0000 UTC --- 4. Enable Auth On Each Cluster # Finally, if your clusters were registered with the Enterprise Server using pachctl, you might choose to activate auth on each (or some) of them. This is an optional step. Clusters can be registered with the enterprise server without authentication being enabled.\nBefore enabling authentication, set up the issuer in the idp config between the enterprise server and your cluster:\necho &#34;issuer: http://&lt;enterprise-server-IP&gt;:658&#34; | pachctl idp set-config --config - Check that your config has been updated properly: pachctl idp get-config\nFor each registered cluster you want to enable auth on:\npachctl auth activate --client-id &lt;my-pachd-config-name&gt; --redirect http://&lt;pachd-IP&gt;:657/authorization-code/callback ‚ÑπÔ∏è Note the /authorization-code/callback appended after &lt;pachd-IP&gt;:657 in --redirect. --client-id is to pachctl auth activate what --id is to pachctl enterprise register: In both cases, enter &lt;my-pachd-config-name&gt;. Make sure than your enterprise context is set up properly: pachctl config get active-enterprise-context If not:\npachctl config set active-enterprise-context &lt;my-enterprise-context-name&gt; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "enterprise",
        "deployment",
        "helm"
      ],
      "id": "eb7bf830acd9d3666c7b8e984a0df976"
    },
    {
      "title": "S3 Gateway API",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Set Up",
      "description": "Learn about the operations exposed by the S3 Gateway API.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/s3gateway-api/",
      "relURI": "/latest/set-up/s3gateway-api/",
      "body": "This section outlines the operations exposed by HPE ML Data Management&rsquo;s HTTP API S3 Gateway.\nüìñ Since 1.13.3, all operations mentioning &lt;branch&gt;.&lt;repo&gt; also accept the syntax &lt;commit&gt;.&lt;repo&gt; and &lt;commit&gt;.&lt;branch&gt;.&lt;repo&gt;.\nListBuckets # Route: GET /.\nLists all of the branches across all of the repos as S3 buckets.\nDeleteBucket # Route: DELETE /&lt;branch&gt;.&lt;repo&gt;/.\nDeletes the branch. If it is the last branch in the repo, the repo is also deleted. Unlike S3, you can delete non-empty branches.\nListObjects # Route: GET /&lt;branch&gt;.&lt;repo&gt;/\nOnly S3&rsquo;s list objects v1 is supported.\nPFS directories are represented via CommonPrefixes. This largely mirrors how S3 is used in practice, but leads to a couple of differences:\nIf you set the delimiter parameter, it must be /. Empty directories are included in listed results. With regard to listed results:\nDue to PFS peculiarities, the LastModified field references when the most recent commit to the branch happened, which may or may not have modified the specific object listed. The HTTP ETag field does not use MD5, but is a cryptographically secure hash of the file contents. The S3 StorageClass and Owner fields always have the same filler value. GetBucketLocation # Route: GET /&lt;branch&gt;.&lt;repo&gt;/?location\nThis will always serve the same location for every bucket, but the endpoint is implemented to provide better compatibility with S3 clients.\nGetBucketVersioning # Route: GET /&lt;branch&gt;.&lt;repo&gt;/?versioning\nThis will get whether versioning is enabled, which is always true.\nListMultipartUploads # Route: GET /&lt;branch&gt;.&lt;repo&gt;/?uploads\nLists the in-progress multipart uploads in the given branch. The delimiter query parameter is not supported.\nCreateBucket # Route: PUT /&lt;branch&gt;.&lt;repo&gt;/.\nIf the repo does not exist, it is created. If the branch does not exist, it is likewise created. As per S3&rsquo;s behavior in some regions (but not all), trying to create the same bucket twice will return a BucketAlreadyOwnedByYou error.\nDeleteObjects # Route: POST /&lt;branch&gt;.&lt;repo&gt;/?delete.\nDeletes multiple files specified in the request payload.\nDeleteObject # Route: DELETE /&lt;branch&gt;.&lt;repo&gt;/&lt;filepath&gt;.\nDeletes the PFS file filepath in an atomic commit on the HEAD of branch.\nGetObject # Route: GET /&lt;branch&gt;.&lt;repo&gt;/&lt;filepath&gt;.\nBy default, this request gets the HEAD version of the file. You can use s3&rsquo;s versioning API to get the object at a non-HEAD commit by specifying either a specific commit ID, or by using the caret syntax &ndash; for example, HEAD^.\nThere is support for range queries and conditional requests, however error response bodies for bad requests using these headers are not standard S3 XML.\nWith regard to HTTP response headers:\nDue to PFS peculiarities, the HTTP Last-Modified header references when the most recent commit to the branch happened, which may or may not have modified this specific object. The HTTP ETag does not use MD5, but is a cryptographically secure hash of the file contents. PutObject # Route: PUT /&lt;branch&gt;.&lt;repo&gt;/&lt;filepath&gt;.\nWrites the PFS file at filepath in an atomic commit on the HEAD of branch.\nAny existing file content is overwritten. Unlike S3, there is no limit to upload size.\nUnlike s3, a 64mb max size is not enforced on this endpoint. Especially, as the file upload size gets larger, we recommend setting the Content-MD5 request header to ensure data integrity.\nAbortMultipartUpload # Route: DELETE /&lt;branch&gt;.&lt;repo&gt;?uploadId=&lt;uploadId&gt;\nAborts an in-progress multipart upload.\nCompleteMultipartUpload # Route: POST /&lt;branch&gt;.&lt;repo&gt;?uploadId=&lt;uploadId&gt;\nCompletes a multipart upload. If ETags are included in the request payload, they must be of the same format as returned by the S3 gateway when the multipart chunks are included. If they are md5 hashes or any other hash algorithm, they are ignored.\nCreateMultipartUpload # Route: POST /&lt;branch&gt;.&lt;repo&gt;?uploads\nInitiates a multipart upload.\nListParts # Route: GET /&lt;branch&gt;.&lt;repo&gt;?uploadId=&lt;uploadId&gt;\nLists the parts of an in-progress multipart upload.\nUploadPart # Route: PUT /&lt;branch&gt;.&lt;repo&gt;?uploadId=&lt;uploadId&gt;&amp;partNumber=&lt;partNumber&gt;\nUploads a chunk of a multipart upload.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "s3"
      ],
      "id": "5b1f90ea11780cf254dd16bc2f5b4e21"
    },
    {
      "title": "TLS (SSL, HTTPS)",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Set Up",
      "description": "Learn how to deploy a cluster with Transport Layer Security (TLS).",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/tls/",
      "relURI": "/latest/set-up/tls/",
      "body": "Secure internet browser connections and transactions via data encryption by deploying HPE ML Data Management with Transport Layer Security (TLS). Once set up, users can access HPE ML Data Management through a secure HTTPS connection (e.g., https://console.yourdomain.com)\nBefore You Start # You must have admin control over the domain you wish to use You must have already set up a subdomain for your HPE ML Data Management cluster (e.g., console.yourdomain.com) You must have added the subdomain to the proxy.host value in your Helm values.yaml file About DNS Records # You will need to access your domain&rsquo;s DNS records to complete the setup. For example, if you are deploying HPE ML Data Management in GCP, you will need to navigate to the Networking section of your GCP project. If your domain is registered with a third-party provider (e.g., Cloudflare), you will also need to log in to your account with that provider as well to register the subdomain with an NS record that matches the values of your subdomain&rsquo;s NS record in GCP.\nThe following guide assumes that you already have the following DNS records set up for your subdomain:\nRecord Type Example DNS Name Value A console.yourdomain.com. &lt;pachyderm.proxy.external.ip&gt; NS console.yourdomain.com. auto generated; do not edit SOA console.yourdomain.com. auto generated; do not edit üí° You can use Google&rsquo;s Dig tool to verify that records are accessible.\nHow to Deploy with TLS Enabled # At a high level, you need to do all of the following to enable TLS:\nCreate a signed certificate (obtainable from any CA, such as Let&rsquo;s Encrypt, HashiCorp Vault, or Venafi). Create a Kubernetes secret with the certificate. Configure the TLS section of your Helm values.yaml file by enabling TLS and specifying the secret name. Upgrade your Cluster. Connect using pachctl connect grpcs://&lt;your.proxy.host.value&gt;:443. ‚ÑπÔ∏è When using custom CA-signed certs (instead of certs from Let&rsquo;s Encrypt, HashiCorp Vault, or Venafi) the .crt must include the full certificate chain (root, intermediates, and leaf). You must also set global.customCaCerts to true in the Helm values.yaml file.\nVia certbot (Let&rsquo;s Encrypt) # The following steps are just one example of how to obtain a signed certificate from Let&rsquo;s Encrypt.\nInstall certbot. Options: Mac Linux brew install certbot sudo snap install --classic certbot sudo ln -s /snap/bin/certbot /usr/bin/certbot Run the following command (replace &lt;your@email.address&gt; and &lt;your.proxy.host.value&gt;):\ncertbot certonly --manual -v \\ --preferred-challenges=dns \\ --email &lt;your@email.address&gt; \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d &lt;your.proxy.host.value&gt; You will be asked to fill out some identifying information. When prompted, create a TXT record with your DNS provider. The TXT record will look something like this:\n_acme-challenge.&lt;your.proxy.host.value&gt;. IN TXT &#34;&lt;random-string&gt;&#34; Once you have created the TXT record, press Enter to continue in the terminal. If the TXT record is not created and accessible, the command will fail.\nCreate a secret (replace &lt;your.proxy.host.value&gt;):\nkubectl create secret tls pachyderm-tls-secret-001 \\ --cert /etc/letsencrypt/live/&lt;your.proxy.host.value&gt;/fullchain.pem \\ --key /etc/letsencrypt/live/&lt;your.proxy.host.value&gt;/privkey.pem \\ --dry-run=client \\ --output=yaml &gt; pachyderm-tls-secret-001.yaml Apply the secret:\nkubectl apply -f pachyderm-tls-secret-001.yaml Configure the TLS section of your Helm values.yaml file by enabling TLS and specifying the secret name:\ntls: enabled: true secretName: pachyderm-tls-secret-001 Upgrade your Helm chart:\nhelm upgrade pachyderm pachyderm/pachyderm -f values.yaml Connect to HPE ML Data Management via SSL:\npachctl connect grpcs://&lt;your.proxy.host.value&gt;:443 That&rsquo;s it! You can now consider setting up Authentication to take advantage of our Authorization (RBAC) system.\nTroubleshooting # Can&rsquo;t Connect to Console Through Subdomain # It&rsquo;s possible that the external IP address of your cluster has changed during a Helm upgrade. To check, run the following command:\nkubectl get service pachyderm-proxy -o wide | grep pachyderm-proxy | awk &#39;{print $4}&#39; If that IP address does not match the IP address found in your A DNS record, you will need to update your DNS records to point to the new IP address.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "deployment",
        "TLS",
        "SSL",
        "HTTPS",
        "security",
        "authentication",
        "authorization",
        "enterprise"
      ],
      "id": "8f5fdf0d5ad1080c85a3b42dec2d6eed"
    },
    {
      "title": "Tracing (Jaeger)",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Set Up",
      "description": "Learn how to trace requests with Jaeger when diagnosing slow cluster performance.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/set-up/tracing/",
      "relURI": "/latest/set-up/tracing/",
      "body": "HPE ML Data Management has the ability to trace requests using Jaeger. This can be useful when diagnosing slow clusters.\nCollecting Traces # To use tracing in HPE ML Data Management, complete the following steps:\nRun Jaeger in Kubernetes\nkubectl apply -f https://raw.githubusercontent.com/pachyderm/pachyderm/v2.7.3/etc/deploy/tracing/jaeger-all-in-one.yaml Point HPE ML Data Management at Jaeger\nFor pachctl, run:\nexport JAEGER_ENDPOINT=localhost:14268 kubectl port-forward svc/jaeger-collector 14268 &amp; # Collector service For pachd, run:\nkubectl delete po -l suite=pachyderm,app=pachd The port-forward command is necessary because pachctl sends traces to Jaeger (it actually initiates every trace), and reads the JAEGER_ENDPOINT environment variable for the address to which it will send the trace info.\nRestarting the pachd pod is necessary because pachd also sends trace information to Jaeger, but it reads the environment variables corresponding to the Jaeger service[1] on startup to find Jaeger (the Jaeger service is created by the jaeger-all-in-one.yaml manifest). Killing the pods restarts them, which causes them to connect to Jaeger.\nSend HPE ML Data Management a traced request by setting the PACH_TRACE environment variable to &ldquo;true&rdquo; before running any pachctl command (note that JAEGER_ENDPOINT must also be set/exported):\nPACH_TRACE=true pachctl list job # for example HPE ML Data Management does not recommend exporting PACH_TRACE because tracing calls can slow them down and make interesting traces hard to find in Jaeger. Therefore, you might want to set this variable for the specific calls you want to trace.\nHowever, HPE ML Data Management&rsquo;s client library reads this variable and implements the relevant tracing, so any binary that uses HPE ML Data Management&rsquo;s go client library can trace calls if these variables are set.\nView Traces # To view traces, run:\nkubectl port-forward svc/jaeger-query 16686:80 &amp; # UI service Then, connect to localhost:16686 in your browser, and you should see all collected traces.\n‚ÑπÔ∏è See Also: Kubernetes Service Environment Variables\nTroubleshooting # If you see &lt;trace-without-root-span&gt;, this likely means that pachd has connected to Jaeger, but pachctl has not. Make sure that the JAEGER_ENDPOINT environment variable is set on your local machine, and that kubectl port-forward &quot;po/${jaeger_pod}&quot; 14268 is running.\nIf you see a trace appear in Jaeger with no subtraces, like so:\nThis might mean that pachd has not connected to Jaeger, but pachctl has. Restart the pachd pods after creating the Jaeger service in Kubernetes.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "logs",
        "jaeger"
      ],
      "id": "fa4556712df4b4946d06b30ce65bba9e"
    },
    {
      "title": "Manage",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "Manage your instance, its GPUs, backups, and more.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/",
      "relURI": "/latest/manage/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "47120c47e2a031b8a8db31dc2f3fa462"
    },
    {
      "title": "Helm Chart Values (HCVs)",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Manage",
      "description": "Learn about the configurable helm chart attributes available.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/",
      "relURI": "/latest/manage/helm-values/",
      "body": "Explore all of the available Helm chart values for HPE ML Data Management in this section.\n‚ÑπÔ∏è Looking for a comprehensive list of all attributes and comments? View a complete values.yml file.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "configuration",
        "helm",
        "helm chart"
      ],
      "id": "586257f10592d26975472d6ad35f37d6"
    },
    {
      "title": "Deploy Target HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Choose where you're deploying (Local, Cloud).",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/deploy-target/",
      "relURI": "/latest/manage/helm-values/deploy-target/",
      "body": " About # The Deploy Target section defines where you&rsquo;re deploying HPE ML Data Management; this is typically located at the top of your values.yaml file.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Amazon Custom Google Local Microsoft Minio # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;AMAZON&#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;CUSTOM&#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;GOOGLE&#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;LOCAL&#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;MICROSOFT&#34; # Deploy Target configures the storage backend and cloud provider settings (storage classes, etc). # options: GOOGLE, AMAZON, MINIO, MICROSOFT, CUSTOM or LOCAL. deployTarget: &#34;MINIO&#34; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "1dd93627f5e01ff949bf20959c08cf95"
    },
    {
      "title": "Global HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Configure the postgresql database connection.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/global/",
      "relURI": "/latest/manage/helm-values/global/",
      "body": " About # The Global section configures the connection to the PostgreSQL database. By default, it uses the included Postgres service.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: With Secrets Without Secrets global: postgresql: postgresqlAuthType: &#34;md5&#34; # sets the auth type used with postgres &amp; pg-bounder; options include &#34;md5&#34; and &#34;scram-sha-256&#34; postgresqlUsername: &#34;pachyderm&#34; # defines the username to access the pachyderm and dex databases postgresqlExistingSecretName: &#34;&#34; # leave blank if using password postgresqlExistingSecretKey: &#34;&#34; # leave blank if using password postgresqlDatabase: &#34;pachyderm&#34; # defines the database name where pachyderm data will be stored postgresqlHost: &#34;postgres&#34; # defines the postgresql database host to connect to postgresqlPort: &#34;5432&#34; # defines he postgresql database port to connect to postgresqlSSL: &#34;disable&#34; # defines the SSL mode used to connect pg-bouncer to postgrs postgresqlSSLCACert: &#34;&#34; # defines the CA Certificate required to connect to Postgres postgresqlSSLSecret: &#34;&#34; # defines the TLS Secret with cert/key to connect to Postgres identityDatabaseFullNameOverride: &#34;&#34; # defines the DB name that dex connects to; defaults to &#34;Dex&#34; imagePullSecrets: [] # allows you to pull images from private repositories; also added to pipeline workers # Example: # imagePullSecrets: # - regcred customCaCerts: false # loads the cert file in pachd-tls-cert as the root cert for pachd, console, and enterprise-server proxy: &#34;&#34; # sets server address for outbound cluster traffic noProxy: &#34;&#34; # if proxy is set, allows a comma-separated list of destinations that bypass the proxy securityContexts: # set security context runAs users. If running on openshift, set enabled to false as openshift creates its own contexts. enabled: true global: postgresql: postgresqlAuthType: &#34;md5&#34; # sets the auth type used with postgres &amp; pg-bounder; options include &#34;md5&#34; and &#34;scram-sha-256&#34; postgresqlUsername: &#34;pachyderm&#34; # defines the username to access the pachyderm and dex databases postgresqlPostgresPassword: &#34;insecure-root-password&#34; # leave blank if using a secret postgresqlDatabase: &#34;pachyderm&#34; # defines the database name where pachyderm data will be stored postgresqlHost: &#34;postgres&#34; # defines the postgresql database host to connect to postgresqlPort: &#34;5432&#34; # defines he postgresql database port to connect to postgresqlSSL: &#34;disable&#34; # defines the SSL mode used to connect pg-bouncer to postgrs postgresqlSSLCACert: &#34;&#34; # defines the CA Certificate required to connect to Postgres postgresqlSSLSecret: &#34;&#34; # defines the TLS Secret with cert/key to connect to Postgres identityDatabaseFullNameOverride: &#34;&#34; # defines the DB name that dex connects to; defaults to &#34;Dex&#34; imagePullSecrets: [] # allows you to pull images from private repositories; also added to pipeline workers customCaCerts: false # loads the cert file in pachd-tls-cert as the root cert for pachd, console, and enterprise-server proxy: &#34;&#34; # sets server address for outbound cluster traffic noProxy: &#34;&#34; # if proxy is set, allows a comma-separated list of destinations that bypass the proxy # Set security context runAs users. If running on openshift, set enabled to false as openshift creates its own contexts securityContexts: enabled: true ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "a6d8d52aa8b7f91c5b389b04e3aed62d"
    },
    {
      "title": "Console HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Configure the platform's UI.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/console/",
      "relURI": "/latest/manage/helm-values/console/",
      "body": " About # Console is the Graphical User Interface (GUI) for HPE ML Data Management. Users that would prefer to navigate and manage through their project resources visually can connect to Console by authenticating against your configured OIDC. For personal-machine installations of HPE ML Data Management, a user may access Console without authentication via localhost.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Enabled No Metrics Enabled With Metrics Disabled console: enabled: true # deploys Console UI annotations: {} image: # defines which image to use for the console; replicates the --console-image &amp; --registry arguments to pachctl repository: &#34;pachyderm/haberdashery&#34; # defines image repo location pullPolicy: &#34;IfNotPresent&#34; tag: &#34;2.3.3-1&#34; # defines the image repo to pull from priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] podLabels: {} # specifies labels to add to the console pod. resources: # specifies the resource request and limits; unset by default. {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; config: # defines primary configuration settings, including authentication. reactAppRuntimeIssuerURI: &#34;&#34; # defines the pachd oauth address accessible to outside clients. oauthRedirectURI: &#34;&#34; # defines the oauth callback address within console that the pachd oauth service would redirect to. oauthClientID: &#34;console&#34; # defines the client identifier for the Console with pachd oauthClientSecret: &#34;&#34; # defines the secret configured for the client with pachd; if blank, autogenerated. oauthClientSecretSecretName: &#34;&#34; # uses the value of an existing k8s secret by pulling from the `OAUTH_CLIENT_SECRET` key. graphqlPort: 4000 # defines the http port that the console service will be accessible on. pachdAddress: &#34;pachd-peer:30653&#34; disableTelemetry: false # disables analytics and error data collection service: annotations: {} labels: {} # specifies labels to add to the console service. type: ClusterIP # specifies the Kubernetes type of the console service; default is `ClusterIP`. console: enabled: true # deploys Console UI annotations: {} image: # defines which image to use for the console; replicates the --console-image &amp; --registry arguments to pachctl repository: &#34;pachyderm/haberdashery&#34; # defines image repo location pullPolicy: &#34;IfNotPresent&#34; tag: &#34;2.3.3-1&#34; # defines the image repo to pull from priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] podLabels: {} # specifies labels to add to the console pod. resources: # specifies the resource request and limits; unset by default. {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; config: # defines primary configuration settings, including authentication. reactAppRuntimeIssuerURI: &#34;&#34; # defines the pachd oauth address accessible to outside clients. oauthRedirectURI: &#34;&#34; # defines the oauth callback address within console that the pachd oauth service would redirect to. oauthClientID: &#34;console&#34; # defines the client identifier for the Console with pachd oauthClientSecret: &#34;&#34; # defines the secret configured for the client with pachd; if blank, autogenerated. oauthClientSecretSecretName: &#34;&#34; # uses the value of an existing k8s secret by pulling from the `OAUTH_CLIENT_SECRET` key. graphqlPort: 4000 # defines the http port that the console service will be accessible on. pachdAddress: &#34;pachd-peer:30653&#34; disableTelemetry: true # disables analytics and error data collection service: annotations: {} labels: {} # specifies labels to add to the console service. type: ClusterIP # specifies the Kubernetes type of the console service; default is `ClusterIP`. console: enabled: false ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "a58f4c940b4ac9c429b47a300142c49d"
    },
    {
      "title": "Enterprise Server HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Configure the Enterprise Server for production deployments.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/enterprise-server/",
      "relURI": "/latest/manage/helm-values/enterprise-server/",
      "body": " About # Enterprise Server is a production management layer that centralizes the licensing registration of multiple HPE ML Data Management clusters for Enterprise use and the setup of user authorization/authentication via OIDC.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: TLS Disabled TLS New Secret TLS Existing Secret ES Disabled enterpriseServer: enabled: true affinity: {} annotations: {} tolerations: [] priorityClassName: &#34;&#34; nodeSelector: {} service: type: ClusterIP apiGRPCPort: 31650 prometheusPort: 31656 oidcPort: 31657 identityPort: 31658 s3GatewayPort: 31600 tls: enabled: false resources: {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; podLabels: {} # specifies labels to add to the pachd pod. clusterDeploymentID: &#34;&#34; image: repository: &#34;pachyderm/pachd&#34; pullPolicy: &#34;IfNotPresent&#34; tag: &#34;&#34; # defaults to the chart‚Äôs specified appVersion. enterpriseServer: enabled: true affinity: {} annotations: {} tolerations: [] priorityClassName: &#34;&#34; nodeSelector: {} service: type: ClusterIP apiGRPCPort: 31650 prometheusPort: 31656 oidcPort: 31657 identityPort: 31658 s3GatewayPort: 31600 tls: enabled: true newSecret: create: true crt: &#34;&#34; key: &#34;&#34; resources: {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; podLabels: {} # specifies labels to add to the pachd pod. clusterDeploymentID: &#34;&#34; image: repository: &#34;pachyderm/pachd&#34; pullPolicy: &#34;IfNotPresent&#34; tag: &#34;&#34; # defaults to the chart‚Äôs specified appVersion. enterpriseServer: enabled: true affinity: {} annotations: {} tolerations: [] priorityClassName: &#34;&#34; nodeSelector: {} service: type: ClusterIP apiGRPCPort: 31650 prometheusPort: 31656 oidcPort: 31657 identityPort: 31658 s3GatewayPort: 31600 tls: enabled: true secretName: &#34;&#34; resources: {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; podLabels: {} # specifies labels to add to the pachd pod. clusterDeploymentID: &#34;&#34; image: repository: &#34;pachyderm/pachd&#34; pullPolicy: &#34;IfNotPresent&#34; tag: &#34;&#34; # defaults to the chart‚Äôs specified appVersion. enterpriseServer: enabled: false ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "a8489a97563a0515c49ab63a3ed61884"
    },
    {
      "title": "ETCD HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Configure your ETCD key-value storage cluster.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/etcd/",
      "relURI": "/latest/manage/helm-values/etcd/",
      "body": " About # The ETCD section configures the ETCD cluster in the deployment.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\netcd: affinity: {} annotations: {} dynamicNodes: 1 # sets the number of nodes in the etcd StatefulSet; analogous to the --dynamic-etcd-nodes argument to pachctl image: repository: &#34;pachyderm/etcd&#34; tag: &#34;v3.5.1&#34; pullPolicy: &#34;IfNotPresent&#34; maxTxnOps: 10000 # sets the --max-txn-ops in the container args priorityClassName: &#34;&#34; nodeSelector: {} podLabels: {} # specifies labels to add to the etcd pod. resources: # specifies the resource request and limits {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; storageClass: &#34;&#34; # defines what existing storage class to use; analogous to --etcd-storage-class argument to pachctl storageSize: 10Gi # specifies the size of the volume to use for etcd. service: annotations: {} # specifies annotations to add to the etcd service. labels: {} # specifies labels to add to the etcd service. type: ClusterIP # specifies the Kubernetes type of the etcd service. tolerations: [] ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "b33df9816c2b483b5ab1e37aad44a5cc"
    },
    {
      "title": "Ingress HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "This section is being deprecated; use proxy instead.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/ingress/",
      "relURI": "/latest/manage/helm-values/ingress/",
      "body": " About # ‚ö†Ô∏è ingress will be removed from the helm chart once the deployment of HPE ML Data Management with a proxy becomes mandatory.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: TLS Existing Secret TLS New Secret TLS Disabled ingress: enabled: true annotations: {} host: &#34;&#34; uriHttpsProtoOverride: false # if true, adds the https protocol to the ingress URI routes without configuring certs tls: enabled: true secretName: &#34;&#34; ingress: enabled: true annotations: {} host: &#34;&#34; uriHttpsProtoOverride: false # if true, adds the https protocol to the ingress URI routes without configuring certs tls: enabled: true newSecret: create: true crt: &#34;&#34; key: &#34;&#34; ingress: enabled: true annotations: {} host: &#34;&#34; uriHttpsProtoOverride: false # if true, adds the https protocol to the ingress URI routes without configuring certs tls: enabled: false ",
      "beta": "<no value>",
      "hidden": "true",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "1a713319c511366b7901de134c77c8b1"
    },
    {
      "title": "Loki HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Set up logs with Loki, Grafana, and Promtail.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/loki/",
      "relURI": "/latest/manage/helm-values/loki/",
      "body": " About # Loki Stack contains values that are passed to the loki-stack subchart. For more details on each service, see their official documentation:\nLoki storage documentation Grafana documentation Promtail documentation Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nloki-stack: loki: serviceAccount: automountServiceAccountToken: false persistence: enabled: true accessModes: - ReadWriteOnce size: 10Gi # More info for setting up storage classes on various cloud providers: # AWS: https://docs.aws.amazon.com/eks/latest/userguide/storage-classes.html # GCP: https://cloud.google.com/compute/docs/disks/performance#disk_types # Azure: https://docs.microsoft.com/en-us/azure/aks/concepts-storage#storage-classes storageClassName: &#34;&#34; annotations: {} priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] config: limits_config: retention_period: 24h retention_stream: - selector: &#39;{suite=&#34;pachyderm&#34;}&#39; priority: 1 period: 168h # = 1 week grafana: enabled: false promtail: config: clients: - url: &#34;http://{{ .Release.Name }}-loki:3100/loki/api/v1/push&#34; snippets: # The scrapeConfigs section is copied from loki-stack-2.6.4 # The pipeline_stages.match stanza has been added to prevent multiple lokis in a cluster from mixing their logs. scrapeConfigs: | - job_name: kubernetes-pods pipeline_stages: {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }} - match: selector: &#39;{namespace!=&#34;{{ .Release.Namespace }}&#34;}&#39; action: drop kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: - __meta_kubernetes_pod_controller_name regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})? action: replace target_label: __tmp_controller_name - source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_name - __meta_kubernetes_pod_label_app - __tmp_controller_name - __meta_kubernetes_pod_name regex: ^;*([^;]+)(;.*)?$ action: replace target_label: app - source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_instance - __meta_kubernetes_pod_label_release regex: ^;*([^;]+)(;.*)?$ action: replace target_label: instance - source_labels: - __meta_kubernetes_pod_label_app_kubernetes_io_component - __meta_kubernetes_pod_label_component regex: ^;*([^;]+)(;.*)?$ action: replace target_label: component {{- if .Values.config.snippets.addScrapeJobLabel }} - replacement: kubernetes-pods target_label: scrape_job {{- end }} {{- toYaml .Values.config.snippets.common | nindent 4 }} {{- with .Values.config.snippets.extraRelabelConfigs }} {{- toYaml . | nindent 4 }} {{- end }} pipelineStages: - cri: {} common: # This is copy and paste of existing actions, so we don&#39;t lose them. # Cf. https://github.com/grafana/loki/issues/3519#issuecomment-1125998705 - action: replace source_labels: - __meta_kubernetes_pod_node_name target_label: node_name - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace replacement: $1 separator: / source_labels: - namespace - app target_label: job - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - action: replace replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - action: replace regex: true/(.*) replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash - __meta_kubernetes_pod_container_name target_label: __path__ - action: keep regex: pachyderm source_labels: - __meta_kubernetes_pod_label_suite # this gets all kubernetes labels as well - action: labelmap regex: __meta_kubernetes_pod_label_(.+) livenessProbe: failureThreshold: 5 tcpSocket: port: http-metrics initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "46cbe906527cbeaf91e526b7a4d54294"
    },
    {
      "title": "PachD HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Configure the core settings.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/pachd/",
      "relURI": "/latest/manage/helm-values/pachd/",
      "body": " Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: With Secrets Without Secrets pachd: enabled: true preflightChecks: enabled: true # runs kube validation preflight checks. affinity: {} annotations: {} clusterDeploymentID: &#34;&#34; # sets HPE ML Data Management cluster ID. configJob: annotations: {} goMaxProcs: 0 # passed as GOMAXPROCS to the pachd container. image: repository: &#34;pachyderm/pachd&#34; pullPolicy: &#34;IfNotPresent&#34; tag: &#34;&#34; # sets worker image tag; defaults to appVersion. logFormat: &#34;json&#34; logLevel: &#34;info&#34; lokiDeploy: true lokiLogging: true metrics: enabled: true endpoint: &#34;&#34; # provide the URL of the metrics endpoint. priorityClassName: &#34;&#34; nodeSelector: {} podLabels: {} # adds labels to the pachd pod. replicas: 1 # sets the number of pachd running pods resources: # specifies the resource requests &amp; limits {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; requireCriticalServersOnly: false externalService: enabled: false # Creates a service that&#39;s safe to expose. loadBalancerIP: &#34;&#34; apiGRPCPort: 30650 s3GatewayPort: 30600 annotations: {} service: labels: {} # adds labels to the pachd service. type: &#34;ClusterIP&#34; # specifies pachd service&#39;s Kubernetes type annotations: {} apiGRPCPort: 30650 prometheusPort: 30656 oidcPort: 30657 identityPort: 30658 s3GatewayPort: 30600 #apiGrpcPort: # expose: true # port: 30650 activateEnterpriseMember: false # connects to an existing enterprise server. activateAuth: true # bootstraps auth via the config job. enterpriseLicenseKey: &#34;&#34; # activates enterprise if provided. enterpriseLicenseKeySecretName: &#34;&#34; # pulls value from k8s secret key &#34;enterprise-license-key&#34; rootToken: &#34;&#34; # autogenerated if not provided; stored in k8s secret &#34;pachyderm-bootstrap-config.rootToken&#34; rootTokenSecretName: &#34;&#34; # passes rooToken value from k8s secret key &#34;root-token&#34; enterpriseSecret: &#34;&#34; # autogenerated if not provided; stored in k8s secret &#34;pachyderm-bootstrap-config.enterpriseSecret&#34; enterpriseSecretSecretName: &#34;&#34; # passes value from k8s secret key &#34;enterprise-secret&#34; oauthClientID: pachd oauthClientSecret: &#34;&#34; # autogenerated if not provided; stored in k8s secret &#34;pachyderm-bootstrap-config.authConfig.clientSecret&#34; oauthClientSecretSecretName: &#34;&#34; # passes value from k8s secret key &#34;pachd-oauth-client-secret&#34; oauthRedirectURI: &#34;&#34; enterpriseServerToken: &#34;&#34; # authenticates to a enterprise server &amp; registers this cluster as a member if activateEnterpriseMember is true. enterpriseServerTokenSecretName: &#34;&#34; # passes value from k8s secret key &#34;enterprise-server-token&#34; if activateEnterpriseMember is true. enterpriseServerAddress: &#34;&#34; enterpriseCallbackAddress: &#34;&#34; localhostIssuer: &#34;&#34; # Indicates to pachd whether dex is embedded in its process; &#34;true&#34;, &#34;false&#34;, or &#34;&#34; pachAuthClusterRoleBindings: {} # map initial users to their list of roles. # robot:wallie: # - repoReader # robot:eve: # - repoWriter additionalTrustedPeers: [] # configures identity service to recognize trusted peers. # - example-app serviceAccount: create: true additionalAnnotations: {} name: &#34;pachyderm&#34; storage: backend: &#34;&#34; # options: GOOGLE, AMAZON, MINIO, MICROSOFT or LOCAL amazon: bucket: &#34;&#34; # sets the S3 bucket to use. cloudFrontDistribution: &#34;&#34; # sets the CloudFront distribution in the storage secrets. customEndpoint: &#34;&#34; disableSSL: false id: &#34;&#34; # sets the Amazon access key ID logOptions: &#34;&#34; # case-sensitive comma-separated list: &#39;Debug&#39;, &#39;Signing&#39;, &#39;HTTPBody&#39;, &#39;RequestRetries&#39;, &#39;EventStreamBody&#39;, or &#39;all&#39; maxUploadParts: 10000 verifySSL: true partSize: &#34;5242880&#34; # sets part size for object storage uploads; must be a string. region: &#34;&#34; # sets AWS region retries: 10 reverse: true secret: &#34;&#34; # sets the Amazon secret access key to use. timeout: &#34;5m&#34; # sets the timeout for object storage requests. token: &#34;&#34; # sets the Amazon token to use. uploadACL: &#34;bucket-owner-full-control&#34; google: bucket: &#34;&#34; cred: &#34;&#34; # sets GCP service account private key as string. # cred: | # { # &#34;type&#34;: &#34;service_account&#34;, # &#34;project_id&#34;: &#34;‚Ä¶&#34;, # &#34;private_key_id&#34;: &#34;‚Ä¶&#34;, # &#34;private_key&#34;: &#34;-----BEGIN PRIVATE KEY-----\\n‚Ä¶\\n-----END PRIVATE KEY-----\\n&#34;, # &#34;client_email&#34;: &#34;‚Ä¶@‚Ä¶.iam.gserviceaccount.com&#34;, # &#34;client_id&#34;: &#34;‚Ä¶&#34;, # &#34;auth_uri&#34;: &#34;https://accounts.google.com/o/oauth2/auth&#34;, # &#34;token_uri&#34;: &#34;https://oauth2.googleapis.com/token&#34;, # &#34;auth_provider_x509_cert_url&#34;: &#34;https://www.googleapis.com/oauth2/v1/certs&#34;, # &#34;client_x509_cert_url&#34;: &#34;https://www.googleapis.com/robot/v1/metadata/x509/‚Ä¶%40‚Ä¶.iam.gserviceaccount.com&#34; # } local: hostPath: &#34;&#34; # path where PFS metadata is stored; must end with &#34;/&#34;. requireRoot: true # root required for hostpath, but we run rootless in CI microsoft: container: &#34;&#34; id: &#34;&#34; secret: &#34;&#34; minio: bucket: &#34;&#34; # sets bucket name. endpoint: &#34;&#34; # format: hostname:port id: &#34;&#34; # username/id with readwrite access to the bucket. secret: &#34;&#34; # the secret/password of the user with readwrite access to the bucket. secure: &#34;false&#34; # enables https for minio if &#34;true&#34; signature: &#34;&#34; # Enables S3v2 support by setting signature to &#34;1&#34;; being deprecated. putFileConcurrencyLimit: 100 # sets the maximum number of files to upload or fetch from remote sources uploadConcurrencyLimit sets the maximum number of concurrent; analogous to --put-file-concurrency-limit argument to pachctl uploadConcurrencyLimit: 100 # object storage uploads per Pachd instance; analogous to --upload-concurrency-limit argument to pachctl compactionShardSizeThreshold: 0 # the total size of the files in a shard. compactionShardCountThreshold: 0 # the total number of files in a shard. memoryThreshold: 0 levelFactor: 0 maxFanIn: 10 maxOpenFileSets: 50 # diskCacheSize and memoryCacheSize are defined in units of 8 Mb chunks. The default is 100 chunks which is 800 Mb. diskCacheSize: 100 memoryCacheSize: 100 ppsWorkerGRPCPort: 1080 storageGCPeriod: 0 # the number of seconds between PFS&#39;s garbage collection cycles; &lt;0 disables garbage collection; 0 defaults to pachyderm&#39;s internal config. storageChunkGCPeriod: 0 # the number of seconds between chunk garbage collection cycles; &lt;0 disables chunk garbage collection; 0 defaults to pachyderm&#39;s internal config. # There are three options for TLS: # 1. Disabled # 2. Enabled, existingSecret, specify secret name # 3. Enabled, newSecret, must specify cert, key and name tls: enabled: false secretName: &#34;&#34; newSecret: create: false crt: &#34;&#34; key: &#34;&#34; tolerations: [] worker: image: repository: &#34;pachyderm/worker&#34; pullPolicy: &#34;IfNotPresent&#34; # Worker tag is set under pachd.image.tag (they should be kept in lock step) serviceAccount: create: true additionalAnnotations: {} name: &#34;pachyderm-worker&#34; # sets the name of the worker service account; analogous to --worker-service-account argument to pachctl. rbac: create: true # indicates whether RBAC resources should be created; analogous to --no-rbac to pachctl # Set up default resources for pipelines that don&#39;t include any requests or limits. The values # are k8s resource quantities, so &#34;1Gi&#34;, &#34;2&#34;, etc. Set to &#34;0&#34; to disable setting any defaults. defaultPipelineCPURequest: &#34;&#34; defaultPipelineMemoryRequest: &#34;&#34; defaultPipelineStorageRequest: &#34;&#34; defaultSidecarCPURequest: &#34;&#34; defaultSidecarMemoryRequest: &#34;&#34; defaultSidecarStorageRequest: &#34;&#34; pachd: enabled: true preflightChecks: # if enabled runs kube validation preflight checks. enabled: true affinity: {} annotations: {} # clusterDeploymentID sets the HPE ML Data Management cluster ID. clusterDeploymentID: &#34;&#34; configJob: annotations: {} # goMaxProcs is passed as GOMAXPROCS to the pachd container. goMaxProcs: 0 image: repository: &#34;pachyderm/pachd&#34; pullPolicy: &#34;IfNotPresent&#34; # tag defaults to the chart‚Äôs specified appVersion. # This sets the worker image tag as well (they should be kept in lock step) tag: &#34;&#34; logFormat: &#34;json&#34; logLevel: &#34;info&#34; # If lokiDeploy is true, a HPE ML Data Management-specific instance of Loki will # be deployed. lokiDeploy: true # lokiLogging enables Loki logging if set. lokiLogging: true metrics: # enabled sets the METRICS environment variable if set. enabled: true # endpoint should be the URL of the metrics endpoint. endpoint: &#34;&#34; priorityClassName: &#34;&#34; nodeSelector: {} # podLabels specifies labels to add to the pachd pod. podLabels: {} # resources specifies the resource requests and limits # replicas sets the number of pachd running pods replicas: 1 resources: {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; # requireCriticalServersOnly only requires the critical pachd # servers to startup and run without errors. It is analogous to the # --require-critical-servers-only argument to pachctl deploy. requireCriticalServersOnly: false # If enabled, External service creates a service which is safe to # be exposed externally externalService: enabled: false # (Optional) specify the existing IP Address of the load balancer loadBalancerIP: &#34;&#34; apiGRPCPort: 30650 s3GatewayPort: 30600 annotations: {} service: # labels specifies labels to add to the pachd service. labels: {} # type specifies the Kubernetes type of the pachd service. type: &#34;ClusterIP&#34; annotations: {} apiGRPCPort: 30650 prometheusPort: 30656 oidcPort: 30657 identityPort: 30658 s3GatewayPort: 30600 #apiGrpcPort: # expose: true # port: 30650 # DEPRECATED: activateEnterprise is no longer used. activateEnterprise: false ## if pachd.activateEnterpriseMember is set, enterprise will be activated and connected to an existing enterprise server. ## if pachd.enterpriseLicenseKey is set, enterprise will be activated. activateEnterpriseMember: false ## if pachd.activateAuth is set, auth will be bootstrapped by the config-job. activateAuth: true ## the license key used to activate enterprise features enterpriseLicenseKey: &#34;&#34; rootToken: &#34;&#34; enterpriseSecret: &#34;&#34; # enterpriseSecretSecretName is used to pass the enterprise secret value via an existing k8s secret. # The value is pulled from the key, &#34;enterprise-secret&#34;. enterpriseSecretSecretName: &#34;&#34; # if a secret is not provided, a secret will be autogenerated on install and stored in the k8s secret &#39;pachyderm-bootstrap-config.authConfig.clientSecret&#39; oauthClientID: pachd oauthClientSecret: &#34;&#34; # oauthClientSecretSecretName is used to set the OAuth Client Secret via an existing k8s secret. # The value is pulled from the key, &#34;pachd-oauth-client-secret&#34;. oauthClientSecretSecretName: &#34;&#34; oauthRedirectURI: &#34;&#34; # DEPRECATED: enterpriseRootToken is deprecated, in favor of enterpriseServerToken # NOTE only used if pachd.activateEnterpriseMember == true enterpriseRootToken: &#34;&#34; # enterpriseServerToken represents a token that can authenticate to a separate Enterprise server, # and is used to complete the enterprise member registration process for this pachyderm cluster. # The user backing this token should have either the licenseAdmin &amp; identityAdmin roles assigned, or # the clusterAdmin role. # NOTE: only used if pachd.activateEnterpriseMember == true enterpriseServerToken: &#34;&#34; # enterpriseServerTokenSecretName is used to pass the enterpriseServerToken value via an existing k8s secret. # The value is pulled from the key, &#34;enterprise-server-token&#34;. enterpriseServerTokenSecretName: &#34;&#34; # only used if pachd.activateEnterpriseMember == true enterpriseServerAddress: &#34;&#34; enterpriseCallbackAddress: &#34;&#34; # Indicates to pachd whether dex is embedded in its process. localhostIssuer: &#34;&#34; # &#34;true&#34;, &#34;false&#34;, or &#34;&#34; (used string as bool doesn&#39;t support empty value) # set the initial pachyderm cluster role bindings, mapping a user to their list of roles # ex. # pachAuthClusterRoleBindings: # robot:wallie: # - repoReader # robot:eve: # - repoWriter pachAuthClusterRoleBindings: {} # additionalTrustedPeers is used to configure the identity service to recognize additional OIDC clients as trusted peers of pachd. # For example, see the following example or the dex docs (https://dexidp.io/docs/custom-scopes-claims-clients/#cross-client-trust-and-authorized-party). # additionalTrustedPeers: # - example-app additionalTrustedPeers: [] serviceAccount: create: true additionalAnnotations: {} name: &#34;pachyderm&#34; #TODO Set default in helpers / Wire up in templates storage: # backend configures the storage backend to use. It must be one # of GOOGLE, AMAZON, MINIO, MICROSOFT or LOCAL. This is set automatically # if deployTarget is GOOGLE, AMAZON, MICROSOFT, or LOCAL backend: &#34;&#34; amazon: # bucket sets the S3 bucket to use. bucket: &#34;&#34; # cloudFrontDistribution sets the CloudFront distribution in the # storage secrets. It is analogous to the # --cloudfront-distribution argument to pachctl deploy. cloudFrontDistribution: &#34;&#34; customEndpoint: &#34;&#34; # disableSSL disables SSL. It is analogous to the --disable-ssl # argument to pachctl deploy. disableSSL: false # id sets the Amazon access key ID to use. Together with secret # and token, it implements the functionality of the # --credentials argument to pachctl deploy. id: &#34;&#34; # logOptions sets various log options in HPE ML Data Management‚Äôs internal S3 # client. Comma-separated list containing zero or more of: # &#39;Debug&#39;, &#39;Signing&#39;, &#39;HTTPBody&#39;, &#39;RequestRetries&#39;, # &#39;RequestErrors&#39;, &#39;EventStreamBody&#39;, or &#39;all&#39; # (case-insensitive). See &#39;AWS SDK for Go&#39; docs for details. # logOptions is analogous to the --obj-log-options argument to # pachctl deploy. logOptions: &#34;&#34; # maxUploadParts sets the maximum number of upload parts. It is # analogous to the --max-upload-parts argument to pachctl # deploy. maxUploadParts: 10000 # verifySSL performs SSL certificate verification. It is the # inverse of the --no-verify-ssl argument to pachctl deploy. verifySSL: true # partSize sets the part size for object storage uploads. It is # analogous to the --part-size argument to pachctl deploy. It # has to be a string due to Helm and YAML parsing integers as # floats. Cf. https://github.com/helm/helm/issues/1707 partSize: &#34;5242880&#34; # region sets the AWS region to use. region: &#34;&#34; # retries sets the number of retries for object storage # requests. It is analogous to the --retries argument to # pachctl deploy. retries: 10 # reverse reverses object storage paths. It is analogous to the # --reverse argument to pachctl deploy. reverse: true # secret sets the Amazon secret access key to use. Together with id # and token, it implements the functionality of the # --credentials argument to pachctl deploy. secret: &#34;&#34; # timeout sets the timeout for object storage requests. It is # analogous to the --timeout argument to pachctl deploy. timeout: &#34;5m&#34; # token optionally sets the Amazon token to use. Together with # id and secret, it implements the functionality of the # --credentials argument to pachctl deploy. token: &#34;&#34; # uploadACL sets the upload ACL for object storage uploads. It # is analogous to the --upload-acl argument to pachctl deploy. uploadACL: &#34;bucket-owner-full-control&#34; google: bucket: &#34;&#34; # cred is a string containing a GCP service account private key, # in object (JSON or YAML) form. A simple way to pass this on # the command line is with the set-file flag, e.g.: # # helm install pachd -f my-values.yaml --set-file storage.google.cred=creds.json pachyderm/pachyderm cred: &#34;&#34; # Example: # cred: | # { # &#34;type&#34;: &#34;service_account&#34;, # &#34;project_id&#34;: &#34;‚Ä¶&#34;, # &#34;private_key_id&#34;: &#34;‚Ä¶&#34;, # &#34;private_key&#34;: &#34;-----BEGIN PRIVATE KEY-----\\n‚Ä¶\\n-----END PRIVATE KEY-----\\n&#34;, # &#34;client_email&#34;: &#34;‚Ä¶@‚Ä¶.iam.gserviceaccount.com&#34;, # &#34;client_id&#34;: &#34;‚Ä¶&#34;, # &#34;auth_uri&#34;: &#34;https://accounts.google.com/o/oauth2/auth&#34;, # &#34;token_uri&#34;: &#34;https://oauth2.googleapis.com/token&#34;, # &#34;auth_provider_x509_cert_url&#34;: &#34;https://www.googleapis.com/oauth2/v1/certs&#34;, # &#34;client_x509_cert_url&#34;: &#34;https://www.googleapis.com/robot/v1/metadata/x509/‚Ä¶%40‚Ä¶.iam.gserviceaccount.com&#34; # } local: # hostPath indicates the path on the host where the PFS metadata # will be stored. It must end in /. It is analogous to the # --host-path argument to pachctl deploy. hostPath: &#34;&#34; requireRoot: true #Root required for hostpath, but we run rootless in CI microsoft: container: &#34;&#34; id: &#34;&#34; secret: &#34;&#34; minio: # minio bucket name bucket: &#34;&#34; # the minio endpoint. Should only be the hostname:port, no http/https. endpoint: &#34;&#34; # the username/id with readwrite access to the bucket. id: &#34;&#34; # the secret/password of the user with readwrite access to the bucket. secret: &#34;&#34; # enable https for minio with &#34;true&#34; defaults to &#34;false&#34; secure: &#34;&#34; # Enable S3v2 support by setting signature to &#34;1&#34;. This feature is being deprecated signature: &#34;&#34; # putFileConcurrencyLimit sets the maximum number of files to # upload or fetch from remote sources (HTTP, blob storage) using # PutFile concurrently. It is analogous to the # --put-file-concurrency-limit argument to pachctl deploy. putFileConcurrencyLimit: 100 # uploadConcurrencyLimit sets the maximum number of concurrent # object storage uploads per Pachd instance. It is analogous to # the --upload-concurrency-limit argument to pachctl deploy. uploadConcurrencyLimit: 100 # The shard size corresponds to the total size of the files in a shard. # The shard count corresponds to the total number of files in a shard. # If either criteria is met, a shard will be created. compactionShardSizeThreshold: 0 compactionShardCountThreshold: 0 memoryThreshold: 0 levelFactor: 0 maxFanIn: 10 maxOpenFileSets: 50 # diskCacheSize and memoryCacheSize are defined in units of 8 Mb chunks. The default is 100 chunks which is 800 Mb. diskCacheSize: 100 ppsWorkerGRPCPort: 1080 # the number of seconds between PFS&#39;s garbage collection cycles. # if this value is set to 0, it will default to pachyderm&#39;s internal configuration. # if this value is less than 0, it will turn off garbage collection. storageGCPeriod: 0 # the number of seconds between chunk garbage collection cycles. # if this value is set to 0, it will default to pachyderm&#39;s internal configuration. # if this value is less than 0, it will turn off chunk garbage collection. storageChunkGCPeriod: 0 # There are three options for TLS: # 1. Disabled # 2. Enabled, existingSecret, specify secret name # 3. Enabled, newSecret, must specify cert, key and name tls: enabled: false secretName: &#34;&#34; newSecret: create: false crt: &#34;&#34; key: &#34;&#34; tolerations: [] worker: image: repository: &#34;pachyderm/worker&#34; pullPolicy: &#34;IfNotPresent&#34; # Worker tag is set under pachd.image.tag (they should be kept in lock step) serviceAccount: create: true additionalAnnotations: {} # name sets the name of the worker service account. Analogous to # the --worker-service-account argument to pachctl deploy. name: &#34;pachyderm-worker&#34; #TODO Set default in helpers / Wire up in templates rbac: # create indicates whether RBAC resources should be created. # Setting it to false is analogous to passing --no-rbac to pachctl # deploy. create: true # Set up default resources for pipelines that don&#39;t include any requests or limits. The values # are k8s resource quantities, so &#34;1Gi&#34;, &#34;2&#34;, etc. Set to &#34;0&#34; to disable setting any defaults. defaultPipelineCPURequest: &#34;&#34; defaultPipelineMemoryRequest: &#34;&#34; defaultPipelineStorageRequest: &#34;&#34; defaultSidecarCPURequest: &#34;&#34; defaultSidecarMemoryRequest: &#34;&#34; defaultSidecarStorageRequest: &#34;&#34; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "71f294939ff0e4eb0c3153ddcd56fe3d"
    },
    {
      "title": "PachW HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Create a pool of pachd instances that dynamically scale storage task handling.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/pachw/",
      "relURI": "/latest/manage/helm-values/pachw/",
      "body": " About # PachW enables fine-grained control of where compaction and object-storage interaction occur by running storage tasks in a dedicated Kubernetes deployment. Users can configure PachW&rsquo;s min and max replicas as well as define nodeSelectors, tolerations, and resource requests. Using PachW allows power users to save on costs by claiming fewer resources and running storage tasks on less expensive nodes.\n‚ö†Ô∏è If you are upgrading to 2.5.0+ for the first time and you wish to use PachW, you must calculate how many maxReplicas you need. By default, PachW is set to maxReplicas:1 &mdash; however, that is not sufficient for production runs.\nmaxReplicas # You should set the maxReplicas value to at least match the number of pipeline replicas that you have. For high performance, we suggest taking the following approach:\nnumber of pipelines * highest parallelism spec * 1.5 = maxReplicas\nLet&rsquo;s say you have 6 pipelines. One of these pipelines has a parallelism spec value of 6, and the rest are 5 or fewer.\n6 * 6 * 1.5 = 54\nminReplicas # Workloads that constantly process storage and compaction tasks because they are committing rapidly may want to increase minReplicas to have instances on standby.\nnodeSelectors # Workloads that utilize GPUs and other expensive resources may want to add a node selector to scope PachW instances to less expensive nodes.\nValues # Options: Enabled With Minimum With Specific Resources As Sidecars (Legacy) pachw: inheritFromPachd: true # defaults below configuration options like &#39;resources&#39; and &#39;tolerations&#39; to values from pachd maxReplicas: 1 minReplicas: 0 inSidecars: false #tolerations: [] #affinity: {} #nodeSelector: {} pachw: inheritFromPachd: true # defaults below configuration options like &#39;resources&#39; and &#39;tolerations&#39; to values from pachd maxReplicas: 6 # set to match the number of pipeline replicas you have; sample formula: pipeline count * parallelism = target maxReplicas minReplicas: 1 #tolerations: [] #affinity: {} #nodeSelector: {} #resources: # sets kubernetes resource configuration for pachw pods. If not defined, config from pachd is reused. We recommend defining resources when running pachw with a high value of maxReplicas (when formula is: target maxReplicas * 1.5). #limits: #cpu: &#34;1&#34; #memory: &#34;2G&#34; #requests: #cpu: &#34;1&#34; #memory: &#34;2G&#34; pachw: inheritFromPachd: false # defaults below configuration options like &#39;resources&#39; and &#39;tolerations&#39; to values from pachd maxReplicas: 6 # set to match the number of pipeline replicas you have; sample formula: pipeline count * parallelism = target maxReplicas minReplicas: 1 #tolerations: [] #affinity: {} #nodeSelector: {} resources: # sets kubernetes resource configuration for pachw pods. If not defined, config from pachd is reused. We recommend defining resources when running pachw with a high value of maxReplicas (when formula is: target maxReplicas * 1.5). limits: cpu: &#34;1&#34; memory: &#34;2G&#34; requests: cpu: &#34;1&#34; memory: &#34;2G&#34; pachw: inheritFromPachd: true # defaults below configuration options like &#39;resources&#39; and &#39;tolerations&#39; to values from pachd inSidecars: true # processes storage related tasks in pipeline storage sidecars like version 2.4.2 or less. maxReplicas: 1 #tolerations: [] #affinity: {} #nodeSelector: {} ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "e147ef3b0cb23cbdb197236d8997fbac"
    },
    {
      "title": "Kube Event Tail HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Deploy lightweight logging for Kubernetes events.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/kube-event-tail/",
      "relURI": "/latest/manage/helm-values/kube-event-tail/",
      "body": " About # Kube Event Tail deploys a lightweight app that watches Kubernetes events and echoes them into logs.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: All Events Namespace Events Disabled kubeEventTail: enabled: true clusterScope: false # if true, watches just events in its namespace image: repository: pachyderm/kube-event-tail pullPolicy: &#34;IfNotPresent&#34; tag: &#34;v0.0.6&#34; resources: limits: cpu: &#34;1&#34; memory: 100Mi requests: cpu: 100m memory: 45Mi kubeEventTail: enabled: true clusterScope: true # if true, watches just events in its namespace image: repository: pachyderm/kube-event-tail pullPolicy: &#34;IfNotPresent&#34; tag: &#34;v0.0.6&#34; resources: limits: cpu: &#34;1&#34; memory: 100Mi requests: cpu: 100m memory: 45Mi kubeEventTail: enabled: false ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "f8f07658fb58866424cae82db0c45c1b"
    },
    {
      "title": "PGBouncer HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Deploy a lightweight connection pooler for PostgreSQL.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/pgbouncer/",
      "relURI": "/latest/manage/helm-values/pgbouncer/",
      "body": " About # The PGBouncer section configures a PGBouncer Postgres connection pooler.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\npgbouncer: service: type: ClusterIP # defines the Kubernetes service type. annotations: {} priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] image: repository: pachyderm/pgbouncer tag: 1.16.1-debian-10-r82 resources: # defines resources in standard kubernetes format; unset by default. {} #limits: # cpu: &#34;1&#34; # memory: &#34;2G&#34; #requests: # cpu: &#34;1&#34; # memory: &#34;2G&#34; maxConnections: 1000 # defines the maximum number of concurrent connections into pgbouncer. defaultPoolSize: 20 # specifies the maximum number of concurrent connections from pgbouncer to the postgresql database. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "abfac03a8fb0079aa0405af1b3fd400e"
    },
    {
      "title": "PostgreSQL Subchart HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Use the bundled version of PostgreSQL (metadata storage) for testing on your personal machine.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/postgressql/",
      "relURI": "/latest/manage/helm-values/postgressql/",
      "body": " About # The PostgresQL section controls the Bitnami PostgreSQL subchart. HPE ML Data Management runs on Kubernetes, is backed by an object store of your choice, and comes with a bundled version of PostgreSQL (metadata storage) by default.\nWe recommended disabling this bundled PostgreSQL and using a managed database instance (such as RDS, CloudSQL, or PostgreSQL Server) for production environments.\nSee storage class details for your provider:\nAWS | Min: 500Gi (GP2) / 1,500 IOP GCP | Min: 50Gi / 1,500 IOPS Azure | Min: 256Gi / 1,100 IOPS Values # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Production Personal Machine postgresql: enabled: false # if false, you must specify a PostgreSQL database server connection @ global.postgresql postgresql: enabled: true # if false, you must specify a PostgreSQL database server connection @ global.postgresql image: tag: &#34;13.3.0&#34; # DEPRECATED from pachyderm 2.1.5 initdbScripts: dex.sh: | #!/bin/bash set -e psql -v ON_ERROR_STOP=1 --username &#34;$POSTGRES_USER&#34; --dbname &#34;$POSTGRES_DB&#34; &lt;&lt;-EOSQL CREATE DATABASE dex; GRANT ALL PRIVILEGES ON DATABASE dex TO &#34;$POSTGRES_USER&#34;; EOSQL fullnameOverride: postgres persistence: # Specify the storage class for the postgresql Persistent Volume (PV) storageClass: &#34;&#34; # specifies the size of the volume to use for postgresql size: 10Gi labels: suite: pachyderm primary: priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] readReplicas: priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "fdb02efdd76913b66fae7b1426606a3b"
    },
    {
      "title": "CloudSQL Auth Proxy HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Deploy on GCP with CloudSQL.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/cloudsql-auth-proxy/",
      "relURI": "/latest/manage/helm-values/cloudsql-auth-proxy/",
      "body": " About # The CloudSQL Auth Proxy section configures the CloudSQL Auth Proxy for deploying HPE ML Data Management on GCP with CloudSQL.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Enabled Disabled cloudsqlAuthProxy: connectionName: &#34;&#34; # may be found by running `gcloud sql instances describe INSTANCE_NAME --project PROJECT_ID` serviceAccount: &#34;&#34; # defines the account used to connect to the cloudSql instance iamLogin: false port: 5432 # the cloudql database port to expose. The default is `5432` enabled: true # controls whether to deploy the cloudsqlAuthProxy. Default is false. image: repository: &#34;gcr.io/cloudsql-docker/gce-proxy&#34; # the image repo to pull from; replicates --registry to pachctl pullPolicy: &#34;IfNotPresent&#34; tag: &#34;1.23.0&#34; # the image repo to pull from; replicates the --dash-image argument to pachctl deploy. priorityClassName: &#34;&#34; nodeSelector: {} tolerations: [] podLabels: {} # specifies labels to add to the dash pod. resources: {} # specifies the resource request and limits. # requests: # # The proxy&#39;s memory use scales linearly with the number of active # # connections. Fewer open connections will use less memory. Adjust # # this value based on your application&#39;s requirements. # memory: &#34;&#34; # # The proxy&#39;s CPU use scales linearly with the amount of IO between # # the database and the application. Adjust this value based on your # # application&#39;s requirements. # cpu: &#34;&#34; service: labels: {} # specifies labels to add to the cloudsql auth proxy service. type: ClusterIP # specifies the Kubernetes type of the cloudsql auth proxy service. The default is `ClusterIP`. cloudsqlAuthProxy: enabled: false ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm",
        "gcp"
      ],
      "id": "08eb1e5abf1c6a8d59ef12ab40d12606"
    },
    {
      "title": "OpenID Connect HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Set up your OIDC authentication and connect to IDPs.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/oidc/",
      "relURI": "/latest/manage/helm-values/oidc/",
      "body": " About # The OIDC section of the helm chart enables you to set up authentication through upstream IDPs. To use authentication, you must have an Enterprise license.\nWe recommend setting up this section alongside the Enterprise Server section of your Helm chart so that you can easily scale multiple clusters using the same authentication configurations.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Mock IDP Upstream IDP Additional Clients oidc: issuerURI: &#34;&#34; # inferred if running locally or using proxy requireVerifiedEmail: false # if true, email verification is required to authenticate IDTokenExpiry: 24h # if set, specifies the duration where OIDC ID Tokens are valid; parsed into golang&#39;s time.Duration: https://pkg.go.dev/time#example-ParseDuration RotationTokenExpiry: 48h # If set, enables OIDC rotation tokens, and specifies the duration where they are valid. userAccessibleOauthIssuerHost: &#34;&#34; # (Optional) Only set in cases where the issuerURI is not user accessible (ie. localhost install) mockIDP: true # if true, ignores upstreamIDPs in favor of a placeholder IDP with the username/password of &#34;admin&#34;/&#34;password&#34; oidc: issuerURI: &#34;&#34; # inferred if running locally or using proxy requireVerifiedEmail: false # if true, email verification is required to authenticate IDTokenExpiry: 24h # if set, specifies the duration where OIDC ID Tokens are valid; parsed into golang&#39;s time.Duration: https://pkg.go.dev/time#example-ParseDuration RotationTokenExpiry: 48h # If set, enables OIDC rotation tokens, and specifies the duration where they are valid. userAccessibleOauthIssuerHost: &#34;&#34; # (Optional) Only set in cases where the issuerURI is not user accessible (ie. localhost install) upstreamIDPs: # defines a list of Identity Providers to use for authentication. https://dexidp.io/docs/connectors/ - id: idpConnector config: issuer: &#34;&#34; clientID: &#34;&#34; clientSecret: &#34;&#34; redirectURI: &#34;http://localhost:30658/callback&#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: idpConnector type: oidc - id: okta config: issuer: &#34;https://dev-84362674.okta.com&#34; clientID: &#34;client_id&#34; clientSecret: &#34;notsecret&#34; redirectURI: &#34;http://localhost:30658/callback&#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: okta type: oidc upstreamIDPsSecretName: &#34;&#34; # passes the upstreamIDPs value via an existing k8s secret (key: `upstream-idps`) dexCredentialSecretName: &#34;&#34; # mounts a credential file to the pachd pod at /dexcreds/ (e.g., serviceAccountFilePath: /dexcreds/googleAuth.json); required for some dex configs like Google. mockIDP: false # if true, ignores upstreamIDPs in favor of a placeholder IDP with the username/password of &#34;admin&#34;/&#34;password&#34; oidc: issuerURI: &#34;&#34; # inferred if running locally or using proxy requireVerifiedEmail: false # if true, email verification is required to authenticate IDTokenExpiry: 24h # if set, specifies the duration where OIDC ID Tokens are valid; parsed into golang&#39;s time.Duration: https://pkg.go.dev/time#example-ParseDuration RotationTokenExpiry: 48h # If set, enables OIDC rotation tokens, and specifies the duration where they are valid. userAccessibleOauthIssuerHost: &#34;&#34; # (Optional) Only set in cases where the issuerURI is not user accessible (ie. localhost install) upstreamIDPs: # defines a list of Identity Providers to use for authentication. https://dexidp.io/docs/connectors/ - id: idpConnector config: issuer: &#34;&#34; clientID: &#34;&#34; clientSecret: &#34;&#34; redirectURI: &#34;http://localhost:30658/callback&#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: idpConnector type: oidc - id: okta config: issuer: &#34;https://dev-84362674.okta.com&#34; clientID: &#34;client_id&#34; clientSecret: &#34;notsecret&#34; redirectURI: &#34;http://localhost:30658/callback&#34; insecureEnableGroups: true insecureSkipEmailVerified: true insecureSkipIssuerCallbackDomainCheck: true forwardedLoginParams: - login_hint name: okta type: oidc upstreamIDPsSecretName: &#34;&#34; # passes the upstreamIDPs value via an existing k8s secret (key: `upstream-idps`) dexCredentialSecretName: &#34;&#34; # mounts a credential file to the pachd pod at /dexcreds/ (e.g., serviceAccountFilePath: /dexcreds/googleAuth.json); required for some dex configs like Google. mockIDP: false # if true, ignores upstreamIDPs in favor of a placeholder IDP with the username/password of &#34;admin&#34;/&#34;password&#34; additionalOIDCClient: - id: example-app secret: example-app-secret name: &#39;Example App&#39; redirectURIs: - &#39;http://127.0.0.1:5555/callback&#39; additionalClientsSecretName: &#34;&#34; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "631cda3769260ff6a4c0d996961b097c"
    },
    {
      "title": "Test Connection HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Used by certain orgs to test connection during installation.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/test-connection/",
      "relURI": "/latest/manage/helm-values/test-connection/",
      "body": " About # The Test Connection section is used by HPE ML Data Management to test the connection during installation. This config is used by organizations that do not have permission to pull Docker images directly from the Internet, and instead need to mirror locally.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\ntestConnection: image: repository: alpine tag: latest ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "configuration",
        "helm"
      ],
      "id": "9d602dfee724fc8147987c09ba05493d"
    },
    {
      "title": "Proxy HCVs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Helm Chart Values (HCVs)",
      "description": "Centralize all traffic on a single port that's safe to expose to the Internet.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/helm-values/proxy/",
      "relURI": "/latest/manage/helm-values/proxy/",
      "body": " About # Proxy is a service that handles all HPE ML Data Management traffic (S3, Console, OIDC, Dex, GRPC) on a single port; It&rsquo;s great for exposing directly to the Internet.\nValues # The following section contains a series of tabs for commonly used configurations for this section of your values.yml Helm chart.\nOptions: Load Balancer With Legacy Ports Node Port proxy: enabled: true host: &#34;&#34; # the external hostname (including port if nonstandard) that the proxy will be reachable at. replicas: 1 # each replica can handle 50,000 concurrent connections. There is an affinity rule to prefer scheduling the proxy pods on the same node as pachd, so a number here that matches the number of pachd replicas is a fine configuration. (Note that we don&#39;t guarantee to keep the proxy&lt;-&gt;pachd traffic on-node or even in-region.) image: repository: &#34;envoyproxy/envoy&#34; tag: &#34;v1.22.0&#34; pullPolicy: &#34;IfNotPresent&#34; resources: requests: cpu: 100m memory: 512Mi limits: memory: 512Mi # proxy sheds traffic before using 500MB of RAM. labels: {} annotations: {} service: # configures the service that routes traffic to the proxy. type: LoadBalancer # can be ClusterIP, NodePort, or LoadBalancer. loadBalancerIP: &#34;&#34; # If the service is a LoadBalancer, you can specify the IP address to use; defaults to 80. httpPort: 80 # The port to serve plain HTTP traffic on. httpsPort: 443 # The port to serve HTTPS traffic on, if enabled below. annotations: {} labels: {} # adds labels to the service itself (not the selector!). tls: # Incompatible with legacy ports. enabled: false secretName: &#34;&#34; # must contain &#34;tls.key&#34; and &#34;tls.crt&#34; keys; generate with kubectl create secret tls &lt;name&gt; --key=tls.key --cert=tls.cert&#34; secret: {} # generate the secret from values here. This is intended only for unit tests. proxy: enabled: true host: &#34;&#34; # the external hostname (including port if nonstandard) that the proxy will be reachable at. replicas: 1 # each replica can handle 50,000 concurrent connections. There is an affinity rule to prefer scheduling the proxy pods on the same node as pachd, so a number here that matches the number of pachd replicas is a fine configuration. (Note that we don&#39;t guarantee to keep the proxy&lt;-&gt;pachd traffic on-node or even in-region.) image: repository: &#34;envoyproxy/envoy&#34; tag: &#34;v1.22.0&#34; pullPolicy: &#34;IfNotPresent&#34; resources: requests: cpu: 100m memory: 512Mi limits: memory: 512Mi # proxy sheds traffic before using 500MB of RAM. labels: {} annotations: {} service: # configures the service that routes traffic to the proxy. type: LoadBalancer # can be ClusterIP, NodePort, or LoadBalancer. loadBalancerIP: &#34;&#34; # If the service is a LoadBalancer, you can specify the IP address to use; defaults to 80. httpPort: 80 # The port to serve plain HTTP traffic on. httpsPort: 443 # The port to serve HTTPS traffic on, if enabled below. annotations: {} labels: {} # adds labels to the service itself (not the selector!). legacyPorts: # proxy can serve backend services on a numbered port if not set to 0. If this service is of type NodePort, the port numbers here will be used for the node port, and will need to be in the node port range. console: 0 # legacy 30080, conflicts with default httpNodePort grpc: 0 # legacy 30650 s3Gateway: 0 # legacy 30600 oidc: 0 # legacy 30657 identity: 0 # legacy 30658 metrics: 0 # legacy 30656 proxy: enabled: true host: &#34;&#34; # the external hostname (including port if nonstandard) that the proxy will be reachable at. replicas: 1 # each replica can handle 50,000 concurrent connections. There is an affinity rule to prefer scheduling the proxy pods on the same node as pachd, so a number here that matches the number of pachd replicas is a fine configuration. (Note that we don&#39;t guarantee to keep the proxy&lt;-&gt;pachd traffic on-node or even in-region.) image: repository: &#34;envoyproxy/envoy&#34; tag: &#34;v1.22.0&#34; pullPolicy: &#34;IfNotPresent&#34; resources: requests: cpu: 100m memory: 512Mi limits: memory: 512Mi # proxy sheds traffic before using 500MB of RAM. labels: {} annotations: {} service: # configures the service that routes traffic to the proxy. type: NodePort # can be ClusterIP, NodePort, or LoadBalancer. httpPort: 80 # The port to serve plain HTTP traffic on. httpsPort: 443 # The port to serve HTTPS traffic on, if enabled below. httpNodePort: 30080 # If the service is a NodePort, you can specify the port to receive HTTP traffic on. httpsNodePort: 30443 annotations: {} labels: {} # adds labels to the service itself (not the selector!). legacyPorts: # proxy can serve backend services on a numbered port if not set to 0. If this service is of type NodePort, the port numbers here will be used for the node port, and will need to be in the node port range. console: 0 # legacy 30080, conflicts with default httpNodePort grpc: 0 # legacy 30650 s3Gateway: 0 # legacy 30600 oidc: 0 # legacy 30657 identity: 0 # legacy 30658 metrics: 0 # legacy 30656 ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "helm"
      ],
      "id": "bb545431a174e5ecfd4127b901873e76"
    },
    {
      "title": "S3 Gateway",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Manage",
      "description": "Learn about the embedded S3 gateway, which is compatible with MinIO, AWS S3 CLI, and boto3.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/s3gateway/",
      "relURI": "/latest/manage/s3gateway/",
      "body": "Use the embedded S3 Gateway to send or receive data through the S3 protocol using object storage tooling such as Minio, boto3, or AWS s3 CLI. Operations available are similar to those officially documented for S3.\nS3 Gateway Syntax # The S3 gateway presents each branch from every HPE ML Data Management repository as an S3 bucket. Buckets are represented via [&lt;commit&gt;.]&lt;branch&gt;.&lt;repo&gt;.&lt;project&gt;, with the commit being optional.\nThe master.foo.bar bucket corresponds to the master branch of the repo foo within the bar project. The be97b64f110643389f171eb64697d4e1.master.foo.bar bucket corresponds to the commit be97b64f110643389f171eb64697d4e1 on the master branch of the foo repo within the bar project. If auth is enabled, credentials must be passed with each S3 gateway endpoint as mentioned in S3 Client configuration.\nCommand Examples # The following command examples assume that you have upgraded to use the embedded proxy, which will become mandatory in future releases.\nPut Data Into HPE ML Data Management Repo # Tool: S3 Client Pachctl CLI aws --endpoint-url &lt;pachyderm-address&gt; s3 cp myfile.csv s3://master.foo.bar pachctl put file data@master:/ -f myfile.csv --project bar Retrieve Data From HPE ML Data Management Repo # Tool: S3 Client Pachctl CLI aws --endpoint-url &lt;pachyderm-address&gt; s3 cp s3://master.foo.bar/myfile.csv pachctl get file data@master:/myfile.csv --project bar Port Forwarding # You can pachctl port-forward to access the s3 gateway through the localhost:30600 endpoint, however, the Kubernetes port forwarder incurs substantial overhead and does not recover well from broken connections.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "5ec606a25c68cf601294ac9f46432d9e"
    },
    {
      "title": "AWS CLI",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway",
      "description": "Learn how to configure AWS CLI for the S3 Gateway",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/s3gateway/aws/",
      "relURI": "/latest/manage/s3gateway/aws/",
      "body": " Before You Start # You must have the AWS CLI installed Configuration Steps # Install the AWS CLI as described in the AWS documentation.\nVerify that the AWS CLI is installed:\naws --version Configure AWS CLI. Use the aws configure command to configure your credentials file: aws configure --profile &lt;name-your-profile&gt; # AWS Access Key ID: YOUR-PACHYDERM-AUTH-TOKEN # AWS Secret Access Key: YOUR-PACHYDERM-AUTH-TOKEN # Default region name: # Default output format [None]: ‚ÑπÔ∏è Note that the --profile flag (named profiles) is optional. If not used, your access information will be stored in the default profile.\nTo reference a given profile when using the S3 client, append --profile &lt;name-your-profile&gt; at the end of your command.\nVerify Setup # To verify your setup, you can check the list of filesystem objects on the master branch of your repository.\naws --endpoint-url http://&lt;localhost_or_externalIP&gt;:30600/ s3 ls s3://master.&lt;repo&gt;.&lt;project&gt; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "fe396833b1ddfb23bee0c390eb277a09"
    },
    {
      "title": "Boto3",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway",
      "description": "Learn how to configure Boto for the S3 Gateway",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/s3gateway/boto3/",
      "relURI": "/latest/manage/s3gateway/boto3/",
      "body": " Before You Start # Before using Boto3, you need to set up authentication credentials for your AWS accountn using the AWS CLI.\nConfiguration Steps # Then follow the Using boto documentation starting with importing boto3 in your python file and creating your S3 resources.\nüìñ Find boto3 full documentation here.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "2085bfd364bcea3b3b8d95b4a6d40a11"
    },
    {
      "title": "Credentials",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway",
      "description": "Learn how to configure an S3 client.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/s3gateway/configure-s3client/",
      "relURI": "/latest/manage/s3gateway/configure-s3client/",
      "body": " Before You Start # You must configure an S3 Client (Boto3, AWS, MinIO) You must have authentication enabled. How to Set Your Credentials # Auth: Enabled Disabled Run the following command: more ~/.pachyderm/config.json Search for your session token: &quot;session_token&quot;: &quot;your-session-token-value&quot;. Make sure to fill both fields Access Key ID and Secret Access Key with that same value. AWS_ACCESS_KEY_ID=&#34;somevalue&#34; AWS_SECRET_ACCESS_KEY=&#34;somevalue&#34; aws --endpoint-url http://pach-address s3 ls s3://branch.repo.project You must set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to any matching, non-empty string.\nFor example, you could set both values to &quot;x&quot;.\nAWS_ACCESS_KEY_ID=x AWS_SECRET_ACCESS_KEY=x aws --endpoint-url http://pach-address s3 ls s3://branch.repo.project Robot Users # Depending on your use case, it might make sense to pass the credentials of a robot-user or another type of user altogether. Refer to the RBAC for more information.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "ab07126f203bac4480c53c7335d2ba37"
    },
    {
      "title": "MinIO",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway",
      "description": "Learn how to configure MinIO for the S3 Gateway",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/s3gateway/minio/",
      "relURI": "/latest/manage/s3gateway/minio/",
      "body": " Before You Start # Configuration Steps # Install the MinIO client as described on the MinIO download page.\nVerify that MinIO components are successfully installed by running the following command:\nminio version mc version System Response:\nVersion: 2019-07-11T19:31:28Z Release-tag: RELEASE.2019-07-11T19-31-28Z Commit-id: 31e5ac02bdbdbaf20a87683925041f406307cfb9 Set up the MinIO configuration file to use the S3 Gateway port 30600 for your host:\nvi ~/.mc/config.json You should see a configuration similar to the following. For a minikube deployment, verify the local configuration:\n&#34;local&#34;: { &#34;url&#34;: &#34;http://localhost:30600&#34;, &#34;accessKey&#34;: &#34;YOUR-PACHYDERM-AUTH-TOKEN&#34;, &#34;secretKey&#34;: &#34;YOUR-PACHYDERM-AUTH-TOKEN&#34;, &#34;api&#34;: &#34;S3v4&#34;, &#34;lookup&#34;: &#34;auto&#34; }, Verify Setup # Check the list of filesystem objects on the master branch of the repository raw_data.\nmc ls local/master.&lt;repo&gt;.&lt;project&gt; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "05ecce84f077c8cd883dd540bd0d6ff4"
    },
    {
      "title": "Unsupported Operations",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway",
      "description": "Learn which S3 operations are not supported.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/s3gateway/unsupported-operations/",
      "relURI": "/latest/manage/s3gateway/unsupported-operations/",
      "body": "Some of the S3 operations are not yet supported by HPE ML Data Management. If you run any of these operations, HPE ML Data Management returns a standard S3 NotImplemented error.\nThe S3 Gateway does not support the following S3 operations:\nAccelerate Analytics Object copying. PFS supports this functionality through gRPC. CORS configuration Encryption HTML form uploads Inventory Legal holds Lifecycles Logging Metrics Notifications Object locks Payment requests Policies Public access blocks Regions Replication Retention policies Tagging Torrents Website configuration ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "b07878eccad21c47e3e273eeed5c73aa"
    },
    {
      "title": "Backup & Restore",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Manage",
      "description": "Learn how to back-up and restore a cluster or standalone Enterprise Server.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/backup-restore/",
      "relURI": "/latest/manage/backup-restore/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "management",
        "backups"
      ],
      "id": "f048e9151d85e074d9b7edf29b23671b"
    },
    {
      "title": "Cluster Backup",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Backup & Restore",
      "description": "Learn how to back-up and restore the state of a production cluster.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/backup-restore/cluster/",
      "relURI": "/latest/manage/backup-restore/cluster/",
      "body": "This page will walk you through the main steps required to manually back up and restore the state of a HPE ML Data Management cluster in production. Details on how to perform those steps might vary depending on your infrastructure and cloud provider / on-premises setup. Refer to your provider&rsquo;s documentation.\nOverview # HPE ML Data Management state is stored in two main places:\nAn object-store holding HPE ML Data Management&rsquo;s data. A PostgreSQL instance made up of one or two databases: pachyderm holding HPE ML Data Management&rsquo;s metadata and dex holding authentication data. Backing up a HPE ML Data Management cluster involves snapshotting both the object store and the PostgreSQL database(s), in a consistent state, at a given point in time.\nRestoring it involves re-populating the database(s) and the object store using those backups, then recreating a HPE ML Data Management cluster.\n‚ÑπÔ∏è Make sure that you have a bucket for backup use, separate from the object store used by your cluster. Depending on the reasons behind your cluster recovery, you might choose to use an existing vs. a new instance of PostgreSQL and/or the object store. Manual Back Up Of A HPE ML Data Management Cluster # Before any manual backup:\nMake sure to retain a copy of the Helm values used to deploy your cluster. Then, suspend any state-mutating operations. ‚ÑπÔ∏è Backups incur downtime until operations are resumed. Operational best practices include notifying HPE ML Data Management users of the outage and providing an estimated time when downtime will cease. Downtime duration is a function of the size of the data be to backed up and the networks involved; Testing before going into production and monitoring backup times on an ongoing basis might help make accurate predictions. Suspend Operations # Pause any external automated process ingressing data to HPE ML Data Management input repos, or queue/divert those as they will fail to connect to the cluster while the backup occurs.\nSuspend all mutation of state by scaling pachd and the worker pods down:\n‚ö†Ô∏è Before starting, make sure that your context points to the server you want to pause by running pachctl config get active-context.\nTo pause HPE ML Data Management:\nIf you are an Enterprise user: Run the pachctl enterprise pause command.\nAlternatively, you can use kubectl:\nBefore starting, make sure that kubectl points to the right cluster.\nRun kubectl config get-contexts to list all available clusters and contexts (the current context is marked with a *), then kubectl config use-context &lt;your-context-name&gt; to set the proper active context.\nkubectl scale deployment pachd --replicas 0 kubectl scale rc --replicas 0 -l suite=pachyderm,component=worker Note that it takes some time for scaling down to take effect;\nRun the watch command to monitor the state of pachd and worker pods terminating:\nwatch -n 5 kubectl get pods Back Up The Databases And The Object Store # This step is specific to your database and object store hosting.\nIf your PostgreSQL instance is solely dedicated to HPE ML Data Management, you can use PostgreSQL&rsquo;s tools, like pg_dumpall, to dump your entire PostgreSQL state.\nAlternatively, you can use targeted pg_dump commands to dump the pachyderm and dex databases, or use your Cloud Provider&rsquo;s backup product. In any case, make sure to use TLS. Note that if you are using a cloud provider, you might choose to use the provider‚Äôs method of making PostgreSQL backups.\n‚ö†Ô∏è A production setting of HPE ML Data Management implies that you are running a managed PostgreSQL instance.\nüìñ PostgreSQL on AWS RDS backup GCP Cloud SQL backup Azure Database for PostgreSQL backup For on-premises Kubernetes deployments, check the vendor documentation for your on-premises PostgreSQL for details on backing up and restoring your databases.\nTo back up the object store, you can either download all objects or use the object store provider‚Äôs backup method.\nThe latter is preferable since it will typically not incur egress costs. üìñ AWS backup for S3 GCP Cloud storage bucket backup Azure blob backup For on-premises Kubernetes deployments, check the vendor documentation for your on-premises object store for details on backing up and restoring a bucket.\nResuming operations # Once your backup is completed, resume your normal operations by scaling pachd back up. It will take care of restoring the worker pods:\nEnterprise users: run pachctl enterprise unpause.\nAlternatively, if you used kubectl:\nkubectl scale deployment pachd --replicas 1 Restore HPE ML Data Management # There are two primary use cases for restoring a cluster:\nYour data have been corrupted, preventing your cluster from functioning correctly. You want the same version of HPE ML Data Management re-installed on the latest uncorrupted data set. You have upgraded a cluster and are encountering problems. You decide to uninstall the current version and restore the latest backup of a previous version of HPE ML Data Management. Depending on your scenario, pick all or a subset of the following steps:\nPopulate new pachyderm and dex (if required) databases on your PostgreSQL instance Populate a new bucket or use the backed-up object-store (note that, in that case, it will no longer be a backup) Create a new empty Kubernetes cluster and give it access to your databases and bucket Deploy HPE ML Data Management into your new cluster Restore The Databases And Objects # Restore PostgreSQL backups into your new databases using the appropriate method (this is most straightforward when using a cloud provider). Copy the objects from the backed-up object store to your new bucket or re-use your backup. Deploy HPE ML Data Management Into The New Cluster # Finally, update the copy of your original Helm values to point HPE ML Data Management to the new databases and the new object store, then use Helm to install HPE ML Data Management into the new cluster.\nConnect &lsquo;pachctl&rsquo; To Your Restored Cluster # And check that your cluster is up and running.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "management",
        "backups"
      ],
      "id": "351e7b45f104ab2fb87f6efbd214aa06"
    },
    {
      "title": "Enterprise Server Backup",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Backup & Restore",
      "description": "Learn how to back-up and restore the state of a production Enterprise Server.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/backup-restore/enterprise-server/",
      "relURI": "/latest/manage/backup-restore/enterprise-server/",
      "body": "This page will walk you through the main steps required to manually back up and restore the state of a HPE ML Data Management cluster in production. Details on how to perform those steps might vary depending on your infrastructure and cloud provider / on-premises setup. Refer to your provider&rsquo;s documentation.\nOverview # HPE ML Data Management state is stored in two main places:\nAn object-store holding HPE ML Data Management&rsquo;s data. A PostgreSQL instance made up of one or two databases: pachyderm holding HPE ML Data Management&rsquo;s metadata and dex holding authentication data. Backing up a HPE ML Data Management cluster involves snapshotting both the object store and the PostgreSQL database(s), in a consistent state, at a given point in time.\nRestoring it involves re-populating the database(s) and the object store using those backups, then recreating a HPE ML Data Management cluster.\n‚ÑπÔ∏è Make sure that you have a bucket for backup use, separate from the object store used by your cluster. Depending on the reasons behind your cluster recovery, you might choose to use an existing vs. a new instance of PostgreSQL and/or the object store. Backup A Standalone Enterprise Server # Backing up and restoring an Enterprise Server is similar to the backing up and restoring of a regular cluster, with three slight variations:\nThe name of its Kubernetes deployment is pach-enterprise versus pachd in the case of a regular cluster. The Enterprise Server does not use an Object Store. An Enterprise server only requires a dex database. ‚ö†Ô∏è Make sure that pachctl and kubectl are pointing to the right cluster. Check your Enterprise Server context: pachctl config get active-enterprise-context, or pachctl config set active-enterprise-context &lt;my-enterprise-context-name&gt; --overwrite to set it.\nPause the Enterprise Server like you would pause a regular cluster by running pachctl enterprise pause (Enterprise users), or using kubectl. ‚ÑπÔ∏è kubectl users: There is a difference with the pause of a regular cluster. The deployment of the enterprise server is named pach-enterprise; therefore, the first command should be:\nkubectl scale deployment pach-enterprise --replicas 0 There is no need to pause all the HPE ML Data Management clusters registered to the Enterprise Server to backup the enterprise server; however, pausing the Enterprise server will result in your clusters becoming unavailable.\nAs a reminder, the Enterprise Server does not use any object-store. Therefore, the backup of the Enterprise Server only consists in backing up the database dex.\nResume the operations on your Enterprise Server by running pachctl enterprise unpause (Enterprise users) to scale the pach-enterprise deployment back up. Alternatively, if you used kubectl, run:\nkubectl scale deployment pach-enterprise --replicas 1 Restore An Enterprise Server # Follow the cluster restoration steps while skipping all tasks related to creating and populating a new object-store.\nOnce your cluster is up and running, check that all your clusters are automatically registered with your new Enterprise Server.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "management",
        "backups"
      ],
      "id": "7377efa0fcba14e398f4cf7249b4b22c"
    },
    {
      "title": "Upgrade",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to upgrade PachCTLand & PachD.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/upgrades/",
      "relURI": "/latest/manage/upgrades/",
      "body": "Learn how to upgrade HPE ML Data Management to access new features and performance enhancements.\nBefore You Start # Check the release notes before upgrading Back up your cluster Update your Helm chart values if applicable How to Upgrade HPE ML Data Management # 1. Run a Preflight Check # PachD has a preflight check mode that you can enable in your Helm chart by setting pachd.preflightchecks.enabled to true. Preflight checks run as a Kubernetes job, and can use a different version of HPE ML Data Management than the rest of the chart. In this case, you will set it to the version you are upgrading to (for example, the latest version is 2.7.3).\nExample configuration:\npreflightCheckJob: enabled: true image: tag: &#34;2.7.3&#34; You&rsquo;ll see a pod named pachyderm-preflight-check was created to perform the preflight checks.\nIf its status says completed, you are ready to continue with the upgrade. If its status does not say completed, reach out to the HPE ML Data Management team for assistance with your upgrade. kubectl get pods NAME READY STATUS RESTARTS AGE console-76f5fd8c58-zwvj5 1/1 Running 0 13m default-edges-v1-bl6cf 2/2 Running 0 12m default-montage-v1-tbj6p 2/2 Running 0 12m etcd-0 1/1 Running 0 13m minio-0 1/1 Running 0 14m pachd-6c99fc7448-vsjbn 1/1 Running 0 13m pachyderm-kube-event-tail-5957785f5d-4557j 1/1 Running 0 13m pachyderm-loki-0 1/1 Running 0 13m pachyderm-preflight-check-rh9rp 0/1 Completed 0 13m pachyderm-promtail-h29zv 1/1 Running 0 13m pachyderm-proxy-7956c766bd-drndd 1/1 Running 0 13m pg-bouncer-686db6477c-rjwgl 1/1 Running 0 13m postgres-0 1/1 Running 0 13m 2. Upgrade # Run the following brew command or download &amp; install the latest release assets: brew tap pachyderm/tap &amp;&amp; brew install pachyderm/tap/pachctl@2.7 Upgrade Helm. Deploy Method: Production Local (Personal Machine) Note that the repo name input (pachd) must match the name you provided upon first install. You can also pass in a specific version (e.g., --version x.x.0-rc.1) if you are testing a pre-released version of HPE ML Data Management.\nhelm repo update helm upgrade pachyderm pachyderm/pachyderm -f my_pachyderm_values.yaml --set proxy.enabled=true --set proxy.service.type=LoadBalancer Note that the repo name input (pachd) must match the name you provided upon first install. You can also pass in a specific version (e.g., --version x.x.0-rc.1) if you are testing a pre-released version of HPE ML Data Management.\nhelm repo update helm upgrade pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Verify that the installation was successful by running pachctl version: pachctl version # COMPONENT VERSION # pachctl 2.7.3 # pachd 2.7.3 ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "management",
        "upgrades",
        "pachctl",
        "pachd"
      ],
      "id": "ae82491900bac26f951a48d4422e1a60"
    },
    {
      "title": "PachCTL Shell",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to use the PachCTL shell.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/pachctl-shell/",
      "relURI": "/latest/manage/pachctl-shell/",
      "body": "The HPE ML Data Management Shell is a special-purpose shell for HPE ML Data Management that provides auto-suggesting as you type. New HPE ML Data Management users will find this user-friendly shell especially appealing as it helps to learn pachctl, type commands faster, and displays useful information about the objects you are interacting with. This new shell does not supersede the classic use of pachctl shell in your standard terminal, but is a compelling convenience for power users and beginners alike. If you prefer to use just pachctl, you can continue to do so.\nTo enter the HPE ML Data Management Shell, type:\npachctl shell When you enter pachctl shell, your prompt changes to display your current HPE ML Data Management context, as well as displays a list of available commands in a drop-down list.\nTo scroll through the list, press TAB and then use arrows to move up or down. Press SPACE to select a command.\nWhen in the HPE ML Data Management Shell, you do not need to prepend your commands with pachctl because HPE ML Data Management does that for you automatically behind the scenes. For example, instead of running pachctl list repo, run list repo:\nWith nested commands, pachctl shell can do even more. For example, if you type list file &lt;repo&gt;@&lt;branch&gt;/, you can preview and select files from that branch:\nSimilarly, you can select a commit:\nExit the HPE ML Data Management Shell # To exit the HPE ML Data Management Shell, press CTRL-D or type exit.\nClearing Cached Completions # To optimize performance and achieve faster response time, the HPE ML Data Management Shell caches completion results. You can clear this cache by pressing F5 forcing the HPE ML Data Management Shell to send requests to the server for new completions.\nClearing the screen # To clear the screen, press CTRL-L.\nLimitations # The HPE ML Data Management Shell does not support standard UNIX commands or kubectl commands. To run them, exit the HPE ML Data Management Shell or run the commands in a different terminal window.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pachctl",
        "cli"
      ],
      "id": "1a10c4786b22e78148509829d3854c1e"
    },
    {
      "title": "Check IdP User",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to check which IdP user you are logged in as using pachctl.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/check-user/",
      "relURI": "/latest/manage/check-user/",
      "body": " Before You Start # Your organization must have an active Enterprise license key. You must have PachCTL installed. How to Check Your Current User # Open a terminal. Run the following command: pachctl auth whoami # You are &#34;user:one-pachyderm-user@gmail.com&#34; # session expires: 08 May 21 13:59 EDT ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "workflows",
        "permissions",
        "management"
      ],
      "id": "86b7b3e8cfe18a52dd8e0e9e4c5e2419"
    },
    {
      "title": "Supported Releases & Features",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Check which release versions are actively supported using this guide.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/supported-releases/",
      "relURI": "/latest/manage/supported-releases/",
      "body": "HPE ML Data Management lists the status for each release and feature, so that you can understand expectations for support and stability.\nSupported Releases # HPE ML Data Management supports the latest Generally Available (GA) release and the previous two major and minor GA releases. Releases three or more major and minor versions back are considered End of Life (EOL).\nRelease Status by Version # Version Release Status Support 2.7.x GA Yes 2.6.x GA Yes 2.5.x GA Yes 2.4.x EOL No 2.3.x EOL No 2.2.x EOL No 2.1.x EOL No 2.0.x EOL No 1.13.x EOL No 1.12.x EOL No 1.11.x EOL No 1.10.x EOL No &lt; 1.9.11 EOL No Releases Under Development # A release under development may undergo several pre-release stages before becoming Generally Available (GA). These pre-releases enable the HPE ML Data Management team to do development and testing in partnership with our users before a release is considered ready for a Generally Availability (GA).\nalpha &gt; beta &gt; Release Candidate (RC) &gt; Generally Available (GA)\nRelease Status # Alpha # alpha releases are a pre-release version of a product, intended for development and testing purposes only. alpha releases include many bugs and unfinished features, and are only suitable for early technical feedback. alpha releases should not be used in a production environment.\nBeta # beta releases are a pre-release version of a product, intended for development and testing purposes only, and include a wider range of users than an alpha release. beta releases should not be used in a production environment.\nRelease Candidate (RC) # Release Candidate or RC releases are a pre-release version of a product, intended for users to prepare for a GA release. RC releases should not be used in a production environment.\nGenerally Available (GA) # Generally Available or GA releases are considered stable and intended for production usage.\nContain new features, fixed defects, and patched security vulnerabilities. Support is available from HPE ML Data Management. End of Life (EOL) # End of Life or EOL indicates the release will no longer receive support.\nDocumentation will be archived. Release artifacts will remain available. We keep release artifacts on Github and Docker Hub. Support is no longer available for End of Life (EOL) releases. Support can assist with upgrading to a newer version. Supported Features # Stable # stable indicates that the HPE ML Data Management team believes the feature is ready for use in a production environment.\nThe feature&rsquo;s API is stable and unlikely to change. There are no major defects for the feature. The HPE ML Data Management team believes there is a sufficient amount of testing, including automated tests, community testing, and user production environments. Support is available from HPE ML Data Management. Experimental # experimental indicates that a feature has not met the HPE ML Data Management team&rsquo;s criteria for production use. Therefore, these features should be used with caution in production environments. experimental features are likely to change, have outstanding defects, and/or missing documentation. Users considering using experimental features should contact HPE ML Data Management for guidance.\nProduction use is not recommended without guidance from HPE ML Data Management. These features may have missing documentation, lack of examples, and lack of content. Support is available from HPE ML Data Management, which may be limited in scope based on our guidance. Deprecated # deprecated indicates that a feature is no longer developed. Users of deprecated features are encouraged to upgrade or migrate to newer versions or compatible features. deprecated features become End of Life (EOL) features after 6 months.\nUsers continuing to use deprecated features should contact support to migrate to features. Support is available from HPE ML Data Management. End of Life (EOL) Features # End of Life or EOL indicates that a feature is no longer supported.\nDocumentation will be archived. Support is no longer available for End of Life (EOL) features. Support can assist upgrading to a newer version. Experimental Features # Feature Version Date Service Pipelines 1.9.9 2019-11-06 JupyterLab Extension 0.6.3 2022-09-21 Deprecated Features # Feature Version EOL Date End of Life (EOL) Features # Feature Version EOL Date Build Pipelines 2.0.0 2021-07-25 Git Inputs 2.0.0 2021-07-25 pachctl deploy 2.0.0 2021-07-25 Spouts: Named Pipes 2.0.0 2021-07-25 Vault Plugin 2.0.0 2021-07-25 pachctl put file --split 2.0.0 2021-07-25 MaxQueueSize 2.0.0 2021-07-25 S3v2 signatures 1.12.0 2021-01-05 atom inputs 1.9.0 2019-06-12 ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "ed774ee56a710afbf968b2339765b287"
    },
    {
      "title": "PostgresSQL Fine-Tuning",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to optimize PostgreSQL for high workloads and table maintenance.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/postgres/",
      "relURI": "/latest/manage/postgres/",
      "body": "PostgreSQL is a powerful relational database management system, but it requires careful configuration to perform optimally, especially when dealing with high workloads and tables with frequent row deletions. In this guide, we&rsquo;ll discuss some best practices and recommended settings to ensure PostgreSQL can efficiently manage your data and prevent unexpected downtime.\nFine-Tuning Recommendations # 1. Adjust Vacuuming Parameters # PostgreSQL uses a process called &ldquo;vacuuming&rdquo; to reclaim space and improve performance. Use the following settings for high-update and -delete-rate tables:\nfillfactor = 0.5: This setting reserves space within pages for future updates, reducing index bloat. autovacuum_vacuum_threshold = 250000: Lower this threshold to trigger vacuum more frequently. autovacuum_vacuum_scale_factor = 0: Disable scale factor-based vacuum scheduling. These settings encourage &ldquo;HOT&rdquo; updates, keeping index updates in check and controlling index bloat. However, remember that modifying these parameters may require additional autovacuum workers.\n2. Increase Autovacuum Workers # When you modify storage parameters for specific tables, PostgreSQL may need more autovacuum workers to keep up with the increased workload. Increase autovacuum_max_workers by the number of tables you&rsquo;ve modified with new storage parameters. For example, if you modified three tables, add three workers to the current value. Ensure that maintenance_work_mem is set to an appropriate value, ideally no more than 1GB.\n3. Regularly Monitor Bloat # To prevent issues, it&rsquo;s crucial to monitor table and index bloat in PostgreSQL. Implement a monitoring system that can use Nagios-compatible plugins, such as check_postgres.pl. This tool allows you to regularly check for bloat and take corrective actions when necessary.\n4. Set a Schedule for pg_repack # For large tables and busy systems, consider scheduling the use of pg_repack weekly during your lowest traffic periods. pg_repack helps you reclaim space by reorganizing tables without causing downtime.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "868e37b58dd63e5fe89f59a66c383d22"
    },
    {
      "title": "Cluster Access",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to manage Kubernetes cluster access using Contexts.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/cluster-access/",
      "relURI": "/latest/manage/cluster-access/",
      "body": "HPE ML Data Management contexts enable you to store configuration parameters for multiple HPE ML Data Management clusters in a single configuration file saved at ~/.pachyderm/config.json. This file stores the information about all HPE ML Data Management clusters that you have deployed from your machine locally or on a remote server.\nFor example, if you have a cluster that is deployed locally in minikube and another one deployed on Amazon EKS, configurations for these clusters are stored in that config.json file. By default, all local cluster configurations have the local prefix. If you have multiple local clusters, HPE ML Data Management adds a consecutive number to the local prefix of each cluster.\nThe following text is an example of a HPE ML Data Management config.json file:\n{ &#34;user_id&#34;: &#34;b4fe4317-be21-4836-824f-6661c68b8fba&#34;, &#34;v2&#34;: { &#34;active_context&#34;: &#34;local-1&#34;, &#34;contexts&#34;: { &#34;default&#34;: {}, &#34;local&#34;: {}, &#34;local-1&#34;: {}, }, &#34;metrics&#34;: true } } View the Active Context # When you have multiple HPE ML Data Management clusters, you can switch between them by setting the current context. The active context is the cluster that you interact with when you run pachctl commands.\nTo view active context, type:\nView the active context:\npachctl config get active-context System response:\nlocal-1 List all contexts and view the current context:\npachctl config list context System response:\nACTIVE NAME default local * local-1 The active context is marked with an asterisk.\nChange the Active Context # To change the active context, type pachctl config set active-context &lt;name&gt;.\nAlso, you can set the PACH_CONTEXT environmental variable that overrides the active context.\nExample:\nexport PACH_CONTEXT=local1 Create a New Context # When you deploy a new HPE ML Data Management cluster, a new context that points to the new cluster is created automatically.\nIn addition, you can create a new context by providing your parameters through the standard input stream (stdin) in your terminal. Specify the parameters as a comma-separated list enclosed in curly brackets.\n‚ÑπÔ∏è By default, the pachd port is 30650.\nTo create a new context with specific parameters, complete the following steps:\nCreate a new HPE ML Data Management context with a specific pachd IP address and a client certificate:\necho &#39;{&#34;pachd_address&#34;:&#34;10.10.10.130:650&#34;, &#34;server_cas&#34;:&#34;insert your base 64 encoded key.pem&#34;}&#39; | pachctl config set context new-local System response:\nReading from stdin Verify your configuration by running the following command:\npachctl config get context new-local { &#34;pachd_address&#34;: &#34;10.10.10.130:650&#34;, &#34;server_cas&#34;: &#34;insert your base 64 encoded key.pem&#34; } Update an Existing Context # You can update an existing context with new parameters, such as a HPE ML Data Management IP address, certificate authority (CA), and others.\nTo update the Active Context, run the following commands:\nUpdate the context with a new pachd address:\npachctl config update context local-1 --pachd-address 10.10.10.131 The pachctl config update command supports the --pachd-address flag only.\nVerify that the context has been updated:\npachctl config get context local-1 System response:\n{ &#34;pachd_address&#34;: &#34;10.10.10.131&#34; } Alternatively, you can update multiple properties by using an echo script:\necho &#39;{&#34;pachd_address&#34;:&#34;10.10.10.132&#34;, &#34;server_cas&#34;:&#34;insert your base 64 encoded key.pem&#34;}&#39; | pachctl config set context local-1 --overwrite System response:\nReading from stdin. Verify that the changes were applied:\npachctl config get context local-1 System response:\n{ &#34;pachd_address&#34;: &#34;10.10.10.132&#34;, &#34;server_cas&#34;: &#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUVEakNDQXZhZ0F3SUJBZ0lERDkyc01BMEdDU3FHU0liM0RRRUJDd1VBTUVVeEN6QUpCZ05WQkFZVEFrUkYKTVJVd0V3WURWUVFLREF4RUxWUnlkWE4wSUVkdFlrZ3hIekFkQmdOVkJBTU1Ga1F0VkZKVlUxUWdVbTl2ZENCRApRU0F6SURJd01UTXdIaGNOTVRNd09USXdNRGd5TlRVeFdoY05Namd3T1RJd01EZ3lOVFV4V2pCRk1Rc3dDUVlEClZRUUdFd0pFUlRFVk1CTUdBMVVFQ2d3TVJDMVVjblZ6ZENCSGJXSklNUjh3SFFZRFZRUUREQlpFTFZSU1ZWTlUKSUZKdmIzUWdRMEVnTXlBeU1ERXpNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQQp4SHRDa29JZjdPMVVtSTRTd01vSjM1TnVPcE5jRytRUWQ1NU9hWWhzOXVGcDh2YWJvbUd4dlFjZ2RKaGw4WXdtCkNNMm9OY3FBTnRGamJlaEVlb0xEYkY3ZXUrZzIwc1JvTm95Zk1yMkVJdURjd3U0UVJqbHRyNU01cm9mbXc3d0oKeVN4cloxdlptM1oxVEF2Z3U4WFh2RDU1OGwrKzBaQlgrYTcyWmw4eHY5TnRqNmU2U3ZNalpidTM3Nk1sMXdycQpXTGJ2aVByNmViSlNXTlh3ckl5aFVYUXBsYXBSTzVBeUE1OGNjblNRM2ozdFlkTGw0LzFrUitXNXQwcXA5eCt1CmxvWUVyQy9qcElGM3Qxb1cvOWdQUC9hM2VNeWtyL3BiUEJKYnFGS0pjdStJODlWRWdZYVZJNTk3M2J6Wk5POTgKbER5cXdFSEM0NTFRR3NEa0dTTDhzd0lEQVFBQm80SUJCVENDQVFFd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVVQNURJZmNjVmIvTWtqNm5ETDB1aUR5R3lMK2N3RGdZRFZSMFBBUUgvQkFRREFnRUdNSUcrCkJnTlZIUjhFZ2JZd2diTXdkS0J5b0hDR2JteGtZWEE2THk5a2FYSmxZM1J2Y25rdVpDMTBjblZ6ZEM1dVpYUXYKUTA0OVJDMVVVbFZUVkNVeU1GSnZiM1FsTWpCRFFTVXlNRE1sTWpBeU1ERXpMRTg5UkMxVWNuVnpkQ1V5TUVkdApZa2dzUXoxRVJUOWpaWEowYVdacFkyRjBaWEpsZG05allYUnBiMjVzYVhOME1EdWdPYUEzaGpWb2RIUndPaTh2ClkzSnNMbVF0ZEhKMWMzUXVibVYwTDJOeWJDOWtMWFJ5ZFhOMFgzSnZiM1JmWTJGZk0xOHlNREV6TG1OeWJEQU4KQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBRGxrT1dPUjBTQ05FenpRaHRad1VHcTJhUzdlemlHMWNxUmR3OENxZgpqWHY1ZTRYNnh6bm9FQWl3TlN0Znp3TFMwNXpJQ3g3dUJWU3VONU1FQ1gxc2o4SjB2UGdjbEw0eEFVQXQ4eVFnCnQ0UlZMRnpJOVhSS0VCbUxvOGZ0TmRZSlNOTU93TG81cUxCR0FyRGJ4b2had3I3OGU3RXJ6MzVpaDFXV3pBRnYKbTJjaGxUV0wrQkQ4Y1J1M1N6ZHBwanZXN0l2dXdiRHpKY21Qa24yaDZzUEtSTDhtcFhTU25PTjA2NTEwMmN0TgpoOWo4dEdsc2k2QkRCMkI0bCtuWmszekNScnliTjFLajdZbzhFNmw3VTB0Sm1oRUZMQXR1VnF3ZkxvSnM0R2xuCnRRNXRMZG5rd0JYeFAvb1ljdUVWYlNkYkxUQW9LNTlJbW1Rcm1lL3lkVWxmWEE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==&#34; } ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "management",
        "permissions"
      ],
      "id": "5d02eb1d2a9011edc81ec742c3847b55"
    },
    {
      "title": "Deactivate Authorization",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to deactivate Authorization (User Access Management).",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/deactivate-auth/",
      "relURI": "/latest/manage/deactivate-auth/",
      "body": "When you deactivate authorization, all permissions granted to users on HPE ML Data Management resources are removed. Everyone that can connect to HPE ML Data Management is back to being a clusterAdmin (can access and modify all data in all repos).\nBefore You Start # You must be logged in as a clusterAdmin. How to Deactivate Auth # pachctl auth deactivate ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "7c2a60a0c2b796627dcefe1b0cdd5104"
    },
    {
      "title": "GPUs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to access GPUs on a Kubernetes cluster for data transformations.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/gpus/",
      "relURI": "/latest/manage/gpus/",
      "body": " Set up a GPU enabled Kubernetes Cluster # HPE ML Data Management leverages Kubernetes Device Plugins to let Kubernetes Pods access specialized hardware such as GPUs. For instructions on how to set up a GPU-enabled Kubernetes cluster through device plugins, see the Kubernetes documentation.\nHPE ML Data Management on NVIDIA DGX A100 # Let‚Äôs walk through the main steps allowing HPE ML Data Management to leverage the AI performance of your DGX A100 GPUs.\nüìñ Read about NVIDIA DGX A100&rsquo;s full user guide.\nüí° Support for scheduling GPU workloads in Kubernetes requires a fair amount of trial and effort. To ease the process:\nThis setup page will walk you through very detailed installation steps to prepare your Kubernetes cluster. Take advantage of a user&rsquo;s past experience in this blog. Here is a quick recap of what will be needed:\nHave a working Kubernetes control plane and worker nodes attached to your cluster. Install the DGX system in a hosting environment. Add the DGX to your K8s API server as a worker node. Now that the DGX is added to your API server, you can then proceed to:\nEnable the GPU worker node in the Kubernetes cluster by installing NVIDIA&rsquo;s dependencies:\nDependencies packages and deployment methods may vary. The following list is not exhaustive and is intended to serve as a general guideline.\nNVIDIA drivers\nFor complete instructions on setting up NVIDIA drivers, visit this quickstart guide or check this summary of the steps.\nNVIDIA Container Toolkit (nvidia-docker2)\nYou may need to use different packages depending on your container engine.\nNVIDIA Kubernetes Device Plugin\nTo use GPUs in Kubernetes, the NVIDIA Device Plugin is required. The NVIDIA Device Plugin is a daemonset that enumerates the number of GPUs on each node of the cluster and allows pods to be run on GPUs. Follow those steps to deploy the device plugin as a daemonset using helm.\nCheckpoint: Run NVIDIA System Management Interface (nvidia-smi) on the CLI. It should return the list of NVIDIA GPUs.\nTest a sample container with GPU:\nTo test whether CUDA jobs can be deployed, run a sample CUDA (vectorAdd) application.\nFor reference, find the pod spec below:\napiVersion: v1 kind: Pod metadata: name: gpu-test spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: &#34;nvidia/samples:vectoradd-cuda10.2&#34; resources: limits: nvidia.com/gpu: 1 Save it as gpu-pod.yaml then deploy the application:\nkubectl apply -f gpu-pod.yaml Check the logs to make sure that the app completed successfully:\nkubectl get pods gpu-test If the container above is scheduled successfully: install HPE ML Data Management. You are ready to start leveraging NVIDIA&rsquo;s GPUs in your HPE ML Data Management pipelines.\nüí° Note that you have the option to use GPUs for compute-intensive workloads on:\nGoogle Container Engine (GKE). Amazon Elastic Kubernetes Service (EKS). Azure Kubermnetes Service (AKS). Configure GPUs in Pipelines # Once your GPU-enabled Kubernetes cluster is set, you can request a GPU tier in your pipeline specifications by setting up GPU resource limits, along with its type and number of GPUs.\nüí° By default, HPE ML Data Management workers are spun up and wait for new input. That works great for pipelines that are processing a lot of new incoming commits. However, for lower volume of input commits, you could have your pipeline workers &rsquo;taking&rsquo; the GPU resource as far as k8s is concerned, but &lsquo;idling&rsquo; as far as you are concerned.\nMake sure to set the autoscaling field to true so that if your pipeline is not getting used, the worker pods get spun down and the GPU resource freed. Additionally, specify how much of GPU your pipeline worker will need via the resource_requests fields in your pipeline specification with resource_requests &lt;= resource_limits. Below is an example of a pipeline spec for a GPU-enabled pipeline from our market sentiment analysis example:\n{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;train_model&#34; }, &#34;description&#34;: &#34;Fine tune a BERT model for sentiment analysis on financial data.&#34;, &#34;input&#34;: { &#34;cross&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;dataset&#34;, &#34;glob&#34;: &#34;/&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;language_model&#34;, &#34;glob&#34;: &#34;/&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python&#34;, &#34;finbert_training.py&#34;, &#34;--lm_path&#34;, &#34;/pfs/language_model/&#34;, &#34;--cl_path&#34;, &#34;/pfs/out&#34;, &#34;--cl_data_path&#34;, &#34;/pfs/dataset/&#34; ], &#34;image&#34;: &#34;pachyderm/market_sentiment:dev0.25&#34; }, &#34;resource_limits&#34;: { &#34;gpu&#34;: { &#34;type&#34;: &#34;nvidia.com/gpu&#34;, &#34;number&#34;: 1 } }, &#34;resource_requests&#34;: { &#34;memory&#34;: &#34;4G&#34;, &#34;cpu&#34;: 1 } } ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "management",
        "deployment"
      ],
      "id": "9aafcaa3618e0dca7b88786f951484c4"
    },
    {
      "title": "Log In via IdP",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to log in to a cluster as an IdP user.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/login/",
      "relURI": "/latest/manage/login/",
      "body": "How to Log in to a Cluster via IdP # Open a terminal. Input the following command: pachctl auth login Select the connector you wish to use. Provide your credentials ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "workflows",
        "permissions",
        "management"
      ],
      "id": "ecee86d9a21425a03c5068fb087cd3f2"
    },
    {
      "title": "Revoke User Access",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to revoke user access to a cluster.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/revoke-user/",
      "relURI": "/latest/manage/revoke-user/",
      "body": " Before You Start # You must have clusterAdmin role permissions. How to Revoke User Access # ‚ö†Ô∏è You must remove the revoked user from your IdP user registry after completing the steps in this guide.\nRevoke a Specific Token # pachctl auth revoke --token=&lt;pach token&gt; Revoke All Tokens # pachctl auth revoke --user=idp:usernamen@pachyderm.io ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "workflows",
        "permissions",
        "management"
      ],
      "id": "8a9e7ce4acf63e5ec58c60689208cbde"
    },
    {
      "title": "Scaling Limits (CE)",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn about the built-in scaling limitations of our Community Edition.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/scaling-limits/",
      "relURI": "/latest/manage/scaling-limits/",
      "body": "Our free HPE ML Data Management Community Edition contains built-in scaling limitations and parallelism thresholds. To scale beyond these limits, request a Enterprise trial token and enjoy unlimited scaling, and more.\nüìñ You might qualify for a free Enterprise license.\nHPE ML Data Management offers activation keys for proofs-of-concept, startups, academic, nonprofit, or open-source projects. Tell us about your project!.\nScaling Limits # Number of concurrent pipelines deployed Number of workers for each pipeline Community Users can deploy up to 16 pipelines. Community Users can run up to 8 workers in parallel on each pipeline. What happens when you exceed those limits? # As a general rule, HPE ML Data Management provides an error message in the STDERR whenever a limit is encountered that prevents you from successfully running a command. In that case, the alert message links to a free trial request form.\nLimit on the number of pipelines # When exceeding the number of pipelines:\npachctl create pipeline fails once the maximum number of pipelines is reached.\npachctl update pipeline and pachctl edit pipeline succeed on existing pipelines, fail when attempting to create pipelines beyond the limit.\n‚ÑπÔ∏è If update pipeline fails for any other reason, it does not log any message related to pipeline limits.\nAll of the commands listed above create a distinct message to STDERR and to the pachd logs. This message includes information such as the limit on the number of pipelines in the Community Edition, the total number of pipelines deployed, and provides a link to request an Enterprise key to lift those limitations.\nall other list, run, start, stop pipeline commands&rsquo; behavior remains unchanged. Limit on the number of workers per pipeline # When constant parallelism &gt; 8:\npachctl create pipeline and pachctl update pipeline fail. A message to STDERR and pachd logs is generated. You will need to update your pipeline specification file accordingly or activate an Enterprise license. What happens when your license expires? # If your Enterprise License has expired and you have more than 16 pipelines, all existing pipelines continue to work. However, you will not be able to create additional pipelines. Same behavior if you upgrade your cluster.\n‚ö†Ô∏è Restoring or installing HPE ML Data Management with an expired license will fail.\n‚ÑπÔ∏è Pipelines automatically generated by the system (for example cron&hellip;) are not considered when assessing the total number of pipelines deployed. The limit applies to user-created pipelines only.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "community-edition"
      ],
      "id": "05239b1c5302637868bd7716031c2fc4"
    },
    {
      "title": "Secrets",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to create and manage Kubernetes secrets.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/secrets/",
      "relURI": "/latest/manage/secrets/",
      "body": "HPE ML Data Management uses Kubernetes&rsquo; Secrets to store and manage sensitive data, such as passwords, OAuth tokens, or ssh keys. You can use any of Kubernetes&rsquo; types of Secrets that match your use case. Namely, generic (or Opaque), tls, or docker-registry.\nAbout Secrets # When you install or upgrade a cluster, you can provide values for the configuration fields in your Helm Chart values.yaml file. However, some of those values are sensitive and should not be stored in your values.yaml file.\nHPE ML Data Management provides a way to inject those values at the time of the deployment or upgrade. We call those values platform secrets.\nHPE ML Data Management Platform Secrets Map # If no Secret KEY name is provided for the Helm Chart&rsquo;s Secret NAME Attribute, HPE ML Data Management will use the Helm Chart&rsquo;s RAW Attribute to populate its own platform secrets at the time of the installation/upgrade. Those that are not marked as required are automatically generated by the platform if not provided.\nRequired Secret KEY Name : Platform Secret Helm Chart&rsquo;s Secret NAME Attribute Helm Chart&rsquo;s RAW Attribute Yes enterprise-license-key : pachyderm-license pachd.enterpriseLicenseKeySecretName pachd.enterpriseLicenseKey Yes upstream-idps : pachyderm-identity oidc.upstreamIDPsSecretName oidc.upstreamIDPs No rootToken : pachyderm-auth pachd.rootTokenSecretName pachd.rootToken No auth-config : pachyderm-auth pachd.oauthClientSecretSecretName pachd.oauthClientSecret No cluster-role-bindings : pachyderm-auth Use plain text in your values.yaml pachd.pachAuthClusterRoleBindings No postgresql-password : postgres global.postgresql.postgresqlExistingSecretName global.postgresql.postgresqlPassword No OAUTH_CLIENT_SECRET : pachyderm-console-secret console.config.oauthClientSecretSecretName console.config.oauthClientSecret No N/A; passed into deployment manifest as plaintext. pachd.enterpriseServerTokenSecretName pachd.enterpriseServerToken No enterprise-secret : pachyderm-enterprise pachd.enterpriseSecretSecretName pachd.enterpriseSecret Create A Secret # The creation of a Secret in HPE ML Data Management requires a JSON configuration file.\nA good way to create this file is:\nTo generate it by calling a dry-run of the kubectl create secret ... --dry-run=client --output=json &gt; myfirstsecret.json command. Then call pachctl create secret -f myfirstsecret.json. ‚ö†Ô∏è Kubernetes Secrets are, by default, stored as unencrypted base64-encoded strings (i.e., the values for all keys in the data field have to be base64-encoded strings). When using the kubectl create secret command, the encoding is done for you. If you choose to manually create your JSON file, make sure to use your own base 64 encoder.\nGenerate Your Secret Config File # Let&rsquo;s first generate your secret configuration file using the kubectl command. For example:\nfor a generic authentication secret: kubectl create secret generic mysecretname --from-literal=username=&lt;myusername&gt; --from-literal=password=&lt;mypassword&gt; --dry-run=client --output=json &gt; myfirstsecret.json for a tls secret: kubectl create secret tls mysecretname --cert=&lt;Path to your certificate&gt; --key=&lt;Path to your SSH key&gt; --dry-run=client --output=json &gt; myfirstsecret.json for a docker registry secret: kubectl create secret docker-registry mysecretname --dry-run=client --docker-server=&lt;DOCKER_REGISTRY_SERVER&gt; --docker-username=&lt;DOCKER_USER&gt; --docker-password=&lt;DOCKER_PASSWORD&gt; --output=json &gt; myfirstsecret.json Generic Secret Example # { &#34;apiVersion&#34;: &#34;v1&#34;, &#34;kind&#34;: &#34;Secret&#34;, &#34;metadata&#34;: { &#34;name&#34;: &#34;clearml&#34; }, &#34;type&#34;: &#34;Opaque&#34;, &#34;stringData&#34;: { &#34;access&#34;: &#34;&lt;CLEARML_API_ACCESS_KEY&gt;&#34;, &#34;secret&#34;: &#34;&lt;CLEARML_API_SECRET_KEY&gt;&#34; } } Find more detailed information on the creation of Secrets in Kubernetes documentation.\nCreate your Secret in HPE ML Data Management # Next, run the following to actually create the secret in the HPE ML Data Management Kubernetes cluster:\npachctl create secret -f myfirstsecret.json You can run pachctl list secret to verify that your secret has been properly created. You should see an output that looks like the following:\nNAME TYPE CREATED mysecret kubernetes.io/dockerconfigjson 11 seconds ago ‚ÑπÔ∏è Use pachctl delete secret to delete a secret given its name, pachctl inspect secret to list a secret given its name.\nYou can now edit your pipeline specification file as follow.\nReference a Secret in a Pipeline Spec # Now that your secret is created on HPE ML Data Management cluster, you will need to notify your pipeline by updating your pipeline specification file. In HPE ML Data Management, a Secret can be used in three different ways:\nAs a container environment variable:\nIn this case, in HPE ML Data Management&rsquo;s pipeline specification file, you need to reference Kubernetes&rsquo; Secret by its:\nname and specify an environment variable named env_var that the value of your key should be bound to. This makes for easy access to your Secret&rsquo;s data in your pipeline&rsquo;s code. For example, this is useful for passing the password to a third-party system to your pipeline&rsquo;s code.\n&#34;transform&#34;: { &#34;image&#34;: &#34;string&#34;, &#34;cmd&#34;: [ string ], ... &#34;secrets&#34;: [ { &#34;name&#34;: &#34;string&#34;, &#34;env_var&#34;: &#34;string&#34;, &#34;key&#34;: string }] } Example # Example of a pipeline specification file assigning a Secret&rsquo;s values to environment variables.\nLook at the pipeline specification in this example and see how we used the &quot;env_var&quot; to pass CLEARML API credentials to the pipeline code.\n{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;mnist&#34; }, &#34;description&#34;: &#34;MNIST example logging to ClearML&#34;, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;/bin/sh&#34; ], &#34;stdin&#34;: [ &#34;python pytorch_mnist.py --lr 0.2 --save-location /pfs/out&#34; ], &#34;image&#34;: &#34;pachyderm/clearml_mnist:dev0.11&#34;, &#34;secrets&#34;: [ { &#34;name&#34;: &#34;clearml&#34;, &#34;env_var&#34;: &#34;CLEARML_API_ACCESS_KEY&#34;, &#34;key&#34;: &#34;access&#34; }, { &#34;name&#34;: &#34;clearml&#34;, &#34;env_var&#34;: &#34;CLEARML_API_SECRET_KEY&#34;, &#34;key&#34;: &#34;secret&#34; } ] } } As a file in a volume mounted on a container:\nIn this case, in HPE ML Data Management&rsquo;s pipeline specification file, you need to reference Kubernetes&rsquo; Secret by its:\nname and specify the mount point (mount_path) to the secret (ex: &quot;/var/my-app-secret&quot;). HPE ML Data Management mounts all of the keys in the secret with file names corresponding to the keys. This is useful for secure configuration files.\n&#34;transform&#34;: { &#34;image&#34;: &#34;string&#34;, &#34;cmd&#34;: [ string ], ... &#34;secrets&#34;: [ { &#34;name&#34;: &#34;string&#34;, &#34;mount_path&#34;: string }] } When pulling images:\nImage pull Secrets are a different kind of secret used to store access credentials to your private image registry.\nYou reference Image Pull Secrets (or Docker Registry Secrets) by setting the image_pull_secrets field of your pipeline specification file to the secret&rsquo;s name you created (ex: &quot;mysecretname&quot;).\n&#34;transform&#34;: { &#34;image&#34;: &#34;string&#34;, &#34;cmd&#34;: [ string ], ... &#34;image_pull_secrets&#34;: [ string ] } ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "secrets",
        "management",
        "kubernetes"
      ],
      "id": "c2af003368fa8037606b6ecc5f9ada08"
    },
    {
      "title": "Sidecar S3 Gateway",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to use S3-protocol-enabled pipelines and interact with external input/output data.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/deploy-s3gateway-sidecar/",
      "relURI": "/latest/manage/deploy-s3gateway-sidecar/",
      "body": "You can interact with input/output data through the S3 protocol using HPE ML Data Management&rsquo;s S3-protocol-enabled pipelines.\nAbout # HPE ML Data Management&rsquo;s S3-protocol-enabled pipelines run a separate S3 gateway instance in a sidecar container within the pipeline-worker pod. Using this approach enables maintaining data provenance since the external code (e.g., within a Kubeflow pod) is executed in (and associated with) a HPE ML Data Management job.\nWhen enabled, input and output repositories are exposed as S3 Buckets via the S3 gateway sidecar instance.\nInput Address: s3://&lt;input_repo_name&gt;. Output Address: s3://out Example with Kubeflow Pod # The following diagram shows communication between the S3 gateway deployed in a sidecar and the Kubeflow pod.\nConfigure an S3-enabled Pipeline # Open your pipeline spec. Add &quot;s3&quot;: true to input.pfs. Add &quot;s3_out&quot;: true to pipeline. Save your spec. Update your pipeline. Example Pipeline Spec # The following spec example reads files in the input bucket labresults and copies them in the pipeline&rsquo;s output bucket:\n{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;s3_protocol_enabled_pipeline&#34; }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/&#34;, &#34;repo&#34;: &#34;labresults&#34;, &#34;name&#34;: &#34;labresults&#34;, &#34;s3&#34;: true } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;sh&#34; ], &#34;stdin&#34;: [ &#34;set -x &amp;&amp; mkdir -p /tmp/result &amp;&amp; aws --endpoint-url $S3_ENDPOINT s3 ls &amp;&amp; aws --endpoint-url $S3_ENDPOINT s3 cp s3://labresults/ /tmp/result/ --recursive &amp;&amp; aws --endpoint-url $S3_ENDPOINT s3 cp /tmp/result/ s3://out --recursive&#34; ], &#34;image&#34;: &#34;pachyderm/ubuntu-with-s3-clients:v0.0.1&#34; }, &#34;s3_out&#34;: true } User Code Requirements # Your user code is responsible for:\nProviding its own S3 client package as part of the image (boto3) reading and writing in the S3 Buckets exposed to the pipeline Accessing the Sidecar # Use the S3_ENDPOINT environment variable to access the sidecar. No authentication is needed; you can only read the input bucket and write in the output bucket.\naws --endpoint-url $S3_ENDPOINT s3 cp /tmp/result/ s3://out --recursive Triggering External Pipelines # If Authentication is enabled, you can access the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY env vars in your pipeline user code to forward your pipeline&rsquo;s auth credentials to third-party tools like Spark.\nConstraints # All files are processed as a single datum, meaning: The glob field in the pipeline must be set to &quot;glob&quot;: &quot;/&quot;. Already processed datums are not skipped. Only cross inputs are supported; join, group, and union are not supported. You can create a cross of an S3-enabled input with a non-S3 input; For a non-S3 input in such a cross, you can still specify a glob pattern. Input bucket(s) are read-only, and the output bucket is initially empty and writable. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "faa3044ce2135025ee7ba472c4efc8d0"
    },
    {
      "title": "Storage Optimization",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to optimize your data storage to increase data-processing performance.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/data-management/",
      "relURI": "/latest/manage/data-management/",
      "body": "This section discusses best practices for minimizing the space needed to store your HPE ML Data Management data, increasing the performance of your data processing as related to data organization, and general good ideas when you are using HPE ML Data Management to version/process your data.\nSetting a root volume size # When planning and configuring your HPE ML Data Management deployment, you need to make sure that each node&rsquo;s root volume is big enough to accommodate your total processing bandwidth. Specifically, you should calculate the bandwidth for your expected running jobs as follows:\n(storage needed per datum) x (number of datums being processed simultaneously) / (number of nodes) Here, the storage needed per datum must be the storage needed for the largest datum you expect to process anywhere on your DAG plus the size of the output files that will be written for that datum. If your root volume size is not large enough, pipelines might fail when downloading the input. The pod would get evicted and rescheduled to a different node, where the same thing might happen (assuming that node had a similar volume).\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "management",
        "storage"
      ],
      "id": "cf195688a7b85f8e74eba4b0f3e3114b"
    },
    {
      "title": "Usage Metrics",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to disable automatically collected usage metrics.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/disable-metrics/",
      "relURI": "/latest/manage/disable-metrics/",
      "body": "HPE ML Data Management automatically collects and reports anonymous usage metrics. These metrics help the HPE ML Data Management team understand how people use HPE ML Data Management to make it better. If you want opt out of anonymous metrics collection, disable them by setting the METRICS environment variable to false in the pachd container.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "61b51deb8c1fb30c8822bfe88259c125"
    },
    {
      "title": "Monitor with Prometheus",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Manage",
      "description": "Learn how to monitor a cluster using Prometheus.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/prometheus/",
      "relURI": "/latest/manage/prometheus/",
      "body": " ‚ÑπÔ∏è To monitor a HPE ML Data Management cluster with Prometheus, a Enterprise License is required.\nHPE ML Data Management&rsquo;s deployment manifest exposes Prometheus metrics, allowing an easy set up of the monitoring of your cluster. Only available for self-managed deployments today.\n‚ö†Ô∏è These installation steps are for Informational Purposes ONLY. Please refer to your full Prometheus documentation for further installation details and any troubleshooting advice.\nPrometheus installation and Service Monitor creation # Helm install kube-prometheus-stack, Prometheus&rsquo; Kubernetes cluster monitoring using the Prometheus Operator:\nGet Repo Info helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Install the Prometheus-operator helm chart helm install &lt;a-release-name&gt; prometheus-community/kube-prometheus-stack Create a ServiceMonitor for HPE ML Data Management in Kubernetes:\nCreate a myprometheusservice.yaml\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: pachyderm-scraper labels: release: &lt;a-release-name&gt; spec: selector: matchLabels: suite: pachyderm namespaceSelector: matchNames: - default endpoints: - port: prom-metrics interval: 30s Create a ServiceMonitor looking to scrape metrics from suite: pachyderm:\nkubectl create -f myprometheusservice.yaml The prometheus-operator will search for the pods based on the label selector &lt;a-release-name&gt; and creates a prometheus target so prometheus will scrape the metrics endpoint prom-metrics.\nIn this case, it looks for anything with the label suite: pachyderm - which is by default associated with all HPE ML Data Management resources.\n‚ÑπÔ∏è Our Service Monitor pachyderm-scraper above maps the endpoint port prom-metrics to a corresponding prom-metrics port described in HPE ML Data Management&rsquo;s deployment manifest. Let&rsquo;s take a quick look at this file:\nkubectl -o json get service/pachd In the json file, find:\n{ &#34;name&#34;: &#34;prom-metrics&#34;, &#34;port&#34;: 1656, &#34;protocol&#34;: &#34;TCP&#34;, &#34;targetPort&#34;: &#34;prom-metrics&#34; } Port-Forward # Connect to Prometheus using the following command:\nkubectl port-forward service/&lt;release-name&gt;-kube-prometheus-prometheus 9090 If you have an existing Prometheus deployment, please navigate to your Prometheus GUI.\nBrowse # You can now browse your targets (http://localhost:9090/targets). Run a pipeline of your choice. The pachyderm-scraper should be visible:\nIn the ClassicUI tab, you should be able to see the new HPE ML Data Managementmetrics.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "c316fabb6180b00a0269901156dbad2d"
    },
    {
      "title": "Metrics",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Monitor with Prometheus",
      "description": "Learn about the job metrics available.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/prometheus/metrics/",
      "relURI": "/latest/manage/prometheus/metrics/",
      "body": " Metric Type Description pachyderm_worker_datum_count Counter Counts the number of datums processed by a pipeline. pachyderm_worker_datum_proc_time Histogram Tracks the time spent in user code for datums processed by a pipeline. pachyderm_worker_datum_proc_seconds_count Counter Counts the total time spent in user code by a pipeline. pachyderm_worker_datum_download_time Histogram Tracks the time spent downloading input data by a pipeline. pachyderm_worker_datum_download_seconds_count Counter Counts the total time spent downloading input data by a pipeline. pachyderm_worker_datum_upload_time Histogram Tracks the time spent uploading output data by a pipeline. pachyderm_worker_datum_upload_seconds_count Counter Counts the total time spent uploading output data by a pipeline. pachyderm_worker_datum_download_size Histogram Tracks the size of input data downloaded by a pipeline. pachyderm_worker_datum_download_bytes_count Counter Counts the total size of input data downloaded by a pipeline. pachyderm_worker_datum_upload_size Histogram Tracks the size of output data uploaded by a pipeline. pachyderm_worker_datum_upload_bytes_count Counter Counts the total size of output data uploaded by a pipeline. pachyderm_auth_dex_approval_errors_total Counter Counts the number of HTTP requests to /approval that ended in error. pachyderm_auth_dex_http_requests_duration_seconds Histogram Histogram of time spent processing Dex requests, by response status code and HTTP method. pachyderm_auth_dex_http_requests_in_flight Gauge Tracks the number of requests currently being handled by Dex. pachyderm_auth_dex_http_requests_total Counter Counts the number of HTTP requests handled by Dex, by response status code and HTTP method. pachyderm_auth_dex_startup_errors_total Counter Counts the number of HTTP requests that were rejected because the server can&rsquo;t start. pachyderm_pachd_admin_v2_inspect_cluster_seconds Histogram Tracks the run time of InspectCluster. pachyderm_pachd_auth_v2_who_am_i_seconds Histogram Tracks the run time of WhoAmI. pachyderm_pachd_enterprise_v2_get_state_seconds Histogram Tracks the run time of GetState. pachyderm_pachd_grpc_check_seconds Histogram Tracks the run time of Check. pachyderm_pachd_pfs_v2_list_repo_seconds Histogram Tracks the run time of ListRepo. pachyderm_pachd_pps_v2_list_pipeline_seconds Histogram Tracks the run time of ListPipeline. pachyderm_pachd_report_metric Gauge Tracks the gauge of the number of calls to reportDuration(). pachyderm_pfs_object_storage_cache_evictions_total Counter Counts the number of objects evicted from the LRU cache. pachyderm_pfs_object_storage_cache_hits_total Counter Counts the number of object storage gets served from cache. pachyderm_pfs_object_storage_cache_misses_total Counter Counts the number of object storage gets that were not served from cache. pachyderm_pfs_object_storage_operation_count_total Counter Counts the number of object storage operations, by storage type and operation name. pachyderm_pfs_object_storage_read_bytes_total Counter Counts the number of bytes read from object storage, by storage type. pachyderm_pfs_object_storage_written_bytes_total Counter Counts the number of bytes written to object storage, by storage type. pachyderm_postgres_tx_start_count Counter Counts the number of transactions that have been started. pachyderm_postgres_tx_underlying_start_count Counter Counts the number of underlying database transactions that have been started. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a7f9ced97b981631f335049a8437f364"
    },
    {
      "title": "Uninstall",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Manage",
      "description": "Learn how to uninstall our platform.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/manage/uninstall/",
      "relURI": "/latest/manage/uninstall/",
      "body": " Uninstall HPE ML Data Management # helm uninstall pachyderm kubectl delete pvc -l suite=pachyderm ‚ÑπÔ∏è The name of the Helm release is pachyderm by default. If you used a different name, replace pachyderm with the name of your Helm release (e.g., pachd)\nUninstall Pachctl # brew uninstall @&lt;major&gt;.&lt;minor&gt; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e77bc743ead01661876be371bc798caf"
    },
    {
      "title": "Prepare Data",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "Prepare your data for transformation.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/prepare-data/",
      "relURI": "/latest/prepare-data/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "872d7861878d992cd04c02f168436bb6"
    },
    {
      "title": "Datum Batching",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Prepare Data",
      "description": "Learn how to batch datums to optimize performance.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/prepare-data/datum-batching/",
      "relURI": "/latest/prepare-data/datum-batching/",
      "body": "By default, HPE ML Data Management processes each datum independently. This means that your user code is called once for each datum. This can be inefficient and costly if you have a large number of small datums or if your user code is slow to start.\nWhen you have a large number of datums, you can batch them to optimize performance. HPE ML Data Management provides a next datum command that you can use to batch datums.\nFlow Diagram # flowchart LR user_code(User Code) ndsuccess(NextDatum) nderror(&#34;NextDatum(error)&#34;) response(NextDatumResponse) process_datum{process datum} cmd_err(Run cmd_err) kill[Kill User Code] datum?{datum exists?} retry?{retry?} cmd_err?{cmd_err defined} user_code ==&gt;ndsuccess ndsuccess =====&gt; datum? datum? ==&gt;|yes| process_datum process_datum ==&gt;|success| response response ==&gt; user_code datum? --&gt;|no| kill process_datum --&gt;|fail| nderror nderror --&gt; cmd_err? cmd_err? --&gt;|yes| cmd_err cmd_err? --&gt;|no|kill cmd_err --&gt; retry? retry? --&gt;|yes| response retry? --&gt;|no| kill How to Batch Datums # Define your user code and build a docker image. Your user code must call pachctl next datum to get the next datum to process.\nLanguage: Bash Python transformation() { # Your transformation code goes here echo &#34;Transformation function executed&#34; } echo &#34;Starting while loop&#34; while true; do pachctl next datum echo &#34;Next datum called&#34; transformation done Your user code can apply the @batch_all_datums convenience decorator to iterate through all datums. This will perform the NextDatum calls for you as well as prepare the environment for each datum.\nimport os from python_pachyderm import batch_all_datums @batch_all_datums def main(): # Processing code goes here. # This function will be run for each datum until all are processed. # Once all datums are processed, the process is terminated. print(f&#39;datum processed: {os.environ[&#34;PACH_DATUM_ID&#34;]}&#39;) def init(): # Initializing code goes here. # When this function is called, no input data is present. print(&#39;Preparing for datum batching job&#39;) if __name__ == &#39;__main__&#39;: init() print(&#39;Starting datum processing&#39;) main() Create a repo (e.g., pachctl create repo repoName).\nDefine a pipeline spec in YAML or JSON that references your Docker image and repo.\nAdd the following to the transform section of your pipeline spec:\ndatum_batching: true pipeline: name: p_datum_batching_example input: pfs: repo: repoName glob: &#34;/*&#34; transform: datum_batching: true image: user/docker-image:tag Create the pipeline (e.g., pachctl update pipeline -f pipeline.yaml).\nMonitor the pipeline&rsquo;s state either via Console or via pachctl list pipeline.\nüí° You can view the printed confirmation of &ldquo;Next datum called&rdquo; in the logs your pipeline&rsquo;s job.\nFAQ # Q: My pipeline started but no files from my input repo are present. Where are they?\nA: Files from the first datum are mounted following the first call to NextDatum or, when using the Python client, when code execution enters the decorated function.\nQ: How can I set environment variables when the datum runs?\nA: You can use the .env file accessible from the /pfs directory. To easily locate your .env file, you can do the following:\ndef find_files(pattern): return [f for f in glob.glob(os.path.join(&#34;/pfs&#34;, &#34;**&#34;, pattern), recursive=True)] env_file = find_files(&#34;.env&#34;) ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "datums",
        "data-operations"
      ],
      "id": "8e39de3fe15770b30c9d698b63215b9f"
    },
    {
      "title": "Defer Processing via Staging Branch",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Prepare Data",
      "description": "Learn how to defer processing of data by using a staging branch in an input repository.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/prepare-data/dp-staging-branch/",
      "relURI": "/latest/prepare-data/dp-staging-branch/",
      "body": "When you want to load data into HPE ML Data Management without triggering a pipeline, you can upload it to a staging branch and then submit accumulated changes in one batch by re-pointing the HEAD of your master branch to a commit in the staging branch. Let&rsquo;s see how this works.\nHow to Use a Staging Branch # Create a repository. For example, data.\npachctl create repo data Create a master branch.\npachctl create branch data@master View the created branch:\npachctl list commit data REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION data master 8090bfb4d4fe44158eac12199c37a591 About a minute ago 0B AUTO HPE ML Data Management automatically created an empty HEAD commit on the new branch, as you can see from the 0B (zero-byte) size and AUTO commit origin.\nCommit a file to a staging branch:\npachctl put file data@staging -f &lt;file&gt; HPE ML Data Management automatically creates the staging branch. Your repo now has 2 branches, staging and master. In this example, the staging name is used, but you can name the branch as you want &ndash; and have as many staging branches as you need.\nVerify that the branches were created:\npachctl list branch data BRANCH HEAD TRIGGER staging f3506f0fab6e483e8338754081109e69 - master 8090bfb4d4fe44158eac12199c37a591 - The master branch still has the same HEAD commit. No jobs have started to process the new file, because there are no pipelines that take staging as inputs. You can continue to commit to staging to add new data to the branch, and the pipeline will not process anything.\nWhen you are ready to process the data, update the master branch to point it to the head of the staging branch:\npachctl create branch data@master --head staging List your branches to verify that the master branch&rsquo;s HEAD commit has changed:\npachctl list branch data staging f3506f0fab6e483e8338754081109e69 master f3506f0fab6e483e8338754081109e69 The master and staging branches now have the same HEAD commit. This means that your pipeline has data to process.\nVerify that the pipeline has new jobs:\npachctl list job data@f3506f0fab6e483e8338754081109e69 ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE f3506f0fab6e483e8338754081109e69 data 32 seconds ago Less than a second 0 6 + 0 / 6 108B 24B success You should see one job that HPE ML Data Management created for all the changes you have submitted to the staging branch, with the same ID. While the commits to the staging branch are ancestors of the current HEAD in master, they were never the actual HEAD of master themselves, so they do not get processed. This behavior works for most of the use cases because commits in HPE ML Data Management are generally additive, so processing the HEAD commit also processes data from previous commits.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "datums",
        "data-operations"
      ],
      "id": "7d9c137aa0b00cfb3b6e122fce00c830"
    },
    {
      "title": "Ingest Data",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Prepare Data",
      "description": "Learn how to ingest data using console and the PachCTL CLI.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/prepare-data/ingest-data/",
      "relURI": "/latest/prepare-data/ingest-data/",
      "body": "You can upload data to an input repo in HPE ML Data Management by using Console or the PachCTL CLI. Console offers an easy-to-use interface for quick drop-in uploads, while the PachCTL CLI provides more advanced options for uploading data by opening commits and transactions.\nBefore You Start # You must have a running HPE ML Data Management cluster. You must have created a project and repository. HPE ML Data Management uses *?[]{}!()@+^ as reserved characters for glob patterns. Because of this, you cannot use these characters in your filepath. How to Ingest Data # Via Console # Open Console. Scroll to a project you wish to upload data to and select View Project. Select a repository from the DAG view. Select Upload ( ). Optionally, provide a File Path to upload your data to a specific directory in the repository. Select an existing Branch; defaults to master. Select Browse Files and select the files you want to upload. Via PachCTL # For a comprehensive list of options for pachctl put file, see the CLI Reference.\nAtomic Commits # Atomic commits automatically open a commit, add data, and close the commit. They are typically useful when you only need to submit one file or directory &ndash; otherwise, you should use manual commits to avoid noisy commit history and overwhelming your pipeline with many jobs.\nOpen terminal. Run the following command: pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file1&gt; -f &lt;file1&gt; Manual Commits # Manual commits allow you to open a commit, add data, and close the commit when you are ready. They are typically useful when you need to submit multiple files or directories that are related. They also help your pipeline process your data faster by using only one job to process all of the data.\nOpen terminal. Run the following command: pachctl start commit &lt;repo&gt;@&lt;branch&gt; Add multiple pieces of data: pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file1&gt; -f &lt;file1&gt; pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file2&gt; -f http://file_2_url_path pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file3&gt; -r -f s3://file_3_obj_store_url/upload-dir pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file3&gt; -i -f gcs://file_4_obj_store_url/dir-contents-ony pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file3&gt; -f as://file_5_obj_store_url Close the commit: pachctl finish commit &lt;repo&gt;@&lt;branch&gt; üí° If you have a large dataset and you wish to only upload a subset of it, you can add a metadata file containing a list of urls/paths to the relevant data. Your pipeline code will retrieve the data following their path without the need to preload it all.\nIn this case, HPE ML Data Management will not keep versions of the source file, but it will keep track and provenance of the resulting output commits.\nTransactions # Transactions allow you to bundle multiple manual commits together and process them simultaneously in one job run. This is particularly useful for pipelines that require multiple inputs to be processed together instead of kicking off a job each time only one of the inputs has been updated.\nOpen terminal. Run the following command: pachctl start transaction Open commits and add data: pachctl create branch &lt;repo&gt;@&lt;branch&gt; pachctl start commit &lt;repo&gt;@&lt;branch&gt; pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file1&gt; -f &lt;file1&gt; pachctl finish commit &lt;repo&gt;@&lt;branch&gt; pachctl start commit &lt;repo&gt;@&lt;branch&gt; pachctl put file &lt;repo&gt;@&lt;branch&gt;:&lt;/path/to/file2&gt; -f &lt;file2&gt; pachctl finish commit &lt;repo&gt;@&lt;branch&gt; Close the transaction: pachctl finish transaction ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "ingest data",
        "upload files"
      ],
      "id": "d615129a6dc4f62f20cebb711a82f0b1"
    },
    {
      "title": "Mount Volumes",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Prepare Data",
      "description": "Learn how to mount local or network-attached storage and use its data in pipelines.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/prepare-data/mount-volume/",
      "relURI": "/latest/prepare-data/mount-volume/",
      "body": "You may have a local or a network-attached storage that you want your pipeline to write files to. You can mount that folder as a volume in Kubernetes and make it available in your pipeline worker by using the pod_patch pipeline parameter. The pod_patch parameter takes a string that specifies the changes that you want to add to your existing manifest. To create a patch, you need to generate a diff of the original ReplicationController and the one with your changes. You can use one of the online JSON patch utilities, such as JSON Patch Generator to create a diff. A diff for mounting a volume might look like this:\n[ { &#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/volumes/-&#34;, &#34;value&#34;: { &#34;name&#34;: &#34;task-pv-storage&#34;, &#34;persistentVolumeClaim&#34;: { &#34;claimName&#34;: &#34;task-pv-claim&#34; } } }, { &#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/containers/0/volumeMounts/-&#34;, &#34;value&#34;: { &#34;mountPath&#34;: &#34;/data&#34;, &#34;name&#34;: &#34;task-pv-volume&#34; } } ] This output needs to be converted into a one-liner and added to the pipeline spec.\nWe will use the OpenCV example. to demonstrate this functionality.\nTo mount a volume, complete the following steps:\nCreate a PersistentVolume and a PersistentVolumeClaim as described in Configure a Pod to Use a PersistentVolume for Storage. Modify mountPath and path as needed.\nFor testing purposes, you might want to add an index.html file as described in Create an index.html file.\nGet the ReplicationController (RC) manifest from your pipeline:\nkubectl get rc &lt;rc-pipeline&gt; -o json &gt; &lt;filename&gt;.yaml Example:\nkubectl get rc pipeline-edges-v7 -o json &gt; test-rc.yaml Open the generated RC manifest for editing.\nUnder spec, find the volumeMounts section.\nAdd your volume in the list of mounts.\nExample:\n{ &#34;mountPath&#34;: &#34;/data&#34;, &#34;name&#34;: &#34;task-pv-storage&#34; } mountPath is where your volume will be mounted inside of the container.\nFind the volumes section.\nAdd the information about the volume.\nExample:\n{ &#34;name&#34;: &#34;task-pv-storage&#34;, &#34;persistentVolumeClaim&#34;: { &#34;claimName&#34;: &#34;task-pv-claim&#34; } } In this section, you need to specify the PersistentVolumeClaim you have created in Step 1.\nSave these changes to a new file.\nCopy the contents of the original RC to the clipboard.\nGo to a JSON patch generator, such as JSON Patch Generator, and paste the contents of the original RC manifest to the Source JSON field.\nCopy the contents of the modified RC manifest to clipboard as described above.\nPaste the contents of the modified RC manifest to the Target JSON field.\nCopy the generated JSON Patch.\nGo to your terminal and open the pipeline manifest for editing.\nFor example, if you are modifying the edges pipeline, open the edges.json file.\nAdd the patch as a one-liner under the pod_patch parameter.\nExample:\n&#34;pod_patch&#34;: &#34;[{\\&#34;op\\&#34;: \\&#34;add\\&#34;,\\&#34;path\\&#34;: \\&#34;/volumes/-\\&#34;,\\&#34;value\\&#34;: {\\&#34;name\\&#34;: \\&#34;task-pv-storage\\&#34;,\\&#34;persistentVolumeClaim\\&#34;: {\\&#34;claimName\\&#34;: \\&#34;task-pv-claim\\&#34;}}}, {\\&#34;op\\&#34;: \\&#34;add\\&#34;,\\&#34;path\\&#34;: \\&#34;/containers/0/volumeMounts/-\\&#34;,\\&#34;value\\&#34;: {\\&#34;mountPath\\&#34;: \\&#34;/data\\&#34;,\\&#34;name\\&#34;: \\&#34;task-pv-storage\\&#34;}}]&#34; You need to add a backslash () before every quote (&quot;) sign that is enclosed in square brackets ([]). Also, you might need to modify the path to volumeMounts and volumes by removing the /spec/template/spec/ prefix and replacing the assigned volume number with a dash (-). For example, if a path in the JSON patch is /spec/template/spec/volumes/5, you might need to replace it with /volumes/-. See the example above for details.\nAfter modifying the pipeline spec, update the pipeline:\npachctl update pipeline -f &lt;pipeline-spec.yaml&gt; A new pod and new replication controller should be created with your modified changes.\nVerify that your file was mounted by connecting to your pod and listing the directory that you have specified as a mountpoint. In this example, it is /data.\nExample:\nkubectl exec -it &lt;pipeline-pod&gt; -- /bin/bash ls /data If you have added the index.html file for testing as described in Step 1, you should see that file in the mounted directory.\nYou might want to adjust your pipeline code to read from or write to the mounted directory. For example, in the aforementioned OpenCV example, the code reads from the /pfs/images directory and writes to the /pfs/out directory. If you want to read or write to the /data directory, you need to change those to /data.\nüìñ HPE ML Data Management has no notion of the files stored in the mounted directory before it is mounted to HPE ML Data Management. Moreover, if you have mounted a network share to which you write files from other than HPE ML Data Management sources, HPE ML Data Management does not guarantee the provenance of those changes.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "data-operations"
      ],
      "id": "b79ef65915885a552d52e8c72c4a280f"
    },
    {
      "title": "Skip Failed Datums",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Prepare Data",
      "description": "Learn how to skip failed datums to prevent jobs from fully failing.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/prepare-data/err-cmd/",
      "relURI": "/latest/prepare-data/err-cmd/",
      "body": " ‚ÑπÔ∏è The err_cmd parameter enables you to fail a datum without failing the whole job.\nüí° Before you read this section, make sure that you understand such concepts as Datum and Pipeline.\nWhen HPE ML Data Management processes your data, it breaks it up into units of computation called datums. Each datum is processed separately. In a basic pipeline configuration, a failed datum results in a failed job. However, in some cases, you might not need all datums to consider a job successful. If your downstream pipelines can be run on only the successful datums instead of needing all the datums to be successful, HPE ML Data Management can mark some datums as recovered which means that they failed with a non-critical error, but the successful datums will be processed.\nTo configure a condition under which you want your failed datums not to fail the whole job, you can add your custom error code in err_cmd and err_stdin fields in your pipeline specification.\nFor example, your DAG consists of two pipelines:\nThe pipeline 1 cleans the data. The pipeline 2 trains your model by using the data from the first pipeline. That means that the second pipeline takes the results of the first pipeline from its output repository and uses that data to train a model. In some cases, you might not need all the datums in the first pipeline to be successful to run the second pipeline.\nThe following diagram describes how HPE ML Data Management transformation and error code work:\nHere is what is happening in the diagram above:\nHPE ML Data Management executes the transformation code that you defined in the cmd field against your datums. If a datum is processed without errors, HPE ML Data Management marks it as processed. If a datum fails, HPE ML Data Management executes your error code (err_cmd) on that datum. If the code in err_cmd successfully runs on the skipped datum, HPE ML Data Management marks the skipped datum as recovered. The datum is in a failed state and, therefore, the pipeline does not put it into the output repository, but successful datums continue onto the next step in your DAG. If the err_cmd code fails on the skipped datum, the datum is marked as failed, and, consequently, the job is marked as failed. You can view the processed, skipped, and recovered datums in the PROGRESS field in the output of the pachctl list job -p &lt;pipeline name&gt; command:\nHPE ML Data Management writes only processed datums of successful jobs to the output commit so that these datums can be processed by downstream pipelines. For example, in your first pipeline, HPE ML Data Management processes three datums. If one of the datums is marked as recovered and two others are successfully processed, only these two successful datums are used in the next pipeline.\nIf you want to let the job proceed with only the successful datums being written to the output, set &quot;err_cmd&quot; : [&quot;true&quot;]. The failed datums, which are &ldquo;recovered&rdquo; by err_cmd in this way, will be retried on the next job, just as failed datums.\n‚ÑπÔ∏è See Also: Example err_cmd pipeline\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "datums",
        "data-operations"
      ],
      "id": "7d64be327c19fc41e80b2b6a32a8928f"
    },
    {
      "title": "SQL Ingest",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Prepare Data",
      "description": "Learn how to set up the SQL Ingest Tool to import data.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/prepare-data/sql-ingest/",
      "relURI": "/latest/prepare-data/sql-ingest/",
      "body": " ‚ö†Ô∏è SQL Ingest is an experimental feature.\nYou can inject database content, collected by your data warehouse, by pulling the result of a given query into HPE ML Data Management and saving it as a CSV or JSON file.\nBefore You Start # You should be familiar with Jsonnet. You should be familiar with creating Jsonnet pipeline specs in HPE ML Data Management. You should be familiar with managing Kubernetes secrets. How to Set Up SQL Ingest # 1. Create &amp; Upload a Secret # You must generate a secret that contains the password granting user access to the database; you will pass the username details through the database connection string in step 2.\nCopy the following: kubectl create secret generic yourSecretName --from-literal=PACHYDERM_SQL_PASSWORD=yourDatabaseUserPassword --dry-run=client --output=json &gt; yourSecretFile.json Swap out yourSecretName, yourDatabaseUserPassword, and yourSecretFile with relevant inputs. Open a terminal and run the command. Copy the following: pachctl create secret -f yourSecretFile.json Swap out yourSecretfile with relevant filename. Run the command. Confirm secret by running pachctl list secret. ‚ÑπÔ∏è Not all secret formats are the same. For a full walkthrough on how to create, edit, and view different types of secrets, see Create and Manage Secrets in HPE ML Data Management.\n2. Create a Database Connection String # HPE ML Data Management&rsquo;s SQL Ingest requires a connection string defined as a Jsonnet URL parameter to connect to your database; the URL is structured as follows:\n&lt;protocol&gt;://&lt;username&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?&lt;param1&gt;=&lt;value1&gt;&amp;&lt;param2&gt;=&lt;value2&gt; 3. Create a Pipeline Spec # HPE ML Data Management provides a default Jsonnet template that has key parameters built in. To use it, you must pass an argument for each parameter.\nCopy the following: pachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.7.x/src/templates/sql_ingest_cron.jsonnet \\ --arg name=&lt;pipelineName&gt; \\ --arg url=&#34;&lt;connectionStringToDdatabase&gt;&#34; \\ --arg query=&#34;&lt;query&gt;&#34; \\ --arg hasHeader=&lt;boolean&gt; \\ --arg cronSpec=&#34;&lt;pullInterval&gt;&#34; \\ --arg secretName=&#34;&lt;youSecretName&gt;&#34; \\ --arg format=&lt;CsvOrJson&gt; --arg outputFile=&#39;&lt;fileName&gt;&#39; Swap out all of the parameter values with relevant inputs. Open terminal. Run the command. 4. View Query &amp; Results # To View Query String: pachctl inspect pipeline &lt;pipelineName&gt; To View Output File Name: pachctl list file &lt;pipelineName&gt;@master To View Output File Contents: pachctl get file &lt;pipelineName&gt;@master:/0000 Example: Snowflake # In this example, we are leveraging Snowflake&rsquo;s support for queries traversing semi-structured data (here, JSON).\nCreate a secret with your password named snowflakeSecret. Create a Snowflake specific database connection URL using the following details: Protocol: snowflake Username: username Host: VCNYTW-MH64356 (account name or locator) Database: SNOWFLAKE_SAMPLE_DATA Schema: WEATHER Warehouse: COMPUTE_WH snowflake://username@VCNYTW-MH64356/SNOWFLAKE_SAMPLE_DATA/WEATHER?warehouse=COMPUTE_WH Build query for the table DAILY_14_TOTAL using information from column V. select T, V:city.name, V:data[0].weather[0].description as morning, V:data[12].weather[0].description as pm FROM DAILY_14_TOTAL LIMIT 1 Define the pipeline spec by populating all of the parameter values: pachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.7.x/src/templates/sql_ingest_cron.jsonnet \\ --arg name=mysnowflakeingest \\ --arg url=&#34;snowflake://username@VCNYTW-MH64356/SNOWFLAKE_SAMPLE_DATA/WEATHER?warehouse=COMPUTE_WH&#34; \\ --arg query=&#34;select T, V:city.name, V:data[0].weather[0].description as morning, V:data[12].weather[0].description as pm FROM DAILY_14_TOTAL LIMIT 1&#34; \\ --arg hasHeader=true \\ --arg cronSpec=&#34;@every 30s&#34; \\ --arg secretName=&#34;snowflakeSecret&#34; \\ --arg format=json Run the command. How Does This Work? # SQL Ingest&rsquo;s Jsonnet pipeline spec, sql_ingest_cron.jsonnet, creates all of the following:\n1 Input Data Repo: Used to store timestamp files at the cronSpec&rsquo;s set interval rate (--arg cronSpec=&quot;pullInterval&quot; \\) to trigger the pipeline. 1 Cron Pipeline: Houses the spec details that define the input type and settings and data transformation. 1 Output Repo: Used to store the data transformed by the cron pipeline; set by the pipeline spec&rsquo;s pipeline.name attribute, which you can define through the Jsonnet parameter --arg name=outputRepoName \\. 1 Output File: Used to save the query results (JSON or CSV) and potentially be used as input for a following pipeline. In the default Jsonnet template, the file generated is obtainable from the output repo, outputRepoName@master:/0000. The filename is hardcoded, however you could paramaterize this as well using a custom Jsonnet pipeline spec and passing --arg outputFile='0000'. The file&rsquo;s contents are the result of the query(--arg query=&quot;query&quot;) being ran against the database--arg url=&quot;connectionStringToDdatabase&quot; ; both are defined in the transform.cmd attribute.\nAbout SQL Ingest Pipeline Specs # To create an SQL Ingest Jsonnet Pipeline spec, you must have a .jsonnet file and several parameters:\npachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.7.x/src/templates/sql_ingest_cron.jsonnet \\ --arg name=&lt;pipelineName&gt; \\ --arg url=&#34;&lt;connectionStringToDdatabase&gt;&#34; \\ --arg query=&#34;&lt;query&gt;&#34; \\ --arg hasHeader=&lt;boolean&gt; \\ --arg cronSpec=&#34;&lt;pullInterval&gt;&#34; \\ --arg secretName=&#34;&lt;secretName&gt;&#34; \\ --arg format=&lt;CsvOrJson&gt; The name of each pipeline (and their related input/output repos) are derived from the name parameter (--arg name=&lt;pipelineName&gt;). Parameters # Parameter Description name The name of output repo where query results will materialize. url The connection string to the database. query The SQL query to be run against the connected database. hasHeader Adds a header to your CSV file if set to true. Ignored if format=&quot;json&quot; (JSON files always display (header,value) pairs for each returned row). Defaults to false. HPE ML Data Management creates the header after each element of the comma separated list of your SELECT clause. For example country.country_name_eng will have country.country_name_eng as header while country.country_name_eng as country_name will have country_name. cronSpec How often to run the query. For example &quot;@every 60s&quot;. format The type of your output file containing the results of your query (either json or csv). secretName The Kubernetes secret name that contains the password to the database. outputFile The name of the file created by your pipeline and stored in your output repo; default 0000 URL Parameter Details # &lt;protocol&gt;://&lt;username&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?&lt;param1&gt;=&lt;value1&gt;&amp;&lt;param2&gt;=&lt;value2&gt; Passwords are not included in the URL; they are retrieved from a secret. The additional parameters after ? are optional and needed on a case-by-case bases (for example, Snowflake). Parameter Description protocol The name of the database protocol. As of today, we support: - postgres and postgresql : connect to Postgresql or compatible (for example Redshift).\n- mysql : connect to MySQL or compatible (for example MariaDB). - snowflake : connect to Snowflake. username The user used to access the database. host The hostname of your database instance. port The port number your instance is listening on. database The name of the database to connect to. Snowflake # HPE ML Data Management supports two connection URL patterns to query Snowflake:\nsnowflake://username@&lt;account_identifier&gt;/&lt;db_name&gt;/&lt;schema_name&gt;?warehouse=&lt;warehouse_name&gt; snowflake://username@hostname:port/&lt;db_name&gt;/&lt;schema_name&gt;?account=&lt;account_identifier&gt;&amp;warehouse=&lt;warehouse_name&gt; The account_identifier takes one of the following forms for most URLs:\nOption 1 - Account Name:organization_name-account_name. Option 2 - Account Locator: account_locator.region.cloud. Formats &amp; SQL Data Types # The following comments on formatting reflect the state of this release and are subject to change.\nFormats # Numeric # All numeric values are converted into strings in your CSV and JSON.\nDatabase CSV JSON 12345 12345 &ldquo;12345&rdquo; 123.45 123.45 &ldquo;123.45&rdquo; ‚ö†Ô∏è Note that infinite (Inf) and not a number (NaN) values will also be stored as strings in JSON files. Use this format #.# for all decimals that you plan to egress back to a database. Date/Timestamps # Type Database CSV JSON Date 2022-05-09 2022-05-09T00:00:00 &ldquo;2022-05-09T00:00:00&rdquo; Timestamp ntz 2022-05-09 16:43:00 2022-05-09T16:43:00 &ldquo;2022-05-09T16:43:00&rdquo; Timestamp tz 2022-05-09 16:43:00-05:00 2022-05-09T16:43:00-05:00 &ldquo;2022-05-09T16:43:00-05:00&rdquo; Strings # Database CSV &ldquo;null&rdquo; null `&quot;&quot;` &quot;&quot;&quot;&quot;&quot;&quot; &quot;&quot; &quot;&quot; nil &quot;my string&quot; &ldquo;&ldquo;&ldquo;my string&rdquo;&rdquo;&rdquo; &ldquo;this will be enclosed in quotes because it has a ,&rdquo; &ldquo;this will be enclosed in quotes because it has a ,&rdquo; üí° When parsing your CSVs in your user code, remember to escape &quot; with &quot;&quot;.\nSupported Data Types # Some of the Data Types listed in this section are specific to a particular database.\nDates/Timestamps Varchars Numerics Booleans DATE TIME\nTIMESTAMP\nTIMESTAMP_LTZ\nTIMESTAMP_NTZ\nTIMESTAMP_TZ\nTIMESTAMPTZ\nTIMESTAMP WITH TIME ZONE\nTIMESTAMP WITHOUT TIME ZONE VARCHAR\nTEXT\nCHARACTER VARYING SMALLINT\nINT2\nINTEGER\nINT\nINT4\nBIGINT\nINT8\nFLOAT\nFLOAT4\nFLOAT8\nREAL\nDOUBLE PRECISION\nNUMERIC\nDECIMAL\nNUMBER BOOL\nBOOLEAN ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "sql",
        "ingest",
        "data-operations"
      ],
      "id": "6e91842f59b9bf8df7667be085067a04"
    },
    {
      "title": "Time-Windowed Data",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Prepare Data",
      "description": "Learn how to process historical data using time windows such as last 24 hours or the past 7 days.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/prepare-data/time-windows/",
      "relURI": "/latest/prepare-data/time-windows/",
      "body": " üìñ Before you read this section, make sure that you understand the concepts described in the following sections:\nDatum Distributed Computing Developer Workflow If you are analyzing data that is changing over time, you might need to analyze historical data. For example, you might need to examine the last two weeks of data, January&rsquo;s data, or some other moving or static time window of data.\nHPE ML Data Management provides the following approaches to this task:\nFixed time windows - for rigid, fixed time windows, such as months (Jan, Feb, and so on) or days‚Äî01-01-17, 01-02-17, and so on.\nMoving time windows\nfor rolling time windows of data, such as three-day windows or two-week windows. Fixed Time Windows # Datum is the basic unit of data partitioning in HPE ML Data Management. The glob pattern property in the pipeline specification defines a datum. When you analyze data within fixed time windows, such as the data that corresponds to fixed calendar dates, HPE ML Data Management recommends that you organize your data repositories so that each of the time windows that you plan to analyze corresponds to a separate file or directory in your repository, and therefore, HPE ML Data Management processes it as a separate datum.\nOrganizing your repository as described above, enables you to do the following:\nAnalyze each time window in parallel. Only re-process data within a time window when that data, or a corresponding data pipeline, changes. For example, if you have monthly time windows of sales data stored in JSON format that needs to be analyzed, you can create a sales data repository with the following data:\nsales ‚îú‚îÄ‚îÄ January | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îî‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ February | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îî‚îÄ‚îÄ ... ‚îî‚îÄ‚îÄ March ‚îú‚îÄ‚îÄ 01-01-17.json ‚îú‚îÄ‚îÄ 01-02-17.json ‚îî‚îÄ‚îÄ ... When you run a pipeline with sales as an input repository and a glob pattern of /*, HPE ML Data Management processes each month&rsquo;s worth of sales data in parallel if workers are available. When you add new data into a subset of the months or add data into a new month, for example, May, HPE ML Data Management processes only these updated datums.\nMore generally, this structure enables you to create the following types of pipelines:\nPipelines that aggregate or otherwise process daily data on a monthly basis by using the /* glob pattern. Pipelines that only analyze a particular month&rsquo;s data by using a /subdir/* or /subdir/ glob pattern. For example, /January/* or /January/. Pipelines that process data on daily by using the /*/* glob pattern. Any combination of the above. Moving Time Windows # In some cases, you need to run analyses for moving or rolling time windows that do not correspond to certain calendar months or days. For example, you might need to analyze the last three days of data, the three days of data before that, or similar. In other words, you need to run an analysis for every rolling length of time.\nFor rolling or moving time windows, there are a couple of recommended patterns:\nBin your data in repository folders for each of the moving time windows.\nMaintain a time-windowed set of data that corresponds to the latest of the moving time windows.\nBin Data into Moving Time Windows # In this method of processing rolling time windows, you create the following two-pipeline DAGs to analyze time windows efficiently:\nPipeline Description Pipeline 1 Reads in data, determines to which bins the data corresponds, and writes the data into those bins. Pipeline 2 Read in and analyze the binned data. By splitting this analysis into two pipelines, you can benefit from using parallelism at the file level. In other words, Pipeline 1 can be easily parallelized for each file, and Pipeline 2 can be parallelized per bin. This structure enables easy pipeline scaling as the number of files increases.\nFor example, you have three-day moving time windows, and you want to analyze three-day moving windows of sales data. In the first repo, called sales, you commit data for the first day of sales:\nsales ‚îî‚îÄ‚îÄ 01-01-17.json In the first pipeline, you specify to bin this data into a directory that corresponds to the first rolling time window from 01-01-17 to 01-03-17:\nbinned_sales ‚îî‚îÄ‚îÄ 01-01-17_to_01-03-17 ‚îî‚îÄ‚îÄ 01-01-17.json When the next day&rsquo;s worth of sales is committed, that data lands in the sales repository:\nsales ‚îú‚îÄ‚îÄ 01-01-17.json ‚îî‚îÄ‚îÄ 01-02-17.json Then, the first pipeline executes again to bin the 01-02-17 data into relevant bins. In this case, the data is placed in the previously created bin named 01-01-17 to 01-03-17. However, the data also goes to the bin that stores the data that is received starting on 01-02-17:\nbinned_sales ‚îú‚îÄ‚îÄ 01-01-17_to_01-03-17 | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îî‚îÄ‚îÄ 01-02-17.json ‚îî‚îÄ‚îÄ 01-02-17_to_01-04-17 ‚îî‚îÄ‚îÄ 01-02-17.json As more and more daily data is added, your repository structure starting to looks as follows:\nbinned_sales ‚îú‚îÄ‚îÄ 01-01-17_to_01-03-17 | ‚îú‚îÄ‚îÄ 01-01-17.json | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îî‚îÄ‚îÄ 01-03-17.json ‚îú‚îÄ‚îÄ 01-02-17_to_01-04-17 | ‚îú‚îÄ‚îÄ 01-02-17.json | ‚îú‚îÄ‚îÄ 01-03-17.json | ‚îî‚îÄ‚îÄ 01-04-17.json ‚îú‚îÄ‚îÄ 01-03-17_to_01-05-17 | ‚îú‚îÄ‚îÄ 01-03-17.json | ‚îú‚îÄ‚îÄ 01-04-17.json | ‚îî‚îÄ‚îÄ 01-05-17.json ‚îî‚îÄ‚îÄ ... The following diagram describes how data accumulates in the repository over time:\nYour second pipeline can then process these bins in parallel according to the glob pattern of /* or as described further. Both pipelines can be easily parallelized.\nIn the above directory structure, it might seem that data is duplicated. However, under the hood, HPE ML Data Management deduplicates all of these files and maintains a space-efficient representation of your data. The binning of the data is merely a structural re-arrangement to enable you to process these types of moving time windows.\nIt might also seem as if HPE ML Data Management performs unnecessary data transfers over the network to bin files. However, HPE ML Data Management ensures that these data operations do not require transferring data over the network.\nMaintaining a Single Time-Windowed Data Set # The advantage of the binning pattern above is that any of the moving time windows are available for processing. They can be compared, aggregated, and combined in any way, and any results or aggregations are kept in sync with updates to the bins. However, you do need to create a process to maintain the binning directory structure.\nThere is another pattern for moving time windows that avoids the binning of the above approach and maintains an up-to-date version of a moving time-windowed data set. This approach involves the creation of the following pipelines:\nPipeline Description Pipeline 1 Reads in data, determines which files belong in your moving time window, and writes the relevant files into an updated\nversion of the moving time-windowed data set. Pipeline 2 Reads in and analyzes the moving time-windowed data set. For example, you have three-day moving time windows, and you want to analyze three-day moving windows of sales data. The input data is stored in the sales repository:\nsales ‚îú‚îÄ‚îÄ 01-01-17.json ‚îú‚îÄ‚îÄ 01-02-17.json ‚îú‚îÄ‚îÄ 01-03-17.json ‚îî‚îÄ‚îÄ 01-04-17.json When the January 4th file, 01-04-17.json, is committed, the first pipeline pulls out the last three days of data and arranges it in the following order:\nlast_three_days ‚îú‚îÄ‚îÄ 01-02-17.json ‚îú‚îÄ‚îÄ 01-03-17.json ‚îî‚îÄ‚îÄ 01-04-17.json When the January 5th file, 01-05-17.json, is committed into the sales repository:\nsales ‚îú‚îÄ‚îÄ 01-01-17.json ‚îú‚îÄ‚îÄ 01-02-17.json ‚îú‚îÄ‚îÄ 01-03-17.json ‚îú‚îÄ‚îÄ 01-04-17.json ‚îî‚îÄ‚îÄ 01-05-17.json the first pipeline updates the moving window:\nlast_three_days ‚îú‚îÄ‚îÄ 01-03-17.json ‚îú‚îÄ‚îÄ 01-04-17.json ‚îî‚îÄ‚îÄ 01-05-17.json The analysis that you need to run on the moving windowed dataset in moving_sales_window can use the / or /* glob pattern, depending on whether you need to process all of the time-windowed files together or if they can be processed in parallel.\n‚ö†Ô∏è When you create this type of moving time-windowed data set, the concept of now or today is relative. You must define the time based on your use case. For example, by configuring to use UTC. Do not use functions such as time.now() to determine the current time. The actual time when this pipeline runs might vary.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "data-operations"
      ],
      "id": "169b25da89c16785cf9004cd60d57241"
    },
    {
      "title": "Transactions",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Prepare Data",
      "description": "Learn how to use transactions to create collections of PachCTL commands that are executed concurrently.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/prepare-data/transactions/",
      "relURI": "/latest/prepare-data/transactions/",
      "body": " ‚ÑπÔ∏è TL;DR:Use transactions to run multiple HPE ML Data Management commands simultaneously in one job run.\nA transaction is a HPE ML Data Management operation that enables you to create a collection of HPE ML Data Management commands and execute them concurrently. Regular HPE ML Data Management operations, that are not in a transaction, are executed one after another. However, when you need to run multiple commands at the same time, you can use transactions. This functionality is useful in particular for pipelines with multiple inputs. If you need to update two or more input repos, you might not want pipeline jobs for each state change. You can issue a transaction to start commits in each of the input repos, which puts them both in the same global commit, creating a single downstream commit in the pipeline repo. After the transaction, you can put files and finish the commits at will, and the pipeline job will run once all the input commits have been finished.\nStart and Finish Transaction Demarcations # A transaction demarcation initializes some transactional behavior before the demarcated area begins, then ends that transactional behavior when the demarcated area ends. You should see those demarcations as a declaration of the group of commands that will be treated together as a single coherent operation.\nTo start a transaction demarcation, run the following command:\npachctl start transaction System Response:\nStarted new transaction: 7a81eab5e6c6430aa5c01deb06852ca5 This command generates a transaction object in the cluster and saves its ID in the local HPE ML Data Management configuration file. By default, this file is stored at ~/.pachyderm/config.json.\nExample # { &#34;user_id&#34;: &#34;b4fe4317-be21-4836-824f-6661c68b8fba&#34;, &#34;v2&#34;: { &#34;active_context&#34;: &#34;local-2&#34;, &#34;contexts&#34;: { &#34;default&#34;: {}, &#34;local-2&#34;: { &#34;source&#34;: 3, &#34;active_transaction&#34;: &#34;7a81eab5e6c6430aa5c01deb06852ca5&#34;, &#34;cluster_name&#34;: &#34;minikube&#34;, &#34;auth_info&#34;: &#34;minikube&#34;, &#34;namespace&#34;: &#34;default&#34; }, After you start a transaction demarcation, you can add supported commands (i.e., transactional commands), such as pachctl start commit, pachctl create branch &hellip;, to the transaction.\nAll commands that are performed in a transaction are queued up and not executed against the actual cluster until you finish the transaction. When you finish the transaction, all queued command are executed atomically.\nTo finish a transaction, run:\npachctl finish transaction System Response:\nCompleted transaction with 1 requests: 7a81eab5e6c6430aa5c01deb06852ca5 üí° As soon as a commit is started (whether through start commit or put file without an open commit, or finishing a transaction that contains a start commit), a new global commit as well as a global job is created. All open commits are in a started state, each of the pipeline jobs created is running, and the workers waiting for the commit(s) to be closed to process the data. In other words, your changes will only be applied when you close the commits.\nIn the case of a transaction, the workers will wait until all of the input commits are finished to process them in one batch. All of those commits and jobs will be part of the same global commit/job and share the same globalID (Transaction ID). Without a transaction, each commit would trigger its own separate job.\nWe have used the inner join pipeline in our joins example to illustrate the difference between no transaction and the use a transaction, all other things being equal. Make sure to follow the example README if you want to run those pachctl commands yourself.\n‚ÑπÔ∏è Note that in the case with the transaction, the put file and following finish commit are happening after the finish transaction instruction.\nYou must finish your transaction before putting files in the corresponding repo for the data to be part of the same batch. Running a &lsquo;put file&rsquo; before closing the transaction would result in a commit being created independently from the transaction itself and a job to run on that commit.\nSupported Operations # While there is a transaction object in the HPE ML Data Management configuration file, all supported API requests append the request to the transaction instead of running directly. These supported commands include:\ncreate repo delete repo update repo start commit finish commit squash commit create branch delete branch create pipeline update pipeline edit pipeline Each time you add a command to a transaction, HPE ML Data Management validates the transaction against the current state of the cluster metadata and obtains any return values, which is important for such commands as start commit. If validation fails for any reason, HPE ML Data Management does not add the operation to the transaction. If the transaction has been invalidated by changing the cluster state, you must delete the transaction and start over, taking into account the new state of the cluster. From a command-line perspective, these commands work identically within a transaction as without. The only differences are that you do not apply your changes until you run finish transaction, and HPE ML Data Management logs a message to stderr to indicate that the command was placed in a transaction rather than run directly.\nOther Transaction Commands # Other supported commands for transactions include:\nCommand Description pachctl list transaction List all unfinished transactions available in the HPE ML Data Management cluster. pachctl stop transaction Remove the currently active transaction from the local HPE ML Data Management config file. The transaction remains in the HPE ML Data Management cluster and can be resumed later. pachctl resume transaction Set an already-existing transaction as the active transaction in the local HPE ML Data Management config file. pachctl delete transaction Deletes a transaction from the HPE ML Data Management cluster. pachctl inspect transaction Provides detailed information about an existing transaction, including which operations it will perform. By default, displays information about the current transaction. If you specify a transaction ID, displays information about the corresponding transaction. Multiple Opened Transactions # Some systems have a notion of nested transactions. That is when you open transactions within an already opened transaction. In such systems, the operations added to the subsequent transactions are not executed until all the nested transactions and the main transaction are finished.\nHPE ML Data Management does not support such behavior. Instead, when you open a transaction, the transaction ID is written to the HPE ML Data Management configuration file. If you begin another transaction while the first one is open, HPE ML Data Management returns an error.\nEvery time you add a command to a transaction, HPE ML Data Management creates a blueprint of the commit and verifies that the command is valid. However, one transaction can invalidate another. In this case, a transaction that is closed first takes precedence over the other. For example, if two transactions create a repository with the same name, the one that is executed first results in the creation of the repository, and the other results in error.\nUse Cases # HPE ML Data Management users implement transactions to their own workflows finding unique ways to benefit from this feature, whether it is a small research team or an enterprise-grade machine learning workflow.\nBelow are examples of the most commonly employed ways of using transactions.\nCommit to Separate Repositories Simultaneously # For example, you have a HPE ML Data Management pipeline with two input repositories. One repository includes training data and the other parameters for your machine learning pipeline. If you need to run specific data against specific parameters, you need to run your pipeline against specific commits in both repositories. To achieve this, you need to commit to these repositories simultaneously.\nIf you use a regular HPE ML Data Management workflow, the data is uploaded sequentially, each time triggering a separate job instead of one job with both commits of new data. One put file operation commits changes to the data repository and the other updates the parameters repository. The following animation shows the standard HPE ML Data Management workflow without a transaction:\nIn HPE ML Data Management, a pipeline starts as soon as a new commit lands in a repository. In the diagram above, as soon as commit 1 is added to the data repository, HPE ML Data Management runs a job for commit 1 and commit 0 in the parameters repository. You can also see that HPE ML Data Management runs the second job and processes commit 1 from the data repository with the commit 1 in the parameters repository. In some cases, this is perfectly acceptable solution. But if your job takes many hours and you are only interested in the result of the pipeline run with commit 1 from both repositories, this approach does not work.\nWith transactions, you can ensure that only one job triggers with both the new data and parameters. The following animation demonstrates how transactions work:\nThe transaction ensures that a single job runs for the two commits that were started within the transaction. While HPE ML Data Management supports some workflows where you can get the same effect by having both data and parameters in the same repo, often separating them and using transactions is much more efficient for organizational and performance reasons.\nSwitching from Staging to Master Simultaneously # If you are using deferred processing in your repositories because you want to commit your changes frequently without triggering jobs every time, then transactions can help you manage deferred processing with multiple inputs. You commit your changes to the staging branch and when needed, switch the HEAD of your master branch to a commit in the staging branch. To do this simultaneously, you can use transactions.\nFor example, you have two repositories data and parameters, both of which have a master and staging branch. You commit your changes to the staging branch while your pipeline is subscribed to the master branch. To switch to these branches simultaneously, you can use transactions like this:\npachctl start transaction System Response:\nStarted new transaction: 0d6f0bc337a0493696e382034a2a2055 pachctl pachctl create branch data@master --head staging Added to transaction: 0d6f0bc337a0493696e382034a2a2055 pachctl create branch parameters@master --head staging Added to transaction: 0d6f0bc337a0493696e382034a2a2055 pachctl finish transaction Completed transaction with 2 requests: 0d6f0bc337a0493696e382034a2a2055 When you finish the transaction, both repositories switch to the master branch at the same time which triggers one job to process those commits together.\nUpdating Multiple Pipelines Simultaneously # If you want to change logic or intermediate data formats in your DAG, you may need to change multiple pipelines. Performing these changes together in a transaction can avoid creating jobs with mismatched pipeline versions and potentially wasting work.\nTo get a better understanding of how transactions work in practice, try Use Transactions with Hyperparameter Tuning.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "data-operations"
      ],
      "id": "bf263a88be574acc58d64de5e01a8bed"
    },
    {
      "title": "Build Pipelines & DAGs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "Build pipelines & DAGs for every use case.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/",
      "relURI": "/latest/build-dags/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "2f210ad8da99579a08889f52bcd34e1a"
    },
    {
      "title": "Pipeline Specification (PPS)",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Build Pipelines & DAGs",
      "description": "Learn about the different attributes of a pipeline spec.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/",
      "relURI": "/latest/build-dags/pipeline-spec/",
      "body": "This document discusses each of the fields present in a pipeline specification.\nBefore You Start # HPE ML Data Management&rsquo;s pipeline specifications can be written in JSON or YAML. HPE ML Data Management uses its json parser if the first character is {. A pipeline specification file can contain multiple pipeline declarations at once. Minimal Spec # Generally speaking, the only attributes that are strictly required for all scenarios are pipeline.name and transform. Beyond those, other attributes are conditionally required based on your pipeline&rsquo;s use case. The following are a few examples of common use cases along with their minimally required attributes.\nUse Case: Cron Egress (DB) Egress (Storage) Input Service Spout S3 Full { &#34;pipeline&#34;: { &#34;name&#34;: &#34;wordcount&#34;, &#34;project&#34;: { name: &#34;projectName&#34; } }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;cron&#34;: { { &#34;name&#34;: string, &#34;spec&#34;: string, &#34;repo&#34;: string, &#34;start&#34;: time, &#34;overwrite&#34;: bool } } } } { &#34;pipeline&#34;: { &#34;name&#34;: &#34;wordcount&#34;, &#34;project&#34;: { name: &#34;projectName&#34; } }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;egress&#34;: { &#34;sql_database&#34;: { &#34;url&#34;: string, &#34;file_format&#34;: { &#34;type&#34;: string, &#34;columns&#34;: [string] }, &#34;secret&#34;: { &#34;name&#34;: string, &#34;key&#34;: &#34;PACHYDERM_SQL_PASSWORD&#34; } } }, } { &#34;pipeline&#34;: { &#34;name&#34;: &#34;wordcount&#34;, &#34;project&#34;: { name: &#34;projectName&#34; } }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;egress&#34;: { &#34;URL&#34;: &#34;s3://bucket/dir&#34; }, } { &#34;pipeline&#34;: { &#34;name&#34;: &#34;wordcount&#34;, &#34;project&#34;: { name: &#34;projectName&#34; } }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } } } { &#34;pipeline&#34;: { &#34;name&#34;: &#34;wordcount&#34;, &#34;project&#34;: { name: &#34;projectName&#34; } }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;service&#34;: { &#34;internal_port&#34;: int, &#34;external_port&#34;: int }, } { &#34;pipeline&#34;: { &#34;name&#34;: &#34;wordcount&#34;, &#34;project&#34;: { name: &#34;projectName&#34; } }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;spout&#34;: { }, } { &#34;pipeline&#34;: { &#34;name&#34;: &#34;wordcount&#34;, &#34;project&#34;: { name: &#34;projectName&#34; } }, &#34;transform&#34;: { &#34;image&#34;: &#34;wordcount-image&#34;, &#34;cmd&#34;: [&#34;/binary&#34;, &#34;/pfs/data&#34;, &#34;/pfs/out&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;data&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;s3_out&#34;: true, } { &#34;pipeline&#34;: { &#34;name&#34;: string, &#34;project&#34;: { name: &#34;projectName&#34; }, }, &#34;description&#34;: string, &#34;metadata&#34;: { &#34;annotations&#34;: { &#34;annotation&#34;: string }, &#34;labels&#34;: { &#34;label&#34;: string } }, &#34;tf_job&#34;: { &#34;tf_job&#34;: string, }, &#34;transform&#34;: { &#34;image&#34;: string, &#34;cmd&#34;: [ string ], &#34;err_cmd&#34;: [ string ], &#34;env&#34;: { string: string }, &#34;secrets&#34;: [ { &#34;name&#34;: string, &#34;mount_path&#34;: string }, { &#34;name&#34;: string, &#34;env_var&#34;: string, &#34;key&#34;: string } ], &#34;image_pull_secrets&#34;: [ string ], &#34;stdin&#34;: [ string ], &#34;err_stdin&#34;: [ string ], &#34;accept_return_code&#34;: [ int ], &#34;debug&#34;: bool, &#34;user&#34;: string, &#34;working_dir&#34;: string, &#34;dockerfile&#34;: string, &#34;memory_volume&#34;: bool, }, &#34;parallelism_spec&#34;: { &#34;constant&#34;: int }, &#34;egress&#34;: { // Egress to an object store &#34;URL&#34;: &#34;s3://bucket/dir&#34; // Egress to a database &#34;sql_database&#34;: { &#34;url&#34;: string, &#34;file_format&#34;: { &#34;type&#34;: string, &#34;columns&#34;: [string] }, &#34;secret&#34;: { &#34;name&#34;: string, &#34;key&#34;: &#34;PACHYDERM_SQL_PASSWORD&#34; } } }, &#34;update&#34;: bool, &#34;output_branch&#34;: string, [ { &#34;worker_id&#34;: string, &#34;job_id&#34;: string, &#34;datum_status&#34; : { &#34;started&#34;: timestamp, &#34;data&#34;: [] } } ], &#34;s3_out&#34;: bool, &#34;resource_requests&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, &#34;resource_limits&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, &#34;sidecar_resource_limits&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, &#34;input&#34;: { &lt;&#34;pfs&#34;, &#34;cross&#34;, &#34;union&#34;, &#34;join&#34;, &#34;group&#34; or &#34;cron&#34; see below&gt; }, &#34;description&#34;: string, &#34;reprocess&#34;: bool, &#34;service&#34;: { &#34;internal_port&#34;: int, &#34;external_port&#34;: int }, &#34;spout&#34;: { \\\\ Optionally, you can combine a spout with a service: &#34;service&#34;: { &#34;internal_port&#34;: int, &#34;external_port&#34;: int } }, &#34;datum_set_spec&#34;: { &#34;number&#34;: int, &#34;size_bytes&#34;: int, &#34;per_worker&#34;: int, } &#34;datum_timeout&#34;: string, &#34;job_timeout&#34;: string, &#34;salt&#34;: string, &#34;datum_tries&#34;: int, &#34;scheduling_spec&#34;: { &#34;node_selector&#34;: {string: string}, &#34;priority_class_name&#34;: string }, &#34;pod_spec&#34;: string, &#34;pod_patch&#34;: string, &#34;spec_commit&#34;: { &#34;option&#34;: false, &#34;branch&#34;: { &#34;option&#34;: false, &#34;repo&#34;: { &#34;option&#34;: false, &#34;name&#34;: string, &#34;type&#34;: string, &#34;project&#34;:{ &#34;option&#34;: false, &#34;name&#34;: string, }, }, &#34;name&#34;: string }, &#34;id&#34;: string, } &#34;metadata&#34;: { }, &#34;reprocess_spec&#34;: string, &#34;autoscaling&#34;: bool } ------------------------------------ &#34;pfs&#34; input ------------------------------------ &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;repo_type&#34;:string, &#34;branch&#34;: string, &#34;commit&#34;:string, &#34;glob&#34;: string, &#34;join_on&#34;:string, &#34;outer_join&#34;: bool, &#34;group_by&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool, &#34;trigger&#34;: { &#34;branch&#34;: string, &#34;all&#34;: bool, &#34;cron_spec&#34;: string, }, } ------------------------------------ &#34;cross&#34; or &#34;union&#34; input ------------------------------------ &#34;cross&#34; or &#34;union&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ... ] ------------------------------------ &#34;join&#34; input ------------------------------------ &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;join_on&#34;: string, &#34;outer_join&#34;: bool, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;join_on&#34;: string, &#34;outer_join&#34;: bool, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ] ------------------------------------ &#34;group&#34; input ------------------------------------ &#34;group&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;group_by&#34;: string, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;group_by&#34;: string, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ] ------------------------------------ &#34;cron&#34; input ------------------------------------ &#34;cron&#34;: { &#34;name&#34;: string, &#34;spec&#34;: string, &#34;repo&#34;: string, &#34;start&#34;: time, &#34;overwrite&#34;: bool } ‚ÑπÔ∏è For a single-page view of all PPS options, go to the PPS series page.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipeline",
        "pipelines",
        "pipeline specification",
        "pps",
        "pipeline spec"
      ],
      "id": "6c207d2f3ac2c28e2737bad2f2fbe05e"
    },
    {
      "title": "Autoscaling PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Enable autoscaling of the worker pool based on datums in queue.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/autoscaling/",
      "relURI": "/latest/build-dags/pipeline-spec/autoscaling/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;autoscaling&#34;: false, ... } Behavior # The autoscaling attribute in a HPE ML Data Management Pipeline Spec is used to specify whether the pipeline should automatically scale up or down based on the processing load.\nIf the autoscaling attribute is set to true, HPE ML Data Management will monitor the processing load of the pipeline, and automatically scale up or down the number of worker nodes as needed to keep up with the demand. This can help to ensure that the pipeline is always running at optimal efficiency, without wasting resources when the load is low.\nautocaling is set to false by default. The maximum number of workers is controlled by the parallelism_spec. A pipeline with no outstanding jobs will go into standby. A pipeline in a standby state consumes no resources. When to Use # You should consider using the autoscaling attribute in a HPE ML Data Management Pipeline Spec when you have a workload that has variable processing requirements or when the processing load of your pipeline is difficult to predict.\nExample scenarios:\nProcessing unpredictable workloads: If you have a workload that has variable processing requirements, it can be difficult to predict the number of worker nodes that will be needed to keep up with the demand. In this case, you could use the autoscaling attribute to automatically scale the number of worker nodes up or down based on the processing load.\nProcessing large datasets: If you have a pipeline that is processing a large dataset, it can be difficult to predict the processing requirements for the pipeline. In this case, you could use the autoscaling attribute to automatically scale the number of worker nodes based on the processing load, in order to keep up with the demand.\nHandling bursty workloads: If you have a workload that has periods of high demand followed by periods of low demand, it can be difficult to predict the processing requirements for the pipeline. In this case, you could use the autoscaling attribute to automatically scale the number of worker nodes up or down based on the processing load, in order to handle the bursty demand.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "cd69099dac9a464ef2e476413506b14e"
    },
    {
      "title": "Datum Set Spec PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Define how a pipeline should group its datums.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/datum-set-spec/",
      "relURI": "/latest/build-dags/pipeline-spec/datum-set-spec/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;datum_set_spec&#34;: { &#34;number&#34;: 0, &#34;size_bytes&#34;: 0, &#34;per_worker&#34;: 0, }, ... } Attributes # Attribute Description number The desired number of datums in each datum set. If specified, each datum set will contain the specified number of datums. If the total number of input datums is not evenly divisible by the number of datums per set, the last datum set may contain fewer datums than the others. size_bytes The desired target size of each datum set in bytes. If specified, HPE ML Data Management will attempt to create datum sets with the specified size, though the actual size may vary due to the size of the input files. per_worker The desired number of datum sets that each worker should process at a time. This field is similar to number, but specifies the number of sets per worker instead of the number of datums per set. Behavior # The datum_set_spec attribute in a HPE ML Data Management Pipeline Spec is used to control how the input data is partitioned into individual datum sets for processing. Datum sets are the unit of work that workers claim, and each worker can claim 1 or more datums. Once done processing, it commits a full datum set.\nnumber if nonzero, specifies that each datum set should contain number datums. Sets may contain fewer if the total number of datums don&rsquo;t divide evenly. If you lower the number to 1 it&rsquo;ll update after every datum,the cost is extra load on etcd which can slow other stuff down. Default is 0.\nsize_bytes , if nonzero, specifies a target size for each set of datums. Sets may be larger or smaller than size_bytes, but will usually be pretty close to size_bytes in size. Default is 0.\nper_worker, if nonzero, specifies how many datum sets should be created for each worker. It can&rsquo;t be set with number or size_bytes. Default is 0.\nWhen to Use # You should consider using the datum_set_spec attribute in your HPE ML Data Management pipeline when you are experiencing stragglers, which are situations where most of the workers are idle but a few are still processing jobs. This can happen when the work is not divided up in a balanced way, which can cause some workers to be overloaded with work while others are idle.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "f503851fcadcb2d0209399ae5a74e6e6"
    },
    {
      "title": "Datum Timeout PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Set the maximum execution time allowed for each datum.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/datum-timeout/",
      "relURI": "/latest/build-dags/pipeline-spec/datum-timeout/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;datum_timeout&#34;: string, ... } Behavior # The datum_timeout attribute in a HPE ML Data Management pipeline is used to specify the maximum amount of time that a worker is allowed to process a single datum in the pipeline.\nWhen a worker begins processing a datum, HPE ML Data Management starts a timer that tracks the elapsed time since the datum was first assigned to the worker. If the worker has not finished processing the datum before the datum_timeout period has elapsed, HPE ML Data Management will automatically mark the datum as failed and reassign it to another worker to retry. This helps to ensure that slow or problematic datums do not hold up the processing of the entire pipeline.\nOther considerations:\nNot set by default, allowing a datum to process for as long as needed. Takes precedence over the parallelism or number of datums; no single datum is allowed to exceed this value. The value must be a string that represents a time value, such as 1s, 5m, or 15h. When to Use # You should consider using the datum_timeout attribute in your HPE ML Data Management pipeline when you are processing large or complex datums that may take a long time to process, and you want to avoid having individual datums hold up the processing of the entire pipeline.\nFor example, if you are processing images or videos that are particularly large, or if your pipeline is doing complex machine learning or deep learning operations that can take a long time to run on individual datums, setting a reasonable datum_timeout can help ensure that your pipeline continues to make progress even if some datums are slow or problematic.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "447b2339589a89d66ac5932e4506daed"
    },
    {
      "title": "Datum Tries PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Define the number of job attempts to run on a datum when a failure occurs.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/datum-tries/",
      "relURI": "/latest/build-dags/pipeline-spec/datum-tries/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n&#34;datum_tries&#34;: int, Behavior # The datum_tries attribute in a HPE ML Data Management pipeline specifies the maximum number of times that Pachyderm will try to process a datum. When a datum fails to process, either because of an error in the processing logic or because it exceeds the datum_timeout value, HPE ML Data Management will automatically retry the datum until it is successful or the number of datum_tries has been reached.\nEach retry of a datum is treated as a new attempt, and the datum is added back to the job queue for processing. The retry process is transparent to the user and happens automatically within the HPE ML Data Management system.\nOther considerations:\ndatum_tries is set to 3 by default if unspecified. Setting to 1 attempts a datum once with no retries. If all tries have been exhausted and processing has not succeeded, the datum is marked as Failed. When to Use # You should consider setting a higher datum_tries count if your pipeline has a large number of datums that are prone to errors or timeouts, or if the datums you are working with have to be imported or fetched (via data ingress) from an external source.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "cc657dd16100a367a1205b82af07c6b8"
    },
    {
      "title": "Description PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Display meaningful information about your pipeline.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/description/",
      "relURI": "/latest/build-dags/pipeline-spec/description/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;description&#34;: string, ... } Behavior # description is displayed in your pipeline details when viewed from pachCTL or console.\nWhen to Use # It&rsquo;s recommended to always provide meaningful descriptions to your HPE ML Data Management resources.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "015864b54cb187fc6858d69735631c0c"
    },
    {
      "title": "Egress PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Push the results of a Pipeline to an external data store or an SQL Database.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/egress/",
      "relURI": "/latest/build-dags/pipeline-spec/egress/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;egress&#34;: { // Egress to an object store &#34;URL&#34;: &#34;s3://bucket/dir&#34; // Egress to a database &#34;sql_database&#34;: { &#34;url&#34;: string, &#34;file_format&#34;: { &#34;type&#34;: string, &#34;columns&#34;: [string] }, &#34;secret&#34;: { &#34;name&#34;: string, &#34;key&#34;: &#34;PACHYDERM_SQL_PASSWORD&#34; } }, }, ... } Attributes # Attribute Description URL The URL of the object store where the pipeline&rsquo;s output data should be written. sql_database An optional field that is used to specify how the pipeline should write output data to a SQL database. url The URL of the SQL database, in the format postgresql://user:password@host:port/database. file_format The file format of the output data, which can be specified as csv or tsv. This field also includes the column names that should be included in the output. secret The name and key of the Kubernetes secret that contains the password for the SQL database. Behavior # The egress field in a HPE ML Data Management Pipeline Spec is used to specify how the pipeline should write the output data. The egress field supports two types of outputs: writing to an object store and writing to a SQL database.\nData is pushed after the user code finishes running but before the job is marked as successful. For more information, see Egress Data to an object store or Egress Data to a database.\nThis is required if the pipeline needs to write output data to an external storage system.\nWhen to Use # You should use the egress field in a HPE ML Data Management Pipeline Spec when you need to write the output data from your pipeline to an external storage system, such as an object store or a SQL database.\nExample scenarios:\nLong-term data storage: If you need to store the output data from your pipeline for a long time, you can use the egress field to write the data to an object store, such as Amazon S3 or Google Cloud Storage.\nData sharing: If you need to share the output data from your pipeline with external users or systems, you can use the egress field to write the data to an object store that is accessible to those users or systems.\nAnalytics and reporting: If you need to perform further analytics or reporting on the output data from your pipeline, you can use the egress field to write the data to a SQL database that can be used for those purposes.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "ec1c39fa36da4c2008fe7197d78ebbd5"
    },
    {
      "title": "Input Cron PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Trigger pipelines based on time.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/input-cron/",
      "relURI": "/latest/build-dags/pipeline-spec/input-cron/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;input&#34;: { &#34;cron&#34;: { { &#34;name&#34;: string, &#34;spec&#34;: string, &#34;repo&#34;: string, &#34;start&#34;: time, &#34;overwrite&#34;: bool } } }, ... } Attributes # Attribute Required? Description name Yes The name of the cron job, which should be unique within the HPE ML Data Management cluster. spec Yes The cron schedule for the job, specified using the standard cron format or macros. See schedule macros for examples. HPE ML Data Management also supports non-standard schedules, such as &quot;@daily&quot;. repo No The name of the input repository that the cron job should read data from; default:&lt;pipeline-name&gt;_&lt;input-name&gt; start No Specifies the start time for the cron job. This is useful for running the job on a specific date in the future. If not specified, starts immediately. Specifying a time enables you to run on matching times from the past or skip times from the present and only start running on matching times in the future. Format the time value according to RFC3339. overwrite No Defines whether you want the timestamp file to be overwritten on each tick; defaults to simply writing new files on each tick. By default, when &quot;overwrite&quot; is disabled, ticks accumulate in the cron input repo. When &quot;overwrite&quot; is enabled, HPE ML Data Management erases the old ticks and adds new ticks with each commit. If you do not add any manual ticks or run pachctl run cron, only one tick file per commit (for the latest tick) is added to the input repo. Behavior # The input field in a HPE ML Data Management Pipeline Spec is used to specify the inputs to the pipeline, which are the HPE ML Data Management repositories that the pipeline should read data from. The input field can include both static and dynamic inputs.\nThe cron field within the input field is used to specify a dynamic input that is based on a cron schedule. This is useful for pipelines that need to process data on a regular schedule, such as daily or hourly.\nA repo is created for each cron input. When a Cron input triggers, pachd commits a single file, named by the current RFC3339 timestamp to the repo which contains the time which satisfied the spec.\nCallouts # Avoid using intervals faster than 1-5 minutes You can use never during development and manually trigger the pipeline If using jsonnet, you can pass arguments like: --arg cronSpec=&quot;@every 5m&quot; You cannot update a cron pipeline after it has been created; instead, you must delete the pipeline and build a new one. When to Use # You should use a cron input in a HPE ML Data Management Pipeline Spec when you need to process data on a regular schedule, such as hourly or daily. A cron input allows you to specify a schedule for the pipeline to run, and HPE ML Data Management will automatically trigger the pipeline at the specified times.\nExample scenarios:\nBatch processing: If you have a large volume of data that needs to be processed on a regular schedule, a cron input can be used to trigger the processing automatically, without the need for manual intervention.\nData aggregation: If you need to collect data from different sources and process it on a regular schedule, a cron input can be used to automate the data collection and processing.\nReport generation: If you need to generate reports on a regular schedule, a cron input can be used to trigger the report generation process automatically.\nExamples # Examples: Every 60s Daily With Overwrites SQL Ingest &#34;input&#34;: { &#34;cron&#34;: { &#34;name&#34;: &#34;tick&#34;, &#34;spec&#34;: &#34;@every 60s&#34; } } &#34;input&#34;: { &#34;cron&#34;: { &#34;name&#34;: &#34;tick&#34;, &#34;spec&#34;: &#34;@daily&#34;, &#34;overwrite&#34;: true } } pachctl update pipeline --jsonnet https://raw.githubusercontent.com/pachyderm/pachyderm/2.7.x/src/templates/sql_ingest_cron.jsonnet \\ --arg name=myingest \\ --arg url=&#34;mysql://root@mysql:3306/test_db&#34; \\ --arg query=&#34;SELECT * FROM test_data&#34; \\ --arg hasHeader=false \\ --arg cronSpec=&#34;@every 60s&#34; \\ --arg secretName=&#34;mysql-creds&#34; \\ --arg format=json ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "7288e0c3ede41b63cf085d69eb1d36d3"
    },
    {
      "title": "Input Cross PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Create a cross product of other inputs.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/input-cross/",
      "relURI": "/latest/build-dags/pipeline-spec/input-cross/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;input&#34;: { &#34;cross&#34;: [ { &#34;pfs&#34;: { &#34;project&#34;: string, &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;project&#34;: string, &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ... ]}, ... } Attributes # Attribute Description name The name of the PFS input that appears in the INPUT field when you run the pachctl list pipeline command. If an input name is not specified, it defaults to the name of the repo. repo Specifies the name of the HPE ML Data Management repository that contains the input data. branch The branch to watch for commits. If left blank, HPE ML Data Management sets this value to master. glob A wildcard pattern that defines how a dataset is broken up into datums for further processing. When you use a glob pattern in a group input, it creates a naming convention that HPE ML Data Management uses to group the files. lazy Controls how the data is exposed to jobs. The default is false which means the job eagerly downloads the data it needs to process and exposes it as normal files on disk. If lazy is set to true, data is exposed as named pipes instead, and no data is downloaded until the job opens the pipe and reads it. If the pipe is never opened, then no data is downloaded. empty_files Controls how files are exposed to jobs. If set to true, it causes files from this PFS to be presented as empty files. This is useful in shuffle pipelines where you want to read the names of files and reorganize them by using symlinks. s3 Indicates whether the input data is stored in an S3 object store. Behavior # input.cross is an array of inputs to cross. The inputs do not have to be pfs inputs. They can also be union and cross inputs.\nA cross input creates tuples of the datums in the inputs. In the example below, each input includes individual datums, such as if foo and bar were in the same repository with the glob pattern set to /*. Alternatively, each of these datums might have come from separate repositories with the glob pattern set to / and being the only file system objects in these repositories.\n| inputA | inputB | inputA ‚®Ø inputB | | ------ | ------ | --------------- | | foo | fizz | (foo, fizz) | | bar | buzz | (foo, buzz) | | | | (bar, fizz) | | | | (bar, buzz) | The cross inputs above do not take a name and maintain the names of the sub-inputs. In the example above, you would see files under /pfs/inputA/... and /pfs/inputB/....\nWhen to Use # You should use a cross input in a HPE ML Data Management Pipeline Spec when you need to perform operations on combinations of data from multiple HPE ML Data Management repositories. The cross input allows you to generate a set of combinations of files between two or more repositories, which can be used as the input to your pipeline.\nExample scenarios:\nData analysis: If you have data from multiple sources that you need to combine and analyze, a cross input can be used to generate a set of combinations of data that can be used as the input to your analysis.\nMachine learning: If you need to train a machine learning model on combinations of data from multiple sources, a cross input can be used to generate a set of combinations of data that can be used as the input to your model.\nReport generation: If you need to generate reports that combine data from multiple sources, a cross input can be used to generate a set of combinations of data that can be used as the input to your report generation process.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "9fd240b1a1cda0d1cc2128f4f7e4a4c3"
    },
    {
      "title": "Input Group PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Group files stored in one or multiple repos.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/input-group/",
      "relURI": "/latest/build-dags/pipeline-spec/input-group/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;input&#34;: { &#34;group&#34;: [ { &#34;pfs&#34;: { &#34;project&#34;: string, &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;group_by&#34;: string, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;project&#34;: string, &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;group_by&#34;: string, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ] }, ... } Attributes # Attribute Description name The name of the PFS input that appears in the INPUT field when you run the pachctl list pipeline command. If an input name is not specified, it defaults to the name of the repo. repo Specifies the name of the HPE ML Data Management repository that contains the input data. branch The branch to watch for commits. If left blank, HPE ML Data Management sets this value to master. glob A wildcard pattern that defines how a dataset is broken up into datums for further processing. When you use a glob pattern in a group input, it creates a naming convention that HPE ML Data Management uses to group the files. group_by A parameter that is used to group input files by a specific pattern. lazy Controls how the data is exposed to jobs. The default is false which means the job eagerly downloads the data it needs to process and exposes it as normal files on disk. If lazy is set to true, data is exposed as named pipes instead, and no data is downloaded until the job opens the pipe and reads it. If the pipe is never opened, then no data is downloaded. empty_files Controls how files are exposed to jobs. If set to true, it causes files from this PFS to be presented as empty files. This is useful in shuffle pipelines where you want to read the names of files and reorganize them by using symlinks. s3 Indicates whether the input data is stored in an S3 object store. Behavior # The group input in a HPE ML Data Management Pipeline Spec allows you to group input files by a specific pattern.\nTo use the group input, you specify one or more PFS inputs with a group_by parameter. This parameter specifies a pattern or field to use for grouping the input files. The resulting groups are then passed to your pipeline as a series of grouped datums, where each datum is a single group of files.\nYou can specify multiple group input fields in a HPE ML Data Management Pipeline Spec, each with their own group_by parameter. This allows you to group files by multiple fields or patterns, and pass each group to your pipeline as a separate datum.\nThe glob and group_by parameters must be configured.\nWhen to Use # You should consider using the group input in a HPE ML Data Management Pipeline Spec when you have large datasets with multiple files that you want to partition or group by a specific field or pattern. This can be useful in a variety of scenarios, such as when you need to perform complex data analysis on a large dataset, or when you need to group files by some attribute or characteristic in order to facilitate further processing.\nExample scenarios:\nPartitioning data by time: If you have a large dataset that spans a long period of time, you might want to partition it by day, week, or month in order to perform time-based analysis or modeling. In this case, you could use the group input field to group files by date or time, and then process each group separately.\nGrouping data by user or account: If you have a dataset that includes data from multiple users or accounts, you might want to group the data by user or account in order to perform user-based analysis or modeling. In this case, you could use the group input field to group files by user or account, and then process each group separately.\nPartitioning data by geography: If you have a dataset that includes data from multiple geographic regions, you might want to partition it by region in order to perform location-based analysis or modeling. In this case, you could use the group input field to group files by region, and then process each group separately.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "af87d1757cc3d8883e362d73c9b179bd"
    },
    {
      "title": "Input Join PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Join files that are stored in separate repositories.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/input-join/",
      "relURI": "/latest/build-dags/pipeline-spec/input-join/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;input&#34;: { &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;project&#34;: string, &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;join_on&#34;: string, &#34;outer_join&#34;: bool, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;project&#34;: string, &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;join_on&#34;: string, &#34;outer_join&#34;: bool, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ] }, ... } Behavior # A join input must have the glob and join_on parameters configured to work properly. A join can combine multiple PFS inputs. You can optionally add &quot;outer_join&quot;: true to your PFS input. In that case, you will alter the join&rsquo;s behavior from a default &ldquo;inner-join&rdquo; (creates a datum if there is a match only) to a &ldquo;outer-join&rdquo; (the repos marked as &quot;outer_join&quot;: true will see a datum even if there is no match). You can set 0 to many PFS input to &quot;outer_join&quot;: true within your join. Capture Groups # When you configure a join input (inner or outer), you must specify a glob pattern that includes a capture group. The capture group defines the specific string in the file path that is used to match files in other joined repos. Capture groups work analogously to the regex capture group. You define the capture group inside parenthesis. Capture groups are numbered from left to right and can also be nested within each other. Numbering for nested capture groups is based on their opening parenthesis.\nBelow you can find a few examples of applying a glob pattern with a capture group to a file path. For example, if you have the following file path:\n/foo/bar-123/ABC.txt The following glob patterns in a joint input create the following capture groups:\nRegular expression Capture groups /(*) foo /*/bar-(*) 123 /(*)/*/(??)*.txt Capture group 1: foo, capture group 2: AB. /*/(bar-(123))/* Capture group 1: bar-123, capture group 2: 123. Also, joins require you to specify a replacement group in the join_on parameter to define which capture groups you want to tryto match.\nFor example, $1 indicates that you want HPE ML Data Management to match based on capture group 1. Similarly, $2 matches the capture group 2. $1$2 means that it must match both capture groups 1 and 2.\nSee the full join input configuration in the pipeline specification.\nYou can test your glob pattern and capture groups by using the pachctl list datum -f &lt;your_pipeline_spec.json&gt; command.\nüí° The content of the capture group defined in the join_on parameter is available to your pipeline&rsquo;s code in an environment variable: PACH_DATUM_&lt;input.name&gt;_JOIN_ON.\nExamples # Inner Join # Per default, a join input has an inner-join behavior.\nFor example, you have two repositories. One with sensor readings and the other with parameters. The repositories have the following structures:\nreadings repo:\n‚îú‚îÄ‚îÄ ID1234 ‚îú‚îÄ‚îÄ file1.txt ‚îú‚îÄ‚îÄ file2.txt ‚îú‚îÄ‚îÄ file3.txt ‚îú‚îÄ‚îÄ file4.txt ‚îú‚îÄ‚îÄ file5.txt parameters repo:\n‚îú‚îÄ‚îÄ file1.txt ‚îú‚îÄ‚îÄ file2.txt ‚îú‚îÄ‚îÄ file3.txt ‚îú‚îÄ‚îÄ file4.txt ‚îú‚îÄ‚îÄ file5.txt ‚îú‚îÄ‚îÄ file6.txt ‚îú‚îÄ‚îÄ file7.txt ‚îú‚îÄ‚îÄ file8.txt HPE ML Data Management runs your code only on the pairs of files that match the glob pattern and capture groups.\nThe following example shows how you can use joins to group matching IDs:\n{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;joins&#34; }, &#34;input&#34;: { &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;readings&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;parameters&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/joins.py&#34;], &#34;image&#34;: &#34;joins-example&#34; } } The glob pattern for the readings repository, /*/(*).txt, indicates all matching files in the ID sub-directory. In the parameters repository, the glob pattern /(*).txt selects all the matching files in the root directory. All files with indices from 1 to 5 match. The files with indices from 6 to 8 do not match. Therefore, you only get five datums for this job.\nTo experiment further, see the full joins example.\nOuter Join # HPE ML Data Management also supports outer joins. Outer joins include everything an inner join does plus the files that didn&rsquo;t match anything. Inputs can be set to outer semantics independently. So while there isn&rsquo;t an explicit notion of &ldquo;left&rdquo; or &ldquo;right&rdquo; outer joins, you can still get those semantics, and even extend them to multiway joins.\nBuilding off the previous example, notice that there are 3 files in the parameters repo, file6.txt, file7.txt and file8.txt, which don&rsquo;t match any files in the readings repo. In an inner join, those files are omitted. If you still want to see the files without a match, you can use an outer join. The change to the pipeline spec is simple:\n{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;joins&#34; }, &#34;input&#34;: { &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;readings&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;parameters&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/(*).txt&#34;, &#34;join_on&#34;: &#34;$1&#34;, &#34;outer_join&#34;: true } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/joins.py&#34;], &#34;image&#34;: &#34;joins-example&#34; } } Your code will see the joined pairs that it saw before. In addition to those five datums, your code will also see three new ones: one for each of the files in parameters that didn&rsquo;t match. Note that this means that your code needs to handle (not crash) the case where input files are missing from /pfs/readings.\nTo experiment further, see the full join example.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "4412ffe879cb2f2f6d0cfa39d2f183a7"
    },
    {
      "title": "Input PFS PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Add data to an input repo.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/input-pfs/",
      "relURI": "/latest/build-dags/pipeline-spec/input-pfs/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;input&#34;: { &#34;pfs&#34;: { &#34;project&#34;: string, &#34;name&#34;: string, &#34;repo&#34;: string, &#34;repo_type&#34;:string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;join_on&#34;:string, &#34;outer_join&#34;: bool, &#34;group_by&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool, &#34;trigger&#34;: { &#34;branch&#34;: string, &#34;all&#34;: bool, &#34;cron_spec&#34;: string, }, } }, ... } Behavior # input.pfs.name is the name of the input. An input with the name XXX is visible under the path /pfs/XXX when a job runs. Input names must be unique if the inputs are crossed, but they may be duplicated between PFSInputs that are combined by using the union operator. This is because when PFSInputs are combined, you only ever see a datum from one input at a time. Overlapping the names of combined inputs allows you to write simpler code since you no longer need to consider which input directory a particular datum comes from. If an input&rsquo;s name is not specified, it defaults to the name of the repo. Therefore, if you have two crossed inputs from the same repo, you must give at least one of them a unique name.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "a28f553daeea0dd056e9ac1c1c2bf212"
    },
    {
      "title": "Input Union PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Create a union of pfs, cross, or other union inputs.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/input-union/",
      "relURI": "/latest/build-dags/pipeline-spec/input-union/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;input&#34;: { &#34;union&#34;: [ { &#34;pfs&#34;: { &#34;project&#34;: string, &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;project&#34;: string, &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ... ]}, ... } Behavior # input.union is an array of inputs to combine. The inputs do not have to be pfs inputs. They can also be union and cross inputs.\nUnion inputs take the union of other inputs. In the example below, each input includes individual datums, such as if foo and bar were in the same repository with the glob pattern set to /*. Alternatively, each of these datums might have come from separate repositories with the glob pattern set to / and being the only file system objects in these repositories.\n| inputA | inputB | inputA ‚à™ inputB | | ------ | ------ | --------------- | | foo | fizz | foo | | bar | buzz | fizz | | | | bar | | | | buzz | The union inputs do not take a name and maintain the names of the sub-inputs. In the example above, you would see files under /pfs/inputA/... or /pfs/inputB/..., but never both at the same time. When you write code to address this behavior, make sure that your code first determines which input directory is present. Starting with HPE ML Data Management 1.5.3, we recommend that you give your inputs the same Name. That way your code only needs to handle data being present in that directory. This only works if your code does not need to be aware of which of the underlying inputs the data comes from.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "0d13138b7676f7a553b771b4a26abdc8"
    },
    {
      "title": "Job Timeout PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Set the maximum execution time allowed for a job.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/job-timeout/",
      "relURI": "/latest/build-dags/pipeline-spec/job-timeout/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;job_timeout&#34;: string, ... } Behavior # Work that is not complete by set timeout is interrupted. Value must be a string that represents a time value, such as 1s, 5m, or 15h. Differs from datum_timeout in that the limit is applied across all workers and all datums. If not set, a job will run indefinitely until it succeeds or fails. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "74aa5fbf49742bcc02b885c9b86e3964"
    },
    {
      "title": "Metadata PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Add metadata to your pipeline pods using Kubernetes' labels and annotations.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/metadata/",
      "relURI": "/latest/build-dags/pipeline-spec/metadata/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;metadata&#34;: { &#34;annotations&#34;: { &#34;annotation&#34;: string }, &#34;labels&#34;: { &#34;label&#34;: string } }, ... } Behavior # Labels help organize and track cluster objects by creating groups of pods based on a given dimension.\nAnnotations enable you to specify any arbitrary metadata.\nBoth parameters require a key-value pair. Do not confuse this parameter with pod_patch, which adds metadata to the user container of the pipeline pod. For more information, see Labels and Selectors and Kubernetes Annotations in the Kubernetes documentation.\nWhen to Use # Use metadata for operation ergonomics and to simplify the querying of Kubernetes objects.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "46a43b35b353f90d027de977ec1bddce"
    },
    {
      "title": "Output Branch PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Define the branch where the pipeline outputs new commits.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/output-branch/",
      "relURI": "/latest/build-dags/pipeline-spec/output-branch/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;output_branch&#34;: string, ... } Behavior # Set to master by default. When to Use # Use this setting to output commits to dev or testing branches.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "68a68b806b2f22aad0cd0e42726859dd"
    },
    {
      "title": "Parallelism Spec PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Define the number of workers used in parallel.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/parallelism/",
      "relURI": "/latest/build-dags/pipeline-spec/parallelism/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;parallelism_spec&#34;: { &#34;constant&#34;: int }, ... } Behavior # HPE ML Data Management starts the number of workers that you specify. For example, set &quot;constant&quot;:10 to use 10 workers.\nThe default value is 1 When to Use # ‚ö†Ô∏è Because spouts and services are designed to be single instances, do not modify the default parallism_spec value for these pipelines.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "c53698e0e03a6528e83f62c74542ffaa"
    },
    {
      "title": "Pod Patch PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Patch a Pod Spec.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/pod-patch/",
      "relURI": "/latest/build-dags/pipeline-spec/pod-patch/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;pod_patch&#34;: string, ... } Behavior # pod_patch is similar to pod_spec but is applied as a JSON Patch. Note, this means that the process outlined above of modifying an existing pod spec and then manually blanking unchanged fields won&rsquo;t work, you&rsquo;ll need to create a correctly formatted patch by diffing the two pod specs.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "577a7d552e38b75c454d8d3d638cdc35"
    },
    {
      "title": "Pod Spec PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Set fields in the Pod Spec that aren't explicitly exposed.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/pod-spec/",
      "relURI": "/latest/build-dags/pipeline-spec/pod-spec/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;pod_spec&#34;: string, ... } Behavior # pod_spec is an advanced option that allows you to set fields in the pod spec that haven&rsquo;t been explicitly exposed in the rest of the pipeline spec. A good way to figure out what JSON you should pass is to create a pod in Kubernetes with the proper settings, then do:\nkubectl get po/&lt;pod-name&gt; -o json | jq .spec this will give you a correctly formatted piece of JSON, you should then remove the extraneous fields that Kubernetes injects or that can be set else where.\nThe JSON is applied after the other parameters for the pod_spec have already been set as a JSON Merge Patch. This means that you can modify things such as the storage and user containers.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "c391c2e6f1385f0916c835e7ac2ab9f1"
    },
    {
      "title": "Reprocess Spec PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Define the reprocessing behavior of a repo upon receiving new or modified. data.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/reprocess-spec/",
      "relURI": "/latest/build-dags/pipeline-spec/reprocess-spec/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;reprocess_spec&#34;: string, ... } Behavior # &quot;reprocess_spec&quot;: &quot;until_success&quot; is the default behavior. To mitigate datums failing for transient connection reasons, HPE ML Data Management automatically retries user code three (3) times before marking a datum as failed. Additionally, you can set the datum_tries field to determine the number of times a job attempts to run on a datum when a failure occurs.\nLet&rsquo;s compare &quot;until_success&quot; and &quot;every_job&quot;:\nSay we have 2 identical pipelines (reprocess_until_success.json and reprocess_at_every_job.json) but for the &quot;reprocess_spec&quot; field set to &quot;every_job&quot; in reprocess_at_every_job.json.\nBoth use the same input repo and have a glob pattern set to /*.\nWhen adding 3 text files to the input repo (file1.txt, file2.txt, file3.txt), the 2 pipelines (reprocess_until_success and reprocess_at_every_job) will process the 3 datums (here, the glob pattern /* creates one datum per file). Now, let&rsquo;s add a 4th file file4.txt to our input repo or modify the content of file2.txt for example. Case of our default reprocess_until_success.json pipeline: A quick check at the list datum on the job id shows 4 datums, of which 3 were skipped. (Only the changed file was processed) Case of reprocess_at_every_job.json: A quick check at the list datum on the job id shows that all 4 datums were reprocessed, none were skipped. ‚ö†Ô∏è &quot;reprocess_spec&quot;: &quot;every_job will not take advantage of HPE ML Data Management&rsquo;s default de-duplication. In effect, this can lead to slower pipeline performance. Before using this setting, consider other options such as including metadata in your file, naming your files with a timestamp, UUID, or other unique identifiers in order to take advantage of de-duplication.\nWhen to Use # Per default, HPE ML Data Management avoids repeated processing of unchanged datums (i.e., it processes only the datums that have changed and skip the unchanged datums). This incremental behavior ensures efficient resource utilization. However, you might need to alter this behavior for specific use cases and force the reprocessing of all of your datums systematically. This is especially useful when your pipeline makes an external call to other resources, such as a deployment or triggering an external pipeline system. Set &quot;reprocess_spec&quot;: &quot;every_job&quot; in order to enable this behavior.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "f69a8cbc4e4d265f77c239508d863d4f"
    },
    {
      "title": "Resource Limits PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Set the upper threshold of allowed resources the user container can consume.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/resource-limits/",
      "relURI": "/latest/build-dags/pipeline-spec/resource-limits/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;resource_limits&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, ... } Behavior # resource_limits describes the upper threshold of allowed resources a given worker can consume. If a worker exceeds this value, it will be evicted.\nThe gpu field is a number that describes how many GPUs each worker needs. Only whole number are supported, Kubernetes does not allow multiplexing of GPUs. Unlike the other resource fields, GPUs only have meaning in Limits, by requesting a GPU the worker will have sole access to that GPU while it is running. It&rsquo;s recommended to enable autoscaling if you are using GPUs so other processes in the cluster will have access to the GPUs while the pipeline has nothing to process. For more information about scheduling GPUs see the Kubernetes docs on the subject.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "544e4a72b905fa6e74f77f156be9410f"
    },
    {
      "title": "Resource Requests PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Set the minimum amount of resources that the user container will reserve.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/resource-request/",
      "relURI": "/latest/build-dags/pipeline-spec/resource-request/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;resource_requests&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, ... } Behavior # resource_requests describes the amount of resources that the pipeline workers will consume. Knowing this in advance enables HPE ML Data Management to schedule big jobs on separate machines, so that they do not conflict, slow down, or terminate.\nThis parameter is optional, and if you do not explicitly add it in the pipeline spec, HPE ML Data Management creates Kubernetes containers with the following default resources:\nThe user and storage containers request 1 CPU, 0 disk space, and 256MB of memory. The init container requests the same amount of CPU, memory, and disk space that is set for the user container. The resource_requests parameter enables you to overwrite these default values.\nThe memory field is a string that describes the amount of memory, in bytes, that each worker needs. Allowed SI suffixes include M, K, G, Mi, Ki, Gi, and other.\nFor example, a worker that needs to read a 1GB file into memory might set &quot;memory&quot;: &quot;1.2G&quot; with a little extra for the code to use in addition to the file. Workers for this pipeline will be placed on machines with at least 1.2GB of free memory, and other large workers will be prevented from using it, if they also set their resource_requests.\nThe cpu field is a number that describes the amount of CPU time in cpu seconds/real seconds that each worker needs. Setting &quot;cpu&quot;: 0.5 indicates that the worker should get 500ms of CPU time per second. Setting &quot;cpu&quot;: 2 indicates that the worker gets 2000ms of CPU time per second. In other words, it is using 2 CPUs, though worker threads might spend 500ms on four physical CPUs instead of one second on two physical CPUs.\nThe disk field is a string that describes the amount of ephemeral disk space, in bytes, that each worker needs. Allowed SI suffixes include M, K, G, Mi, Ki, Gi, and other.\nIn both cases, the resource requests are not upper bounds. If the worker uses more memory than it is requested, it does not mean that it will be shut down. However, if the whole node runs out of memory, Kubernetes starts deleting pods that have been placed on it and exceeded their memory request, to reclaim memory. To prevent deletion of your worker node, you must set your memory request to a sufficiently large value. However, if the total memory requested by all workers in the system is too large, Kubernetes cannot schedule new workers because no machine has enough unclaimed memory. cpu works similarly, but for CPU time.\nFor more information about resource requests and limits see the Kubernetes docs on the subject.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "0615db6626998887ede4eec7a92b6f1e"
    },
    {
      "title": "s3 Out PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Write results out to an S3 gateway endpoint.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/s3-out/",
      "relURI": "/latest/build-dags/pipeline-spec/s3-out/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;s3_out&#34;: bool, ... } Behavior # s3_out allows your pipeline code to write results out to an S3 gateway endpoint instead of the typical pfs/out directory. When this parameter is set to true, HPE ML Data Management includes a sidecar S3 gateway instance container in the same pod as the pipeline container. The address of the output repository will be s3://&lt;output_repo&gt;.\nIf you want to expose an input repository through an S3 gateway, see input.pfs.s3 in PFS Input.\nWhen to Use # You should use the s3 Out attribute when you&rsquo;d like to access and store the results of your HPE ML Data Management transformations externally.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "10ab31aa06930377b5e52ac6fa7400a4"
    },
    {
      "title": "Scheduling Spec PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Define how the pipeline pods should be scheduled.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/scheduling-spec/",
      "relURI": "/latest/build-dags/pipeline-spec/scheduling-spec/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;scheduling_spec&#34;: { &#34;node_selector&#34;: {string: string}, &#34;priority_class_name&#34;: string }, ... } Attributes # Attribute Description node_selector Allows you to select which nodes your pipeline will run on. Refer to the Kubernetes docs on node selectors for more information about how this works. priority_class_name Allows you to select the priority class for the pipeline, which is how Kubernetes chooses to schedule and de-schedule the pipeline. Refer to the Kubernetes docs on priority and preemption for more information about how this works. Behavior # When you include a node_selector in the scheduling_spec, it tells Kubernetes to schedule the pipeline&rsquo;s Pods on nodes that match the specified key-value pairs. For example, if you specify {&quot;gpu&quot;: &quot;true&quot;} in the node_selector, Kubernetes will only schedule the pipeline&rsquo;s Pods on nodes that have a label gpu=true. This is useful when you have specific hardware or other node-specific requirements for your pipeline.\nWhen you specify a priority_class_name in the scheduling_spec, it tells Kubernetes to assign the specified priority class to the pipeline&rsquo;s Pods. The priority class determines the priority of the Pods relative to other Pods in the cluster, and can affect the order in which Pods are scheduled and the resources they are allocated. For example, if you have a high-priority pipeline that needs to complete as quickly as possible, you can assign it a higher priority class than other Pods in the cluster to ensure that it gets scheduled and allocated resources first.\nWhen to Use # You should use the scheduling_spec field in a HPE ML Data Management Pipeline Spec when you have specific requirements for where and when your pipeline runs. This can include requirements related to hardware, node labels, scheduling priority, and other factors.\nExample requirements:\nHardware requirements: If your pipeline requires specific hardware, such as GPUs, you can use the node_selector field to ensure that your pipeline runs on nodes that have the necessary hardware.\nNode labels: If you have specific requirements for node labels, such as data locality, you can use the node_selector field to schedule your pipeline on nodes with the appropriate labels.\nPriority: If you have a high-priority pipeline that needs to complete as quickly as possible, you can use the priority_class_name field to assign a higher priority class to your pipeline&rsquo;s Pods.\nResource constraints: If your pipeline requires a large amount of resources, such as CPU or memory, you can use the node_selector field to ensure that your pipeline runs on nodes with sufficient resources.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "55617b3203c7e12015f7fa255951846d"
    },
    {
      "title": "Service PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Enable a pipeline to be treated as a long-running service.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/service/",
      "relURI": "/latest/build-dags/pipeline-spec/service/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;service&#34;: { &#34;internal_port&#34;: int, &#34;external_port&#34;: int }, ... } Attributes # Attribute Description internal_port The port that the user code binds to inside the container. external_port The port on which it is exposed through the NodePorts functionality of Kubernetes services. Behavior # When enabled, transform.cmd is not expected to exit and will restart if it does. The service becomes exposed outside the container using a Kubernetes service. You can access the service at http://&lt;kubernetes-host&gt;:&lt;external_port&gt;. The Service starts running at the first commit in the input repo. When to Use # You should use the service field in a HPE ML Data Management Pipeline Spec when you want to expose your pipeline as a Kubernetes service, and allow other Kubernetes services or external clients to connect to it.\nExample scenarios:\nMicroservices architecture: If you are building a microservices architecture, you may want to expose individual pipelines as services that can be accessed by other services in the cluster. By using the service field to expose your pipeline as a Kubernetes service, you can easily connect it to other services in the cluster.\nClient access: If you want to allow external clients to access the output of your pipeline, you can use the service field to expose your pipeline as a Kubernetes service and provide clients with the service&rsquo;s IP address and external_port.\nLoad balancing: By exposing your pipeline as a Kubernetes service, you can take advantage of Kubernetes&rsquo; built-in load balancing capabilities. Kubernetes automatically load balances traffic to the service&rsquo;s IP address and external_port across all the replicas of the pipeline&rsquo;s container.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "8e77b32ea4adecd6c9bdbf31a58932b2"
    },
    {
      "title": "Sidecar Resource Limits PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Set the upper threshold of resources allocated to the storage container.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/sidecar-resource-limits/",
      "relURI": "/latest/build-dags/pipeline-spec/sidecar-resource-limits/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;sidecar_resource_limits&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, ... } Attributes # Attribute Description cpu The maximum number of CPU cores that the sidecar container can use. memory The maximum amount of memory that the sidecar container can use. This can be specified in bytes, or with a unit such as &ldquo;Mi&rdquo; or &ldquo;Gi&rdquo;. gpu An optional field that specifies the number and type of GPUs that the sidecar container can use. type The type of GPU to use, such as &ldquo;nvidia&rdquo; or &ldquo;amd&rdquo;. number The number of GPUs that the sidecar container can use. disk The maximum amount of disk space that the sidecar container can use. This can be specified in bytes, or with a unit such as &ldquo;Mi&rdquo; or &ldquo;Gi&rdquo;. Behavior # The sidecar_resource_limits field in a HPE ML Data Management Pipeline Spec is used to specify the resource limits for any sidecar containers that are run alongside the main pipeline container.\nIn a HPE ML Data Management Pipeline, sidecar containers can be used to perform additional tasks alongside the main pipeline container, such as logging, monitoring, or handling external dependencies. By specifying resource limits for these sidecar containers, you can ensure that they don&rsquo;t consume too many resources and impact the performance of the main pipeline container.\nThis field can also be useful in deployments where Kubernetes automatically applies resource limits to containers, which might conflict with HPE ML Data Management pipelines&rsquo; resource requests. Such a deployment might fail if HPE ML Data Management requests more than the default Kubernetes limit. The sidecar_resource_limits enables you to explicitly specify these resources to fix the issue.\nWhen to Use # You should use the sidecar_resource_limits field in a HPE ML Data Management Pipeline Spec when you have sidecar containers that perform additional tasks alongside the main pipeline container, and you want to set resource limits for those sidecar containers.\nExample scenarios:\nLogging: If you have a sidecar container that is responsible for logging, you may want to limit its CPU and memory usage to prevent it from consuming too many resources and impacting the performance of the main pipeline container.\nMonitoring: If you have a sidecar container that is responsible for monitoring the pipeline, you may want to limit its CPU and memory usage to prevent it from competing with the main pipeline container for resources.\nExternal dependencies: If you have a sidecar container that provides external dependencies, such as a database, you may want to limit its CPU and memory usage to ensure that the main pipeline container has sufficient resources to perform its task.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "7ef263902261ed9826d96cec3921d89e"
    },
    {
      "title": "Sidecar Resource Requests PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Set the minimum amount of resources that the storage container will reserve.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/sidecar-resource-requests/",
      "relURI": "/latest/build-dags/pipeline-spec/sidecar-resource-requests/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;sidecar_resource_requests&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, ... } Attributes # Attribute Description cpu The minimum number of CPU cores that the storage container will reserve. memory The minimum amount of memory that the storage container will reserve. This can be specified in bytes, or with a unit such as &ldquo;Mi&rdquo; or &ldquo;Gi&rdquo;. gpu An optional field that specifies the number and type of GPUs that the storage container will reserve. type The type of GPU to use, such as &ldquo;nvidia&rdquo; or &ldquo;amd&rdquo;. number The number of GPUs that the storage container will reserve. disk The minimum amount of disk space that the storage container will reserve. This can be specified in bytes, or with a unit such as &ldquo;Mi&rdquo; or &ldquo;Gi&rdquo;. Behavior # The sidecar_resource_requests field in a HPE ML Data Management Pipeline Spec is used to specify the resource requests for the storage container that runs alongside the user container.\nIn a HPE ML Data Management Pipeline, the storage container is used to perform additional tasks alongside the user pipeline container, such as logging, monitoring, or handling external dependencies. By specifying resource requests for this sidecar container, you can ensure that the storage container has enough resources reserved as to not impact the performance of the user container.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "18c39d682bf0d6a1bd309d2f87dffe1d"
    },
    {
      "title": "Spec Commit PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "This attribute is auto-generated and is not configurable.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/spec-commit/",
      "relURI": "/latest/build-dags/pipeline-spec/spec-commit/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;spec_commit&#34;: { &#34;option&#34;: false, &#34;branch&#34;: { &#34;option&#34;: false, &#34;repo&#34;: { &#34;option&#34;: false, &#34;name&#34;: string, &#34;type&#34;: string, &#34;project&#34;:{ &#34;option&#34;: false, &#34;name&#34;: string, }, }, &#34;name&#34;: string }, &#34;id&#34;: string, }, ... } When to Use # You do not need to ever configure this attribute; its details are auto-generated.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "2eb83eb11b8b3878b2afeff0502ce23c"
    },
    {
      "title": "Spout PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Ingest streaming data using a spout pipeline.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/spout/",
      "relURI": "/latest/build-dags/pipeline-spec/spout/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: {...}, &#34;spout&#34;: { \\\\ Optionally, you can combine a spout with a service: &#34;service&#34;: { &#34;internal_port&#34;: int, &#34;external_port&#34;: int } }, ... } Attributes # Attribute Description service An optional field that is used to specify how to expose the spout as a Kubernetes service. internal_port Used for the spout&rsquo;s container. external_port Used for the Kubernetes service that exposes the spout. Behavior # Does not have a PFS input; instead, it consumes data from an outside source. Can have a service added to it. See Service. Its code runs continuously, waiting for new events. The output repo, pfs/out is not directly accessible. To write into the output repo, you must to use the put file API call via any of the following: pachctl put file A HPE ML Data Management SDK (for golang or Python ) Your own API client. HPE ML Data Management CLI (PachCTL) is packaged in the base image of your spout as well as your authentication information. As a result, the authentication is seamless when using PachCTL. Diagram # When to Use # You should use the spout field in a HPE ML Data Management Pipeline Spec when you want to read data from an external source that is not stored in a HPE ML Data Management repository. This can be useful in situations where you need to read data from a service that is not integrated with HPE ML Data Management, such as an external API or a message queue.\nExample scenarios:\nData ingestion: If you have an external data source, such as a web service, that you want to read data from and process with HPE ML Data Management, you can use the spout field to read the data into HPE ML Data Management.\nReal-time data processing: If you need to process data in real-time and want to continuously read data from an external source, you can use the spout field to read the data into HPE ML Data Management and process it as it arrives.\nData integration: If you have data stored in an external system, such as a message queue or a streaming service, and you want to integrate it with data stored in HPE ML Data Management, you can use the spout field to read the data from the external system and process it in HPE ML Data Management.\nExample # { &#34;pipeline&#34;: { &#34;name&#34;: &#34;my-spout&#34; }, &#34;spout&#34;: { }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;go&#34;, &#34;run&#34;, &#34;./main.go&#34; ], &#34;image&#34;: &#34;myaccount/myimage:0.1&#34;, &#34;env&#34;: { &#34;HOST&#34;: &#34;kafkahost&#34;, &#34;TOPIC&#34;: &#34;mytopic&#34;, &#34;PORT&#34;: &#34;9092&#34; } } } üí° For a first overview of how spouts work, see our spout101 example.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "565289ed67af0f206bb217ba7b1fa6a5"
    },
    {
      "title": "Transform PPS",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "Set the name of the Docker image that your jobs use.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/transform/",
      "relURI": "/latest/build-dags/pipeline-spec/transform/",
      "body": " Spec # This is a top-level attribute of the pipeline spec.\n{ &#34;pipeline&#34;: {...}, &#34;transform&#34;: { &#34;image&#34;: string, &#34;cmd&#34;: [ string ], &#34;datum_batching&#34;: bool, &#34;err_cmd&#34;: [ string ], &#34;env&#34;: { string: string }, &#34;secrets&#34;: [ { &#34;name&#34;: string, &#34;mount_path&#34;: string }, { &#34;name&#34;: string, &#34;env_var&#34;: string, &#34;key&#34;: string } ], &#34;image_pull_secrets&#34;: [ string ], &#34;stdin&#34;: [ string ], &#34;err_stdin&#34;: [ string ], &#34;accept_return_code&#34;: [ int ], &#34;debug&#34;: bool, &#34;user&#34;: string, &#34;working_dir&#34;: string, &#34;dockerfile&#34;: string, &#34;memory_volume&#34;: bool, }, ... } Attributes # Attribute Description cmd Passes a command to the Docker run invocation. datum_batching Enables you to call your user code once for a batch of datums versus calling it per each datum. stdin Passes an array of lines to your command on stdin. err_cmd Passes a command executed on failed datums. err_stdin Passes an array of lines to your error command on stdin. env Enables a key-value map of environment variables that HPE ML Data Management injects into the container. secrets Passes an array of secrets to embed sensitive data. image_pull_secrets Passes an array of secrets that are mounted before the containers are created. accept_return_code Passes an array of return codes that are considered acceptable when your Docker command exits. debug Enables debug logging for the pipeline user Sets the user that your code runs as. working_dir Sets the directory that your command runs from. memory_volume Sets pachyderm-worker&rsquo;s emptyDir.Medium to Memory, allowing Kubernetes to mount a memory-backed volume (tmpfs). Behavior # cmd is not run inside a shell which means that wildcard globbing (*), pipes (|), and file redirects (&gt; and &gt;&gt;) do not work. To specify these settings, you can set cmd to be a shell of your choice, such as sh and pass a shell script to stdin. err_cmd can be used to ignore failed datums while still writing successful datums to the output repo, instead of failing the whole job when some datums fail. The transform.err_cmd command has the same limitations as transform.cmd. stdin lines do not have to end in newline characters. The following environment variables are automatically injected into the container: PACH_JOB_ID ‚Äì the ID of the current job. PACH_OUTPUT_COMMIT_ID ‚Äì the ID of the commit in the output repo for the current job. &lt;input&gt;_COMMIT - the ID of the input commit. For example, if your input is the images repo, this will be images_COMMIT. secrets reference Kubernetes secrets by name and specify a path to map the secrets or an environment variable (env_var) that the value should be bound to. 0 is always considered a successful exit code. tmpfs is cleared on node reboot and any files you write count against your container&rsquo;s memory limit. This may be useful for workloads that are IO heavy or use memory caches. üí° **Using a private registry? **\nYou can use image_pull_secrets to mount a secret that contains your registry credentials.\n{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;pipeline-a&#34; }, &#34;description&#34;: &#34;...&#34;, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/example.py&#34; ], &#34;image&#34;: &#34;&lt;private container registry&gt;/image:1.0&#34;, &#34;image_pull_secrets&#34;: [ &#34;k8s-secret-with-creds&#34; ] }, ... } When to Use # You must always use the transform attribute when making a pipeline.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps"
      ],
      "id": "816944b2a3400ac5730e84843e3b06a8"
    },
    {
      "title": "Full Pipeline Specification",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Specification (PPS)",
      "description": "View the full pipeline spec file in JSON",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-spec/full-spec/",
      "relURI": "/latest/build-dags/pipeline-spec/full-spec/",
      "body": " { &#34;pipeline&#34;: { &#34;name&#34;: string, &#34;project&#34;: { name: &#34;projectName&#34; }, }, &#34;description&#34;: string, &#34;metadata&#34;: { &#34;annotations&#34;: { &#34;annotation&#34;: string }, &#34;labels&#34;: { &#34;label&#34;: string } }, &#34;tf_job&#34;: { &#34;tf_job&#34;: string, }, &#34;transform&#34;: { &#34;image&#34;: string, &#34;cmd&#34;: [ string ], &#34;err_cmd&#34;: [ string ], &#34;env&#34;: { string: string }, &#34;secrets&#34;: [ { &#34;name&#34;: string, &#34;mount_path&#34;: string }, { &#34;name&#34;: string, &#34;env_var&#34;: string, &#34;key&#34;: string } ], &#34;image_pull_secrets&#34;: [ string ], &#34;stdin&#34;: [ string ], &#34;err_stdin&#34;: [ string ], &#34;accept_return_code&#34;: [ int ], &#34;debug&#34;: bool, &#34;user&#34;: string, &#34;working_dir&#34;: string, &#34;dockerfile&#34;: string, &#34;memory_volume&#34;: bool, }, &#34;parallelism_spec&#34;: { &#34;constant&#34;: int }, &#34;egress&#34;: { // Egress to an object store &#34;URL&#34;: &#34;s3://bucket/dir&#34; // Egress to a database &#34;sql_database&#34;: { &#34;url&#34;: string, &#34;file_format&#34;: { &#34;type&#34;: string, &#34;columns&#34;: [string] }, &#34;secret&#34;: { &#34;name&#34;: string, &#34;key&#34;: &#34;PACHYDERM_SQL_PASSWORD&#34; } } }, &#34;update&#34;: bool, &#34;output_branch&#34;: string, [ { &#34;worker_id&#34;: string, &#34;job_id&#34;: string, &#34;datum_status&#34; : { &#34;started&#34;: timestamp, &#34;data&#34;: [] } } ], &#34;s3_out&#34;: bool, &#34;resource_requests&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, &#34;resource_limits&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, &#34;sidecar_resource_limits&#34;: { &#34;cpu&#34;: number, &#34;memory&#34;: string, &#34;gpu&#34;: { &#34;type&#34;: string, &#34;number&#34;: int } &#34;disk&#34;: string, }, &#34;input&#34;: { &lt;&#34;pfs&#34;, &#34;cross&#34;, &#34;union&#34;, &#34;join&#34;, &#34;group&#34; or &#34;cron&#34; see below&gt; }, &#34;description&#34;: string, &#34;reprocess&#34;: bool, &#34;service&#34;: { &#34;internal_port&#34;: int, &#34;external_port&#34;: int }, &#34;spout&#34;: { \\\\ Optionally, you can combine a spout with a service: &#34;service&#34;: { &#34;internal_port&#34;: int, &#34;external_port&#34;: int } }, &#34;datum_set_spec&#34;: { &#34;number&#34;: int, &#34;size_bytes&#34;: int, &#34;per_worker&#34;: int, } &#34;datum_timeout&#34;: string, &#34;job_timeout&#34;: string, &#34;salt&#34;: string, &#34;datum_tries&#34;: int, &#34;scheduling_spec&#34;: { &#34;node_selector&#34;: {string: string}, &#34;priority_class_name&#34;: string }, &#34;pod_spec&#34;: string, &#34;pod_patch&#34;: string, &#34;spec_commit&#34;: { &#34;option&#34;: false, &#34;branch&#34;: { &#34;option&#34;: false, &#34;repo&#34;: { &#34;option&#34;: false, &#34;name&#34;: string, &#34;type&#34;: string, &#34;project&#34;:{ &#34;option&#34;: false, &#34;name&#34;: string, }, }, &#34;name&#34;: string }, &#34;id&#34;: string, } &#34;metadata&#34;: { }, &#34;reprocess_spec&#34;: string, &#34;autoscaling&#34;: bool } ------------------------------------ &#34;pfs&#34; input ------------------------------------ &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;repo_type&#34;:string, &#34;branch&#34;: string, &#34;commit&#34;:string, &#34;glob&#34;: string, &#34;join_on&#34;:string, &#34;outer_join&#34;: bool, &#34;group_by&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool, &#34;trigger&#34;: { &#34;branch&#34;: string, &#34;all&#34;: bool, &#34;cron_spec&#34;: string, }, } ------------------------------------ &#34;cross&#34; or &#34;union&#34; input ------------------------------------ &#34;cross&#34; or &#34;union&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;lazy&#34; bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ... ] ------------------------------------ &#34;join&#34; input ------------------------------------ &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;join_on&#34;: string, &#34;outer_join&#34;: bool, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;join_on&#34;: string, &#34;outer_join&#34;: bool, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ] ------------------------------------ &#34;group&#34; input ------------------------------------ &#34;group&#34;: [ { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;group_by&#34;: string, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } }, { &#34;pfs&#34;: { &#34;name&#34;: string, &#34;repo&#34;: string, &#34;branch&#34;: string, &#34;glob&#34;: string, &#34;group_by&#34;: string, &#34;lazy&#34;: bool, &#34;empty_files&#34;: bool, &#34;s3&#34;: bool } } ] ------------------------------------ &#34;cron&#34; input ------------------------------------ &#34;cron&#34;: { &#34;name&#34;: string, &#34;spec&#34;: string, &#34;repo&#34;: string, &#34;start&#34;: time, &#34;overwrite&#34;: bool }``` ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "pps",
        "pipeline spec"
      ],
      "id": "e34b408be7bff4f1307c6237f04a74aa"
    },
    {
      "title": "Pipeline Ops",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Build Pipelines & DAGs",
      "description": "Learn how to create, delete, and update pipelines using PachCTL and jsonnet templating.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-operations/",
      "relURI": "/latest/build-dags/pipeline-operations/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e071175d2ef885183ad643f6c8bc327e"
    },
    {
      "title": "Create a Pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Ops",
      "description": "Learn how to create a pipeline using the pachctl create command.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-operations/create-pipeline/",
      "relURI": "/latest/build-dags/pipeline-operations/create-pipeline/",
      "body": "To create a pipeline, you need to define a pipeline specification in YAML, JSON, or Jsonnet.\nBefore You Start # A basic pipeline must have all of the following:\npipeline.name: The name of your pipeline. transform.cmd: The command that executes your user code. transform.img: The image that contains your user code. input.pfs.repo: The output repository for the transformed data. input.pfs.glob: The glob pattern used to identify the shape of datums. How to Create a Pipeline # Via Local File # Define a pipeline specification in YAML, JSON, or Jsonnet.\nPass the pipeline configuration to HPE ML Data Management:\npachctl create pipeline -f &lt;pipeline_spec&gt; Via URL # Find a pipeline specification hosted in a public or internal repository. Pass the pipeline configuration to HPE ML Data Management: pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/2.7.x/examples/opencv/edges.json Via Jsonnet # Jsonnet Pipeline specs let you create pipelines while passing a set of parameters dynamically, allowing you to reuse the baseline of a given pipeline while changing the values of chosen fields. You can, for example, create multiple pipelines out of the same jsonnet pipeline spec file while pointing each of them at different input repositories, parameterize a command line in the transform field of your pipelines, or dynamically pass various docker images to train different models on the same dataset.\nFor illustration purposes, in the following example, we are creating a pipeline named edges-1 and pointing its input repository at the repo &lsquo;images&rsquo;:\npachctl create pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg src=images üìñ You can define multiple pipeline specifications in one file by separating the specs with the following separator: ---. This works in both JSON and YAML files.\nExamples # JSON # { &#34;pipeline&#34;: { &#34;name&#34;: &#34;edges&#34; }, &#34;description&#34;: &#34;A pipeline that performs image edge detection by using the OpenCV library.&#34;, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python3&#34;, &#34;/edges.py&#34; ], &#34;image&#34;: &#34;pachyderm/opencv&#34; }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;images&#34;, &#34;glob&#34;: &#34;/*&#34; } } } YAML # pipeline: name: edges description: A pipeline that performs image edge detection by using the OpenCV library. transform: cmd: - python3 - &#34;/edges.py&#34; image: pachyderm/opencv input: pfs: repo: images glob: &#34;/*&#34; Considerations # When you create a pipeline, HPE ML Data Management automatically creates an eponymous output repository. However, if such a repo already exists, your pipeline will take over the master branch. The files that were stored in the repo before will still be in the HEAD of the branch. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines"
      ],
      "id": "e985dc0a1f8d1149231d630d28db9a69"
    },
    {
      "title": "Delete a Pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Ops",
      "description": "Learn how to delete a pipeline using the pachctl delete command.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-operations/delete-pipeline/",
      "relURI": "/latest/build-dags/pipeline-operations/delete-pipeline/",
      "body": "You can delete a pipeline by running:\npachctl delete pipeline &lt;pipeline_name&gt; To delete all of your pipelines (be careful with this feature), use the additional --all flag.\nWhen you delete a pipeline:\nKubernetes deletes all resources associated with the pipeline - pods (if any), services, and replication controllers. HPE ML Data Management deletes the user output repository with all its data as well as the system meta (stats) and spec (historical versions of the pipeline specification file) repositories. ‚ÑπÔ∏è If you are using HPE ML Data Management authorization features, only authorized users will be able to delete a given pipeline. In particular, they will have to be repoOwner of the output repo of the pipeline (i.e., have created the pipeline) or clusterAdmin.\nYou can use the --keep-repo flag to preserve the output repo with all its branches. However, important job metadata will still be deleted (including all historical versions of the pipeline specification file). As a result, you will not be able to recreate the deleted pipeline with the same name unless that repo is deleted first.\nExample # For example, if a pipeline &ldquo;xyz&rdquo; exists, then there is an output repo &ldquo;xyz&rdquo;. If a user deletes the pipeline with --keep-repo, the output repo &ldquo;xyz&rdquo; will remain, but the pipeline will be gone. If the user tries to create a new pipeline called &ldquo;xyz&rdquo;, it will fail (there is already an output repo with that name). For the pipeline creation to be successful, the user would have to delete repo &ldquo;xyz&rdquo; first.\n‚ÑπÔ∏è You can use the output repo of a pipeline deleted with --keep-repo as an input repo and add more data.\nWhen HPE ML Data Management cannot delete a pipeline with the standard command, you might need to enforce deletion using the --force flag. Because this option can break dependent components in your DAG, use this option withextreme caution.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines"
      ],
      "id": "8101e14cee8de4e8cd30462d6357370a"
    },
    {
      "title": "Draw a Pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Ops",
      "description": "Learn how to draw a pipeline using the PachCTL draw command or Console.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-operations/draw-pipeline/",
      "relURI": "/latest/build-dags/pipeline-operations/draw-pipeline/",
      "body": "Sometimes it&rsquo;s easier to share and reason about pipelines when you can see them drawn out. You can draw ASCII pipelines via PachCTL, or download highly detailed pipeline canvases (in the form of SVGs) from Console.\nHow to Draw a Pipeline # Tool: PachCTL CLI Console pachctl draw pipeline &lt;pipeline-name&gt; Example output:\n+-----------+ |housing_d..| +-----------+ | | | | | +-----------+ |regression | +-----------+ Open the Console UI. Navigate to the project containing the pipeline. Select Download Canvas. An SVG version of your DAG will be saved to your computer.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines"
      ],
      "id": "d423751415cbafdd6752b96999b11030"
    },
    {
      "title": "Inspect a Pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Ops",
      "description": "Learn how to inspect a pipeline using the pachctl inspect command or console.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-operations/inspect-pipeline/",
      "relURI": "/latest/build-dags/pipeline-operations/inspect-pipeline/",
      "body": "You can inspect a pipeline via PachCTL or the Console. When you inspect a pipeline, you can see key details such as its input configuration, output configuration, and transformation image/command details.\nHow to Inspect a Pipeline # Tool: PachCTL CLI Console pachctl inspect pipeline &lt;pipeline-name&gt; Example output:\nName: regression Description: A pipeline that trains produces a regression model for housing prices. Created: 4 weeks ago State: running Reason: Workers Available: 1/1 Stopped: false Parallelism Spec: &lt;nil&gt; Datum Timeout: (duration: nil Duration) Job Timeout: (duration: nil Duration) Input: { &#34;pfs&#34;: { &#34;project&#34;: &#34;standard-ml-tutorial&#34;, &#34;name&#34;: &#34;housing_data&#34;, &#34;repo&#34;: &#34;housing_data&#34;, &#34;repo_type&#34;: &#34;user&#34;, &#34;branch&#34;: &#34;master&#34;, &#34;glob&#34;: &#34;/*&#34; } } Output Branch: master Transform: { &#34;image&#34;: &#34;lbliii/housing-prices:latest&#34;, &#34;cmd&#34;: [ &#34;python&#34;, &#34;regression.py&#34;, &#34;--input&#34;, &#34;/pfs/housing_data/&#34;, &#34;--target-col&#34;, &#34;MEDV&#34;, &#34;--output&#34;, &#34;/pfs/out/&#34; ], &#34;datum_batching&#34;: true } Open the Console UI. Navigate to the project containing the pipeline. From the DAG view, select the pipeline name. A slideout panel appears with three tabs you can inspect: Job Overview, Pipeline Info, and Spec. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines"
      ],
      "id": "99ced02eae8d86966acfcf3c592edf28"
    },
    {
      "title": "Jsonnet Pipeline Specifications",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Ops",
      "description": "Learn how to configure a pipeline using the Jsonnet templating language.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-operations/jsonnet-pipeline-specs/",
      "relURI": "/latest/build-dags/pipeline-operations/jsonnet-pipeline-specs/",
      "body": " ‚ö†Ô∏è Jsonnet pipeline specifications is an experimental feature.\nHPE ML Data Management pipeline&rsquo;s specification files are intuitive, simple, and language agnostic. They are, however, very static.\nA jsonnet pipeline specification file is a thin wrapping layer atop of your JSON file, allowing you to parameterize a pipeline specification file, thus adding a dynamic component to the creation and update of pipelines.\nWith jsonnet pipeline specs, you can easily reuse the baseline of a given pipeline spec while experimenting with various values of given fields.\nJsonnet Specs # HPE ML Data Management&rsquo;s Jsonnet pipeline specs are written in the open-source templating language jsonnet. Jsonnet wraps the baseline of a JSON file into a function, allowing the injection of parameters to a pipeline specification file.\nAll jsonnet pipeline specs have a .jsonnet extension.\nAs an example, check the file edges.jsonnet below. It is a parameterized version of the edges pipeline spec edges.json in the opencv example, used to inject a name modifier and an input repository name into the original pipeline specifications.\nExample 1 # In this snippet of edges.jsonnet, the parameter src sits in place of what would have been the value of the field repo, as a placeholder for any parameter that will be passed to the Jsonnet pipeline spec.\ninput: { pfs: { name: &#34;images&#34;, glob: &#34;/*&#34;, repo: src, } }, See the full edges.jsonnet here:\n//// // Template arguments: // // suffix : An arbitrary suffix appended to the name of this pipeline, for // disambiguation when multiple instances are created. // src : the repo from which this pipeline will read the images to which // it applies edge detection. //// function(suffix, src) { pipeline: { name: &#34;edges-&#34;+suffix }, description: &#34;OpenCV edge detection on &#34;+src, input: { pfs: { name: &#34;images&#34;, glob: &#34;/*&#34;, repo: src, } }, transform: { cmd: [ &#34;python3&#34;, &#34;/edges.py&#34; ], image: &#34;pachyderm/opencv:0.0.1&#34; } } Or check our full &ldquo;jsonnet-ed&rdquo; opencv example.\nTo create or update a pipeline using a jsonnet pipeline specification file:\nadd the --jsonnet flag to your pipeline create or pipeline update commands, followed by a local path to your jsonnet file or an url. add --arg &lt;parameter-name&gt;=value for each variable. Example 2 # pachctl create pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg src=images The command above will generate a JSON file named edges-1.json then create a pipeline of the same name taking the repository images as its input.\nüìñ Read Jsonnet&rsquo;s complete standard library documentation to learn about all the variables types, string manipulation and mathematical functions, or assertions available to you.\nAt the minimum, your function should always have a parameter that acts as a name modifier. HPE ML Data Management&rsquo;s pipeline names are unique. You can quickly generate several pipelines from the same jsonnet pipeline specification file by adding a prefix or a suffix to its generic name.\nüìñ Your .jsonnet file can create multiple pipelines at once as illustrated in our group example.\nUse Cases # Using jsonnet pipeline specifications, you could pass different images to the transform section of an otherwise identical JSON specification file to train multiple models on the same dataset, or switch between one input repo holding test data to another holding production data by parameterizing the input repo field.\nDuring the development phase of a pipeline, it can be helpful to pass the tag of an image as a parameter: each re-build of the pipeline&rsquo;s code requires you to increment your tag value; passing it as a parameter will save you the time to update your JSON specifications. You could also consider preparing a library of ready-made jsonnet pipeline specs for data science teams to instantiate, according to their own set of parameters.\nWe will let you imagine more use cases in which those jsonnet specs can be helpful to you.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines",
        "jsonnet"
      ],
      "id": "9e964d15ec3c5df4b8b07d05d34dfa9d"
    },
    {
      "title": "Update a Pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Ops",
      "description": "",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-operations/updating-pipelines/",
      "relURI": "/latest/build-dags/pipeline-operations/updating-pipelines/",
      "body": "While working with your data, you often need to modify an existing pipeline with new transformation code or pipeline parameters.\nUse the pachctl update pipeline command to make changes to a pipeline, whether you have re-built a docker image after a code change and/or need to update pipeline parameters in the pipeline specification file.\nAlternatively, you can update a pipeline using jsonnet pipeline specification files.\nAfter You Changed Your Specification File # Run the pachctl update pipeline command to apply any change to your pipeline specification JSON file, such as change to the parallelism settings, change of an image tag, change of an input repository, etc&hellip;\nBy default, a pipeline update does not trigger the reprocessing of the data that has already been processed. Instead, it processes only the new data you submit to the input repo. If you want to run the changes in your pipeline against the data in your input repo&rsquo;s HEAD commit, use the --reprocess flag. The updated pipeline will then continue to process new input data only. Previous results remain accessible through the corresponding commit IDs.\nTo update a pipeline, run the following command after you have updated your pipeline specification JSON file.\npachctl update pipeline -f pipeline.json ‚ÑπÔ∏è Similar to create pipeline, update pipeline with the -f flag can take a URL if your JSON manifest is hosted on GitHub or other remote location.\nUsing Jsonnet Pipeline Specification Files # Jsonnet pipeline specs allow you to bypass the &ldquo;update-your -specification-file&rdquo; step and apply your changes at once by running:\npachctl update pipeline --jsonnet &lt;your jsonnet pipeline specs path or URL&gt; --arg &lt;param 1&gt;=&lt;value 1&gt; --arg &lt;param 2&gt;=&lt;value 2&gt; Example # pachctl update pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg tag=1.0.2 Update the Code in a Pipeline # To update the code in your pipeline, complete the following steps:\nMake the code changes.\nVerify that the Docker daemon is running. Depending on your operating system and the Docker distribution that you use, steps for enabling it might vary:\ndocker ps If you get an error message similar to the following:\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? enable the Docker daemon (see the Docker documentation for your operating system and platform). For example, if you use minikube on macOS, run the following command:\neval $(minikube docker-env) Then build, tag, and push the new image to your image registry and update the pipeline. This step comes in 3 flavors:\nIf you prefer to use instructions from your image registry # Build, tag, and push a new image as described in your image registry documentation. For example, if you use DockerHub, see Docker Documentation.\nUpdate the transform.image field of your pipeline spec with your new tag.\nüí° Make sure to update your tag every time you re-build. Our pull policy is IfNotPresent (Only pull the image if it does not already exist on the node.). Failing to update your tag will result in your pipeline running on a previous version of your code.\nUpdate the pipeline:\npachctl update pipeline -f &lt;pipeline.json&gt; If you chose to use a jsonnet version of your pipeline specs # Pass the tag of your image to your jsonnet specs.\nAs an example, see the tag parameter in this jsonnet version of opencv&rsquo;s edges pipeline (edges.jsonnet):\n//// // Template arguments: // // suffix : An arbitrary suffix appended to the name of this pipeline, for // disambiguation when multiple instances are created. // src : the repo from which this pipeline will read the images to which // it applies edge detection. //// function(suffix, src) { pipeline: { name: &#34;edges-&#34;+suffix }, description: &#34;OpenCV edge detection on &#34;+src, input: { pfs: { name: &#34;images&#34;, glob: &#34;/*&#34;, repo: src, } }, transform: { cmd: [ &#34;python3&#34;, &#34;/edges.py&#34; ], image: &#34;pachyderm/opencv:0.0.1&#34; } } Once your pipeline code is updated and your image is built, tagged, and pushed, update your pipeline using this command line. In this case, there is no need to edit the pipeline specification file to update the value of your new tag. This command will take care of it:\npachctl update pipeline --jsonnet jsonnet/edges.jsonnet --arg suffix=1 --arg tag=1.0.2 If you use HPE ML Data Management commands # Build your new image using docker build (for example, in a makefile: @docker build --platform linux/amd64 -t $(DOCKER_ACCOUNT)/$(CONTAINER_NAME) .). No tag needed, the following --push-images flag will take care of it.\nRun the following command:\npachctl update pipeline -f &lt;pipeline name&gt; --push-images --registry &lt;registry&gt; --username &lt;registry user&gt; If you use DockerHub, omit the --registry flag.\nExample:\npachctl update pipeline -f edges.json --push-images --username testuser When prompted, type your image registry password:\nExample:\nPassword for docker.io/testuser: Building pachyderm/opencv:f1e0239fce5441c483b09de425f06b40, this may take a while. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines"
      ],
      "id": "0fdce58e314e7eb4fa4d9831c7386e79"
    },
    {
      "title": "View Pipeline Jobs & Runtimes",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pipeline Ops",
      "description": "Learn how to view pipeline runtimes from Console",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/pipeline-operations/view-runtimes/",
      "relURI": "/latest/build-dags/pipeline-operations/view-runtimes/",
      "body": "You can easily view and compare pipeline jobs and job runtimes from within Console. This can be a great way to measure and compare performance across different job sizes and pipeline versions.\nHow to View Pipeline Jobs # This view is great for quickly understanding how many datums were processed, if the job required restarts, and how much data was downloaded/uploaded.\nLog in to Console. Navigate to a project. Select Pipelines. Find and select the pipeline(s) you wish to list jobs for. üí° You can achieve a similar output from the terminal by using the follwing PachCTL command:\npachctl list jobs --project &lt;project-name&gt; --pipeline &lt;pipeline-name&gt; How to View Pipeline Runtimes # This view is great for visually comparing processing times between jobs.\nLog in to Console. Navigate to a project. Select Pipelines. Find and select the pipeline(s) you wish to view runtimes for. Select the Runtimes tab. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines"
      ],
      "id": "f458ae961066924138665ae8969b0981"
    },
    {
      "title": "Project Ops",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Build Pipelines & DAGs",
      "description": "Learn how to create, delete, and update projects",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/project-operations/",
      "relURI": "/latest/build-dags/project-operations/",
      "body": "Projects are logical collections of related work (such as repos and pipelines). Each HPE ML Data Management cluster ships with an initial project named default. PachCTL supports all Project operations, such as adding/removing team members, resources, etc. HPE ML Data Management Console can be used to view and access Projects. HPE ML Data Management&rsquo;s integrations with JupyterLab, Seldon, S3 Gateway, and DeterminedAI also support projects.\nBenefits of Projects # Logical Organization of DAGs: Similar to a file system, you can organize your work within a HPE ML Data Management instance.\nStandardizable: Resources like repos can have the same name if they belong to different projects, making it easier to create and adhere to project templates in a collaborative environment. For example, ProjectA.Repo1 and ProjectB.Repo1.\nMulti-team Enablement: With Enterprise HPE ML Data Management, You can grant access to projects based on roles; projects are hidden from users without access by default.\nExample # In the following example there are two projects: DOGS and CATS. They have similarly named repositories and pipelines. With Enterprise HPE ML Data Management, you could scope access to each project by user or user group.\ngraph TD A[(Project DOGS)] --&gt; B[Picture Repo] A[(Project DOGS)] --&gt; C[Text Repo] A[(Project DOGS)] --&gt; D[Audio Repo] B --&gt; X(Cleanup Pipeline A) C --&gt; Y(Cleanup Pipeline B) D --&gt; Z(Cleanup Pipeline C) X -- from output repo --&gt; 1(Grouping Pipeline) Y -- from output repo --&gt; 1(Grouping Pipeline) Z -- from output repo --&gt; 1(Grouping Pipeline) J[(Project CATS)] --&gt; M[Picture Repo] J[(Project CATS)] --&gt; N[Text Repo] J[(Project CATS)] --&gt; O[Audio Repo] M --&gt; P(Cleanup Pipeline A) N --&gt; Q(Cleanup Pipeline B) O --&gt; R(Cleanup Pipeline C) P -- from output repo --&gt; 2(Grouping Pipeline) Q -- from output repo --&gt; 2(Grouping Pipeline) R -- from output repo --&gt; 2(Grouping Pipeline) Limitations # You currently cannot move existing repos or pipelines between different projects. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "projects"
      ],
      "id": "051f013f21a155c0c928c6d1987b46ae"
    },
    {
      "title": "Create a Project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Project Ops",
      "description": "Learn how to create a new project.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/project-operations/create-project/",
      "relURI": "/latest/build-dags/project-operations/create-project/",
      "body": " Before You Start # Project names should be less than 51 characters long Project names cannot start with special characters and cannot contain periods (.) at all. Regex example: /^[a-zA-Z0-9-_]+$/. How to Create a Project # 1. Create a Project # Tool: Pachctl CLI Console pachctl create project foo Open Console. Click Create Project. Provide the following details: Name Description Click Create. 2. Verify Creation # You can verify that your project has been created by running pachctl list projects or by opening Console (localhost for non-production personal-machine installations) and viewing the home page.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "projects"
      ],
      "id": "2a29c21067505da73e9b054f903e018f"
    },
    {
      "title": "Set a Project as Current",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Project Ops",
      "description": "Learn how to switch to a different project.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/project-operations/set-project/",
      "relURI": "/latest/build-dags/project-operations/set-project/",
      "body": " Before You Start # HPE ML Data Management ships with an initial project named default that is automatically set to your active context. How to Set a Project to Your Current Context # Tool: Pachctl CLI Console In order to begin working on a project other than the default project, you must assign it to a pachCTL context. This enables you to safely add or update resources such as pipelines and repos without affecting other projects.\npachctl config update context --project foo You can check the details of your active context using the following commands:\npachctl config get active-context # returns contextName pachctl config get context &lt;contextName&gt; # { # &#34;source&#34;: &#34;IMPORTED&#34;, # &#34;cluster_name&#34;: &#34;docker-desktop&#34;, # &#34;auth_info&#34;: &#34;docker-desktop&#34;, # &#34;cluster_deployment_id&#34;: &#34;dev&#34;, # &#34;project&#34;: &#34;foo&#34; # } Open the Console UI. Navigate to the top-level Projects view. Scroll to a project you wish to work on. Select the Ellipsis Icon. Copy and paste the PachCTL command into your terminal. You can now work within the project from Console.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "projects"
      ],
      "id": "b2aeb01f351d3f832f868198ea67aba5"
    },
    {
      "title": "Add a Project Resource",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Project Ops",
      "description": "Learn how to add a resource (like a repo) to a project.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/project-operations/add-project-resource/",
      "relURI": "/latest/build-dags/project-operations/add-project-resource/",
      "body": " Tool: Pachctl CLI Console There are two main ways to add a resource to a project, depending on whether or not the project has been set to your current pachyderm context.\nAdd Resource to Unset Project:\npachctl create repo bar --project foo Add Resource to Set Project:\npachctl create repo bar Open the Console UI. Scroll to the project you wish to work in. Select View Project. Select Create Repo. For more information about Repos.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "projects"
      ],
      "id": "c5a2a029a031002a2e20893a6286fcea"
    },
    {
      "title": "Grant Project Access",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Project Ops",
      "description": "Learn how to grant a user access to a project.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/project-operations/grant-project-access/",
      "relURI": "/latest/build-dags/project-operations/grant-project-access/",
      "body": "Cluster Admins and Project Owners can grant or revoke user access to projects within a cluster (in addition to the individual resources within the project). View roles and permissions available.\nHow to Grant Project Access to a User # Tool: Pachctl CLI Console pachctl auth set project foo repoReader,repoWriter user:edna Open the Console UI. Navigate to the project you wish to grant user permissions to. Select Edit Project Permissions. Search for and select the user or user&rsquo;s group. Choose which permissions to grant from the dropdown. Select Add. How to Check Project Access for a User # Tool: Pachctl CLI pachctl auth check project foo user:bob ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "projects"
      ],
      "id": "8afde5058df652cc38284dcdb63569ea"
    },
    {
      "title": "Delete a Project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Project Ops",
      "description": "Learn how to delete a project.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/project-operations/delete-project/",
      "relURI": "/latest/build-dags/project-operations/delete-project/",
      "body": " How to Delete a Project # Tool: Pachctl CLI Console pachctl delete project &lt;project&gt; If the project you removed was set to your currently active context, make sure to assign a new one:\npachctl config update context --project foo Open Console. Navigate to Projects (the main page). Scroll to the project you want to delete. Select the ellipsis (&hellip;) icon and click Delete Project. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "projects"
      ],
      "id": "468679ef16e023c765648e8d6dc5fdac"
    },
    {
      "title": "Branch Ops",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Build Pipelines & DAGs",
      "description": "Learn how to create, delete, and update branches.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/branch-operations/",
      "relURI": "/latest/build-dags/branch-operations/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "edb591ff12cb1d5cda93aeacfba1d306"
    },
    {
      "title": "Copy Files",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Branch Ops",
      "description": "Learn how to copy files between branches.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/branch-operations/copy-files/",
      "relURI": "/latest/build-dags/branch-operations/copy-files/",
      "body": "Sometimes you need to be able to copy files between branches. This can be especially useful if you want to commit data in an ad-hoc, disorganized manner to a staging branch and then organize it later.\nWhen you run copy file, HPE ML Data Management only copies references to the files and does not move the actual data for the files around.\nHow to Copy Files Between Branches # Start a commit:\npachctl start commit data@master Copy files:\npachctl copy file data@staging:file1 data@master:file1 pachctl copy file data@staging:file2 data@master:file2 ... Close the commit:\npachctl finish commit data@master ‚ÑπÔ∏è While the commit is open, you can run pachctl delete file if you want to remove something from the parent commit or pachctl put file if you want to upload something that is not in a repo yet.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "deferred processing",
        "branches"
      ],
      "id": "c7be1e6e21d7fde49adc7a379c1dae34"
    },
    {
      "title": "Process Specific Commits",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Branch Ops",
      "description": "Learn how to process specific commits in a branch.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/branch-operations/process-specific-commits/",
      "relURI": "/latest/build-dags/branch-operations/process-specific-commits/",
      "body": "HPE ML Data Management enables you to process specific commits in a branch that are not the HEAD commit, which can be useful in many scenarios. For example, you can use this feature for:\nTesting and debugging: By processing specific intermediary commits, you can identify issues, test changes, or debug the pipeline at different stages of the data processing flow. This helps you ensure the quality and correctness of your data processing tasks.\nSelective processing: In some cases, you might want to process only specific subsets of data instead of the entire dataset. By targeting specific commits, you can selectively process the data as needed.\nHistorical data reprocessing: If your pipeline has been updated or you need to re-process data with new parameters, you can target specific commits to re-run the pipeline on previously processed data.\nResource utilization: By processing only the necessary commits, you can optimize the use of resources and reduce the overall time and cost of data processing.\nHow to Process Specific Commits # To process a specific commit, you need to set the master branch of your repo to have your specified commit as HEAD.\nFor example, if you submitted ten commits in the staging branch and you want to process the seventh, third, and most recent commits, you need to run the following commands respectively:\npachctl create branch data@master --head staging^7 pachctl create branch data@master --head staging^3 pachctl create branch data@master --head staging When you run the commands above, HPE ML Data Management creates a job for each of the commands one after another. Therefore, when one job is completed, HPE ML Data Management starts the next one. To verify that HPE ML Data Management created jobs for these commands, run the following:\npachctl list job -p &lt;pipeline_name&gt; --history all How to Change the Branch HEAD # You can move backward to previous commits as easily as advancing to the latest commits. For example, if you want to change the final output to be the result of processing staging^1, you can roll back your HEAD commit by running the following command:\npachctl create branch data@master --head staging^1 This command starts a new job to process staging^1. The HEAD commit on your output repo will be the result of processing staging^1 instead of staging.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "06d420c0d23c7bb2ca6a23b3ae2d8522"
    },
    {
      "title": "Set Branch Triggers",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Branch Ops",
      "description": "Learn how to use branch triggers to automate deferred processing.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/branch-operations/set-branch-triggers/",
      "relURI": "/latest/build-dags/branch-operations/set-branch-triggers/",
      "body": "You can automate re-pointing from one branch to another by using branch triggers. A branch trigger is a relationship between two branches, such as master and staging. When the head commit of staging meets a certain condition, it triggers master to update its head to that same commit. In other words, it does pachctl create branch data@master --head staging automatically when the trigger condition is met.\nYou can set branch triggers to fire when:\nA certain amount of time has passed (--trigger-cron) A specific number of commits have been made (--trigger-size) The amount of unprocessed data reaches a certain size (--trigger-commits) When more than one is specified, a branch repoint will be triggered when any of the conditions is met. To guarantee that they all must be met, add --trigger-all.\nHow to Automate Deferred Processing via Branch Triggers # Let&rsquo;s make the master branch automatically trigger when there&rsquo;s 1 Megabyte of new data on the staging branch.\nCreate a repo. pachctl create repo data Create a master branch with trigger settings (see pachctl create branch options) and a staging branch. pachctl create branch data@master --trigger staging --trigger-size 1MB pachctl create branch data@staging View your branches. pachctl list branch data BRANCH HEAD TRIGGER staging f35c5e5d6b4c499eaae0ab0c733ad7a6 - master 383c2acb298e4d6aa8327ea49aaeede6 staging on Size(1MB) üí° Triggers can point to branches that don&rsquo;t exist yet. You can test your trigger using a command similar to the following:\ndd if=/dev/urandom bs=1M count=1 | pachctl put file data@staging:/file pachctl list branch data BRANCH HEAD TRIGGER staging 5e27464aab4c4857bfd5d402afe043c6 - master 5e27464aab4c4857bfd5d402afe043c6 staging on Size(1MB) How to Manually Trigger Master # Triggers automate deferred processing, but they don&rsquo;t prevent manually updating the head of a branch. If you ever want to trigger master even though the trigger condition hasn&rsquo;t been met, you can run:\npachctl create branch data@master --head staging Notice that you don&rsquo;t need to re-specify the trigger when you call create branch to change the head. If you do want to clear the trigger delete the branch and recreate it.\nTo experiment further, see the full triggers example.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "deferred processing",
        "branches"
      ],
      "id": "63e94c4d0750233ef07e8b76fdad2c25"
    },
    {
      "title": "Set Output Branch",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Branch Ops",
      "description": "Learn how to use an output branch to defer processing.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/branch-operations/set-output-branch/",
      "relURI": "/latest/build-dags/branch-operations/set-output-branch/",
      "body": "You can defer processing operations for data in output repositories by configuring an output branch field in your pipeline specification. This allows you to accumulate data in an output branch before processing it.\nHow to Defer Processing in an Output Branch # In the pipeline specification, add the output_branch field with the name of the branch in which you want to accumulate your data before processing:\n&#34;output_branch&#34;: &#34;staging&#34; When you want to process data, run:\npachctl create branch pipeline@master --head staging ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "deferred processing",
        "branches"
      ],
      "id": "f8f0dc0a2a709cd86964560d231671d9"
    },
    {
      "title": "Datum Ops",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Build Pipelines & DAGs",
      "description": "Learn how to create, delete, and update datums.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/datum-operations/",
      "relURI": "/latest/build-dags/datum-operations/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "41c458b6af5c0f1923a7d7b605d76295"
    },
    {
      "title": "Get Metadata",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Datum Ops",
      "description": "Learn how to view additional datum metadata using the pachctl get file command.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/datum-operations/get-metadata/",
      "relURI": "/latest/build-dags/datum-operations/get-metadata/",
      "body": "Once a pipeline has finished a job, you can access additional execution metadata about the datums processed in the associated meta system repo. Note that all the inspect datum information above is stored in this meta repo, along with a couple more. For example, you can find the reason in meta/&lt;datumID&gt;/meta: the error message when the datum failed.\nSee the detail of the meta repo structure below.\nüìñ A meta repo contains 2 directories:\n/meta/: The meta directory holds datums&rsquo; statistics /pfs: The pfs directory holds the input data of datums, and their resulting output data pachctl list file edges.meta@master System response:\nNAME TAG TYPE SIZE /meta/ dir 1.956KiB /pfs/ dir 371.9KiB Meta directory # The meta directory holds each datum&rsquo;s JSON metadata, and can be accessed using a get file:\nExample # pachctl get file edges.meta@master:/meta/002f991aa9db9f0c44a92a30dff8ab22e788f86cc851bec80d5a74e05ad12868/meta | jq System response:\n{ &#34;job&#34;: { &#34;pipeline&#34;: { &#34;name&#34;: &#34;edges&#34; }, &#34;id&#34;: &#34;efca9595bdde4c0ba46a444a5877fdfe&#34; }, &#34;inputs&#34;: [ { &#34;fileInfo&#34;: { ... } ], &#34;hash&#34;: &#34;28e6675faba53383ac84b899d853bb0781c6b13a90686758ce5b3644af28cb62f763&#34;, &#34;stats&#34;: { &#34;downloadTime&#34;: &#34;0.103591200s&#34;, &#34;processTime&#34;: &#34;0.374824700s&#34;, &#34;uploadTime&#34;: &#34;0.001807800s&#34;, &#34;downloadBytes&#34;: &#34;80588&#34;, &#34;uploadBytes&#34;: &#34;38046&#34; }, &#34;index&#34;: &#34;1&#34; } Usepachctl list file edges.meta@master:/meta/ to list the files in the meta directory.\nPfs Directory # The pfs directory has both the input files of datums, and the resulting output files that were committed to the output repo:\nExample # pachctl list file montage.meta@master:/pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/ System response:\nNAME TAG TYPE SIZE /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/edges/ dir 133.6KiB /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/images/ dir 238.3KiB /pfs/47be06d9e614700397d8d56272a1a5e039df82bf931e8e3c9d34bccbfbc8b349/out/ dir 1.292MiB Use pachctl list file montage.meta@master:/pfs/ to list the files in the pfs directory.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "datums"
      ],
      "id": "ea3e99b96cc13a3377749a1c52565f64"
    },
    {
      "title": "Inspect Datum",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Datum Ops",
      "description": "Learn how to inspect a datum using the pachctl inspect datum command.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/datum-operations/inspect-datum/",
      "relURI": "/latest/build-dags/datum-operations/inspect-datum/",
      "body": "HPE ML Data Management stores information about each datum that a pipeline processes, including timing information, size information, and /pfs snapshots. You can view these statistics by running the pachctl inspect datum command (or its language client equivalents).\nIn particular, HPE ML Data Management provides the following information for each datum processed by your pipelines:\nThe amount of data that was uploaded and downloaded The time spend uploading and downloading data The total time spend processing Success/failure information, including any error encountered for failed datums The directory structure of input data that was seen by the job. Use the pachctl list datum &lt;pipeline&gt;@&lt;job ID&gt; to retrieve the list of datums processed by a given job, and pick the datum ID you want to inspect. That information can be useful when troubleshooting a failed job.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "datums"
      ],
      "id": "8f68d8c899bd417c79531bad7736db5c"
    },
    {
      "title": "Provenance Ops",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Build Pipelines & DAGs",
      "description": "Learn how to track and view the provenance of your data.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/provenance-operations/",
      "relURI": "/latest/build-dags/provenance-operations/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "projects"
      ],
      "id": "13949a2dbe9f3c02a16d1cec866936c4"
    },
    {
      "title": "List Global Commits & Jobs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Provenance Ops",
      "description": "Learn how to list global commits and jobs.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/provenance-operations/list-globals/",
      "relURI": "/latest/build-dags/provenance-operations/list-globals/",
      "body": " How to List Global Commits # You can list all global commits by running the following command:\npachctl list commit Each global commit displays how many (sub) commits it is made of.\nID SUBCOMMITS PROGRESS CREATED MODIFIED 1035715e796f45caae7a1d3ffd1f93ca 7 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 7 seconds ago 7 seconds ago 28363be08a8f4786b6dd0d3b142edd56 6 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 24 seconds ago 24 seconds ago e050771b5c6f4082aed48a059e1ac203 4 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 24 seconds ago 24 seconds ago How to List Global Jobs # Similarly, if you run the equivalent command for global jobs:\npachctl list job you will notice that the job IDs are shared with the global commit IDs.\nID SUBJOBS PROGRESS CREATED MODIFIED 1035715e796f45caae7a1d3ffd1f93ca 2 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 55 seconds ago 55 seconds ago 28363be08a8f4786b6dd0d3b142edd56 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá About a minute ago About a minute ago e050771b5c6f4082aed48a059e1ac203 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá About a minute ago About a minute ago For example, in this example, 7 commits and 2 jobs are involved in the changes occured in the global commit ID 1035715e796f45caae7a1d3ffd1f93ca.\n‚ÑπÔ∏è The progress bar is equally divided to the number of steps, or pipelines, you have in your DAG. In the example above,1035715e796f45caae7a1d3ffd1f93ca is two steps. If one of the sub-jobs fails, you will see the progress bar turn red for that pipeline step. To troubleshoot, look into that particular pipeline execution.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "provenance"
      ],
      "id": "2e881de4cc300a1850e6a2d1341e2a51"
    },
    {
      "title": "List Global ID Sub Commits",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Provenance Ops",
      "description": "Learn how to list global commits and jobs.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/provenance-operations/list-sub-commits/",
      "relURI": "/latest/build-dags/provenance-operations/list-sub-commits/",
      "body": " How to List Commits via Global ID # To list all (sub) commits involved in a global commit:\npachctl list commit 1035715e796f45caae7a1d3ffd1f93ca REPO BRANCH COMMIT FINISHED SIZE ORIGIN DESCRIPTION images master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 238.3KiB USER edges.spec master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 244B ALIAS montage.spec master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 405B ALIAS montage.meta master 1035715e796f45caae7a1d3ffd1f93ca 4 minutes ago 1.656MiB AUTO edges master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 133.6KiB AUTO edges.meta master 1035715e796f45caae7a1d3ffd1f93ca 5 minutes ago 373.9KiB AUTO montage master 1035715e796f45caae7a1d3ffd1f93ca 4 minutes ago 1.292MiB AUTO How to List Jobs via Global ID # Tto list all (sub) jobs linked to your global job ID:\npachctl list job 1035715e796f45caae7a1d3ffd1f93ca ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE 1035715e796f45caae7a1d3ffd1f93ca montage 5 minutes ago 4 seconds 0 1 + 0 / 1 79.49KiB 381.1KiB success 1035715e796f45caae7a1d3ffd1f93ca edges 5 minutes ago 2 seconds 0 1 + 0 / 1 57.27KiB 22.22KiB success For each pipeline execution (sub job) within this global job, HPE ML Data Management shows the time since each sub job started and its duration, the number of datums in the PROGRESS section, and other information. The format of the progress column is DATUMS PROCESSED + DATUMS SKIPPED / TOTAL DATUMS.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "provenance"
      ],
      "id": "a0d8cde8b63fecd52b0097095d872f5f"
    },
    {
      "title": "Track Downstream",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Provenance Ops",
      "description": "Learn how to track downstream commits and jobs.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/provenance-operations/track-downstream/",
      "relURI": "/latest/build-dags/provenance-operations/track-downstream/",
      "body": " Track Provenance Downstream # HPE ML Data Management provides the wait commit &lt;commitID&gt; command that enables you to track your commits downstream as they are produced.\nUnlike the list commit &lt;commitID&gt;, each line is printed as soon as a new (sub) commit of your global commit finishes.\nChange commit in job to list the jobs related to your global job as they finish processing a commit.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "provenance"
      ],
      "id": "8b536af2875ab9bd34386b6edce2511f"
    },
    {
      "title": "Delete Branch Head",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Provenance Ops",
      "description": "Learn how to delete the HEAD of a branch.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/provenance-operations/delete-head/",
      "relURI": "/latest/build-dags/provenance-operations/delete-head/",
      "body": "To fix a broken HEAD, run the following command:\npachctl delete commit &lt;commit-ID&gt; When you delete a HEAD commit, HPE ML Data Management performs the following actions:\nChanges HEADs of all the branches that had the bad commit as their HEAD to their bad commit&rsquo;s parent and deletes the commit. The data in the deleted commit is lost. If the bad commit does not have a parent, HPE ML Data Management sets the branch&rsquo;s HEAD to a new empty commit. Interrupts all running jobs, including not only the jobs that use the bad commit as a direct input but also the ones farther downstream in your DAG. Deletes the output commits from the deleted jobs. All the actions listed above are applied to those commits as well. ‚ö†Ô∏è This command will only succeed if the HEAD commit has no children on any branch. pachctl delete commit will error when attempting to delete a HEAD commit with children.\n‚ÑπÔ∏è Are you wondering how a HEAD commit can have children?\nA commit can be the head of a branch and still have children. For instance, given a master branch in a repository named repo, if you branch master by running pachctl create branch repo@staging --head repo@master, the master&rsquo;s HEAD will have an alias child on staging.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "data-operations"
      ],
      "id": "2752cf14d981313521eb8104bc9c8b1c"
    },
    {
      "title": "Squash Non-Head Commits",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Provenance Ops",
      "description": "Learn how to squash commits that are not the HEAD of a branch.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/provenance-operations/squash-nonhead/",
      "relURI": "/latest/build-dags/provenance-operations/squash-nonhead/",
      "body": "If your commit has children, you have the option to use the squash commit command. Squashing is a way to rewrite your commit history; this helps clean up and simplify your commit history before sharing your work with team members.\nSquashing a commit in HPE ML Data Management means that you are combining all the file changes in the commits of a global commit into their children and then removing the global commit. This behavior is inspired by the squash option in git rebase. No data stored in PFS is removed since they remain in the child commits.\npachctl squash commit &lt;commit-ID&gt; Example # In the simple example below, we create three successive commits on the master branch of a repo repo:\nIn commit ID1, we added files A and B. In commit ID2, we added file C. In commit ID3, the latest commit, we altered the content of files A and C. We then run pachctl squash commit ID1, then pachctl squash commit ID2, and look at our branch and remaining commit(s).\nA‚Äô and C&rsquo; are altered versions of files A and C. At any moment, pachctl list file repo@master invariably returns the same files A‚Äô, B, C‚Äô. pachctl list commit however, differs in each case, since, by squashing commits, we have deleted them from the branch.\nConsiderations # Squashing a global commit on the head of a branch (no children) will fail. Use pachctl delete commit instead. pachctl squash commit stops (but does not delete) associated jobs. Limitations # Squash commit only applies to user repositories. For example, you cannot squash a commit that updated a pipeline (Commit that lives in a spec repository). You cannot squash a commit set that contains a commit that is a dependency for another commit set. For example, if you have RepoB@master that depends on RepoA@master as an input, you cannot squash the commit set for RepoA@master. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "data-operations"
      ],
      "id": "e93f406d4b33c57e674cb69b35e6e576"
    },
    {
      "title": "Delete File From History",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Provenance Ops",
      "description": "Learn how to delete a file from history.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/provenance-operations/delete-history/",
      "relURI": "/latest/build-dags/provenance-operations/delete-history/",
      "body": " üìñ It is important to note that this use case is limited to simple cases where the &ldquo;bad&rdquo; changes were made relatively recently, as any pipeline update since then will make it impossible.\nIn rare cases, you might need to delete a particular file from a given commit and further choose to delete its complete history. In such a case, you will need to:\nCreate a new commit in which you surgically remove the problematic file.\nStart a new commit:\npachctl start commit &lt;repo&gt;@&lt;branch&gt; Delete all corrupted files from the newly opened commit:\npachctl delete file &lt;repo&gt;@&lt;branch or commitID&gt;:/path/to/files Finish the commit:\npachctl finish commit &lt;repo&gt;@&lt;branch&gt; Optionally, wipe this file from your history by squashing the initial bad commit and all its children up to the newly finished commit.\nUnless the subsequent commits overwrote or deleted the bad data, the data might still be present in the children commits. Squashing those commits cleans up your commit history and ensures that the errant data is not available when non-HEAD versions of the data are read.\nExample # In the simple example below, we want to delete file C in commit 2. How would we do that?\nFor now, pachctl list file repo@master returns the files A‚Äô, B, C‚Äô, E, F.\nA‚Äô and C&rsquo; are altered versions of files A and C. We create a new commit in which we surgically remove file C:\npachctl start commit repo@master pachctl delete file repo@master:path/to/C pachctl finish commit repo@master At this point, pachctl list file repo@master returns the files A‚Äô, B, E, F. We removed file C. However, it still exists in the commit history.\nTo remove C from the commit history, we squash the commits in which C appears, all the way down to the last commit.\npachctl squash commitID2 pachctl squash commitID3 It is as if C never existed.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "data-operations"
      ],
      "id": "c05941ab7a5ce463407f7ef7632a85a1"
    },
    {
      "title": "Tutorials",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Build Pipelines & DAGs",
      "description": "Follow these tutorials to learn how to build ML, NLP, and other pipelines.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/tutorials/",
      "relURI": "/latest/build-dags/tutorials/",
      "body": "The following pipeline tutorials in this section walk you through how to build a variety of pipelines using HPE ML Data Management. It is recommended to complete these tutorials in order from left to right, as each tutorial builds upon the skills and concepts learned in the previous tutorial.\nIf you are already familiar with how to build your own Docker images, you can start with the Standard ML Pipeline tutorial.\n",
      "beta": "false",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "automl",
        "mljar"
      ],
      "id": "8078fa7397ba6a384bb3f35b441694a9"
    },
    {
      "title": "Standard ML Pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Tutorials",
      "description": "Learn how to build a basic machine learning pipeline.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/tutorials/basic-ml/",
      "relURI": "/latest/build-dags/tutorials/basic-ml/",
      "body": "In this tutorial, we&rsquo;ll build a simple machine learning pipeline in HPE ML Data Management to train a regression model on housing market data to predict the value of homes in Boston.\nBefore You Start # You must have a HPE ML Data Management cluster up and running You should have some basic familiarity with HPE ML Data Management pipeline specs &ndash; see the Transform and PFS Input sections in particular Tutorial # Our Docker image&rsquo;s user code for this tutorial is built on top of the civisanalytics/datascience-python base image, which includes the necessary dependencies. It uses pandas to import the structured dataset and the scikit-learn library to train the model.\n1. Create a Project &amp; Input Repo # Create a project named standard-ml-tutorial. pachctl create project standard-ml-tutorial Set the project as current. pachctl config update context --project standard-ml-tutorial Create a repo named housing_data. pachctl create repo housing_data 2. Create a Regression Pipeline # Create a file named regression.json with the following contents:\n# regression.json { &#34;pipeline&#34;: { &#34;name&#34;: &#34;regression&#34; }, &#34;description&#34;: &#34;A pipeline that trains produces a regression model for housing prices.&#34;, &#34;input&#34;: { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;housing_data&#34; } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python&#34;, &#34;regression.py&#34;, &#34;--input&#34;, &#34;/pfs/housing_data/&#34;, &#34;--target-col&#34;, &#34;MEDV&#34;, &#34;--output&#34;, &#34;/pfs/out/&#34; ], &#34;image&#34;: &#34;pachyderm/housing-prices:1.11.0&#34; } } Save the file.\nRun the following command to create the pipeline:\npachctl create pipeline -f regression.json üí° The pipeline writes the output to a PFS repo (/pfs/out/) created with the same name as the pipeline.\n3. Upload the Housing Dataset # Download our first example data set, housing-simplified-1.csv.\nAdd the data to your repo. Processing begins automatically &mdash; anytime you add new data, the pipeline will re-run.\npachctl put file housing_data@master:housing-simplified.csv -f /path/to/housing-simplified-1.csv Verify that the data is in the repository.\npachctl list file housing_data@master # NAME TYPE SIZE # /housing-simplified.csv file 2.482KiB Verify that the pipeline is running by looking at the status of the job(s).\npachctl list job # ID SUBJOBS PROGRESS CREATED MODIFIED # e7dd14d201a64edc8bf61beed6085ae0 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 48 seconds ago 48 seconds ago # df117068124643299d46530859851a4b 1 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá About a minute ago About a minute ago 4. Download Output Files # Once the pipeline is completed, we can download the files that were created.\nView a list of the files in the output repo. pachctl list file regression@master # NAME TYPE SIZE # /housing-simplified_corr_matrix.png file 18.66KiB # /housing-simplified_cv_reg_output.png file 86.07KiB # /housing-simplified_model.sav file 798.5KiB # /housing-simplified_pairplot.png file 100.8KiB Download the files. pachctl get file regression@master:/ --recursive --output . When we inspect the learning curve, we can see that there is a large gap between the training score and the validation score. This typically indicates that our model could benefit from the addition of more data.\nNow let&rsquo;s update our dataset with additional examples.\n5. Update the Dataset # Download our second example data set, housing-simplified-2.csv.\nAdd the data to your repo.\npachctl put file housing_data@master:housing-simplified.csv -f /path/to/housing-simplified-2.csv ‚ÑπÔ∏è We could also append new examples to the existing file, but in this tutorial we&rsquo;re overwriting our previous file to one with more data.\nThis is where HPE ML Data Management truly starts to shine. The new commit of data to the housing_data repository automatically kicks off a job on the regression pipeline without us having to do anything.\nWhen the job is complete we can download the new files and see that our model has improved, given the new learning curve.\n6. Inspect the Pipeline Lineage # Since the pipeline versions all of our input and output data automatically, we can continue to iterate on our data and code while HPE ML Data Management tracks all of our experiments.\nFor any given output commit, HPE ML Data Management can tell us exactly which input commit of data was run. In this tutorial, we have only run 2 experiments so far, but this becomes incredibly valuable as your experiments continue to evolve and scale.\nInspect the commits to your repo.\npachctl list commit # ID SUBCOMMITS PROGRESS CREATED MODIFIED # 3037785cc56c4387bbb897f1887b4a68 4 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 11 seconds ago 11 seconds ago # e7dd14d201a64edc8bf61beed6085ae0 4 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá About a minute ago About a minute ago # df117068124643299d46530859851a4b 4 ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá 2 minutes ago 2 minutes ago Use the commit ID to check for what dataset was used to create the model.\npachctl list file housing_data@3037785cc56c4387bbb897f1887b4a68 # NAME TYPE SIZE # /housing-simplified.csv file 12.14KiB Use the commit ID to check the commit&rsquo;s details (such as parent commit, branch, size, etc.)\npachctl inspect commit housing_data@3037785cc56c4387bbb897f1887b4a68 # Commit: housing_data@3037785cc56c4387bbb897f1887b4a68 # Original Branch: master # Parent: e7dd14d201a64edc8bf61beed6085ae0 # Started: 2 minutes ago # Finished: 2 minutes ago # Size: 12.14KiB User Code Assets # The Docker image used in this tutorial was built with the following assets:\nAssets: Dockerfile Regression.py Utils.py FROM civisanalytics/datascience-python RUN pip install seaborn WORKDIR /workdir/ COPY *.py /workdir/ import argparse import os from os import path import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import joblib from utils import plot_learning_curve import subprocess from sklearn.model_selection import ShuffleSplit from sklearn import datasets, ensemble, linear_model from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import cross_val_score parser = argparse.ArgumentParser(description=&#34;Structured data regression&#34;) parser.add_argument(&#34;--input&#34;, type=str, help=&#34;csv file with all examples&#34;) parser.add_argument(&#34;--target-col&#34;, type=str, help=&#34;column with target values&#34;) parser.add_argument(&#34;--output&#34;, metavar=&#34;DIR&#34;, default=&#39;./output&#39;, help=&#34;output directory&#34;) def load_data(input_csv, target_col): # Load the Boston housing dataset data = pd.read_csv(input_csv, header=0) targets = data[target_col] features = data.drop(target_col, axis = 1) print(&#34;Dataset has {} data points with {} variables each.&#34;.format(*data.shape)) return data, features, targets def create_pairplot(data): plt.clf() # Calculate and show pairplot sns.pairplot(data, height=2.5) plt.tight_layout() def create_corr_matrix(data): plt.clf() # Calculate and show correlation matrix sns.set() corr = data.corr() # Generate a mask for the upper triangle mask = np.triu(np.ones_like(corr, dtype=np.bool)) # Generate a custom diverging colormap cmap = sns.diverging_palette(220, 10, as_cmap=True) # Draw the heatmap with the mask and correct aspect ratio sns_plot = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, annot=True, cbar_kws={&#34;shrink&#34;: .5}) def train_model(features, targets): # Train a Random Forest Regression model reg = ensemble.RandomForestRegressor(random_state=1) scores = cross_val_score(reg, features, targets, cv=10) print(&#34;Score: {:2f} (+/- {:2f})&#34;.format(scores.mean(), scores.std() * 2)) reg.fit(features,targets) return reg def create_learning_curve(estimator, features, targets): plt.clf() title = &#34;Learning Curves (Random Forest Regressor)&#34; cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0) plot_learning_curve(estimator, title, features, targets, ylim=(0.5, 1.01), cv=cv, n_jobs=4) def main(): print(&#34;User code is starting&#34;) subprocess.run([&#34;pachctl&#34;, &#34;connect&#34;, &#34;grpc://localhost:1650&#34;]) print(&#34;starting while loop&#34;) while True: subprocess.run([&#34;pachctl&#34;, &#34;next&#34;, &#34;datum&#34;]) print(&#34;next datum called&#34;) args = parser.parse_args() if os.path.isfile(args.input): input_files = [args.input] else: # Directory for dirpath, dirs, files in os.walk(args.input): input_files = [ os.path.join(dirpath, filename) for filename in files if filename.endswith(&#39;.csv&#39;) ] print(&#34;Datasets: {}&#34;.format(input_files)) os.makedirs(args.output, exist_ok=True) for filename in input_files: experiment_name = os.path.basename(os.path.splitext(filename)[0]) # Data loading and Exploration data, features, targets = load_data(filename, args.target_col) create_pairplot(data) plt.savefig(path.join(args.output,experiment_name + &#39;_pairplot.png&#39;)) create_corr_matrix(data) plt.savefig(path.join(args.output, experiment_name + &#39;_corr_matrix.png&#39;)) # Fit model reg = train_model(features, targets) create_learning_curve(reg, features, targets) plt.savefig(path.join(args.output, experiment_name + &#39;_cv_reg_output.png&#39;)) # Save model joblib.dump(reg, path.join(args.output,experiment_name + &#39;_model.sav&#39;)) if __name__ == &#34;__main__&#34;: main() import numpy as np import matplotlib.pyplot as plt from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from sklearn.datasets import load_digits from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)): &#34;&#34;&#34; Generate 3 plots: the test and training learning curve, the training samples vs fit times curve, the fit times vs score curve. Parameters ---------- estimator : object type that implements the &#34;fit&#34; and &#34;predict&#34; methods An object of that type which is cloned for each validation. title : string Title for the chart. X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape (n_samples) or (n_samples, n_features), optional Target relative to X for classification or regression; None for unsupervised learning. axes : array of 3 axes, optional (default=None) Axes to use for plotting the curves. ylim : tuple, shape (ymin, ymax), optional Defines minimum and maximum yvalues plotted. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if ``y`` is binary or multiclass, :class:`StratifiedKFold` used. If the estimator is not a classifier or if ``y`` is neither binary nor multiclass, :class:`KFold` is used. Refer :ref:`User Guide &lt;cross_validation&gt;` for the various cross-validators that can be used here. n_jobs : int or None, optional (default=None) Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` for more details. train_sizes : array-like, shape (n_ticks,), dtype float or int Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class. (default: np.linspace(0.1, 1.0, 5)) &#34;&#34;&#34; if axes is None: _, axes = plt.subplots(1, 3, figsize=(20, 5)) axes[0].set_title(title) if ylim is not None: axes[0].set_ylim(*ylim) axes[0].set_xlabel(&#34;Training examples&#34;) axes[0].set_ylabel(&#34;Score&#34;) train_sizes, train_scores, test_scores, fit_times, _ = \\ learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, return_times=True) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) fit_times_mean = np.mean(fit_times, axis=1) fit_times_std = np.std(fit_times, axis=1) # Plot learning curve axes[0].grid() axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=&#34;r&#34;) axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=&#34;g&#34;) axes[0].plot(train_sizes, train_scores_mean, &#39;o-&#39;, color=&#34;r&#34;, label=&#34;Training score&#34;) axes[0].plot(train_sizes, test_scores_mean, &#39;o-&#39;, color=&#34;g&#34;, label=&#34;Cross-validation score&#34;) axes[0].legend(loc=&#34;best&#34;) # Plot n_samples vs fit_times axes[1].grid() axes[1].plot(train_sizes, fit_times_mean, &#39;o-&#39;) axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std, fit_times_mean + fit_times_std, alpha=0.1) axes[1].set_xlabel(&#34;Training examples&#34;) axes[1].set_ylabel(&#34;fit_times&#34;) axes[1].set_title(&#34;Scalability of the model&#34;) # Plot fit_time vs score axes[2].grid() axes[2].plot(fit_times_mean, test_scores_mean, &#39;o-&#39;) axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1) axes[2].set_xlabel(&#34;fit_times&#34;) axes[2].set_ylabel(&#34;Score&#34;) axes[2].set_title(&#34;Performance of the model&#34;) return plt ",
      "beta": "false",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "automl",
        "mljar"
      ],
      "id": "d09917d1d60d342a2a54e6628392a766"
    },
    {
      "title": "AutoML Pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Tutorials",
      "description": "Learn how to build an automated machine learning pipeline.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/tutorials/auto-ml/",
      "relURI": "/latest/build-dags/tutorials/auto-ml/",
      "body": "You can use HPE ML Data Management to build an automated machine learning pipeline that trains a model on a CSV file.\nBefore You Start # You must have HPE ML Data Management installed and running on your cluster You should have already completed the Standard ML Pipeline tutorial You must be familiar with jsonnet This tutorial assumes your active context is localhost:80 Tutorial # Our Docker image&rsquo;s user code for this tutorial is built on top of the python:3.7-slim-buster base image. It also uses the mljar-supervised package to perform automated feature engineering, model selection, and hyperparameter tuning, making it easy to train high-quality machine learning models on structured data.\n1. Create a Project &amp; Input Repo # Create a project named automl-tutorial. pachctl create project automl-tutorial Set the project as current. pachctl config update context --project automl-tutorial Create a new csv-data repo. pachctl create repo csv-data Upload the housing-simplified-1.csv file to the repo. pachctl put file csv_data@master:housing-simplified.csv -f /path/to/housing-simplified-1.csv 2. Create a Jsonnet Pipeline # Download or save our automl.jsonnet template.\n//// // Template arguments: // // name : The name of this pipeline, for disambiguation when // multiple instances are created. // input : the repo from which this pipeline will read the csv file to which // it applies automl. // target_col : the column of the csv to be used as the target // args : additional parameters to pass to the automl regressor (e.g. &#34;--random_state 42&#34;) //// function(name=&#39;regression&#39;, input, target_col, args=&#39;&#39;) { pipeline: { name: name}, input: { pfs: { glob: &#34;/&#34;, repo: input } }, transform: { cmd: [ &#34;python&#34;,&#34;/workdir/automl.py&#34;,&#34;--input&#34;,&#34;/pfs/&#34;+input+&#34;/&#34;, &#34;--target-col&#34;, target_col, &#34;--output&#34;,&#34;/pfs/out/&#34;]+ std.split(args, &#39; &#39;), image: &#34;jimmywhitaker/automl:dev0.02&#34; } } Create the AutoML pipeline by referencing and filling out the template&rsquo;s arguments:\npachctl update pipeline --jsonnet /path/to/automl.jsonnet \\ --arg name=&#34;regression&#34; \\ --arg input=&#34;csv_data&#34; \\ --arg target_col=&#34;MEDV&#34; \\ --arg args=&#34;--mode Explain --random_state 42&#34; The model automatically starts training. Once complete, the trained model and evaluation metrics are output to the AutoML output repo.\n3. Upload the Dataset # Update the dataset using housing-simplified-2.csv; HPE ML Data Management retrains the model automatically. pachctl put file csv_data@master:housing-simplified.csv -f /path/to/housing-simplified-2.csv Repeat the previous step as many times as you want. Each time, HPE ML Data Management automatically retrains the model and outputs the new model and evaluation metrics to the AutoML output repo. User Code Assets # The Docker image used in this tutorial was built with the following assets:\nAssets: Dockerfile Automl.py Requirements.txt FROM python:3.7-slim-buster RUN apt-get update &amp;&amp; apt-get -y update RUN apt-get install -y build-essential python3-pip python3-dev RUN pip3 -q install pip --upgrade WORKDIR /workdir/ COPY requirements.txt /workdir/ RUN pip3 install -r requirements.txt COPY *.py /workdir/ import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from supervised.automl import AutoML import argparse import os parser = argparse.ArgumentParser(description=&#34;Structured data regression&#34;) parser.add_argument(&#34;--input&#34;, type=str, help=&#34;csv file with all examples&#34;) parser.add_argument(&#34;--target-col&#34;, type=str, help=&#34;column with target values&#34;) parser.add_argument(&#34;--mode&#34;, type=str, default=&#39;Explain&#39;, help=&#34;mode&#34;) parser.add_argument(&#34;--random_state&#34;, type=int, default=42, help=&#34;random seed&#34;) parser.add_argument(&#34;--output&#34;, metavar=&#34;DIR&#34;, default=&#39;./output&#39;, help=&#34;output directory&#34;) def load_data(input_csv, target_col): # Load the data data = pd.read_csv(input_csv, header=0) targets = data[target_col] features = data.drop(target_col, axis = 1) # Create data splits X_train, X_test, y_train, y_test = train_test_split( features, targets, test_size=0.25, random_state=123, ) return X_train, X_test, y_train, y_test def main(): args = parser.parse_args() if os.path.isfile(args.input): input_files = [args.input] else: # Directory for dirpath, dirs, files in os.walk(args.input): input_files = [ os.path.join(dirpath, filename) for filename in files if filename.endswith(&#39;.csv&#39;) ] print(&#34;Datasets: {}&#34;.format(input_files)) os.makedirs(args.output, exist_ok=True) for filename in input_files: experiment_name = os.path.basename(os.path.splitext(filename)[0]) # Data loading and Exploration X_train, X_test, y_train, y_test = load_data(filename, args.target_col) # Fit model automl = AutoML(total_time_limit=60*60, results_path=args.output) # 1 hour automl.fit(X_train, y_train) # compute the MSE on test data predictions = automl.predict_all(X_test) print(&#34;Test MSE:&#34;, mean_squared_error(y_test, predictions)) if __name__ == &#34;__main__&#34;: main() mljar-supervised ",
      "beta": "false",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "automl",
        "mljar"
      ],
      "id": "78ece1ba158c7c2774e6a1ec38d6c4fa"
    },
    {
      "title": "Multi-Pipeline DAG",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Tutorials",
      "description": "Learn how to build a DAG with multiple pipelines.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/tutorials/multi-ml/",
      "relURI": "/latest/build-dags/tutorials/multi-ml/",
      "body": "In this tutorial, we&rsquo;ll build a multi-pipeline DAG to train a regression model on housing market data to predict the value of homes in Boston. This tutorial builds on the skills learned from the previous tutorials, (Standard ML Pipeline and AutoML Pipeline.\nBefore You Start # You must have HPE ML Data Management installed and running on your cluster You should have already completed the Standard ML Pipeline tutorial You must be familiar with jsonnet This tutorial assumes your active context is localhost:80 Tutorial # Our Docker image&rsquo;s user code for this tutorial is built on top of the civisanalytics/datascience-python base image, which includes the necessary dependencies. It uses pandas to import the structured dataset and the scikit-learn library to train the model.\nEach pipeline in this tutorial executes a Python script, versions the artifacts (datasets, models, etc.), and gives you a full lineage of the model. Once it is set up, you can change, add, or remove data and Pachyderm will automatically keep everything up to date, creating data splits, computing data analysis metrics, and training the model.\n1. Create an Input Repo # Create a project named multipipeline-tutorial.\npachctl create project multipipeline-tutorial Set the project as current.\npachctl config update context --project multipipeline-tutorial Create a new data repository called csv_data where we will put our dataset.\npachctl create repo csv_data 2. Create the Pipelines # We&rsquo;ll deploy each stage in our ML process as a Pachyderm pipeline. Organizing our work into pipelines allows us to keep track of artifacts created in our ML development process. We can extend or add pipelines at any point to add new functionality or features, while keeping track of code and data changes simultaneously.\n1. Data Analysis Pipeline # The data analysis pipeline creates a pair plot and a correlation matrix showing the relationship between features. By seeing what features are positively or negatively correlated to the target value (or each other), it can helps us understand what features may be valuable to the model.\nCreate a file named data_analysis.json with the following contents:\n{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;data_analysis&#34; }, &#34;description&#34;: &#34;Data analysis pipeline that creates pairplots and correlation matrices for csv files.&#34;, &#34;input&#34;: { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;csv_data&#34; } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python&#34;, &#34;data_analysis.py&#34;, &#34;--input&#34;, &#34;/pfs/csv_data/&#34;, &#34;--target-col&#34;, &#34;MEDV&#34;, &#34;--output&#34;, &#34;/pfs/out/&#34; ], &#34;image&#34;: &#34;jimmywhitaker/housing-prices-int:dev0.2&#34; } } Save the file.\nCreate the pipeline.\npachctl create pipeline -f /path/to/data_analysis.json 2. Split Pipeline # Split the input csv files into train and test sets. As we new data is added, we will always have access to previous versions of the splits to reproduce experiments and test results.\nBoth the split pipeline and the data_analysis pipeline take the csv_data as input but have no dependencies on each other. Pachyderm is able to recognize this. It can run each pipeline simultaneously, scaling each horizontally.\nCreate a file named split.json with the following contents:\n{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;split&#34; }, &#34;description&#34;: &#34;A pipeline that splits tabular data into training and testing sets.&#34;, &#34;input&#34;: { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/*&#34;, &#34;repo&#34;: &#34;csv_data&#34; } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python&#34;, &#34;split.py&#34;, &#34;--input&#34;, &#34;/pfs/csv_data/&#34;, &#34;--test-size&#34;, &#34;0.1&#34;, &#34;--output&#34;, &#34;/pfs/out/&#34; ], &#34;image&#34;: &#34;jimmywhitaker/housing-prices-int:dev0.2&#34; } } Save the file.\nCreate the pipeline.\npachctl create pipeline -f /path/to/split.json 3. Regression Pipeline # To train the regression model using scikit-learn. In our case, we will train a Random Forest Regressor ensemble. After splitting the data into features and targets (X and y), we can fit the model to our parameters. Once the model is trained, we will compute our score (r^2) on the test set.\nAfter the model is trained we output some visualizations to evaluate its effectiveness of it using the learning curve and other statistics.\nCreate a file named regression.json with the following contents:\n{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;regression&#34; }, &#34;description&#34;: &#34;A pipeline that trains and tests a regression model for tabular.&#34;, &#34;input&#34;: { &#34;pfs&#34;: { &#34;glob&#34;: &#34;/*/&#34;, &#34;repo&#34;: &#34;split&#34; } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;python&#34;, &#34;regression.py&#34;, &#34;--input&#34;, &#34;/pfs/split/&#34;, &#34;--target-col&#34;, &#34;MEDV&#34;, &#34;--output&#34;, &#34;/pfs/out/&#34; ], &#34;image&#34;: &#34;jimmywhitaker/housing-prices-int:dev0.2&#34; } } Save the file.\nCreate the pipeline.\npachctl create pipeline -f /path/to/regression.json 3. Upload the Dataset # Download our first example data set, housing-simplified-1.csv. Add the data to your repo. pachctl put file csv_data@master:housing-simplified.csv -f /path/to/housing-simplified-1.csv 4. Download the Results # Once the pipeline has finished, download the results.\npachctl get file regression@master:/ --recursive --output . 5. Update the Dataset # Download our second example data set, housing-simplified-2.csv. Add the data to your repo. pachctl put file csv_data@master:housing-simplified.csv -f /path/to/housing-simplified-2.csv 6. Inspect the Data # We can use the diff command and ancestry syntax to see what has changed between the two versions of the dataset.\npachctl diff file csv_data@master csv_data@master^ Bonus Step: Rolling Back # If you need to roll back to a previous dataset commit, you can do so with the create branch command and ancestry syntax.\npachctl create branch csv_data@master --head csv_data@master^ User Code Assets # The Docker image used in this tutorial was built with the following assets:\nAssets: Dockerfile Regression.py Split.py Utils.py Nb_utils.py FROM civisanalytics/datascience-python RUN pip install seaborn WORKDIR /workdir/ COPY *.py /workdir/ import argparse import os from os import path import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import joblib from utils import plot_learning_curve from sklearn.model_selection import ShuffleSplit from sklearn import datasets, ensemble, linear_model from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import cross_val_score from sklearn.metrics import r2_score parser = argparse.ArgumentParser(description=&#34;Structured data regression&#34;) parser.add_argument(&#34;--input&#34;, type=str, help=&#34;directory with train.csv and test.csv&#34;) parser.add_argument(&#34;--target-col&#34;, type=str, default=&#34;MEDV&#34;, help=&#34;column with target values&#34;) parser.add_argument(&#34;--output&#34;, metavar=&#34;DIR&#34;, default=&#39;./output&#39;, help=&#34;output directory&#34;) def load_data(input_csv, target_col): # Load the Boston housing dataset data = pd.read_csv(input_csv, header=0) targets = data[target_col] features = data.drop(target_col, axis = 1) return data, features, targets def train_model(features, targets): # Train a Random Forest Regression model reg = ensemble.RandomForestRegressor(random_state=1) scores = cross_val_score(reg, features, targets, cv=10) print(&#34;Cross Val Score: {:2f} (+/- {:2f})&#34;.format(scores.mean(), scores.std() * 2)) reg.fit(features,targets) return reg def test_model(model, features, targets): # Train a Random Forest Regression model score = r2_score(model.predict(features), targets) return &#34;Test Score: {:2f}&#34;.format(score) def create_learning_curve(estimator, features, targets): plt.clf() title = &#34;Learning Curves (Random Forest Regressor)&#34; cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0) plot_learning_curve(estimator, title, features, targets, ylim=(0.5, 1.01), cv=cv, n_jobs=4) def main(): args = parser.parse_args() input_dirs = [] file_list = os.listdir(args.input) if &#39;train.csv&#39; in file_list and &#39;test.csv&#39; in file_list: input_dirs = [args.input] else: # Directory of directories for root, dirs, files in os.walk(args.input): for dir in dirs: file_list = os.listdir(os.path.join(root, dir)) if &#39;train.csv&#39; in file_list and &#39;test.csv&#39; in file_list: input_dirs.append(os.path.join(root,dir)) print(&#34;Datasets: {}&#34;.format(input_dirs)) os.makedirs(args.output, exist_ok=True) for dir in input_dirs: experiment_name = os.path.basename(os.path.splitext(dir)[0]) train_filename = os.path.join(dir,&#39;train.csv&#39;) test_filename = os.path.join(dir,&#39;test.csv&#39;) # Data loading train_data, train_features, train_targets = load_data(train_filename, args.target_col) print(&#34;Training set has {} data points with {} variables each.&#34;.format(*train_data.shape)) test_data, test_features, test_targets = load_data(test_filename, args.target_col) print(&#34;Testing set has {} data points with {} variables each.&#34;.format(*test_data.shape)) reg = train_model(train_features, train_targets) test_results = test_model(reg, test_features, test_targets) create_learning_curve(reg, train_features, train_targets) plt.savefig(path.join(args.output, experiment_name + &#39;_cv_reg_output.png&#39;)) print(test_results) # Save model and test score joblib.dump(reg, path.join(args.output, experiment_name + &#39;_model.sav&#39;)) with open(path.join(args.output, experiment_name + &#39;_test_results.txt&#39;), &#34;w&#34;) as text_file: text_file.write(test_results) if __name__ == &#34;__main__&#34;: main() import argparse import os from os import path import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import joblib from utils import load_data from sklearn.model_selection import train_test_split parser = argparse.ArgumentParser(description=&#34;Structured data regression&#34;) parser.add_argument(&#34;--input&#34;, type=str, help=&#34;csv file with all examples&#34;) parser.add_argument(&#34;--output&#34;, metavar=&#34;DIR&#34;, default=&#39;./output&#39;, help=&#34;output directory&#34;) parser.add_argument(&#34;--test-size&#34;, type=float, default=0.2, help=&#34;percentage of data to use for testing (\\&#34;0.2\\&#34; = 20% used for testing, 80% for training&#34;) parser.add_argument(&#34;--seed&#34;, type=int, default=42, help=&#34;random seed&#34;) def main(): args = parser.parse_args() if os.path.isfile(args.input): input_files = [args.input] else: # Directory for dirpath, dirs, files in os.walk(args.input): input_files = [ os.path.join(dirpath, filename) for filename in files if filename.endswith(&#39;.csv&#39;) ] print(&#34;Datasets: {}&#34;.format(input_files)) for filename in input_files: file_basename = os.path.basename(os.path.splitext(filename)[0]) os.makedirs(os.path.join(args.output,file_basename), exist_ok=True) # Data loading data = load_data(filename) train, test = train_test_split(data, test_size=args.test_size, random_state=args.seed) train.to_csv(os.path.join(args.output, file_basename, &#39;train.csv&#39;), header=True, index=False) test.to_csv(os.path.join(args.output, file_basename, &#39;test.csv&#39;), header=True, index=False) if __name__ == &#34;__main__&#34;: main() import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from sklearn.datasets import load_digits from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit def load_data(input_csv, target_col=None): # Load the Boston housing dataset data = pd.read_csv(input_csv, header=0) if target_col: targets = data[target_col] features = data.drop(target_col, axis = 1) print(&#34;Dataset has {} data points with {} variables each.&#34;.format(*data.shape)) return data, features, targets return data def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)): &#34;&#34;&#34; Generate 3 plots: the test and training learning curve, the training samples vs fit times curve, the fit times vs score curve. Parameters ---------- estimator : object type that implements the &#34;fit&#34; and &#34;predict&#34; methods An object of that type which is cloned for each validation. title : string Title for the chart. X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape (n_samples) or (n_samples, n_features), optional Target relative to X for classification or regression; None for unsupervised learning. axes : array of 3 axes, optional (default=None) Axes to use for plotting the curves. ylim : tuple, shape (ymin, ymax), optional Defines minimum and maximum yvalues plotted. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if ``y`` is binary or multiclass, :class:`StratifiedKFold` used. If the estimator is not a classifier or if ``y`` is neither binary nor multiclass, :class:`KFold` is used. Refer :ref:`User Guide &lt;cross_validation&gt;` for the various cross-validators that can be used here. n_jobs : int or None, optional (default=None) Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` for more details. train_sizes : array-like, shape (n_ticks,), dtype float or int Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class. (default: np.linspace(0.1, 1.0, 5)) &#34;&#34;&#34; if axes is None: _, axes = plt.subplots(1, 3, figsize=(20, 5)) axes[0].set_title(title) if ylim is not None: axes[0].set_ylim(*ylim) axes[0].set_xlabel(&#34;Training examples&#34;) axes[0].set_ylabel(&#34;Score&#34;) train_sizes, train_scores, test_scores, fit_times, _ = \\ learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, return_times=True) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) fit_times_mean = np.mean(fit_times, axis=1) fit_times_std = np.std(fit_times, axis=1) # Plot learning curve axes[0].grid() axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=&#34;r&#34;) axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=&#34;g&#34;) axes[0].plot(train_sizes, train_scores_mean, &#39;o-&#39;, color=&#34;r&#34;, label=&#34;Training score&#34;) axes[0].plot(train_sizes, test_scores_mean, &#39;o-&#39;, color=&#34;g&#34;, label=&#34;Cross-validation score&#34;) axes[0].legend(loc=&#34;best&#34;) # Plot n_samples vs fit_times axes[1].grid() axes[1].plot(train_sizes, fit_times_mean, &#39;o-&#39;) axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std, fit_times_mean + fit_times_std, alpha=0.1) axes[1].set_xlabel(&#34;Training examples&#34;) axes[1].set_ylabel(&#34;fit_times&#34;) axes[1].set_title(&#34;Scalability of the model&#34;) # Plot fit_time vs score axes[2].grid() axes[2].plot(fit_times_mean, test_scores_mean, &#39;o-&#39;) axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1) axes[2].set_xlabel(&#34;fit_times&#34;) axes[2].set_ylabel(&#34;Score&#34;) axes[2].set_title(&#34;Performance of the model&#34;) return plt import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import joblib from os import path from sklearn.model_selection import ShuffleSplit from sklearn import datasets, ensemble, linear_model from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import cross_val_score from sklearn.metrics import r2_score def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)): &#34;&#34;&#34; Generate 3 plots: the test and training learning curve, the training samples vs fit times curve, the fit times vs score curve. Parameters ---------- estimator : object type that implements the &#34;fit&#34; and &#34;predict&#34; methods An object of that type which is cloned for each validation. title : string Title for the chart. X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape (n_samples) or (n_samples, n_features), optional Target relative to X for classification or regression; None for unsupervised learning. axes : array of 3 axes, optional (default=None) Axes to use for plotting the curves. ylim : tuple, shape (ymin, ymax), optional Defines minimum and maximum yvalues plotted. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if ``y`` is binary or multiclass, :class:`StratifiedKFold` used. If the estimator is not a classifier or if ``y`` is neither binary nor multiclass, :class:`KFold` is used. Refer :ref:`User Guide &lt;cross_validation&gt;` for the various cross-validators that can be used here. n_jobs : int or None, optional (default=None) Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` for more details. train_sizes : array-like, shape (n_ticks,), dtype float or int Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class. (default: np.linspace(0.1, 1.0, 5)) &#34;&#34;&#34; if axes is None: _, axes = plt.subplots(1, 1, figsize=(5, 5)) axes.set_title(title) if ylim is not None: axes.set_ylim(*ylim) axes.set_xlabel(&#34;Training examples&#34;) axes.set_ylabel(&#34;Score&#34;) train_sizes, train_scores, test_scores, fit_times, _ = \\ learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, return_times=True) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) fit_times_mean = np.mean(fit_times, axis=1) fit_times_std = np.std(fit_times, axis=1) # Plot learning curve axes.grid() axes.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=&#34;r&#34;) axes.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=&#34;g&#34;) axes.plot(train_sizes, train_scores_mean, &#39;o-&#39;, color=&#34;r&#34;, label=&#34;Training score&#34;) axes.plot(train_sizes, test_scores_mean, &#39;o-&#39;, color=&#34;g&#34;, label=&#34;Cross-validation score&#34;) axes.legend(loc=&#34;best&#34;) return plt def create_pairplot(data): plt.clf() # Calculate and show pairplot sns.pairplot(data, height=2.5) plt.tight_layout() def create_corr_matrix(data): plt.clf() # Calculate and show correlation matrix sns.set() corr = data.corr() # Generate a mask for the upper triangle mask = np.triu(np.ones_like(corr, dtype=np.bool)) # Generate a custom diverging colormap cmap = sns.diverging_palette(220, 10, as_cmap=True) # Draw the heatmap with the mask and correct aspect ratio sns_plot = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, annot=True, cbar_kws={&#34;shrink&#34;: .5}) def data_analysis(data): create_pairplot(data) plt.show() create_corr_matrix(data) plt.show() def load_data(input_data, target_col): # Load the Boston housing dataset data = input_data targets = data[target_col] features = data.drop(target_col, axis = 1) return data, features, targets def train_model(features, targets): # Train a Random Forest Regression model reg = ensemble.RandomForestRegressor(random_state=1) scores = cross_val_score(reg, features, targets, cv=10) print(&#34;Cross Val Score: {:2f} (+/- {:2f})&#34;.format(scores.mean(), scores.std() * 2)) reg.fit(features,targets) return reg def test_model(model, features, targets): # Train a Random Forest Regression model score = r2_score(model.predict(features), targets) return &#34;Test Score: {:2f}&#34;.format(score) def create_learning_curve(estimator, features, targets): plt.clf() title = &#34;Learning Curves (Random Forest Regressor)&#34; cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0) plot_learning_curve(estimator, title, features, targets, ylim=(0.5, 1.01), cv=cv, n_jobs=4) def set_dtypes(data): for key in data: data[key] = data[key].astype(float) return data ",
      "beta": "false",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "automl",
        "mljar"
      ],
      "id": "9bd758b358d510e94ccbe342dc709013"
    },
    {
      "title": "Data Parallelism Pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Tutorials",
      "description": "Learn how to build a scalable inference pipeline using data parallelism.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/tutorials/data-parallelism/",
      "relURI": "/latest/build-dags/tutorials/data-parallelism/",
      "body": "In this tutorial, we‚Äôll build a scalable inference data parallelism pipeline for breast cancer detection using data parallelism.\nBefore You Start # You must have a HPE ML Data Management cluster up and running You should have some basic familiarity with HPE ML Data Management pipeline specs &ndash; see the Transform, Cross Input, Resource Limits, Resource Requests, and Parallelism sections in particular Tutorial # Our Docker image&rsquo;s user code for this tutorial is built on top of the pytorch/pytorch base image, which includes necessary dependencies. The underlying code and pre-trained breast cancer detection model comes from this repo, developed by the Center of Data Science and Department of Radiology at NYU. Their original paper can be found here.\n1. Create a Project &amp; Input Repos # Create a project named data-parallelism-tutorial.\npachctl create project data-parallelism-tutorial Set the project as current.\npachctl config update context --project data-parallelism-tutorial Create the following repos:\npachctl create repo models pachctl create repo sample_data 2. Create a Classification Pipeline # We&rsquo;re going to need to first build a pipeline that will classify the breast cancer images. We&rsquo;ll use a cross input to combine the sample data and models.\nCreate a file named bc_classification.json with the following contents:\nResource: GPU CPU { &#34;pipeline&#34;: { &#34;name&#34;: &#34;bc_classification&#34; }, &#34;description&#34;: &#34;Run breast cancer classification.&#34;, &#34;input&#34;: { &#34;cross&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;sample_data&#34;, &#34;glob&#34;: &#34;/*&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;models&#34;, &#34;glob&#34;: &#34;/&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;/bin/bash&#34;, &#34;run.sh&#34;, &#34;gpu&#34; ], &#34;image&#34;: &#34;pachyderm/breast_cancer_classifier:1.11.6&#34; }, &#34;resource_limits&#34;: { &#34;gpu&#34;: { &#34;type&#34;: &#34;nvidia.com/gpu&#34;, &#34;number&#34;: 1 } }, &#34;resource_requests&#34;: { &#34;memory&#34;: &#34;4G&#34;, &#34;cpu&#34;: 1 }, &#34;parallelism_spec&#34;: { &#34;constant&#34;: 8 } } { &#34;pipeline&#34;: { &#34;name&#34;: &#34;bc_classification_cpu&#34; }, &#34;description&#34;: &#34;Run breast cancer classification.&#34;, &#34;input&#34;: { &#34;cross&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;sample_data&#34;, &#34;glob&#34;: &#34;/*&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;models&#34;, &#34;glob&#34;: &#34;/&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;/bin/bash&#34;, &#34;run.sh&#34;, &#34;cpu&#34; ], &#34;image&#34;: &#34;pachyderm/breast_cancer_classifier:1.11.6&#34; }, &#34;parallelism_spec&#34;: { &#34;constant&#34;: 4 } } Save the file.\nCreate the pipeline.\npachctl create pipeline -f /path/to/bc_classification.json üí° Datum Shape # When you define a glob pattern in your pipeline, you are defining how HPE ML Data Management should split the data so that the code can execute as parallel jobs without having to modify the underlying implementation.\nIn this case, we are treating each exam (4 images and a list file) as a single datum. Each datum is processed individually, allowing parallelized computation for each exam that is added. The file structure for our sample_data is organized as follows:\nsample_data/ ‚îú‚îÄ‚îÄ &lt;unique_exam_id&gt; ‚îÇ ‚îú‚îÄ‚îÄ L_CC.png ‚îÇ ‚îú‚îÄ‚îÄ L_MLO.png ‚îÇ ‚îú‚îÄ‚îÄ R_CC.png ‚îÇ ‚îú‚îÄ‚îÄ R_MLO.png ‚îÇ ‚îî‚îÄ‚îÄ gen_exam_list_before_cropping.pkl ‚îú‚îÄ‚îÄ &lt;unique_exam_id&gt; ‚îÇ ‚îú‚îÄ‚îÄ L_CC.png ‚îÇ ‚îú‚îÄ‚îÄ L_MLO.png ‚îÇ ‚îú‚îÄ‚îÄ R_CC.png ‚îÇ ‚îú‚îÄ‚îÄ R_MLO.png ‚îÇ ‚îî‚îÄ‚îÄ gen_exam_list_before_cropping.pkl ... The gen_exam_list_before_cropping.pkl is a pickled version of the image list, a requirement of the underlying library being used.\n3. Upload Dataset # Open or download this github repo.\ngh repo clone pachyderm/docs-content Navigate to this tutorial.\ncd content/latest/build-dags/tutorials/data-parallelism Upload the sample_data and models folders to your repos.\npachctl put file -r sample_data@master -f sample_data/ pachctl put file -r models@master -f models/ User Code Assets # The Docker image used in this tutorial was built with the following assets:\nAssets: Dockerfile Run.sh FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-devel # Update NVIDIA&#39;s apt-key # Announcement: https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772 ENV DISTRO ubuntu1804 ENV CPU_ARCH x86_64 RUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/$DISTRO/$CPU_ARCH/3bf863cc.pub RUN apt-get update &amp;&amp; apt-get install -y git libgl1-mesa-glx libglib2.0-0 WORKDIR /workspace RUN git clone https://github.com/jimmywhitaker/breast_cancer_classifier.git /workspace RUN pip install --upgrade pip &amp;&amp; pip install -r requirements.txt RUN pip install matplotlib --ignore-installed RUN apt-get -y install tree COPY . /workspace #!/bin/bash NUM_PROCESSES=10 DEVICE_TYPE=$1 NUM_EPOCHS=10 HEATMAP_BATCH_SIZE=100 GPU_NUMBER=0 ID=$(ls /pfs/sample_data/ | head -n 1) DATA_FOLDER=&#34;/pfs/sample_data/${ID}/&#34; INITIAL_EXAM_LIST_PATH=&#34;/pfs/sample_data/${ID}/gen_exam_list_before_cropping.pkl&#34; PATCH_MODEL_PATH=&#34;/pfs/models/sample_patch_model.p&#34; IMAGE_MODEL_PATH=&#34;/pfs/models/sample_image_model.p&#34; IMAGEHEATMAPS_MODEL_PATH=&#34;/pfs/models/sample_imageheatmaps_model.p&#34; CROPPED_IMAGE_PATH=&#34;/pfs/out/${ID}/cropped_images&#34; CROPPED_EXAM_LIST_PATH=&#34;/pfs/out/${ID}/cropped_images/cropped_exam_list.pkl&#34; EXAM_LIST_PATH=&#34;/pfs/out/${ID}/data.pkl&#34; HEATMAPS_PATH=&#34;/pfs/out/${ID}/heatmaps&#34; IMAGE_PREDICTIONS_PATH=&#34;/pfs/out/${ID}/image_predictions.csv&#34; IMAGEHEATMAPS_PREDICTIONS_PATH=&#34;/pfs/out/${ID}/imageheatmaps_predictions.csv&#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo &#39;Stage 1: Crop Mammograms&#39; python3 src/cropping/crop_mammogram.py \\ --input-data-folder $DATA_FOLDER \\ --output-data-folder $CROPPED_IMAGE_PATH \\ --exam-list-path $INITIAL_EXAM_LIST_PATH \\ --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH \\ --num-processes $NUM_PROCESSES echo &#39;Stage 2: Extract Centers&#39; python3 src/optimal_centers/get_optimal_centers.py \\ --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH \\ --data-prefix $CROPPED_IMAGE_PATH \\ --output-exam-list-path $EXAM_LIST_PATH \\ --num-processes $NUM_PROCESSES echo &#39;Stage 3: Generate Heatmaps&#39; python3 src/heatmaps/run_producer.py \\ --model-path $PATCH_MODEL_PATH \\ --data-path $EXAM_LIST_PATH \\ --image-path $CROPPED_IMAGE_PATH \\ --batch-size $HEATMAP_BATCH_SIZE \\ --output-heatmap-path $HEATMAPS_PATH \\ --device-type $DEVICE_TYPE \\ --gpu-number $GPU_NUMBER echo &#39;Stage 4a: Run Classifier (Image)&#39; python3 src/modeling/run_model.py \\ --model-path $IMAGE_MODEL_PATH \\ --data-path $EXAM_LIST_PATH \\ --image-path $CROPPED_IMAGE_PATH \\ --output-path $IMAGE_PREDICTIONS_PATH \\ --use-augmentation \\ --num-epochs $NUM_EPOCHS \\ --device-type $DEVICE_TYPE \\ --gpu-number $GPU_NUMBER echo &#39;Stage 4b: Run Classifier (Image+Heatmaps)&#39; python3 src/modeling/run_model.py \\ --model-path $IMAGEHEATMAPS_MODEL_PATH \\ --data-path $EXAM_LIST_PATH \\ --image-path $CROPPED_IMAGE_PATH \\ --output-path $IMAGEHEATMAPS_PREDICTIONS_PATH \\ --use-heatmaps \\ --heatmaps-path $HEATMAPS_PATH \\ --use-augmentation \\ --num-epochs $NUM_EPOCHS \\ --device-type $DEVICE_TYPE \\ --gpu-number $GPU_NUMBER ",
      "beta": "false",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "automl",
        "mljar",
        "inference"
      ],
      "id": "0f8e765abe382a5d95b259668b1aab34"
    },
    {
      "title": "Task Parallelism Pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Tutorials",
      "description": "Learn how to build a scalable inference pipeline using task parallelism.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/tutorials/task-parallelism/",
      "relURI": "/latest/build-dags/tutorials/task-parallelism/",
      "body": "In this tutorial, we‚Äôll build a scalable inference pipeline for breast cancer detection using task parallelism.\nBefore You Start # You must have a HPE ML Data Management cluster up and running You should have some basic familiarity with HPE ML Data Management pipeline specs &ndash; see the Transform, Join Input, Resource Limits, Resource Requests, and Parallelism sections in particular Tutorial # Our Docker image&rsquo;s user code for this tutorial is built on top of the pytorch/pytorch base image, which includes necessary dependencies. The underlying code and pre-trained breast cancer detection model comes from this repo, developed by the Center of Data Science and Department of Radiology at NYU. Their original paper can be found here.\n1. Create an Input Repo # Make sure your Tutorials project we created in the Standard ML Pipeline tutorial is set to your active context. (This would only change if you have updated your active context since completing the first tutorial.)\npachctl config get context localhost:80 # { # &#34;pachd_address&#34;: &#34;grpc://localhost:80&#34;, # &#34;cluster_deployment_id&#34;: &#34;KhpCZx7c8prdB268SnmXjELG27JDCaji&#34;, # &#34;project&#34;: &#34;Tutorials&#34; # } Create the following repos:\npachctl create repo models pachctl create repo sample_data 2. Create CPU Pipelines # In task parallelism, we separate out the CPU-based preprocessing and GPU-related tasks, saving us cloud costs when scaling. By separating inference into multiple tasks, each task pipeline can be updated independently, allowing ease of model deployment and collaboration.\nWe can split the run.sh script used in the previous tutorial (Data Parallelism Pipeline) into 5 separate processing steps (4 already defined in the script + a visualization step) which will become Pachyderm pipelines, so each can be scaled separately.\nCrop Pipeline # Create a file named crop.json with the following contents: { &#34;pipeline&#34;: { &#34;name&#34;: &#34;crop&#34; }, &#34;description&#34;: &#34;Remove background of image and save cropped files.&#34;, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;sample_data&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;/bin/bash&#34;, &#34;multi-stage/crop.sh&#34; ], &#34;image&#34;: &#34;pachyderm/breast_cancer_classifier:1.11.6&#34; } } Save the file. Create the pipeline. pachctl create pipeline -f /path/to/crop.json Extract Centers Pipeline # Create a file named extract_centers.json with the following contents: { &#34;pipeline&#34;: { &#34;name&#34;: &#34;extract_centers&#34; }, &#34;description&#34;: &#34;Compute and Extract Optimal Image Centers.&#34;, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;crop&#34;, &#34;glob&#34;: &#34;/*&#34; } }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;/bin/bash&#34;, &#34;multi-stage/extract_centers.sh&#34; ], &#34;image&#34;: &#34;pachyderm/breast_cancer_classifier:1.11.6&#34; } } Save the file. Create the pipeline. pachctl create pipeline -f /path/to/extract_centers.json 3. Create GPU Pipelines # Generate Heatmaps Pipeline # Create a file named generate_heatmaps.json with the following contents: { &#34;pipeline&#34;: { &#34;name&#34;: &#34;generate_heatmaps&#34; }, &#34;description&#34;: &#34;Generates benign and malignant heatmaps for cropped images using patch classifier.&#34;, &#34;input&#34;: { &#34;cross&#34;: [ { &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;crop&#34;, &#34;glob&#34;: &#34;/(*)&#34;, &#34;join_on&#34;: &#34;$1&#34;, &#34;lazy&#34;: false } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;extract_centers&#34;, &#34;glob&#34;: &#34;/(*)&#34;, &#34;join_on&#34;: &#34;$1&#34;, &#34;lazy&#34;: false } } ] }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;models&#34;, &#34;glob&#34;: &#34;/&#34;, &#34;lazy&#34;: false } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;/bin/bash&#34;, &#34;multi-stage/generate_heatmaps.sh&#34; ], &#34;image&#34;: &#34;pachyderm/breast_cancer_classifier:1.11.6&#34; }, &#34;resource_limits&#34;: { &#34;gpu&#34;: { &#34;type&#34;: &#34;nvidia.com/gpu&#34;, &#34;number&#34;: 1 } }, &#34;resource_requests&#34;: { &#34;memory&#34;: &#34;4G&#34;, &#34;cpu&#34;: 1 } } Save the file. Create the pipeline. pachctl create pipeline -f /path/to/generate_heatmaps.json Classify Pipeline # Create a file named classify.json with the following contents: { &#34;pipeline&#34;: { &#34;name&#34;: &#34;classify&#34; }, &#34;description&#34;: &#34;Runs the image only model and image+heatmaps model for breast cancer prediction.&#34;, &#34;input&#34;: { &#34;cross&#34;: [ { &#34;join&#34;: [ { &#34;pfs&#34;: { &#34;repo&#34;: &#34;crop&#34;, &#34;glob&#34;: &#34;/(*)&#34;, &#34;join_on&#34;: &#34;$1&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;extract_centers&#34;, &#34;glob&#34;: &#34;/(*)&#34;, &#34;join_on&#34;: &#34;$1&#34; } }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;generate_heatmaps&#34;, &#34;glob&#34;: &#34;/(*)&#34;, &#34;join_on&#34;: &#34;$1&#34; } } ] }, { &#34;pfs&#34;: { &#34;repo&#34;: &#34;models&#34;, &#34;glob&#34;: &#34;/&#34; } } ] }, &#34;transform&#34;: { &#34;cmd&#34;: [ &#34;/bin/bash&#34;, &#34;multi-stage/classify.sh&#34; ], &#34;image&#34;: &#34;pachyderm/breast_cancer_classifier:1.11.6&#34; }, &#34;resource_limits&#34;: { &#34;gpu&#34;: { &#34;type&#34;: &#34;nvidia.com/gpu&#34;, &#34;number&#34;: 1 } }, &#34;resource_requests&#34;: { &#34;memory&#34;: &#34;4G&#34;, &#34;cpu&#34;: 1 } } Save the file. Create the pipeline pachctl create pipeline -f /path/to/classify.json 4. Upload Dataset # Open or download this github repo. gh repo clone pachyderm/docs-content Navigate to this tutorial. cd content/latest/build-dags/tutorials/task-parallelism Upload the sample_data and models folders to your repos. pachctl put file -r sample_data@master -f sample_data/ pachctl put file -r models@master -f models/ User Code Assets # The Docker image used in this tutorial was built with the following assets:\nAssets: Dockerfile Crop.sh Extract_Centers.sh Generate_Heatmaps.sh Classify.sh Visualize_Heatmaps.sh FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-devel # Update NVIDIA&#39;s apt-key # Announcement: https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772 ENV DISTRO ubuntu1804 ENV CPU_ARCH x86_64 RUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/$DISTRO/$CPU_ARCH/3bf863cc.pub RUN apt-get update &amp;&amp; apt-get install -y git libgl1-mesa-glx libglib2.0-0 WORKDIR /workspace RUN git clone https://github.com/jimmywhitaker/breast_cancer_classifier.git /workspace RUN pip install --upgrade pip &amp;&amp; pip install -r requirements.txt RUN pip install matplotlib --ignore-installed RUN apt-get -y install tree COPY . /workspace #!/bin/bash NUM_PROCESSES=10 ID=$(ls /pfs/sample_data/ | head -n 1) DATA_FOLDER=&#34;/pfs/sample_data/${ID}/&#34; INITIAL_EXAM_LIST_PATH=&#34;/pfs/sample_data/${ID}/gen_exam_list_before_cropping.pkl&#34; CROPPED_IMAGE_PATH=&#34;/pfs/out/${ID}/cropped_images&#34; CROPPED_EXAM_LIST_PATH=&#34;/pfs/out/${ID}/cropped_images/cropped_exam_list.pkl&#34; EXAM_LIST_PATH=&#34;/pfs/out/${ID}/data.pkl&#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo &#39;Stage 1: Crop Mammograms&#39; python3 src/cropping/crop_mammogram.py \\ --input-data-folder $DATA_FOLDER \\ --output-data-folder $CROPPED_IMAGE_PATH \\ --exam-list-path $INITIAL_EXAM_LIST_PATH \\ --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH \\ --num-processes $NUM_PROCESSES #!/bin/bash DEVICE_TYPE=&#39;gpu&#39; NUM_EPOCHS=10 HEATMAP_BATCH_SIZE=100 GPU_NUMBER=0 ID=$(ls /pfs/crop/ | head -n 1) IMAGEHEATMAPS_MODEL_PATH=&#34;/pfs/models/sample_imageheatmaps_model.p&#34; CROPPED_IMAGE_PATH=&#34;/pfs/crop/${ID}/cropped_images&#34; EXAM_LIST_PATH=&#34;/pfs/extract_centers/${ID}/data.pkl&#34; HEATMAPS_PATH=&#34;/pfs/generate_heatmaps/${ID}/heatmaps&#34; IMAGEHEATMAPS_PREDICTIONS_PATH=&#34;/pfs/out/${ID}/imageheatmaps_predictions.csv&#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo &#39;Stage 4b: Run Classifier (Image+Heatmaps)&#39; python3 src/modeling/run_model.py \\ --model-path $IMAGEHEATMAPS_MODEL_PATH \\ --data-path $EXAM_LIST_PATH \\ --image-path $CROPPED_IMAGE_PATH \\ --output-path $IMAGEHEATMAPS_PREDICTIONS_PATH \\ --use-heatmaps \\ --heatmaps-path $HEATMAPS_PATH \\ --use-augmentation \\ --num-epochs $NUM_EPOCHS \\ --device-type $DEVICE_TYPE \\ --gpu-number $GPU_NUMBER #!/bin/bash NUM_PROCESSES=10 ID=$(ls /pfs/crop/ | head -n 1) CROPPED_IMAGE_PATH=&#34;/pfs/crop/${ID}/cropped_images&#34; CROPPED_EXAM_LIST_PATH=&#34;/pfs/crop/${ID}/cropped_images/cropped_exam_list.pkl&#34; EXAM_LIST_PATH=&#34;/pfs/out/${ID}/data.pkl&#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo &#39;Stage 2: Extract Centers&#39; python3 src/optimal_centers/get_optimal_centers.py \\ --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH \\ --data-prefix $CROPPED_IMAGE_PATH \\ --output-exam-list-path $EXAM_LIST_PATH \\ --num-processes $NUM_PROCESSES #!/bin/bash DEVICE_TYPE=&#39;gpu&#39; HEATMAP_BATCH_SIZE=100 GPU_NUMBER=0 ID=$(ls /pfs/crop/ | head -n 1) PATCH_MODEL_PATH=&#34;/pfs/models/sample_patch_model.p&#34; CROPPED_IMAGE_PATH=&#34;/pfs/crop/${ID}/cropped_images&#34; EXAM_LIST_PATH=&#34;/pfs/extract_centers/${ID}/data.pkl&#34; HEATMAPS_PATH=&#34;/pfs/out/${ID}/heatmaps&#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo &#39;Stage 3: Generate Heatmaps&#39; python3 src/heatmaps/run_producer.py \\ --model-path $PATCH_MODEL_PATH \\ --data-path $EXAM_LIST_PATH \\ --image-path $CROPPED_IMAGE_PATH \\ --batch-size $HEATMAP_BATCH_SIZE \\ --output-heatmap-path $HEATMAPS_PATH \\ --device-type $DEVICE_TYPE \\ --gpu-number $GPU_NUMBER #!/bin/bash ID=$(ls /pfs/crop/ | head -n 1) CROPPED_IMAGE_PATH=&#34;/pfs/crop/${ID}/cropped_images/&#34; HEATMAPS_PATH=&#34;/pfs/generate_heatmaps/${ID}/heatmaps/&#34; OUTPUT_PATH=&#34;/pfs/out/${ID}/&#34; export PYTHONPATH=$(pwd):$PYTHONPATH echo &#39;Stage 5: Visualize Heatmaps&#39; python3 src/heatmaps/visualize_heatmaps.py \\ --image-path $CROPPED_IMAGE_PATH \\ --heatmap-path $HEATMAPS_PATH \\ --output-path $OUTPUT_PATH ",
      "beta": "false",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "automl",
        "mljar",
        "inference"
      ],
      "id": "1921ce7e0907bc6eb5a0b89c1fd23198"
    },
    {
      "title": "Docker Image + User Code",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Tutorials",
      "description": "Learn how to build a Docker image that contains your user code and all required dependencies.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/build-dags/tutorials/user-code/",
      "relURI": "/latest/build-dags/tutorials/user-code/",
      "body": "In this tutorial, you&rsquo;ll learn how to build a Docker image that contains your user code and all required dependencies. You&rsquo;ll then learn how to reference this Docker image in a pipeline spec.\nBefore You Start # You must have Docker installed on your machine. You should be familiar with the HPE ML Data Management pipeline specification. You should be comfortable in a programming language of your choice (Python is most common). Tutorial # Pipelines in HPE ML Data Management use Docker images to execute user code. When you specify an image in a pipeline spec, HPE ML Data Management deploys the image to the cluster. During pipeline execution, Pachyderm pulls the image from the registry and creates containers from it to process data in the pipeline.\nNone of our tutorials require that you build your own Docker images. But eventually, you&rsquo;ll want to build your own Docker images so that you can precisely control how your data is transformed. We recommend that you read through this tutorial for context on how pipelines use Docker images to transform your data, but you don&rsquo;t need to follow along with the steps until you are ready to build your first custom pipeline/DAG.\nTo jump into a hands-on tutorial that uses a pre-built Docker image, check out our other tutorials in this section, starting with the Standard ML Pipeline tutorial.\n1. Create a Working Directory # Create a working directory for your all the files we&rsquo;ll need to create a Docker image with your code.\nmkdir -p ~/HPE ML Data Management/tutorials/my-first-image cd ~/HPE ML Data Management/tutorials/my-first-image 2. Create a Dockerfile # To create a Docker image with your user code, you&rsquo;ll need to first create a Dockerfile. The Dockerfile defines the environment and dependencies needed to run your code using the files located inside of your working directory. The following is an example of a simple working directory with a Dockerfile.\n. ‚îú‚îÄ‚îÄ Dockerfile ‚îú‚îÄ‚îÄ app.py ‚îî‚îÄ‚îÄ requirements.txt Create a file named Dockerfile in your working directory. Add a base image to your Dockerfile. The base image is the starting point for your Docker image. You can use any image that has the dependencies you need to run your code. Add a WORKDIR command to your Dockerfile. The WORKDIR command sets the working directory for your Docker image. This is the directory where your code will be run. Add a COPY command to your Dockerfile. The COPY command copies the contents of your working directory into the Docker image. This is where you&rsquo;ll copy your code and any other files needed to run your code. Add a RUN command to your Dockerfile. The RUN command executes a command in your Docker image. This is where you&rsquo;ll install any dependencies needed to run your code. Add a CMD command to your Dockerfile. The CMD command defines the command that will be run when the Docker image is run. This is where you&rsquo;ll define the command to run your actual user code. Python Example:\nFROM python:3.9-slim-buster # Set the working directory to /app WORKDIR /app # Copy the current directory contents into the container at /app COPY . /app # Install any needed packages specified in requirements.txt RUN pip install --trusted-host pypi.python.org -r requirements.txt # Define environment variable ENV NAME World # Run app.py when the container launches CMD [&#34;python&#34;, &#34;app.py&#34;] 3. Add Your Code # Create a file named app.py in your working directory. Write your desired transformation code in the app.py file. Create a file named requirements.txt in your working directory. List all of the dependencies needed to run your code in the requirements.txt file. 4. Build the Docker Image # Open the terminal and navigate to your working directory. Run the following command. docker build -t &lt;image-name&gt;:&lt;tag&gt; . 4. Register the Docker Image # In order to make your Docker image available to your HPE ML Data Management cluster, you&rsquo;ll need to push it to a Docker registry. In this tutorial, we&rsquo;ll be using Docker Hub.\nCreate a Docker Hub account. Run the following command to log into Docker Hub. docker login Run the following commands to tag and push your Docker image to Docker Hub. docker tag &lt;image-name&gt;:&lt;tag&gt; &lt;docker-hub-username&gt;/&lt;image-name&gt;:&lt;tag&gt; docker push &lt;docker-hub-username&gt;/&lt;image-name&gt;:&lt;tag&gt; Run the following command to verify that your Docker image was pushed to Docker Hub. docker images Run the following command to verify that your Docker image is available on Docker Hub. docker pull &lt;docker-hub-username&gt;/&lt;image-name&gt;:&lt;tag&gt; 5. Use Your Docker Image # Now that you have a Docker image with your code, you can reference it in a pipeline spec. The following is an example of a pipeline spec that references a Docker image with user code.\nCreate a file named my-pipeline.json in your working directory.\nDefine the pipeline spec using the options found in the pipeline specification docs.\n{ &#34;pipeline&#34;: { &#34;name&#34;: &#34;my-pipeline&#34; }, &#34;transform&#34;: { &#34;image&#34;: &#34;&lt;dockerhub-username&gt;/&lt;image-name&gt;:&lt;tag&gt;&#34;, &#34;cmd&#34;: [&#34;python&#34;, &#34;app.py&#34;], &#34;stdin&#34;: [&#34;input&#34;], &#34;stdout&#34;: [&#34;output&#34;] }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;input-repo&#34;, &#34;glob&#34;: &#34;/*&#34; } } } Create a pipeline using the my-pipeline.json file.\npachctl create pipeline -f my-pipeline.json Inspect your pipeline to verify that it was created successfully.\npachctl inspect pipeline my-pipeline ",
      "beta": "false",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "tutorials"
      ],
      "id": "9f77a9557fb0acae17450d7969557344"
    },
    {
      "title": "Export Data",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "Export transformed data using data egress and other methods.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/",
      "relURI": "/latest/export-data/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e2ce9dc6d479ebd494c97688c9566f47"
    },
    {
      "title": "Egress To An SQL Database",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Export Data",
      "description": "Learn how to use data egress to end data to an SQL database.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/sql-egress/",
      "relURI": "/latest/export-data/sql-egress/",
      "body": " ‚ö†Ô∏è SQL Egress is an experimental feature.\nHPE ML Data Management already implements egress to object storage as an optional egress field in the pipeline specification. Similarly, our SQL egress lets you seamlessly export data from a HPE ML Data Management-powered pipeline output repo to an SQL database.\nSpecifically, we help you connect to a remote database and push the content of CSV files to interface tables, matching their column names and casting their content into their respective SQL datatype.\nInterface tables are intermediate tables used for staging the data being egressed from HPE ML Data Management to your data warehouse. They are the tables your SQL Egress pipeline inserts its data into and should be dedicated tables. The content of your interface tables matches the content of the latest output commit of your pipeline.\n‚ÑπÔ∏è Best Practice.\nA new output commit will trigger a delete of all data in the interface tables before inserting more recent values. As a best practice, we strongly recommend to create a separate database for HPE ML Data Management Egress.\nAs of today, we support the following drivers:\npostgres and postgresql : connect to Postgresql (or compatible databases such as Redshift). mysql : connect to MySQL (or compatible databases such as MariaDB). snowflake : connect to Snowflake. Use SQL Egress # To egress data from the output commit of a pipeline to an SQL database, you will need to:\nOn your cluster\nCreate a secret containing your database password.\nIn the Specification file of your egress pipeline\nReference your secret by providing its name, provide the connection string to the database and choose the format of the files (CSV for now - we are planning on adding JSON soon) containing the data to insert.\nIn your user code\nWrite your data into CSV files placed in root directories named after the table you want to insert them into. You can have multiple directories.\n1. Create a Secret # Create a secret containing your database password in the field PACHYDERM_SQL_PASSWORD. This secret is identical to the database secret of HPE ML Data Management SQL Ingest. Refer to the SQL Ingest page for instructions on how to create your secret.\n2. Update your Pipeline Spec # Append an egress section to your pipeline specification file, then fill in:\nthe url: the connection string to your database.\nthe file_format type: CSV for now.\nthe name: the Kubernetes secret name.\nthe columns: Optional array for egress of CSV files with headers only. The order of the columns in this array must match the order of the schema columns; however, the CSV columns can be any order. So if the array is [&ldquo;foo&rdquo;, &ldquo;bar&rdquo;] and the CSV file is:\nbar,foo 1,&#34;string&#34; 2,&#34;text!&#34; The following table will be written to the database:\nfoo | bar =============== string | 1 text! | 2 Example # { &#34;pipeline&#34;: { &#34;name&#34;: &#34;egress&#34; }, &#34;input&#34;: { &#34;pfs&#34;: { &#34;repo&#34;: &#34;input_repo&#34;, &#34;glob&#34;: &#34;/&#34;, &#34;name&#34;: &#34;in&#34; } }, &#34;transform&#34;: { ... }, &#34;egress&#34;: { &#34;sql_database&#34;: { &#34;url&#34;: &#34;snowflake://pachyderm@WHMUWUD-CJ80657/PACH_DB/PUBLIC?warehouse=COMPUTE_WH&#34;, &#34;file_format&#34;: { &#34;type&#34;: &#34;CSV&#34;, &#34;columns&#34;: [&#34;foo&#34;, &#34;bar&#34;] }, &#34;secret&#34;: { &#34;name&#34;: &#34;snowflakesecret&#34;, &#34;key&#34;: &#34;PACHYDERM_SQL_PASSWORD&#34; } } } } 3. In your User Code, Write Your Data to Directories Named After Each Table # The user code of your pipeline determines what data should be egressed and to which tables. Data (in the form of CSV files) that the pipeline writes to the output repo is interpreted as tables corresponding to directories.\nEach top-level directory is named after the table you want to egress its content to. All of the files reachable in the walk of each root directory are parsed in the given format indicated in the egress section of the pipeline specification file (CSV for now), then inserted in their corresponding table. Find more information on how to format your CSV file depending on your targeted SQL Data Type in our SQL Ingest Formatting section.\n‚ö†Ô∏è All interface tables must pre-exist before an insertion. Files in the root produce an error as they do not correspond to a table. The directory structure below the top level does not matter. The first directory in the path is the table; everything else is walked until a file is found. All the data in those files is inserted into the table. The order of the values in each line of a CSV must match the order of the columns in the schema of your interface table unless you were using headers AND specified the &quot;columns&quot;: [&quot;foo&quot;, &quot;bar&quot;], field in your pipeline specification file. Example # &#34;1&#34;,&#34;Tim&#34;,&#34;2017-03-12T21:51:45Z&#34;,&#34;true&#34; &#34;12&#34;,&#34;Tom&#34;,&#34;2017-07-25T21:51:45Z&#34;,&#34;true&#34; &#34;33&#34;,&#34;Tam&#34;,&#34;2017-01-01T21:51:45Z&#34;,&#34;false&#34; &#34;54&#34;,&#34;Pach&#34;,&#34;2017-05-15T21:51:45Z&#34;,&#34;true&#34; ‚ÑπÔ∏è HPE ML Data Management queries the schema of the interface tables before insertion then parses the data into their SQL data types. Each insertion creates a new row in your table. Troubleshooting # You have a pipeline running but do not see any update in your database?\nCheck your logs:\npachctl list pipeline pachctl logs -p &lt;your-pipeline-name&gt; --master ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "sql",
        "data-operations",
        "export"
      ],
      "id": "788e1ce8de49e90cb8a76970f89c3d00"
    },
    {
      "title": "Export via Egress",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Export Data",
      "description": "Learn how to push pipeline results to an external datastore using the egress pipeline spec attribute.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/export-data-egress/",
      "relURI": "/latest/export-data/export-data-egress/",
      "body": "The egress field in the HPE ML Data Management pipeline specification enables you to push the results of a pipeline to an external datastore such as Amazon S3, Google Cloud Storage, or Azure Blob Storage. After the user code has finished running, but before the job is marked as successful, HPE ML Data Management pushes the data to the specified destination.\n‚ÑπÔ∏è Make sure that your cluster has been configured to work with your object store.\nPick the egress protocol that applies to your storage:\nCloud Platform Protocol Example Google Cloud Storage gs:// gs://gs-bucket/gs-dir Amazon S3 s3:// s3://s3-endpoint/s3-bucket/s3-dir Azure Blob Storage wasb:// wasb://default-container@storage-account/az-dir Example # &#34;egress&#34;: { &#34;URL&#34;: &#34;s3://bucket/dir&#34; }, ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "egress",
        "data-operations",
        "export"
      ],
      "id": "96b16efc992ac7b96323e8b2af848083"
    },
    {
      "title": "Export via PachCTL",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Export Data",
      "description": "Learn how to export data using the pachctl get command.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/export-data-pachctl/",
      "relURI": "/latest/export-data/export-data-pachctl/",
      "body": "To export your data with PachCTL:\nList the files in the given directory:\npachctl list file &lt;repo&gt;@&lt;branch&gt;:&lt;dir&gt; Example:\npachctl list myrepo@master:labresults System Response:\nNAME TYPE SIZE /labresults/T1606331395-LIPID-PATID2-CLIA24D9871327.txt file 101B /labresults/T1606707557-LIPID-PATID1-CLIA24D9871327.txt file 101B /labresults/T1606707579-LIPID-PATID3-CLIA24D9871327.txt file 100B /labresults/T1606707597-LIPID-PATID4-CLIA24D9871327.txt file 101B Get the contents of a specific file:\npachctl get file &lt;repo&gt;@&lt;branch&gt;:&lt;path/to/file&gt; Example:\npachctl get file myrepo@master:/labresults/T1606331395-LIPID-PATID2-CLIA24D9871327.txt System Response:\nPID|PATID2 ORC|ORD777889 OBX|1|NM|2093-3^Cholesterol|212|mg/dL OBX|2|NM|2571-8^Triglyceride|110|mg/dL ‚ÑπÔ∏è You can view the parent, grandparent, and any previous commit by using the caret (^) symbol followed by a number that corresponds to an ancestor in sequence:\nView a parent commit\npachctl list commit &lt;repo&gt;@&lt;branch-or-commit&gt;^:&lt;path/to/file&gt; pachctl get file &lt;repo&gt;@&lt;branch-or-commit&gt;^:&lt;path/to/file&gt; View an &lt;n&gt; parent of a commit\npachctl list commit &lt;repo&gt;@&lt;branch-or-commit&gt;^&lt;n&gt;:&lt;path/to/file&gt; pachctl get file &lt;repo&gt;@&lt;branch-or-commit&gt;^&lt;n&gt;:&lt;path/to/file&gt; Example:\npachctl get file datas@master^4:user_data.csv If the file does not exist in that revision, HPE ML Data Management displays an error message.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pachctl",
        "data-operations",
        "export"
      ],
      "id": "20fa2ab056ca395c4e816ee4a366c378"
    },
    {
      "title": "Mount a Repo Locally",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Export Data",
      "description": "Learn how to mount a repository as a local filesystem using the pachctl mount command.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/mount-repo-to-local-computer/",
      "relURI": "/latest/export-data/mount-repo-to-local-computer/",
      "body": " ‚ö†Ô∏è HPE ML Data Management uses FUSE to mount repositories as local filesystems. Because Apple has announced phasing out support for macOS kernel extensions, including FUSE, this functionality is no longer stable on macOS Catalina (10.15) or later.\nHPE ML Data Management enables you to mount a repository as a local filesystem on your computer by using the pachctl mount command. This command uses the Filesystem in Userspace (FUSE) user interface to export a HPE ML Data Management File System (PFS) to a Unix computer system. This functionality is useful when you want to pull data locally to experiment, review the results of a pipeline, or modify the files in the input repository directly.\nYou can mount a HPE ML Data Management repo in one of the following modes:\nRead-only ‚Äî you can read the mounted files to further experiment with them locally, but cannot modify them. Read-write ‚Äî you can read mounted files, modify their contents, and push them back into your centralized HPE ML Data Management input repositories. Prerequisites # You must have the following configured for this functionality to work:\nUnix or Unix-like operating system, such as Ubuntu 16.04 or macOS Yosemite or later.\nFUSE for your operating system installed:\nOn macOS, run:\nbrew install osxfuse On Ubuntu, run:\nsudo apt-get install -y fuse For more information, see:\nFUSE for macOS\nMounting Repositories in Read-Only Mode # By default, HPE ML Data Management mounts all repositories in read-only mode. You can access the files through your file browser or enable third-party applications access. Read-only access enables you to explore and experiment with the data, without modifying it. For example, you can mount your repo to a local computer and then open that directory in a Jupyter Notebook for exploration.\n‚ÑπÔ∏è The pachctl mount command allows you to mount not only the default branch, typically a master branch, but also other HPE ML Data Management branches. By default, HPE ML Data Management mounts the master branch. However, if you add a branch to the name of the repo, the HEAD of that branch will be mounted.\nExample:\npachctl mount images --repos images@staging You can also mount a specific commit, but because commits might be on multiple branches, modifying them might result in data deletion in the HEAD of the branches. Therefore, you can only mount commits in read-only mode. If you want to write to a specific commit that is not the HEAD of a branch, you can create a new branch with that commit as HEAD.\nMounting Repositories in Read-Write Mode # Running the pachctl mount command with the --write flag grants you write access to the mounted repositories, which means that you can open the files for editing and put them back to the HPE ML Data Management repository.\n‚ö†Ô∏è Your changes are saved to the HPE ML Data Management repository only after you interrupt the pachctl mount with CTRL+C or with pachctl unmount, unmount /&lt;path-to-mount&gt;, or fusermount -u /&lt;path-to-mount&gt;.\nFor example, you have the OpenCV example pipeline up and running. If you want to edit files in the images repository, experiment with brightness and contrast settings in liberty.png, and finally have your edges pipeline process those changes. If you do not mount the images repo, you would have to first download the files to your computer, edit them, and then put them back to the repository. The pachctl mount command automates all these steps for you. You can mount just the images repo or all HPE ML Data Management repositories as directories on you machine, edit as needed, and, when done, exit the pachctl mount command. Upon exiting the pachctl mount command, HPE ML Data Management uploads all the changes to the corresponding repository.\nIf someone else modifies the files while you are working on them locally, their changes will likely be overwritten when you exit pachctl mount. This happens because Therefore, make sure that you do not work on the same files while someone else is working on them.\n‚ÑπÔ∏è Use writable mount ONLY when you have sole ownership over the mounted data. Otherwise, merge conflicts or unexpected data overwrites can occur.\nBecause output repositories are created by the HPE ML Data Management pipelines, they are immutable. Only a pipeline can change and update files in these repositories. If you try to change a file in an output repo, you will get an error message.\nHow to Mount/Unmount a HPE ML Data Management Repo # To mount a HPE ML Data Management repo on a local computer, complete the following steps:\nIn a terminal, go to a directory in which you want to mount a HPE ML Data Management repo. It can be any new empty directory on your local computer. For example, mydirectory.\nRun pachctl mount for a repository and branch that you want to mount:\npachctl mount &lt;path-on-your-computer&gt; [flags] Example:\nIf you want to mount all the repositories in your HPE ML Data Management cluster to a mydirectory directory on your computer and give WRITE access to them, run: pachctl mount mydirectory --write If you want to mount the master branch of the images repo and enable file editing in this repository, run: pachctl mount mydirectory --repos images@master+w To give read-only access, omit +w.\nSystem Response:\nro for images: &amp;{Branch:master Write:true} ri: repo:&lt;name:&#34;montage&#34; &gt; created:&lt;seconds:1591812554 nanos:348079652 &gt; size_bytes:1345398 description:&#34;Output repo for pipeline montage.&#34; branches:&lt;repo:&lt;name:&#34;montage&#34; &gt; name:&#34;master&#34; &gt; continue ri: repo:&lt;name:&#34;edges&#34; &gt; created:&lt;seconds:1591812554 nanos:201592492 &gt; size_bytes:136795 description:&#34;Output repo for pipeline edges.&#34; branches:&lt;repo:&lt;name:&#34;edges&#34; &gt; name:&#34;master&#34; &gt; continue ri: repo:&lt;name:&#34;images&#34; &gt; created:&lt;seconds:1591812554 nanos:28450609 &gt; size_bytes:244068 branches:&lt;repo:&lt;name:&#34;images&#34; &gt; name:&#34;master&#34; &gt; MkdirAll /var/folders/jl/mm3wrxqd75l9r1_d0zktphdw0000gn/T/pfs201409498/images The command runs in your terminal until you terminate it by pressing CTRL+C.\nTip Mount multiple repos at once by appending each mount instruction to the same command. For example, the following will mount both repos to the /mydirectory directory. pachctl mount ./mydirectory -r first_repo@master -r second_repo@master You can check that the repo was mounted by running the mount command in your terminal:\nmount /dev/disk1s1 on / (apfs, local, read-only, journaled) devfs on /dev (devfs, local, nobrowse) /dev/disk1s2 on /System/Volumes/Data (apfs, local, journaled, nobrowse) /dev/disk1s5 on /private/var/vm (apfs, local, journaled, nobrowse) map auto_home on /System/Volumes/Data/home (autofs, automounted, nobrowse) pachctl@osxfuse0 on /Users/testuser/mydirectory (osxfuse, nodev, nosuid, synchronous, mounted by testuser) Access your mountpoint.\nFor example, in macOS, open Finder, press CMD + SHIFT + G, and type the mountpoint location. If you have mounted the repo to ~/mydirectory, type ~/mydirectory.\nEdit the files as needed.\nWhen ready, add your changes to the HPE ML Data Management repo by stopping the pachctl mount command with CTRL+C or by running pachctl unmount &lt;mountpoint&gt; (or unmount /&lt;path-to-mount&gt;, or fusermount -u /&lt;path-to-mount&gt;).\nIf you have mounted a writable HPE ML Data Management share, interrupting the pachctl mount command results in the upload of your changes to the corresponding repo and branch, which is equivalent to running the pachctl put file command. You can check that HPE ML Data Management runs a new job for this work by listing current jobs with pachctl list job.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pachctl",
        "data-operations",
        "export"
      ],
      "id": "649cc08d7340c388dd2716581b307f4e"
    },
    {
      "title": "S3 Gateway Operations",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Export Data",
      "description": "Learn which S3 Gateway operations are supported.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/s3-gateway-operations/",
      "relURI": "/latest/export-data/s3-gateway-operations/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f8569d2e8f99454388fe0a07d90efff6"
    },
    {
      "title": "Create S3 Bucket",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway Operations",
      "description": "Learn how to create an S3 bucket through the S3 Gateway.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/s3-gateway-operations/create-bucket/",
      "relURI": "/latest/export-data/s3-gateway-operations/create-bucket/",
      "body": "Call the create an S3 bucket command on your S3 client to create a branch in a HPE ML Data Management repository. For example, let&rsquo;s create the master branch of the repo foo in project bar.\nTool: AWS S3 CLI MinIO Create a bucket named foo in project bar. aws --endpoint-url http://localhost:30600/ s3 mb s3://master.foo.bar # make_bucket: master.foo.bar verify that the S3 bucket has been created: aws --endpoint-url http://localhost:30600/ s3 ls # 2022-12-7 22:46:08 master.foo.bar Create a bucket named foo in project bar. mc mb local/master.foo.bar # Bucket created successfully `local/master.foo.bar`. Verify that the S3 bucket has been created: mc ls local # [2021-04-26 22:46:08] 0B master.foo.bar/ ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "4d49f67a97434d4f485bab209798b704"
    },
    {
      "title": "Delete an S3 Object",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway Operations",
      "description": "Learn how to delete an S3 object through the S3 Gateway.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/s3-gateway-operations/remove-object/",
      "relURI": "/latest/export-data/s3-gateway-operations/remove-object/",
      "body": "You can call the delete an S3 Object command on your S3 client to delete a file from a HPE ML Data Management repository. For example, let&rsquo;s delete the file test.csv from the master branch of the foo repo within the bar project.\nTool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 rm s3://master.foo.bar/test.csv # delete: s3://master.foo.bar/test.csv mc rm local/master.foo.bar/test.csv # Removing `local/master.foo.bar/test.csv`. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "94f06350f9004ce53b7b87cef64b2c78"
    },
    {
      "title": "Delete Empty S3 Bucket",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway Operations",
      "description": "Learn how to delete an empty S3 bucket through the S3 Gateway.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/s3-gateway-operations/delete-bucket/",
      "relURI": "/latest/export-data/s3-gateway-operations/delete-bucket/",
      "body": "You can call the delete an empty S3 bucket command on your S3 client to delete a HPE ML Data Management repository. For example, let&rsquo;s delete the the repo foo in project bar.\nTool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 rb s3://master.foo.bar # remove_bucket: master.foo.bar mc rb local/master.foo.bar # Removed `local/master.foo.bar` successfully. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "c1b2268ded2c7d866dbfd229fcb44a8a"
    },
    {
      "title": "Get an S3 Object",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway Operations",
      "description": "Learn how to get an S3 object through the S3 Gateway.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/s3-gateway-operations/get-object/",
      "relURI": "/latest/export-data/s3-gateway-operations/get-object/",
      "body": "You can call the get an S3 object command on your S3 client to download a file by specifying the branch.repo.project it lives in. For example, let&rsquo;s get the test.csv file from master.foo.bar.\nTool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 cp s3://master.foo.bar/test.csv . # download: s3://master.foo.bar/test.csv to ./test.csv mc cp local/master.foo.bar/test.csv . # test.csv: 2.56 MiB / 2.56 MiB ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì 100.00% 1.26 MiB/s 2s Versioning # Most operations act on the HEAD of the given branch. However, if your object store library or tool supports versioning, you can get objects in non-HEAD commits by using the commit ID as the S3 object version ID or use the following syntax --bucket &lt;commit&gt;.&lt;branch&gt;.&lt;repo&gt;.&lt;project&gt;\nTo retrieve the file file.txt in the commit a5984442ce6b4b998879513ff3da17da on the master branch of the repo foo in project bar:\nGet Object By: path id aws s3api get-object --bucket a5984442ce6b4b998879513ff3da17da.master.foo.bar --profile gcp-pf --endpoint http://localhost:30600 --key file.txt export.txt aws s3api get-object --bucket master.foo.bar --profile gcp-pf --endpoint http://localhost:30600 --key file.txt --version-id a5984442ce6b4b998879513ff3da17da export.txt { &#34;AcceptRanges&#34;: &#34;bytes&#34;, &#34;LastModified&#34;: &#34;2021-06-03T01:31:36+00:00&#34;, &#34;ContentLength&#34;: 5, &#34;ETag&#34;: &#34;\\&#34;b5fdc0b3557bd4de47045f9c69fa8e54102bcecc36f8743ab88df90f727ff899\\&#34;&#34;, &#34;VersionId&#34;: &#34;a5984442ce6b4b998879513ff3da17da&#34;, &#34;ContentType&#34;: &#34;text/plain; charset=utf-8&#34;, &#34;Metadata&#34;: {} } ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "06a88204fbb04a90e0f938e4b4a57383"
    },
    {
      "title": "List S3 Buckets",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway Operations",
      "description": "Learn how to list S3 buckets through the S3 Gateway.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/s3-gateway-operations/list-buckets/",
      "relURI": "/latest/export-data/s3-gateway-operations/list-buckets/",
      "body": "You can check the list of filesystem objects in your HPE ML Data Management repository by running an S3 client ls command.\nTool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600 s3 ls # 2021-04-26 15:09:50 master.train.myproject # 2021-04-26 14:58:50 master.pre_process.myproject # 2021-04-26 14:58:09 master.split.myproject # 2021-04-26 14:58:09 stats.split.myproject mc ls local # [2021-04-26 15:09:50 PDT] 0B master.train.myproject/ # [2021-04-26 14:58:50 PDT] 0B master.pre_process.myproject/ # [2021-04-26 14:58:09 PDT] 0B master.split.myproject/ # [2021-04-26 14:58:09 PDT] 0B stats.split.myproject/ ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "339947da42b489544bce46953a599042"
    },
    {
      "title": "List S3 Objects",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway Operations",
      "description": "Learn how to list S3 objects through the S3 Gateway.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/s3-gateway-operations/list-objects/",
      "relURI": "/latest/export-data/s3-gateway-operations/list-objects/",
      "body": "You can list the contents of a given HPE ML Data Management repository using the following commands.\nTool: AWS S3 CLI MinIO aws --endpoint-url http://localhost:30600/ s3 ls s3://master.raw_data.myproject # 2021-04-26 11:22:23 2685061 github_issues_medium.csv mc ls local/master.raw_data.myproject # [2021-04-26 12:11:37 PDT] 2.6MiB github_issues_medium.csv ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "8fa4f7acb1c002459f62256f3e1b60f3"
    },
    {
      "title": "Write an S3 Object",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "S3 Gateway Operations",
      "description": "Learn how to write an S3 object through the S3 Gateway.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/export-data/s3-gateway-operations/write-object/",
      "relURI": "/latest/export-data/s3-gateway-operations/write-object/",
      "body": "You can write an S3 object to a HPE ML Data Management repo within a project by performing the following commands:\nTool: AWS S3 CLI MinIO Create the object: aws --endpoint-url http://localhost:30600/ s3 cp test.csv s3://master.foo.bar # upload: ./test.csv to s3://master.foo.bar/test.csv Check that the object was added: aws --endpoint-url http://localhost:30600/ s3 ls s3://master.foo.bar/ # 2021-04-26 12:11:37 2685061 github_issues_medium.csv # 2021-04-26 12:11:37 62 test.csv Create the object: mc cp test.csv local/master.foo.bar/test.csv # test.csv: 62 B / 62 B ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì 100.00% 206 B/s 0s Check that the object was added: mc ls local/master.foo.bar # [2021-04-26 12:11:37 PDT] 2.6MiB github_issues_medium.csv # [2021-04-26 12:11:37 PDT] 62B test.csv ‚ÑπÔ∏è Not all the repositories that you see in the results of the ls command are repositories that can be written to. Some of them might be read-only. Note that you should have writing access to the input repo in order to be able to add files to it.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "4c6e9befd7d9a91ab79caf46e51951c2"
    },
    {
      "title": "Integrate",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "Integrate with popular tools such as JupyterLab.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/",
      "relURI": "/latest/integrate/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "0b0b6c2050a45a33ca03289ac500a050"
    },
    {
      "title": "Determined",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Integrate",
      "description": "Learn about Determined, a deep learning platform for training machine learning models.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/determined/",
      "relURI": "/latest/integrate/determined/",
      "body": "Determined is a deep learning platform for training machine learning models that can be used alongside HPE ML Data Management.\nUse HPE ML Data Management to store and version your data via repositories and pipelines Use Determined to train your models on HPE ML Data Management data Resources # Batch Inferencing with Determined &amp; Pachyderm End-to-End Deep Learning with Pachyderm, Determined, and Seldon Core Determined Documentation ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations"
      ],
      "id": "1916a5a52e7d447d753616a6dc28de81"
    },
    {
      "title": "Google BigQuery",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Integrate",
      "description": "Learn how to use the Google BigQuery connector to ingest data.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/bigquery/",
      "relURI": "/latest/integrate/bigquery/",
      "body": "This connector ingests the result of a BigQuery query into Pachyderm. With this connector, you can easily create pipelines that read data from BigQuery and process it using Pachyderm&rsquo;s powerful data parallelism and versioning capabilities. It uses python and the python-bigquery-pandas library built by Google.\nBefore You Start # You must have a Google Cloud Platform account with a project that has BigQuery enabled.\nYou must download the following files:\ndockerfile\ngbq_ingest.jsonnet\ngbq_ingest.py\nrequirements.txt\n1. Create a Dataset &amp; Service Account # Before using this connector, you will need to create a BigQuery dataset and a service account with the necessary permissions.\nGo to the BigQuery web UI and create a new dataset. In the GCP Console, go to the IAM &amp; admin section and create a new service account. Grant the service account the BigQuery Data Viewer and BigQuery Data Editor roles for the dataset you created in step 1. Download the private key file for the service account, give it a descriptive name (e.g., gbq-pachyderm-creds.json), and save it to a secure location. Once you have completed these steps, you can use the connector by simply specifying your service account key file and the name of your BigQuery query in the jsonnet pipeline spec for your pipeline.\n2. Create a Pachyderm Secret # Create the secret (be sure to add the namespace if your cluster is deployed in one).\nNamespaced: Yes No kubectl create secret generic gbqsecret --from-file=gbq-pachyderm-creds.json -n mynamespace kubectl create secret generic gbqsecret --from-file=gbq-pachyderm-creds.json 3. Create the Jsonnet Pipeline # Run the pipeline template with jsonnet. Note, this pipeline will not run immediately, but rather will wait until the next cron tick. E.g. if you have it set to 5m, you will wait 5 minutes until the first run of the pipeline.\n$ pachctl update pipeline --jsonnet gbq_ingest.jsonnet \\ --arg inputQuery=&#34;SELECT country_name, alpha_2_code FROM bigquery-public-data.utility_us.country_code_iso WHERE alpha_2_code LIKE &#39;A%&#39;&#34; \\ --arg outFile=&#34;gbq_output.parquet&#34; \\ --arg project=&#34;&lt;gcp_project_name&gt;&#34; \\ --arg cronSpec=&#34;@every 5m&#34; --arg credsFile=&#34;gbq-pachyderm-creds.json&#34; Additional Details # Cron Spec # In some cases, you may not want your cron pipeline to run every 5 minutes as shown in the example above. For example, when testing a large ingestion, you may want to run the pipeline manually. To do this, you can use a cron specification that never triggers, such as --arg cronSpec=&quot;* * 31 2 *&quot;. This value will never trigger the cron pipeline (it refers to a nonexistent date - 31st of Feb).\nTo manually trigger the cron pipeline, run the following command:\npachctl run cron gbq_ingest For more information on using cron with Pachyderm pipelines, see the cron documentation.\nConfiguring your own pipeline spec # You can configure your own pipeline spec with the secret by using these parameters in the pipeline spec.\n&#34;secrets&#34;: [ { &#34;name&#34;: &#34;gbqsecret&#34;, &#34;mount_path&#34;: &#34;/kubesecret/&#34;}] and\n&#34;env&#34;: { &#34;GOOGLE_APPLICATION_CREDENTIALS&#34;: &#34;/kubesecret/gbq-pachyderm-creds.json&#34; }, ",
      "beta": "false",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "bigquery",
        "google"
      ],
      "id": "0dae4317d4cca12be72c6e9ac6a9c337"
    },
    {
      "title": "JupyterLab",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Integrate",
      "description": "Learn how to install and use the JupyterLab Mount Extension.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/jupyterlab-extension/",
      "relURI": "/latest/integrate/jupyterlab-extension/",
      "body": "Use the JupyterLab extension to:\nConnect your Notebook to a HPE ML Data Management cluster Browse, explore, and analyze data stored in HPE ML Data Management directly from your Notebook Run and test out your pipeline code before creating a Docker image Install the Extension # There are two main ways to install the Jupyter Lab extension:\n‚≠ê Via Docker: Fastest implementation! üß™ Locally: Great for development and testing Examples # Make sure to check our data science notebook examples running on HPE ML Data Management, from a market sentiment NLP implementation using a FinBERT model to pipelines training a regression model on the Boston Housing Dataset. You will also find integration examples with open-source products, such as labeling or model serving applications.\n",
      "beta": "true",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "jupyterlab",
        "notebooks"
      ],
      "id": "0250417fccfdb82118804c8f67def556"
    },
    {
      "title": "Docker Installation Guide",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "JupyterLab",
      "description": "Learn how to install and use the JupyterLab Mount Extension using a Docker image.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/jupyterlab-extension/docker-install/",
      "relURI": "/latest/integrate/jupyterlab-extension/docker-install/",
      "body": " Install to Existing Docker Image # You can choose between HPE ML Data Management&rsquo;s pre-built image (a custom version of jupyter/scipy-notebook) or add the extension to your own image. HPE ML Data Management&rsquo;s image includes:\nThe extension jupyterlab-pachyderm FUSE A pre-created /pfs directory that mounts to and grants ownership to the JupyterLab User A mount-server binary Option 1: Pre-Built Image # Open your terminal. Run the following: docker run -it -p 8888:8888 -e GRANT_SUDO=yes --user root --device /dev/fuse --privileged --entrypoint /opt/conda/bin/jupyter pachyderm/notebooks-user:v2.7.3 lab --allow-root Open the UI using the link provided in the terminal following: [I 2023-01-26 19:07:00.245 ServerApp] Jupyter Server 1.16.0 is running at: [I 2023-01-26 19:07:00.245 ServerApp] http://fb66b212ca13:8888/lab?token=013dbb47fc32c0f1ec8277a399e8ccf0e4eb87055942a21d [I 2023-01-26 19:07:00.245 ServerApp] or http://127.0.0.1:8888/lab?token=013dbb47fc32c0f1ec8277a399e8ccf0e4eb87055942a21d Navigate to the connection tab. You will need to provide a link formatted like the following: http://localhost:80 Navigate to the Launcher view in Jupyter and select Terminal. Input the following command: pachctl version If you see a pachctl and pachd version, you are good to go. Option 2: Custom Dockerfile # Replace the following ${PACHCTL_VERSION} with the version of pachctl that matches your cluster&rsquo;s, and update &lt;version&gt; with the release number of the extension.\nYou can find the latest available version of our HPE ML Data Management Mount Extension in PyPi.\n# This runs the following section as root; if adding to an existing Dockerfile, set the user back to whatever you need. USER root # This is the directory files will be mounted to, mirroring how pipelines are run. RUN mkdir -p /pfs # If you are not using &#34;jovyan&#34; as your notebook user, replace the user here. RUN chown $NB_USER /pfs # Fuse is a requirement for the mount extension RUN apt-get clean &amp;&amp; RUN apt-get update &amp;&amp; apt-get -y install curl fuse # Install the mount-server binary RUN curl -f -o mount-server.deb -L https://github.com/pachyderm/pachyderm/releases/download/v${PACHCTL_VERSION}/mount-server_${PACHCTL_VERSION}_amd64.deb RUN dpkg -i mount-server.deb # Optionally Install Pachctl - Set the version of Pachctl that matches your cluster deployment. RUN curl -f -o pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v${PACHCTL_VERSION}/pachctl_${PACHCTL_VERSION}_amd64.deb RUN dpkg -i pachctl.deb # This sets the user back to the notebook user account (i.e., Jovyan) USER $NB_UID # Replace the version here with the version of the extension you would like to install from https://pypi.org/project/jupyterlab-pachyderm/ RUN pip install jupyterlab-pachyderm==&lt;version&gt; Then, build, tag, and push your image.\n",
      "beta": "true",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "jupyterlab",
        "notebooks",
        "docker"
      ],
      "id": "f778ab504b03160bd7790073e1a67919"
    },
    {
      "title": "Run in Determined",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "JupyterLab",
      "description": "Learn how to run the JupyterLab Mount Extension inside of Determined.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/jupyterlab-extension/determined/",
      "relURI": "/latest/integrate/jupyterlab-extension/determined/",
      "body": "You can run the HPE ML Data Management Jupyterlab Extension inside of Determined.\nBefore You Start # You must have Determined 0.23.4 or greater installed You must have HPE ML Data Management JupyterLab Extension 2.7.0 or greater installed How to Run the Extension in Determined # Open Determined. Navigate to the Notebook Launcher. Click Show Full Config. Replace the pod_spec section with the following: pod_spec: spec: containers: - name: determined-container securityContext: privileged: true Update the cpu image to the following: pachderm/notebooks-user:v2.7.3 Limitations # GPUs are not yet supported ",
      "beta": "true",
      "hidden": "false",
      "categories": [],
      "tags": [
        "integrations",
        "jupyterlab",
        "notebooks",
        "docker",
        "determined"
      ],
      "id": "53b22b14e3aba137bf21fdff0b39b40b"
    },
    {
      "title": "Local Installation Guide",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "JupyterLab",
      "description": "Learn how to locally install and use the JupyterLab Mount Extension.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/jupyterlab-extension/local-install/",
      "relURI": "/latest/integrate/jupyterlab-extension/local-install/",
      "body": " Before You Start # You must have a HPE ML Data Management cluster running. Install Jupyter Lab (pip install jupyterlab) Install FUSE ‚ö†Ô∏è Local installation of FUSE requires a reboot to access your Startup Security Utility and enable kernel extensions (kexts) after you have downloaded all of the necessary pre-requisites.\nInstall jupyterlab pachyderm (pip install jupyterlab-pachyderm) Download mount-server binary Local Installation Steps # Open your terminal. Navigate to your downloads folder. Copy the mount-server binary you downloaded from the pre-requisites into a folder included within your $PATH so that your jupyterlab-pachyderm extension can find it: sudo cp mount-server /usr/local/bin Open your zshrc profile: vim ~/.zshrc Create a /pfs directory to mount your data to. This is the default directory used; alternatively, you can define an empty output folder that PFS should mount by adding export PFS_MOUNT_DIR=/&lt;directory&gt;/&lt;path&gt; to your bash/zshrc profile. Update the source by restarting your computer or executing the following command: source ~/.zshrc Run jupyter lab. If you have an existing pachyderm config file at ~/.pachyderm/config.json, the extension automatically connects to the active context. Otherwise, you must enter the cluster address manually in the extension UI.\n",
      "beta": "true",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "jupyterlab",
        "notebooks"
      ],
      "id": "71386baab744c0613e6bf0c3c35fbbc7"
    },
    {
      "title": "User Guide",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "JupyterLab",
      "description": "Learn how to use the JupyterLab Mount Extension.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/jupyterlab-extension/user-guide/",
      "relURI": "/latest/integrate/jupyterlab-extension/user-guide/",
      "body": " Select a Project # You can filter mountable repositories by selecting a project.\nOpen the JupyterLab UI. Navigate to the Pachyderm Mount tab (). Navigate to the Project dropdown. Select an existing project or the default project. Create a Repo &amp; Repo Branch # Open the JupyterLab UI.\nOpen a Terminal from the launcher.\nInput the following:\npachctl create repo demo pachctl create branch demo@master Open the Pachyderm Mount tab ().\nCheck the Unmounted Repositories section.\nüí° Your repo is created within the project set to your current context.\nCreate a Pipeline # Open the JupyterLab UI. Create a notebook from the launcher (it can be left blank). Navigate to the Pachyderm Mount tab (). Select Pipeline in the side panel. Input values for all of the following: Name: The name of your pipeline. Image: The Docker Hub image that has your user code. Requirements: A requirements.txt file that contains the dependencies for your code. Input Spec: The input spec for your pipeline in YAML format. See the Pipeline Specification for input options. Select Save. Select Create Pipeline. Track the status of your pipeline using the command pachctl list pipelines in a terminal or view the pipeline in Console. üí° You can view the full compiled pipeline spec from the Pipline Spec Preview section.\nMount a Repo Branch # Open the JupyterLab UI. Navigate to the Pachyderm Mount tab (). Navigate to the Unmounted Repositories section. Scroll to a repository&rsquo;s row. Select Mount. Mount (and Test) a Datum # You can mount to a specific datum in your repository from the JupyterLab UI using an input spec. This is useful when:\nWorking on data that is deeply nested within a specific directory of your repository. Testing and exploring viable glob patterns to use for your datums. Open the JupyterLab UI.\nNavigate to the Pachyderm Mount tab ().\nMount to a repo from the Unmounted Repositories section. (e.g., mounting to demo would look like /pfs/demo/ in the file browser).\nNavigate to the Mounted Repositories section and select Datum.\nYou should see the following:\npfs: repo: demo branch: master glob: / Update the glob pattern to match the datums you wish to focus on.\nDirectory Example # pfs: repo: demo branch: master glob: /images/2022/* Extension Example # pfs: repo: demo branch: master glob: /images/**.png Select Mount Datums.\nThe file browser updates to display the matching datums.\nWhen you return to the mounted view by selecting Back, the file browser will return to displaying datums that match your default glob pattern.\nExplore Directories &amp; Files # At the bottom of the Mounted Repositories tab, you&rsquo;ll find the file browser.\nMounted repositories are nested within the root /pfs (HPE ML Data Management&rsquo;s File System) These repositories are read-only Mounted repositories have a / glob pattern applied to their directories and files Files only downloaded locally when you access them (saving you time) Using the previous example, while the Demo repository is mounted, you can select the demo folder to reveal the example myfile.txt.\n",
      "beta": "true",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "jupyterlab",
        "notebooks"
      ],
      "id": "40554dec346f95c575751d745dbf5479"
    },
    {
      "title": "Troubleshooting",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "JupyterLab",
      "description": "Learn how to troubleshoot the JupyterLab Mount Extension.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/jupyterlab-extension/troubleshooting/",
      "relURI": "/latest/integrate/jupyterlab-extension/troubleshooting/",
      "body": "In general, restarting your server should resolve most JupyterLab Mount Extension issues. To restart your server, run the following command from the terminal window in Jupyterlab:\npkill -f &#34;mount-server&#34; The server restarts by itself.\nKnown Issues # M1 Users With Docker Desktop &lt; 4.6 # A documented issue between qemu and Docker Desktop prevents you from running our pre-built Mount Extension Image in Docker Desktop.\nWe recommend the following:\nUse Podman (See installation instructions) brew install podman podman machine init --disk-size 50 podman machine start podman machine ssh sudo rpm-ostree install qemu-user-static &amp;&amp; sudo systemctl reboot THEN then replace the keyword docker with podman in all the commands above.\nOr make sure that your qemu version is &gt; 6.2 ",
      "beta": "true",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "jupyterlab",
        "notebooks"
      ],
      "id": "175946bb2fa95181e05bac24ea8aaba8"
    },
    {
      "title": "Label Studio",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Integrate",
      "description": "Learn how to use the Label Studio connector to ingest data.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/label-studio/",
      "relURI": "/latest/integrate/label-studio/",
      "body": "Label Studio supports many different types of data labeling tasks, while HPE ML Data Management allows you to incorporate data versioning and data-driven pipelines, enabling the management of the data loop. This integration connects a HPE ML Data Management versioned data backend with Label Studio to support versioning datasets and tracking the data lineage of pipelines built off the versioned datasets.\nBefore You Start # You must have a HPE ML Data Management cluster deployed. 1. Deploy Label Studio # The following one-liner will map your local configuration into the container to connect to HPE ML Data Management. If you are performing another form of authentication, then you may need to use the entrypoint /bin/bash to configure the container before running /usr/local/bin/label-studio.\n$ docker run -it --rm -p8080:8080 -v ~/.pachyderm/config.json:/root/.pachyderm/config.json --device=/dev/fuse --cap-add SYS_ADMIN --name label-studio --entrypoint=/usr/local/bin/label-studio jimmywhitaker/label-studio:pach2.2-ls1.4v3 Once running, we can access label studio by visiting: http://localhost:8080/.\nOnce we create a user, new project , and select our labeling task (in our case we&rsquo;ll use the &ldquo;Object Detection with Bounding Boxes&rdquo; template), we can configure the Cloud Storage settings to point to HPE ML Data Management, using the HPE ML Data Management Storage Type.\n2. Configure Source and Target Storage # Selecting Cloud Storage from the Label Studio settings will allow us to add Source and Target Storage sync our data with.\nFirst, let&rsquo;s create two data repositories in HPE ML Data Management for source and target storage. The source will be where we pull our unlabeled data from, and our target storage is where we&rsquo;ll write our labels.\npachctl create repo images pachctl put file images@master:liberty.png -f http://imgur.com/46Q8nDz.png pachctl create repo labels Next, we can add HPE ML Data Management as our source and target storages by configuring them in the Label Studio Settings as shown below:\nSource Storage # First, we will select HPE ML Data Management as our storage type. The &ldquo;Storage Title&rdquo; will be the mounted directory name in the Label Studio container. In general, you shouldn&rsquo;t have to worry about this very much, it&rsquo;s mainly used as a way to keep track of things, should you have many source repos. The Repository Name will be the name and branch of our repo where we should pull our data from. Here, for example, images@master.\nWe can check the connection to make sure it is correct by pressing the &ldquo;Check Connection&rdquo; button and then once it is saved, we can sync our data from the source storage. When sync all of the data from that branch of the repo is downloaded into the Label Studio container.\nNote: This is a one time operation, so we must press this button whenever we want to sync new examples.\nTarget Storage # Configuring our target storage is roughly the same as our source storage. We configure the title and repo name.\nHowever, with Target Storage, the &ldquo;sync storage&rdquo; button has two roles:\nWhen it is pressed the first time, the repository is mounted, but no data is transferred. This is necessary to have a place to accumulate our annotations. (Think of it as a staging area for an upcoming commit.) The second time it is pressed, it commits all files that have been labeled to the HPE ML Data Management repository (in our case labels@master). Note: In the image we only show labels but under the hood the code defaults to master if no other branch if provided. This is also the case with Source Storage.\nThis functionality is very beneficial because it means that we can have a single commit that contains all of our annotations instead of a commit per annotation, improving the speed of our data labeling.\nOnce configured, our storage should look like this:\nAnd when we move to our labeling environment, we see our example image present.\nCommit our Annotations to HPE ML Data Management # After we have annotated our image data, we can commit all of our annotations to HPE ML Data Management.\nTo do this we navigate back to Cloud Storage in our settings, and press the Sync Storage button on our Target Storage (labels@master). Under the hood, this will unmount the repo (committing the data) and then remount it again with the newest version of the branch. After the data is committed, it should look like the following:\nIn HPE ML Data Management, we can verify that our data was committed by running:\n$ pachctl list file labels@master NAME TYPE SIZE /1.json file 1.228KiB Building the LS Docker image from scratch # If you want to build the Label Studio docker image from scratch with the HPE ML Data Management cloud storage backend, you can run the following:\n$ git clone https://github.com/pachyderm/label-studio.git $ cd label-studio $ git checkout -b pachyderm $ docker build -t jimmywhitaker/label-studio:pach2.2-ls1.4v3 . ",
      "beta": "false",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "label",
        "studio",
        "label-studio"
      ],
      "id": "93eb4e700f5551f99a6535f81463576b"
    },
    {
      "title": "Superb AI",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Integrate",
      "description": "Learn how to use the Superb AI connector to ingest data.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/superb-ai/",
      "relURI": "/latest/integrate/superb-ai/",
      "body": "Connect your Superb.ai project to HPE ML Data Management to automatically version and save data you&rsquo;ve labeled in Superb AI to use in downstream machine learning workflows.\nThis integration ingests the data into HPE ML Data Management on a cron schedule. Once your data is ingested into HPE ML Data Management, you can perform data tests, train a model, or any other type of data automation you may want to do, all while having full end-to-end reproducibility.\nBefore You Start # You must have a Superb.AI account You must have a HPE ML Data Management cluster Download the example code and unzip it. (or download this repo. gh repo clone pachyderm/docs-content and navigate to docs-content/docs/latest/integrate/superb-ai) How to Use the Superb AI Connector # Generate an Access API Key in SuperbAI. Put the key and your user name in the secrets.json file. Create the Pachyderm secret pachctl create secret -f secrets.json Create the cron pipeline to synchronize your Sample project from SuperbAI to HPE ML Data Management. This pipeline will run every minute to check for new data (you can configure it to run more or less often in the cron spec in sample_project.yml). pachctl create pipeline -f sample_project.yml HPE ML Data Management will automatically kick off the pipeline and import the data from your sample project. ",
      "beta": "false",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "superb-ai"
      ],
      "id": "a4e5ef62b3018994d8f7466efff0c060"
    },
    {
      "title": "Weights and Biases",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Integrate",
      "description": "Learn how to use the Weights and Biases connector to track your data science experiments.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/integrate/weights-and-biases/",
      "relURI": "/latest/integrate/weights-and-biases/",
      "body": "Connect HPE ML Data Management to Weights and Biases to track your data science experiments. Using HPE ML Data Management as our execution platform, we can version our executions, code, data, and models while still tracking everything in W&amp;B.\nHere we&rsquo;ll use Pachyderm to manage our data and train our model.\nBefore You Start # You must have a W&amp;B account You must have a HPE ML Data Management cluster Download the example code and unzip it. (or download this repo. gh repo clone pachyderm/docs-content and navigate to docs-content/docs/latest/integrate/weights-and-biases) How to Use the Weights and Biases Connector # Create a HPE ML Data Management cluster. Create a W&amp;B Account Copy your W&amp;B API Key into the secrets.json file. We&rsquo;ll use this file to make a HPE ML Data Management secret. This keeps our access keys from being built into our container or put in plaintext somewhere. Create the secret with pachctl create secret -f secrets.json Run make all to create a data repository and the pipeline. üí° Downloading the data locally and then pushing it to a remote cluster seems like an extra step, especially when dealing with a standard dataset like MNIST. However, if we think about a real-world use case where multiple teams may be manipulating the data (removing examples, adding classes, etc.) then having a history for each of these models can be very useful. In most production settings with supervised learning, the labeling environment can be directly connected to the data repository, automating this step.\nAbout the MNIST example # Creates a project in W&amp;B with the name of the HPE ML Data Management pipeline. Trains an MNIST classifier in a HPE ML Data Management Job. Logs training info from training to W&amp;B. If the Data or HPE ML Data Management Pipeline changes, it kicks off a new training process. ",
      "beta": "false",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "integrations",
        "weights-and-biases"
      ],
      "id": "bab594f314b83b6d13f3f0a93a4eb6dc"
    },
    {
      "title": "Run Commands",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "Access the platform's API using PachCTL commands.",
      "date": "October 11, 2022",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/",
      "relURI": "/latest/run-commands/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e383479f197ed1e2d71b5cac64287956"
    },
    {
      "title": "pachctl",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl/",
      "relURI": "/latest/run-commands/pachctl/",
      "body": " pachctl # Synopsis # Access the Pachyderm API.\nPachCTL Environment Variables: PACH_CONFIG= | (Req) PachCTL config location. PACH_TRACE={true,false} | (Opt) Attach Jaeger trace to outgoing RPCs; JAEGER_ENDPOINT must be specified. [req. PACH_TRACE={true}]: JAEGER_ENDPOINT=: | Jaeger server to connect to. PACH_TRACE_DURATION= | Duration to trace pipelines after &lsquo;pachctl create-pipeline&rsquo;.\nDocumentation: https://docs.pachyderm.com/latest/\nOptions # -h, --help help for pachctl --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster pachctl buildinfo\t- Print go buildinfo. pachctl completion\t- Print or install terminal completion code. pachctl config\t- Manages the pachyderm config. pachctl connect\t- Connect to a Pachyderm Cluster pachctl copy\t- Copy a Pachyderm resource. pachctl create\t- Create a new instance of a Pachyderm resource. pachctl debug\t- Debug commands for analyzing a running cluster. pachctl delete\t- Delete an existing Pachyderm resource. pachctl diff\t- Show the differences between two Pachyderm resources. pachctl draw\t- Draw an ASCII representation of an existing Pachyderm resource. pachctl edit\t- Edit the value of an existing Pachyderm resource. pachctl enterprise\t- Enterprise commands enable Pachyderm Enterprise features pachctl exit\t- Exit the pachctl shell. pachctl find\t- Find a file addition, modification, or deletion in a commit. pachctl finish\t- Finish a Pachyderm resource. pachctl fsck\t- Run a file system consistency check on PFS. pachctl get\t- Get the raw data represented by a Pachyderm resource. pachctl glob\t- Print a list of Pachyderm resources matching a glob pattern. pachctl idp\t- Commands to manage identity provider integrations pachctl inspect\t- Show detailed information about a Pachyderm resource. pachctl kube-events\t- Return the kubernetes events. pachctl license\t- License commmands manage the Enterprise License service pachctl list\t- Print a list of Pachyderm resources of a specific type. pachctl logs\t- Return logs from a job. pachctl loki\t- Query the loki logs. pachctl mount\t- Mount pfs locally. This command blocks. pachctl port-forward\t- Forward a port on the local machine to pachd. This command blocks. pachctl put\t- Insert data into Pachyderm. pachctl restart\t- Cancel and restart an ongoing task. pachctl resume\t- Resume a stopped task. pachctl run\t- Manually run a Pachyderm resource. pachctl shell\t- Run the pachyderm shell. pachctl squash\t- Squash an existing Pachyderm resource. pachctl start\t- Start a Pachyderm resource. pachctl stop\t- Cancel an ongoing task. pachctl subscribe\t- Wait for notifications of changes to a Pachyderm resource. pachctl unmount\t- Unmount pfs. pachctl update\t- Change the properties of an existing Pachyderm resource. pachctl validate\t- Validate the specification of a Pachyderm resource. pachctl version\t- Print Pachyderm version information. pachctl wait\t- Wait for the side-effects of a Pachyderm resource to propagate. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a35c55612cf1dd4bb5f6c5913ef8add3"
    },
    {
      "title": "pachctl auth",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth/",
      "relURI": "/latest/run-commands/pachctl_auth/",
      "body": " pachctl auth # Auth commands manage access to data in a Pachyderm cluster\nSynopsis # Auth commands manage access to data in a Pachyderm cluster\nOptions # -h, --help help for auth Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl auth activate\t- Activate Pachyderm&rsquo;s auth system pachctl auth check\t- Check whether a subject has a permission on a resource pachctl auth deactivate\t- Delete all ACLs, tokens, admins, IDP integrations and OIDC clients, and deactivate Pachyderm auth pachctl auth get\t- Get the role bindings for a resource pachctl auth get-config\t- Retrieve Pachyderm&rsquo;s current auth configuration pachctl auth get-groups\t- Get the list of groups a user belongs to pachctl auth get-robot-token\t- Get an auth token for a robot user with the specified name. pachctl auth login\t- Log in to Pachyderm pachctl auth logout\t- Log out of Pachyderm by deleting your local credential pachctl auth revoke\t- Revoke a Pachyderm auth token pachctl auth roles-for-permission\t- List roles that grant the given permission pachctl auth rotate-root-token\t- Rotate the root user&rsquo;s auth token pachctl auth set\t- Set the role bindings for a resource pachctl auth set-config\t- Set Pachyderm&rsquo;s current auth configuration pachctl auth use-auth-token\t- Read a Pachyderm auth token from stdin, and write it to the current user&rsquo;s Pachyderm config file pachctl auth whoami\t- Print your Pachyderm identity ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "20656c923cc3d4400c4fa26107dc0789"
    },
    {
      "title": "pachctl auth activate",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_activate command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_activate/",
      "relURI": "/latest/run-commands/pachctl_auth_activate/",
      "body": " pachctl auth activate # Activate Pachyderm&rsquo;s auth system\nSynopsis # Activate Pachyderm&rsquo;s auth system, and restrict access to existing data to the root user\npachctl auth activate [flags] Options # --client-id string The client ID for this pachd (default &#34;pachd&#34;) --enterprise Activate auth on the active enterprise context -h, --help help for activate --issuer string The issuer for the OIDC service (default &#34;http://pachd:1658/&#34;) --only-activate Activate auth without configuring the OIDC service --redirect string The redirect URL for the OIDC service (default &#34;http://localhost:30657/authorization-code/callback&#34;) --scopes strings Comma-separated list of scopes to request (default [email,profile,groups,openid]) --supply-root-token Prompt the user to input a root token on stdin, rather than generating a random one. --trusted-peers strings Comma-separated list of OIDC client IDs to trust Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "d9d5e79d50b637ca5e9bdd902bcdb92d"
    },
    {
      "title": "pachctl auth check",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_check command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_check/",
      "relURI": "/latest/run-commands/pachctl_auth_check/",
      "body": " pachctl auth check # Check whether a subject has a permission on a resource\nSynopsis # Check whether a subject has a permission on a resource\nOptions # -h, --help help for check Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster pachctl auth check project\t- Check the permissions a user has on &lsquo;project&rsquo; pachctl auth check repo\t- Check the permissions a user has on &lsquo;repo&rsquo; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "af5fdfa18b852e93b7555d437da1a0b5"
    },
    {
      "title": "pachctl auth check project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_check_project command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_check_project/",
      "relURI": "/latest/run-commands/pachctl_auth_check_project/",
      "body": " pachctl auth check project # Check the permissions a user has on &lsquo;project&rsquo;\nSynopsis # Check the permissions a user has on &lsquo;project&rsquo;\npachctl auth check project &lt;project&gt; [user] [flags] Options # -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth check\t- Check whether a subject has a permission on a resource ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "321d2c698c9f3c083d88457a20940c1b"
    },
    {
      "title": "pachctl auth check repo",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_check_repo command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_check_repo/",
      "relURI": "/latest/run-commands/pachctl_auth_check_repo/",
      "body": " pachctl auth check repo # Check the permissions a user has on &lsquo;repo&rsquo;\nSynopsis # Check the permissions a user has on &lsquo;repo&rsquo;\npachctl auth check repo &lt;repo&gt; [&lt;user&gt;] [flags] Options # -h, --help help for repo --project string The project containing the repo. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth check\t- Check whether a subject has a permission on a resource ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "ae8dee25e6754673da816e29b8d7a965"
    },
    {
      "title": "pachctl auth deactivate",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_deactivate command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_deactivate/",
      "relURI": "/latest/run-commands/pachctl_auth_deactivate/",
      "body": " pachctl auth deactivate # Delete all ACLs, tokens, admins, IDP integrations and OIDC clients, and deactivate Pachyderm auth\nSynopsis # Deactivate Pachyderm&rsquo;s auth and identity systems, which will delete ALL auth tokens, ACLs and admins, IDP integrations and OIDC clients, and expose all data in the cluster to any user with cluster access. Use with caution.\npachctl auth deactivate [flags] Options # --enterprise Deactivate auth on the active enterprise context -h, --help help for deactivate Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "88248a9ced779a1e979c2fc6eb510720"
    },
    {
      "title": "pachctl auth get",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_get command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_get/",
      "relURI": "/latest/run-commands/pachctl_auth_get/",
      "body": " pachctl auth get # Get the role bindings for a resource\nSynopsis # Get the role bindings for a resource\nOptions # -h, --help help for get Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster pachctl auth get cluster\t- Get the role bindings for &lsquo;cluster&rsquo; pachctl auth get enterprise\t- Get the role bindings for the enterprise server pachctl auth get project\t- Get the role bindings for &lsquo;project&rsquo; pachctl auth get repo\t- Get the role bindings for &lsquo;repo&rsquo; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "080f23131579fd798afaa9aa568ec2b0"
    },
    {
      "title": "pachctl auth get cluster",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_get_cluster command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_get_cluster/",
      "relURI": "/latest/run-commands/pachctl_auth_get_cluster/",
      "body": " pachctl auth get cluster # Get the role bindings for &lsquo;cluster&rsquo;\nSynopsis # Get the role bindings for &lsquo;cluster&rsquo;\npachctl auth get cluster [flags] Options # -h, --help help for cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth get\t- Get the role bindings for a resource ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "7105091a31d331b5784ba51d24a28b8a"
    },
    {
      "title": "pachctl auth get enterprise",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_get_enterprise command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_get_enterprise/",
      "relURI": "/latest/run-commands/pachctl_auth_get_enterprise/",
      "body": " pachctl auth get enterprise # Get the role bindings for the enterprise server\nSynopsis # Get the role bindings for the enterprise server\npachctl auth get enterprise [flags] Options # -h, --help help for enterprise Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth get\t- Get the role bindings for a resource ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "d7329caa65bbbefbbd41cf8e0f30987b"
    },
    {
      "title": "pachctl auth get project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_get_project command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_get_project/",
      "relURI": "/latest/run-commands/pachctl_auth_get_project/",
      "body": " pachctl auth get project # Get the role bindings for &lsquo;project&rsquo;\nSynopsis # Get the role bindings for &lsquo;project&rsquo;\npachctl auth get project &lt;project&gt; [flags] Options # -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth get\t- Get the role bindings for a resource ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "76c1f847742954501af63b59d33a3543"
    },
    {
      "title": "pachctl auth get repo",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_get_repo command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_get_repo/",
      "relURI": "/latest/run-commands/pachctl_auth_get_repo/",
      "body": " pachctl auth get repo # Get the role bindings for &lsquo;repo&rsquo;\nSynopsis # Get the role bindings for &lsquo;repo&rsquo;\npachctl auth get repo &lt;repo&gt; [flags] Options # -h, --help help for repo --project string The project containing the repo. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth get\t- Get the role bindings for a resource ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "3a857dc5ed72dc6e55d1a7636c5178c1"
    },
    {
      "title": "pachctl auth get-config",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_get-config command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_get-config/",
      "relURI": "/latest/run-commands/pachctl_auth_get-config/",
      "body": " pachctl auth get-config # Retrieve Pachyderm&rsquo;s current auth configuration\nSynopsis # Retrieve Pachyderm&rsquo;s current auth configuration\npachctl auth get-config [flags] Options # --enterprise Get auth config for the active enterprise context -h, --help help for get-config -o, --output-format string output format (&#34;json&#34; or &#34;yaml&#34;) (default &#34;json&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "0354358c7651ee57b6ba5a3cc96162fa"
    },
    {
      "title": "pachctl auth get-groups",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_get-groups command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_get-groups/",
      "relURI": "/latest/run-commands/pachctl_auth_get-groups/",
      "body": " pachctl auth get-groups # Get the list of groups a user belongs to\nSynopsis # Get the list of groups a user belongs to. If no user is specified, the current user&rsquo;s groups are listed.\npachctl auth get-groups [username] [flags] Options # --enterprise Get group membership info from the enterprise server -h, --help help for get-groups Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "afce283847d1bc4a12c5f2b0104ae5e2"
    },
    {
      "title": "pachctl auth get-robot-token",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_get-robot-token command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_get-robot-token/",
      "relURI": "/latest/run-commands/pachctl_auth_get-robot-token/",
      "body": " pachctl auth get-robot-token # Get an auth token for a robot user with the specified name.\nSynopsis # Get an auth token for a robot user with the specified name.\npachctl auth get-robot-token [username] [flags] Options # --enterprise Get a robot token for the enterprise context -h, --help help for get-robot-token -q, --quiet if set, only print the resulting token (if successful). This is useful for scripting, as the output can be piped to use-auth-token --ttl string if set, the resulting auth token will have the given lifetime. If not set, the token does not expire. This flag should be a golang duration (e.g. &#34;30s&#34; or &#34;1h2m3s&#34;). Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "eb729b44280403b0fbb383c556ea279a"
    },
    {
      "title": "pachctl auth login",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_login command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_login/",
      "relURI": "/latest/run-commands/pachctl_auth_login/",
      "body": " pachctl auth login # Log in to Pachyderm\nSynopsis # Login to Pachyderm. Any resources that have been restricted to the account you have with your ID provider (e.g. GitHub, Okta) account will subsequently be accessible.\npachctl auth login [flags] Options # --enterprise Login for the active enterprise context -h, --help help for login -t, --id-token If set, read an ID token on stdin to authenticate the user -b, --no-browser If set, don&#39;t try to open a web browser Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "1fdd0f8e1876c30521fe6b4ec8e0df1e"
    },
    {
      "title": "pachctl auth logout",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_logout command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_logout/",
      "relURI": "/latest/run-commands/pachctl_auth_logout/",
      "body": " pachctl auth logout # Log out of Pachyderm by deleting your local credential\nSynopsis # Log out of Pachyderm by deleting your local credential. Note that it&rsquo;s not necessary to log out before logging in with another account (simply run &lsquo;pachctl auth login&rsquo; twice) but &rsquo;logout&rsquo; can be useful on shared workstations.\npachctl auth logout [flags] Options # --enterprise Log out of the active enterprise context -h, --help help for logout Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "b951852bc13ab63d28c6027757c30595"
    },
    {
      "title": "pachctl auth revoke",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_revoke command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_revoke/",
      "relURI": "/latest/run-commands/pachctl_auth_revoke/",
      "body": " pachctl auth revoke # Revoke a Pachyderm auth token\nSynopsis # Revoke a Pachyderm auth token.\npachctl auth revoke [flags] Options # --enterprise Revoke an auth token (or all auth tokens minted for one user) on the enterprise server -h, --help help for revoke --token string Pachyderm auth token that should be revoked (one of --token or --user must be set) --user string User whose Pachyderm auth tokens should be revoked (one of --token or --user must be set) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "3723ed6c6a226b43feaf42491dcdc14b"
    },
    {
      "title": "pachctl auth roles-for-permission",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_roles-for-permission command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_roles-for-permission/",
      "relURI": "/latest/run-commands/pachctl_auth_roles-for-permission/",
      "body": " pachctl auth roles-for-permission # List roles that grant the given permission\nSynopsis # List roles that grant the given permission\npachctl auth roles-for-permission &lt;permission&gt; [flags] Options # -h, --help help for roles-for-permission Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e305513151ff87df40bbedb600386244"
    },
    {
      "title": "pachctl auth rotate-root-token",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_rotate-root-token command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_rotate-root-token/",
      "relURI": "/latest/run-commands/pachctl_auth_rotate-root-token/",
      "body": " pachctl auth rotate-root-token # Rotate the root user&rsquo;s auth token\nSynopsis # Rotate the root user&rsquo;s auth token\npachctl auth rotate-root-token [flags] Options # -h, --help help for rotate-root-token --supply-token string An auth token to rotate to. If left blank, one will be auto-generated. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "75244e5c64911eecf632c298a8c9b01f"
    },
    {
      "title": "pachctl auth set",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_set command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_set/",
      "relURI": "/latest/run-commands/pachctl_auth_set/",
      "body": " pachctl auth set # Set the role bindings for a resource\nSynopsis # Set the role bindings for a resource\nOptions # -h, --help help for set Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster pachctl auth set cluster\t- Set the roles that &lsquo;subject&rsquo; has on the &lsquo;cluster&rsquo; pachctl auth set enterprise\t- Set the roles that &lsquo;subject&rsquo; has on the enterprise server pachctl auth set project\t- Set the roles that &lsquo;subject&rsquo; has on &lsquo;project&rsquo; pachctl auth set repo\t- Set the roles that &lsquo;subject&rsquo; has on &lsquo;repo&rsquo; ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "785a75019462396ed955ba0ae18f2c5c"
    },
    {
      "title": "pachctl auth set cluster",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_set_cluster command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_set_cluster/",
      "relURI": "/latest/run-commands/pachctl_auth_set_cluster/",
      "body": " pachctl auth set cluster # Set the roles that &lsquo;subject&rsquo; has on the &lsquo;cluster&rsquo;\nSynopsis # Set the roles that &lsquo;subject&rsquo; has on the &lsquo;cluster&rsquo;\npachctl auth set cluster [role1,role2 | none ] subject [flags] Options # -h, --help help for cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth set\t- Set the role bindings for a resource ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "bc4a79327dd26c31ebc3bc6669a5bbff"
    },
    {
      "title": "pachctl auth set enterprise",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_set_enterprise command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_set_enterprise/",
      "relURI": "/latest/run-commands/pachctl_auth_set_enterprise/",
      "body": " pachctl auth set enterprise # Set the roles that &lsquo;subject&rsquo; has on the enterprise server\nSynopsis # Set the roles that &lsquo;subject&rsquo; has on the enterprise server\npachctl auth set enterprise [role1,role2 | none ] subject [flags] Options # -h, --help help for enterprise Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth set\t- Set the role bindings for a resource ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "72101dbc9c3c1f3509e6b291792c0c32"
    },
    {
      "title": "pachctl auth set project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_set_project command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_set_project/",
      "relURI": "/latest/run-commands/pachctl_auth_set_project/",
      "body": " pachctl auth set project # Set the roles that &lsquo;subject&rsquo; has on &lsquo;project&rsquo;\nSynopsis # Set the roles that &lsquo;subject&rsquo; has on &lsquo;project&rsquo;\npachctl auth set project &lt;project&gt; [role1,role2 | none ] &lt;subject&gt; [flags] Options # -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth set\t- Set the role bindings for a resource ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a27e46e3649724d8f8dfaec73780e17a"
    },
    {
      "title": "pachctl auth set repo",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_set_repo command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_set_repo/",
      "relURI": "/latest/run-commands/pachctl_auth_set_repo/",
      "body": " pachctl auth set repo # Set the roles that &lsquo;subject&rsquo; has on &lsquo;repo&rsquo;\nSynopsis # Set the roles that &lsquo;subject&rsquo; has on &lsquo;repo&rsquo;\npachctl auth set repo &lt;repo&gt; [role1,role2 | none ] &lt;subject&gt; [flags] Options # -h, --help help for repo --project string The project containing the repo. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth set\t- Set the role bindings for a resource ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "b25891223e062a3f625a0af4538d6a34"
    },
    {
      "title": "pachctl auth set-config",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_set-config command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_set-config/",
      "relURI": "/latest/run-commands/pachctl_auth_set-config/",
      "body": " pachctl auth set-config # Set Pachyderm&rsquo;s current auth configuration\nSynopsis # Set Pachyderm&rsquo;s current auth configuration\npachctl auth set-config [flags] Options # --enterprise Set auth config for the active enterprise context -f, --file string input file (to use as the new config (default &#34;-&#34;) -h, --help help for set-config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "28eaf825cbc7e79be5d495433f3408d1"
    },
    {
      "title": "pachctl auth use-auth-token",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_use-auth-token command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_use-auth-token/",
      "relURI": "/latest/run-commands/pachctl_auth_use-auth-token/",
      "body": " pachctl auth use-auth-token # Read a Pachyderm auth token from stdin, and write it to the current user&rsquo;s Pachyderm config file\nSynopsis # Read a Pachyderm auth token from stdin, and write it to the current user&rsquo;s Pachyderm config file\npachctl auth use-auth-token [flags] Options # --enterprise Use the token for the enterprise context -h, --help help for use-auth-token Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "585deb05fe353a9e81e4e3478c9bb04e"
    },
    {
      "title": "pachctl auth whoami",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_auth_whoami command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_auth_whoami/",
      "relURI": "/latest/run-commands/pachctl_auth_whoami/",
      "body": " pachctl auth whoami # Print your Pachyderm identity\nSynopsis # Print your Pachyderm identity.\npachctl auth whoami [flags] Options # --enterprise -h, --help help for whoami Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl auth\t- Auth commands manage access to data in a Pachyderm cluster ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "d17473cd5d7c398fcc8ed6d3ac40204d"
    },
    {
      "title": "pachctl buildinfo",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_buildinfo command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_buildinfo/",
      "relURI": "/latest/run-commands/pachctl_buildinfo/",
      "body": " pachctl buildinfo # Print go buildinfo.\nSynopsis # Print information about the build environment.\npachctl buildinfo [flags] Options # -h, --help help for buildinfo Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9ae77939735ff54f3cb905c45fb19cd5"
    },
    {
      "title": "pachctl completion",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_completion command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_completion/",
      "relURI": "/latest/run-commands/pachctl_completion/",
      "body": " pachctl completion # Print or install terminal completion code.\nSynopsis # Print or install terminal completion code.\nOptions # -h, --help help for completion Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl completion bash\t- Print or install the bash completion code. pachctl completion zsh\t- Print or install the zsh completion code. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "291f8e2f7eb99727123ecda1970ac69f"
    },
    {
      "title": "pachctl completion bash",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_completion_bash command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_completion_bash/",
      "relURI": "/latest/run-commands/pachctl_completion_bash/",
      "body": " pachctl completion bash # Print or install the bash completion code.\nSynopsis # Print or install the bash completion code.\npachctl completion bash [flags] Options # -h, --help help for bash --install Install the completion. --path string Path to install the completions to. (default &#34;/etc/bash_completion.d/pachctl&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl completion\t- Print or install terminal completion code. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "3794679213534a8ca7b93d3da1d1ad16"
    },
    {
      "title": "pachctl completion zsh",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_completion_zsh command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_completion_zsh/",
      "relURI": "/latest/run-commands/pachctl_completion_zsh/",
      "body": " pachctl completion zsh # Print or install the zsh completion code.\nSynopsis # Print or install the zsh completion code.\npachctl completion zsh [flags] Options # -h, --help help for zsh --install Install the completion. --path string Path to install the completions to. (default &#34;_pachctl&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl completion\t- Print or install terminal completion code. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "4bf82f314746fd249e8955af9ebc563c"
    },
    {
      "title": "pachctl config",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config/",
      "relURI": "/latest/run-commands/pachctl_config/",
      "body": " pachctl config # Manages the pachyderm config.\nSynopsis # Gets/sets pachyderm config values.\nOptions # -h, --help help for config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl config delete\t- Commands for deleting pachyderm config values pachctl config get\t- Commands for getting pachyderm config values pachctl config import-kube\t- Import a kubernetes context as a Pachyderm context, and set the active Pachyderm context. pachctl config list\t- Commands for listing pachyderm config values pachctl config set\t- Commands for setting pachyderm config values pachctl config update\t- Commands for updating pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "98524a4c2ff8fcc61b3ea8441e7e670c"
    },
    {
      "title": "pachctl config delete",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_delete command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_delete/",
      "relURI": "/latest/run-commands/pachctl_config_delete/",
      "body": " pachctl config delete # Commands for deleting pachyderm config values\nSynopsis # Commands for deleting pachyderm config values\nOptions # -h, --help help for delete Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config\t- Manages the pachyderm config. pachctl config delete context\t- Deletes a context. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "11a8d2ee7efa2587c8253bf2981bacd6"
    },
    {
      "title": "pachctl config delete context",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_delete_context command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_delete_context/",
      "relURI": "/latest/run-commands/pachctl_config_delete_context/",
      "body": " pachctl config delete context # Deletes a context.\nSynopsis # Deletes a context.\npachctl config delete context &lt;context&gt; [flags] Options # -h, --help help for context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config delete\t- Commands for deleting pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "94c43de335286ea01a86348cb3584b99"
    },
    {
      "title": "pachctl config get",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_get command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_get/",
      "relURI": "/latest/run-commands/pachctl_config_get/",
      "body": " pachctl config get # Commands for getting pachyderm config values\nSynopsis # Commands for getting pachyderm config values\nOptions # -h, --help help for get Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config\t- Manages the pachyderm config. pachctl config get active-context\t- Gets the currently active context. pachctl config get active-enterprise-context\t- Gets the currently active enterprise context. pachctl config get context\t- Gets a context. pachctl config get metrics\t- Gets whether metrics are enabled. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "04e2dca941f2eaaba4a53f923cd425d1"
    },
    {
      "title": "pachctl config get active-context",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_get_active-context command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_get_active-context/",
      "relURI": "/latest/run-commands/pachctl_config_get_active-context/",
      "body": " pachctl config get active-context # Gets the currently active context.\nSynopsis # Gets the currently active context.\npachctl config get active-context [flags] Options # -h, --help help for active-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config get\t- Commands for getting pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a175e3dcfca32db14c8a7a87dfcbbf30"
    },
    {
      "title": "pachctl config get active-enterprise-context",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_get_active-enterprise-context command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_get_active-enterprise-context/",
      "relURI": "/latest/run-commands/pachctl_config_get_active-enterprise-context/",
      "body": " pachctl config get active-enterprise-context # Gets the currently active enterprise context.\nSynopsis # Gets the currently active enterprise context.\npachctl config get active-enterprise-context [flags] Options # -h, --help help for active-enterprise-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config get\t- Commands for getting pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "0e028447871ea7144e35c3a457a26ffb"
    },
    {
      "title": "pachctl config get context",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_get_context command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_get_context/",
      "relURI": "/latest/run-commands/pachctl_config_get_context/",
      "body": " pachctl config get context # Gets a context.\nSynopsis # Gets the config of a context by its name.\npachctl config get context &lt;context&gt; [flags] Options # -h, --help help for context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config get\t- Commands for getting pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "8abccd4c735bba22cc99071e2ea1e4de"
    },
    {
      "title": "pachctl config get metrics",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_get_metrics command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_get_metrics/",
      "relURI": "/latest/run-commands/pachctl_config_get_metrics/",
      "body": " pachctl config get metrics # Gets whether metrics are enabled.\nSynopsis # Gets whether metrics are enabled.\npachctl config get metrics [flags] Options # -h, --help help for metrics Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config get\t- Commands for getting pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "77ebe5eef256105d0a064e491849d608"
    },
    {
      "title": "pachctl config import-kube",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_import-kube command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_import-kube/",
      "relURI": "/latest/run-commands/pachctl_config_import-kube/",
      "body": " pachctl config import-kube # Import a kubernetes context as a Pachyderm context, and set the active Pachyderm context.\nSynopsis # Import a kubernetes context as a Pachyderm context. By default the current kubernetes context is used.\npachctl config import-kube &lt;context&gt; [flags] Options # -e, --enterprise Configure an enterprise server context. -h, --help help for import-kube -k, --kubernetes string Specify the kubernetes context&#39;s values to import. -n, --namespace string Specify a namespace where Pachyderm is deployed. -o, --overwrite Overwrite a context if it already exists. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config\t- Manages the pachyderm config. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "bd2db0939507ce584efb3be063eb3734"
    },
    {
      "title": "pachctl config list",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_list command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_list/",
      "relURI": "/latest/run-commands/pachctl_config_list/",
      "body": " pachctl config list # Commands for listing pachyderm config values\nSynopsis # Commands for listing pachyderm config values\nOptions # -h, --help help for list Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config\t- Manages the pachyderm config. pachctl config list context\t- Lists contexts. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "bc4ea8d4ed026e7e26d1cd8ca3d14f42"
    },
    {
      "title": "pachctl config list context",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_list_context command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_list_context/",
      "relURI": "/latest/run-commands/pachctl_config_list_context/",
      "body": " pachctl config list context # Lists contexts.\nSynopsis # Lists contexts.\npachctl config list context [flags] Options # -h, --help help for context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config list\t- Commands for listing pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "bfa12734a2479bc50a46956d57d7644f"
    },
    {
      "title": "pachctl config set",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_set command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_set/",
      "relURI": "/latest/run-commands/pachctl_config_set/",
      "body": " pachctl config set # Commands for setting pachyderm config values\nSynopsis # Commands for setting pachyderm config values\nOptions # -h, --help help for set Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config\t- Manages the pachyderm config. pachctl config set active-context\t- Sets the currently active context. pachctl config set active-enterprise-context\t- Sets the currently active enterprise context. pachctl config set context\t- Set a context. pachctl config set metrics\t- Sets whether metrics are enabled. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "02f43ca01c02920d1e5c0b6207c5511f"
    },
    {
      "title": "pachctl config set active-context",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_set_active-context command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_set_active-context/",
      "relURI": "/latest/run-commands/pachctl_config_set_active-context/",
      "body": " pachctl config set active-context # Sets the currently active context.\nSynopsis # Sets the currently active context.\npachctl config set active-context &lt;context&gt; [flags] Options # -h, --help help for active-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config set\t- Commands for setting pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "fc9482234f0c179f67ce3dfb143d6c34"
    },
    {
      "title": "pachctl config set active-enterprise-context",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_set_active-enterprise-context command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_set_active-enterprise-context/",
      "relURI": "/latest/run-commands/pachctl_config_set_active-enterprise-context/",
      "body": " pachctl config set active-enterprise-context # Sets the currently active enterprise context.\nSynopsis # Sets the currently active enterprise context.\npachctl config set active-enterprise-context &lt;context&gt; [flags] Options # -h, --help help for active-enterprise-context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config set\t- Commands for setting pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "00b4b34230c499cec51130287780d5ed"
    },
    {
      "title": "pachctl config set context",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_set_context command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_set_context/",
      "relURI": "/latest/run-commands/pachctl_config_set_context/",
      "body": " pachctl config set context # Set a context.\nSynopsis # Set a context config from a given name and a JSON configuration file on stdin\npachctl config set context &lt;context&gt; [flags] Options # -h, --help help for context --overwrite Overwrite a context if it already exists. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config set\t- Commands for setting pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "0b2a726e9114956006467b6b39e771e6"
    },
    {
      "title": "pachctl config set metrics",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_set_metrics command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_set_metrics/",
      "relURI": "/latest/run-commands/pachctl_config_set_metrics/",
      "body": " pachctl config set metrics # Sets whether metrics are enabled.\nSynopsis # Sets whether metrics are enabled.\npachctl config set metrics (true | false) [flags] Options # -h, --help help for metrics Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config set\t- Commands for setting pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "4f9d7b5dab75f02a238a862abd869d17"
    },
    {
      "title": "pachctl config update",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_update command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_update/",
      "relURI": "/latest/run-commands/pachctl_config_update/",
      "body": " pachctl config update # Commands for updating pachyderm config values\nSynopsis # Commands for updating pachyderm config values\nOptions # -h, --help help for update Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config\t- Manages the pachyderm config. pachctl config update context\t- Updates a context. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "963bd889fd2d7507c643fee39cf069c5"
    },
    {
      "title": "pachctl config update context",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_config_update_context command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_config_update_context/",
      "relURI": "/latest/run-commands/pachctl_config_update_context/",
      "body": " pachctl config update context # Updates a context.\nSynopsis # Updates an existing context config from a given name (or the currently-active context, if no name is given).\npachctl config update context [&lt;context&gt;] [flags] Options # --auth-info string Set a new k8s auth info. --cluster-name string Set a new cluster name. -h, --help help for context --namespace string Set a new namespace. --pachd-address string Set a new name pachd address. --project string Set a new project. --remove-cluster-deployment-id Remove the cluster deployment ID field, which will be repopulated on the next &#39;pachctl&#39; call using this context. --server-cas string Set new trusted CA certs. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl config update\t- Commands for updating pachyderm config values ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "8a483f4c6d37a67f3c8adc549cf09f16"
    },
    {
      "title": "pachctl connect",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_connect command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_connect/",
      "relURI": "/latest/run-commands/pachctl_connect/",
      "body": " pachctl connect # Connect to a Pachyderm Cluster\nSynopsis # Creates a Pachyderm context at the given address and sets it as active\npachctl connect &lt;address&gt; [flags] Options # --alias string Alias for the context that is created -h, --help help for connect Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "108ca440575388b0a0982431e2b508c9"
    },
    {
      "title": "pachctl copy",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_copy command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_copy/",
      "relURI": "/latest/run-commands/pachctl_copy/",
      "body": " pachctl copy # Copy a Pachyderm resource.\nSynopsis # Copy a Pachyderm resource.\nOptions # -h, --help help for copy Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl copy file\t- Copy files between pfs paths. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "3d009a60d24645a0246d17a72b397a13"
    },
    {
      "title": "pachctl copy file",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_copy_file command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_copy_file/",
      "relURI": "/latest/run-commands/pachctl_copy_file/",
      "body": " pachctl copy file # Copy files between pfs paths.\nSynopsis # This command files between pfs paths. While using this command, take special note of which project is set to your active context by running pachctl list projects and checking for the * in the ACTIVE column.\nTo append to an existing file, use the &ndash;append flag. To specify the project where both the source and destination repos are located, use the &ndash;project flag. This is only necessary if the project in question is not set to your active context. To copy a file from one project to another, use the &ndash;src-project and &ndash;dest-project flags. Needing to use one (or both) depends on whether or not either project is set to your active context. pachctl copy file &lt;src-repo&gt;@&lt;src-branch-or-commit&gt;:&lt;src-path&gt; &lt;dst-repo&gt;@&lt;dst-branch-or-commit&gt;:&lt;dst-path&gt; [flags] Examples # pachctl copy file foo@master:/file bar@master:/file pachctl copy file foo@0001a0100b1c10d01111e001fg00h00i:/file bar@master:/file pachctl copy file foo@master:/file bar@master:/file --project ProjectContainingFooAndBar pachctl copy file foo@master:/file bar@master:/file --dest-project ProjectContainingBar pachctl copy file foo@master:/file bar@master:/file --src-project ProjectContainingFoo pachctl copy file foo@master:/file bar@master:/file --src-project ProjectContainingFoo --dest-project ProjectContainingBar Options # -a, --append Append to the existing content of the file, either from previous commits or previous calls to &#39;put file&#39; within this commit. --dest-project string Specify the project (by name) where the destination repo is located; this overrides --project. -h, --help help for file --project string Specify the project (by name) where both source and destination repos are located. (default &#34;video-to-frame-traces&#34;) --src-project string Specify the project (by name) where the source repo is located; this overrides --project. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl copy\t- Copy a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "db3bfe9187b147b4ca4b35c9eb4ff31b"
    },
    {
      "title": "pachctl create",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_create command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_create/",
      "relURI": "/latest/run-commands/pachctl_create/",
      "body": " pachctl create # Create a new instance of a Pachyderm resource.\nSynopsis # Create a new instance of a Pachyderm resource.\nOptions # -h, --help help for create Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl create branch\t- Create a new branch, or update an existing branch, on a repo. pachctl create pipeline\t- Create a new pipeline. pachctl create project\t- Create a new project. pachctl create repo\t- Create a new repo in your active project. pachctl create secret\t- Create a secret on the cluster. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "71ad0aa6d0d35e4c1ce00805ccafaffa"
    },
    {
      "title": "pachctl create branch",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_create_branch command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_create_branch/",
      "relURI": "/latest/run-commands/pachctl_create_branch/",
      "body": " pachctl create branch # Create a new branch, or update an existing branch, on a repo.\nSynopsis # This command creates or updates a branch on a repo.\nTo create a branch for a repo in a particular project, use the --project flag; this requires the repo to already exist in that project To attach an existing commit as the head commit of the new branch, use the --head flag To set a trigger, use the --trigger flag, pass in the branch (from same repo, without repo@), and set conditions using any of the --trigger options To require all defined triggering conditions to be met, use the --trigger-all flag; otherwise, each condition can execute the trigger To attach provenance to the new branch, use the --provenance flag. You can inspect provenance using pachctl inspect branch foo@bar Note: Starting a commit on the branch also creates it, so there&rsquo;s often no need to call this.\npachctl create branch &lt;repo&gt;@&lt;branch&gt; [flags] Examples # pachctl create branch foo@master pachctl create branch foo@master --project bar pachctl create branch foo@master --head 0001a0100b1c10d01111e001fg00h00i pachctl create branch foo@master=0001a0100b1c10d01111e001fg00h00i pachctl create branch foo@master --provenance=foo@branch1,foo@branch2 pachctl create branch foo@master --trigger staging pachctl create branch foo@master --trigger staging --trigger-size=100M pachctl create branch foo@master --trigger staging --trigger-cron=&#39;@every 1h&#39; pachctl create branch foo@master --trigger staging --trigger-commits=10&#39; pachctl create branch foo@master --trigger staging --trigger-size=100M --trigger-cron=&#39;@every 1h --trigger-commits=10 --trigger-all Options # --head string Set the head of the newly created branch using &lt;branch-or-commit&gt; or &lt;repo&gt;@&lt;branch&gt;=&lt;id&gt; -h, --help help for branch --project string Specify the project (by name) where the repo for this branch is located. (default &#34;video-to-frame-traces&#34;) -p, --provenance []string Set the provenance for the branch. format: &lt;repo&gt;@&lt;branch&gt; (default []) -t, --trigger string Specify the branch name that triggers this branch. --trigger-all Specify that all set conditions must be met for the trigger. --trigger-commits int Set the number of commits as a condition for the trigger. --trigger-cron string Set a cron spec interval as a condition for the trigger. --trigger-size string Set data size as a condition for the trigger. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl create\t- Create a new instance of a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "399bcfc58a7080667b87d35af6ae54de"
    },
    {
      "title": "pachctl create pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_create_pipeline command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_create_pipeline/",
      "relURI": "/latest/run-commands/pachctl_create_pipeline/",
      "body": " pachctl create pipeline # Create a new pipeline.\nSynopsis # This command creates a new pipeline from a pipeline specification.\nYou can create a pipeline using a JSON/YAML file or a jsonnet template file &ndash; via either a local filepath or URL. Multiple pipelines can be created from one file.For details on the format, see https://docs.pachyderm.com/latest/reference/pipeline_spec/.\nTo create a pipeline from a JSON/YAML file, use the --file flag To create a pipeline from a jsonnet template file, use the --jsonnet flag; you can optionally pay multiple arguments separately using --arg To push your local images to docker registry, use the --push-images and --username flags To push your local images to custom registry, use the --push-images, --registry, and --username flags pachctl create pipeline [flags] Examples # pachctl create pipeline -file regression.json pachctl create pipeline -file foo.json --project bar pachctl create pipeline -file foo.json --push-images --username lbliii pachctl create pipeline --jsonnet /templates/foo.jsonnet --arg myimage=bar --arg src=image Options # --arg stringArray Provide a top-level argument in the form of &#39;param=value&#39; passed to the Jsonnet template; requires --jsonnet. For multiple args, --arg may be set more than once. --dry-run If true, pipeline will not actually be created. -f, --file string Provide a JSON/YAML file (url or filepath) for one or more pipelines. &#34;-&#34; reads from stdin (the default behavior). Exactly one of --file and --jsonnet must be set. -h, --help help for pipeline --jsonnet string Provide a Jsonnet template file (url or filepath) for one or more pipelines. &#34;-&#34; reads from stdin. Exactly one of --file and --jsonnet must be set. Jsonnet templates must contain a top-level function; strings can be passed to this function with --arg (below) -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) in which to create the pipeline. (default &#34;video-to-frame-traces&#34;) -p, --push-images Specify that the local docker images should be pushed into the registry (docker by default). --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml -r, --registry string Specify an alternative registry to push images to. (default &#34;index.docker.io&#34;) -u, --username string Specify the username to push images as. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl create\t- Create a new instance of a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "ee97a2d6de258c63ed08c1d1c6744869"
    },
    {
      "title": "pachctl create project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_create_project command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_create_project/",
      "relURI": "/latest/run-commands/pachctl_create_project/",
      "body": " pachctl create project # Create a new project.\nSynopsis # This command creates a new project.\nTo set a description for the project, use the --description flag pachctl create project &lt;project&gt; [flags] Examples # pachctl create project foo-project pachctl create project foo-project --description &#39;This is a project for foo.&#39; Options # -d, --description string Set a description for the newly-created project. -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl create\t- Create a new instance of a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "b254c3d7baa0c3113d3c05877f3a6301"
    },
    {
      "title": "pachctl create repo",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_create_repo command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_create_repo/",
      "relURI": "/latest/run-commands/pachctl_create_repo/",
      "body": " pachctl create repo # Create a new repo in your active project.\nSynopsis # This command creates a repo in the project that is set to your active context (initially the default project).\nTo specify which project to create the repo in, use the --project flag To add a description to the project, use the --description flag pachctl create repo &lt;repo&gt; [flags] Examples # pachctl create repo foo ‚ûî /&lt;active-project&gt;/foo pachctl create repo bar --description &#39;my new repo&#39; ‚ûî /&lt;active-project&gt;/bar pachctl create repo baz --project myproject ‚ûî /myproject/baz Options # -d, --description string Set a repo description. -h, --help help for repo --project string Specify an existing project (by name) for where the new repo will be located. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl create\t- Create a new instance of a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a04416c194cb2ca2e4d7789c608dcd41"
    },
    {
      "title": "pachctl create secret",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_create_secret command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_create_secret/",
      "relURI": "/latest/run-commands/pachctl_create_secret/",
      "body": " pachctl create secret # Create a secret on the cluster.\nSynopsis # This command creates a secret on the cluster.\npachctl create secret [flags] Examples # pachctl create secret --file my-secret.json Options # -f, --file string File containing Kubernetes secret. -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl create\t- Create a new instance of a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "1267e452680bb02adf60b35416582920"
    },
    {
      "title": "pachctl debug",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_debug command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_debug/",
      "relURI": "/latest/run-commands/pachctl_debug/",
      "body": " pachctl debug # Debug commands for analyzing a running cluster.\nSynopsis # Debug commands for analyzing a running cluster.\nOptions # -h, --help help for debug Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl debug analyze\t- Start a local pachd server to analyze a debug dump. pachctl debug binary\t- Collect a set of binaries. pachctl debug dump\t- Collect a standard set of debugging information. pachctl debug log-level\t- Change the log level across Pachyderm. pachctl debug profile\t- Collect a set of pprof profiles. pachctl debug template\t- Collect a standard set of debugging information. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "16d50ca5fbcb59c8f4e1de264c05e37d"
    },
    {
      "title": "pachctl debug analyze",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_debug_analyze command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_debug_analyze/",
      "relURI": "/latest/run-commands/pachctl_debug_analyze/",
      "body": " pachctl debug analyze # Start a local pachd server to analyze a debug dump.\nSynopsis # Start a local pachd server to analyze a debug dump.\npachctl debug analyze &lt;file&gt; [flags] Options # -h, --help help for analyze -p, --port int launch a debug server on the given port. If unset, choose a free port automatically Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl debug\t- Debug commands for analyzing a running cluster. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "21492ea67cce2fb36c4b050b0124c9ac"
    },
    {
      "title": "pachctl debug binary",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_debug_binary command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_debug_binary/",
      "relURI": "/latest/run-commands/pachctl_debug_binary/",
      "body": " pachctl debug binary # Collect a set of binaries.\nSynopsis # Collect a set of binaries.\npachctl debug binary &lt;file&gt; [flags] Options # -h, --help help for binary --pachd Only collect the binary from pachd. -p, --pipeline string Only collect the binary from the worker pods for the given pipeline. -w, --worker string Only collect the binary from the given worker pod. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl debug\t- Debug commands for analyzing a running cluster. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "002b74a7278ad4ae17d0898f4abc1471"
    },
    {
      "title": "pachctl debug dump",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_debug_dump command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_debug_dump/",
      "relURI": "/latest/run-commands/pachctl_debug_dump/",
      "body": " pachctl debug dump # Collect a standard set of debugging information.\nSynopsis # Collect a standard set of debugging information.\npachctl debug dump &lt;file&gt; [flags] Options # -h, --help help for dump -t, --template string A template to customize the output of the debug dump operation. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl debug\t- Debug commands for analyzing a running cluster. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "819a878f2f1cabbf30aa28c309a94a6a"
    },
    {
      "title": "pachctl debug log-level",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_debug_log-level command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_debug_log-level/",
      "relURI": "/latest/run-commands/pachctl_debug_log-level/",
      "body": " pachctl debug log-level # Change the log level across Pachyderm.\nSynopsis # Change the log level across Pachyderm.\npachctl debug log-level &lt;level&gt; [flags] Options # -d, --duration duration how long to log at the non-default level (default 5m0s) -g, --grpc adjust the grpc log level instead of the pachyderm log level -h, --help help for log-level -r, --recursive set the log level on all pachyderm pods; if false, only the pachd that handles this RPC (default true) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl debug\t- Debug commands for analyzing a running cluster. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "922818cd5c4382f91152753f3528fc3b"
    },
    {
      "title": "pachctl debug profile",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_debug_profile command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_debug_profile/",
      "relURI": "/latest/run-commands/pachctl_debug_profile/",
      "body": " pachctl debug profile # Collect a set of pprof profiles.\nSynopsis # Collect a set of pprof profiles.\npachctl debug profile &lt;profile&gt; &lt;file&gt; [flags] Options # -d, --duration duration Duration to run a CPU profile for. (default 1m0s) -h, --help help for profile --pachd Only collect the profile from pachd. -p, --pipeline string Only collect the profile from the worker pods for the given pipeline. -w, --worker string Only collect the profile from the given worker pod. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl debug\t- Debug commands for analyzing a running cluster. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "4d89f69b071b0aafb518cc5b15f12b56"
    },
    {
      "title": "pachctl debug template",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_debug_template command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_debug_template/",
      "relURI": "/latest/run-commands/pachctl_debug_template/",
      "body": " pachctl debug template # Collect a standard set of debugging information.\nSynopsis # Collect a standard set of debugging information.\npachctl debug template &lt;file&gt; [flags] Options # -h, --help help for template Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl debug\t- Debug commands for analyzing a running cluster. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "30c7a22b87a2ddc024983116ba27efd6"
    },
    {
      "title": "pachctl delete",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_delete command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_delete/",
      "relURI": "/latest/run-commands/pachctl_delete/",
      "body": " pachctl delete # Delete an existing Pachyderm resource.\nSynopsis # Delete an existing Pachyderm resource.\nOptions # -h, --help help for delete Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl delete all\t- Delete everything. pachctl delete branch\t- Delete a branch pachctl delete commit\t- Delete the sub-commits of a commit. pachctl delete file\t- Delete a file. pachctl delete job\t- Delete a job. pachctl delete pipeline\t- Delete a pipeline. pachctl delete project\t- Delete a project. pachctl delete repo\t- Delete a repo. pachctl delete secret\t- Delete a secret from the cluster. pachctl delete transaction\t- Cancel and delete an existing transaction. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "8a6b862de4aa6c36ef9ec03c31e6a4fd"
    },
    {
      "title": "pachctl delete all",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_delete_all command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_delete_all/",
      "relURI": "/latest/run-commands/pachctl_delete_all/",
      "body": " pachctl delete all # Delete everything.\nSynopsis # Delete all repos, commits, files, pipelines and jobs. This resets the cluster to its initial state.\npachctl delete all [flags] Options # -h, --help help for all Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl delete\t- Delete an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "5b1bbcb6e051b842a49b736d5012d6fa"
    },
    {
      "title": "pachctl delete branch",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_delete_branch command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_delete_branch/",
      "relURI": "/latest/run-commands/pachctl_delete_branch/",
      "body": " pachctl delete branch # Delete a branch\nSynopsis # This command deletes a branch while leaving its commits intact.\nTo delete a branch from a repo in another project, use the --project flag To delete a branch regardless of errors, use the --force flag pachctl delete branch &lt;repo&gt;@&lt;branch&gt; [flags] Examples # pachctl delete branch foo@master pachctl delete branch foo@master --project bar pachctl delete branch foo@master --force pachctl delete branch foo@master --project bar --force Options # -f, --force Force branch deletion regardless of errors; use with caution. -h, --help help for branch --project string Specify the project (by name) containing branch&#39;s repo. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl delete\t- Delete an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "1d7d728356b1b7efe72f421199fe677a"
    },
    {
      "title": "pachctl delete commit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_delete_commit command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_delete_commit/",
      "relURI": "/latest/run-commands/pachctl_delete_commit/",
      "body": " pachctl delete commit # Delete the sub-commits of a commit.\nSynopsis # This command deletes the sub-commits of a commit; data in sub-commits will be lost, so use with caution. This operation is only supported if none of the sub-commits have children.\npachctl delete commit &lt;commit-id&gt; [flags] Examples # pachctl delete commit 0001a0100b1c10d01111e001fg00h00i Options # -h, --help help for commit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl delete\t- Delete an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "8432498e6ffdf3c58fe776f8a1b4502f"
    },
    {
      "title": "pachctl delete file",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_delete_file command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_delete_file/",
      "relURI": "/latest/run-commands/pachctl_delete_file/",
      "body": " pachctl delete file # Delete a file.\nSynopsis # This command deletes a file.\npachctl delete file &lt;repo&gt;@&lt;branch-or-commit&gt;:&lt;path/in/pfs&gt; [flags] Examples # pachctl delete file foo@bar:image.png pachctl delete file foo@0001a0100b1c10d01111e001fg00h00i:/images/image.png pachctl delete file -r foo@master:/images pachctl delete file -r foo@master:/images --project projectContainingFoo Options # -h, --help help for file --project string Specify the project (by name) where the file&#39;s repo is located. (default &#34;video-to-frame-traces&#34;) -r, --recursive Recursively delete the files in a directory. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl delete\t- Delete an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9e2a35ceb145e45d2a98a3f3e930e761"
    },
    {
      "title": "pachctl delete job",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_delete_job command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_delete_job/",
      "relURI": "/latest/run-commands/pachctl_delete_job/",
      "body": " pachctl delete job # Delete a job.\nSynopsis # This command deletes a job.\npachctl delete job &lt;pipeline&gt;@&lt;job&gt; [flags] Examples # pachctl delete job 5f93d03b65fa421996185e53f7f8b1e4 pachctl delete job 5f93d03b65fa421996185e53f7f8b1e4 --project foo Options # -h, --help help for job --project string Specify the project (by name) containing the parent pipeline for this job. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl delete\t- Delete an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "3103fc600abfe64add8f0641c990f133"
    },
    {
      "title": "pachctl delete pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_delete_pipeline command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_delete_pipeline/",
      "relURI": "/latest/run-commands/pachctl_delete_pipeline/",
      "body": " pachctl delete pipeline # Delete a pipeline.\nSynopsis # This command deletes a pipeline.\npachctl delete pipeline (&lt;pipeline&gt;|--all) [flags] Examples # pachctl delete pipeline foo pachctl delete pipeline --all pachctl delete pipeline foo --force pachctl delete pipeline foo --keep-repo pachctl delete pipeline foo --project bar --keep-repo Options # --all Delete all pipelines -A, --all-projects Delete pipelines from all projects; only valid with --all -f, --force Delete the pipeline regardless of errors; use with care -h, --help help for pipeline --keep-repo Specify that the pipeline&#39;s output repo should be saved after pipeline deletion; to reuse this pipeline&#39;s name, you&#39;ll also need to delete this output repo. --project string Specify the project (by name) containing project (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl delete\t- Delete an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9f6aa66591b593171757b8a68e3b6341"
    },
    {
      "title": "pachctl delete project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_delete_project command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_delete_project/",
      "relURI": "/latest/run-commands/pachctl_delete_project/",
      "body": " pachctl delete project # Delete a project.\nSynopsis # This command deletes a project.\npachctl delete project &lt;project&gt; [flags] Examples # pachctl delete project foo-project pachctl delete project foo-project --force Options # -f, --force Force delete the project regardless of errors; use with caution. -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl delete\t- Delete an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f9fed7f1622a3e3ddb2a0b8ea4ac329d"
    },
    {
      "title": "pachctl delete repo",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_delete_repo command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_delete_repo/",
      "relURI": "/latest/run-commands/pachctl_delete_repo/",
      "body": " pachctl delete repo # Delete a repo.\nSynopsis # This command deletes a repo. If this is a shared resource, it will be deleted for other users as well.\nTo force delete a repo, use the --force flag; use with caution To delete all repos across all projects, use the --all flag To delete a repo of a specific type, use the --type flag; options include USER, META, &amp; SPEC To delete all repos of a specific type across all projects, use the --all and --type flags To delete all repos of a specific type in a specific project, use the --all, --type, and --project flags pachctl delete repo &lt;repo&gt; [flags] Examples # pachctl delete repo foo pachctl delete repo foo --force pachctl delete repo --type user pachctl delete repo --all pachctl delete repo --all --type user pachctl delete repo --all --type user --project default Options # --all remove all repos -A, --all-projects Delete repo(s) across all projects; only valid with --all. -f, --force Force repo deletion, regardless of errors; use with caution. -h, --help help for repo --project string Specify the project (by name) where the to-be-deleted repo is located. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl delete\t- Delete an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9d15878793f9cd908cdf3046f600b825"
    },
    {
      "title": "pachctl delete secret",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_delete_secret command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_delete_secret/",
      "relURI": "/latest/run-commands/pachctl_delete_secret/",
      "body": " pachctl delete secret # Delete a secret from the cluster.\nSynopsis # This command deletes a secret from the cluster.\npachctl delete secret [flags] Examples # pachctl delete secret my-secret Options # -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl delete\t- Delete an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "8ba609f2bad33b3ab7c46f3d8be3503d"
    },
    {
      "title": "pachctl delete transaction",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_delete_transaction command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_delete_transaction/",
      "relURI": "/latest/run-commands/pachctl_delete_transaction/",
      "body": " pachctl delete transaction # Cancel and delete an existing transaction.\nSynopsis # Cancel and delete an existing transaction.\npachctl delete transaction [&lt;transaction&gt;] [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl delete\t- Delete an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "c86abac74ef82f706e58f6e21bb60718"
    },
    {
      "title": "pachctl diff",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_diff command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_diff/",
      "relURI": "/latest/run-commands/pachctl_diff/",
      "body": " pachctl diff # Show the differences between two Pachyderm resources.\nSynopsis # Show the differences between two Pachyderm resources.\nOptions # -h, --help help for diff Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl diff file\t- Return a diff of two file trees stored in Pachyderm ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "b7ad910d8f7929317e5202ea4ee6ae29"
    },
    {
      "title": "pachctl diff file",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_diff_file command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_diff_file/",
      "relURI": "/latest/run-commands/pachctl_diff_file/",
      "body": " pachctl diff file # Return a diff of two file trees stored in Pachyderm\nSynopsis # This command returns a diff of two file trees stored in Pachyderm. The file trees are specified by two files, one from the new tree and one from the old tree.\nTo specify the project where the repos are located, use the --project flag To specify the project where the second older repo is located, use the --old-project flag To prevent descending into sub-directories, use the --shallow flag To use an alternative (non-git) diff command, use the --diff-command flag To get only the names of changed files, use the --name-only flag pachctl diff file &lt;new-repo&gt;@&lt;new-branch-or-commit&gt;:&lt;new-path&gt; [&lt;old-repo&gt;@&lt;old-branch-or-commit&gt;:&lt;old-path&gt;] [flags] Examples # pachctl diff file foo@master:/logs/log.txt pachctl diff file foo@0001a0100b1c10d01111e001fg00h00i:log.txt pachctl diff file foo@master:path1 bar@master:path2 Options # --diff-command string Set a git-alternative program to diff files. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for file --name-only Specify results should only return the names of changed files. --no-pager Don&#39;t pipe output into a pager (i.e. less). --old-project string Specify the project (by name) where the second, older repo is located. --project string Specify the project (by name) where the first repo is located. (default &#34;video-to-frame-traces&#34;) -s, --shallow Specify results should not to descend into sub directories. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl diff\t- Show the differences between two Pachyderm resources. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "4e40dc9894ab43f37fe17ea8d70d281d"
    },
    {
      "title": "pachctl draw",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_draw command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_draw/",
      "relURI": "/latest/run-commands/pachctl_draw/",
      "body": " pachctl draw # Draw an ASCII representation of an existing Pachyderm resource.\nSynopsis # Draw an ASCII representation of an existing Pachyderm resource.\nOptions # -h, --help help for draw Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl draw pipeline\t- Draw a DAG ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "5833afbe077b48ba0391539bc6eb2d17"
    },
    {
      "title": "pachctl draw pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_draw_pipeline command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_draw_pipeline/",
      "relURI": "/latest/run-commands/pachctl_draw_pipeline/",
      "body": " pachctl draw pipeline # Draw a DAG\nSynopsis # This command draws a DAG\npachctl draw pipeline [flags] Examples # pachctl draw pipeline --commit 5f93d03b65fa421996185e53f7f8b1e4 pachctl draw pipeline --box-width 20 pachctl draw pipeline --edge-height 8 pachctl draw pipeline --project foo pachctl draw pipeline --box-width 20 --edge-height 8 --commit 5f93d03b65fa421996185e53f7f8b1e4 Options # --box-width int Character width of each box in the DAG (default 11) -c, --commit string Commit at which you would to draw the DAG --edge-height int Number of vertical lines spanned by each edge (default 5) -h, --help help for pipeline --project string Project containing pipelines. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl draw\t- Draw an ASCII representation of an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f3864cfcfb07ca81ede1e880001062a3"
    },
    {
      "title": "pachctl edit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_edit command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_edit/",
      "relURI": "/latest/run-commands/pachctl_edit/",
      "body": " pachctl edit # Edit the value of an existing Pachyderm resource.\nSynopsis # Edit the value of an existing Pachyderm resource.\nOptions # -h, --help help for edit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl edit pipeline\t- Edit the manifest for a pipeline in your text editor. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "b8f14bdc1c1a3f0d861d9c7fc9a095cb"
    },
    {
      "title": "pachctl edit pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_edit_pipeline command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_edit_pipeline/",
      "relURI": "/latest/run-commands/pachctl_edit_pipeline/",
      "body": " pachctl edit pipeline # Edit the manifest for a pipeline in your text editor.\nSynopsis # This command edits the manifest for a pipeline in your text editor.\npachctl edit pipeline &lt;pipeline&gt; [flags] Examples # pachctl edit pipeline foo pachctl edit pipeline foo --project bar pachctl edit pipeline foo --project bar --editor vim pachctl edit pipeline foo --project bar --editor vim --output yaml pachctl edit pipeline foo --project bar --editor vim --reprocess Options # --editor string Specify the editor to use for modifying the manifest. -h, --help help for pipeline -o, --output string Specify the output format: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) containing pipeline to edit. (default &#34;video-to-frame-traces&#34;) --reprocess If true, reprocess datums that were already processed by previous version of the pipeline. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl edit\t- Edit the value of an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a1c8abca5f3923a8a5f95135cdc010b7"
    },
    {
      "title": "pachctl enterprise",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_enterprise command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_enterprise/",
      "relURI": "/latest/run-commands/pachctl_enterprise/",
      "body": " pachctl enterprise # Enterprise commands enable Pachyderm Enterprise features\nSynopsis # Enterprise commands enable Pachyderm Enterprise features\nOptions # -h, --help help for enterprise Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl enterprise deactivate\t- Deactivate the enterprise service pachctl enterprise get-state\t- Check whether the Pachyderm cluster has enterprise features activated pachctl enterprise heartbeat\t- Sync the enterprise state with the license server immediately. pachctl enterprise pause\t- Pause the cluster. pachctl enterprise pause-status\t- Get the pause status of the cluster. pachctl enterprise register\t- Register the cluster with an enterprise license server pachctl enterprise sync-contexts\t- Pull all available Pachyderm Cluster contexts into your pachctl config pachctl enterprise unpause\t- Unpause the cluster. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "6c966bbd50c72f0920b658e61ac1e9b4"
    },
    {
      "title": "pachctl enterprise deactivate",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_enterprise_deactivate command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_enterprise_deactivate/",
      "relURI": "/latest/run-commands/pachctl_enterprise_deactivate/",
      "body": " pachctl enterprise deactivate # Deactivate the enterprise service\nSynopsis # Deactivate the enterprise service\npachctl enterprise deactivate [flags] Options # -h, --help help for deactivate Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl enterprise\t- Enterprise commands enable Pachyderm Enterprise features ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f166994a2faf91caf2db519650dee2c4"
    },
    {
      "title": "pachctl enterprise get-state",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_enterprise_get-state command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_enterprise_get-state/",
      "relURI": "/latest/run-commands/pachctl_enterprise_get-state/",
      "body": " pachctl enterprise get-state # Check whether the Pachyderm cluster has enterprise features activated\nSynopsis # Check whether the Pachyderm cluster has enterprise features activated\npachctl enterprise get-state [flags] Options # --enterprise Activate auth on the active enterprise context -h, --help help for get-state Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl enterprise\t- Enterprise commands enable Pachyderm Enterprise features ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "0b3fa75b46b8a71903527a83886c77b0"
    },
    {
      "title": "pachctl enterprise heartbeat",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_enterprise_heartbeat command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_enterprise_heartbeat/",
      "relURI": "/latest/run-commands/pachctl_enterprise_heartbeat/",
      "body": " pachctl enterprise heartbeat # Sync the enterprise state with the license server immediately.\nSynopsis # Sync the enterprise state with the license server immediately.\npachctl enterprise heartbeat [flags] Options # --enterprise Make the enterprise server refresh its state -h, --help help for heartbeat Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl enterprise\t- Enterprise commands enable Pachyderm Enterprise features ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f32ac02976c3f54549b2a37927d7f3fe"
    },
    {
      "title": "pachctl enterprise pause",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_enterprise_pause command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_enterprise_pause/",
      "relURI": "/latest/run-commands/pachctl_enterprise_pause/",
      "body": " pachctl enterprise pause # Pause the cluster.\nSynopsis # Pause the cluster.\npachctl enterprise pause [flags] Options # -h, --help help for pause Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl enterprise\t- Enterprise commands enable Pachyderm Enterprise features ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "955756eded8ab88eee1fdc5f87f61ae4"
    },
    {
      "title": "pachctl enterprise pause-status",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_enterprise_pause-status command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_enterprise_pause-status/",
      "relURI": "/latest/run-commands/pachctl_enterprise_pause-status/",
      "body": " pachctl enterprise pause-status # Get the pause status of the cluster.\nSynopsis # Get the pause the cluster: normal, partially-paused or paused.\npachctl enterprise pause-status [flags] Options # -h, --help help for pause-status Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl enterprise\t- Enterprise commands enable Pachyderm Enterprise features ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "573e601ea40737d6503179ca234adb93"
    },
    {
      "title": "pachctl enterprise register",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_enterprise_register command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_enterprise_register/",
      "relURI": "/latest/run-commands/pachctl_enterprise_register/",
      "body": " pachctl enterprise register # Register the cluster with an enterprise license server\nSynopsis # Register the cluster with an enterprise license server\npachctl enterprise register [flags] Options # --cluster-deployment-id string the deployment id of the cluster being registered --enterprise-server-address string the address for the pachd to reach the enterprise server -h, --help help for register --id string the id for this cluster --pachd-address string the address for the enterprise server to reach this pachd --pachd-user-address string the address for a user to reach this pachd Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl enterprise\t- Enterprise commands enable Pachyderm Enterprise features ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "334fbd84e579a5fcfa75ed09737d48a9"
    },
    {
      "title": "pachctl enterprise sync-contexts",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_enterprise_sync-contexts command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_enterprise_sync-contexts/",
      "relURI": "/latest/run-commands/pachctl_enterprise_sync-contexts/",
      "body": " pachctl enterprise sync-contexts # Pull all available Pachyderm Cluster contexts into your pachctl config\nSynopsis # Pull all available Pachyderm Cluster contexts into your pachctl config\npachctl enterprise sync-contexts [flags] Options # -h, --help help for sync-contexts Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl enterprise\t- Enterprise commands enable Pachyderm Enterprise features ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "8555adf4df3828e87767affb7c455fda"
    },
    {
      "title": "pachctl enterprise unpause",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_enterprise_unpause command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_enterprise_unpause/",
      "relURI": "/latest/run-commands/pachctl_enterprise_unpause/",
      "body": " pachctl enterprise unpause # Unpause the cluster.\nSynopsis # Unpause the cluster.\npachctl enterprise unpause [flags] Options # -h, --help help for unpause Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl enterprise\t- Enterprise commands enable Pachyderm Enterprise features ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "399cf70ff9d2212c70cf4d4afd0eab50"
    },
    {
      "title": "pachctl exit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_exit command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_exit/",
      "relURI": "/latest/run-commands/pachctl_exit/",
      "body": " pachctl exit # Exit the pachctl shell.\nSynopsis # Exit the pachctl shell.\npachctl exit [flags] Options # -h, --help help for exit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "37d7a7895b4dc222fe6af0abeb771cb7"
    },
    {
      "title": "pachctl find",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_find command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_find/",
      "relURI": "/latest/run-commands/pachctl_find/",
      "body": " pachctl find # Find a file addition, modification, or deletion in a commit.\nSynopsis # fInd a file addition, modification, or deletion in a commit.\nOptions # -h, --help help for find Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl find commit\t- Find commits with reference to within a branch starting from repo@commitID ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "225a821419929698954a38ff49f756ed"
    },
    {
      "title": "pachctl find commit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_find_commit command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_find_commit/",
      "relURI": "/latest/run-commands/pachctl_find_commit/",
      "body": " pachctl find commit # Find commits with reference to within a branch starting from repo@commitID\nSynopsis # This command returns a list of commits using a reference to their file/path within a branch, starting from repo@&lt;commitID&gt;.\nTo find commits from a repo in another project, use the --project flag To set a limit on the number of returned commits, use the --limits flag To set a timeout for your commit search, use the --timeout flag To print the results as json, use the --json flag pachctl find commit &lt;repo&gt;@&lt;branch-or-commit&gt;:&lt;path/in/pfs&gt; [flags] Examples # pachctl find commit foo@master:file pachctl find commit foo@master:file --project bar pachctl find commit foo@master:file --limit 10 pachctl find commit foo@master:file --timeout 10s pachctl find commit foo@master:file --json pachctl find commit foo@master:file --project bar --json --limit 100 --timeout 20s Options # -h, --help help for commit --json Print the response in json. --limit uint32 Set the number of matching commits to return. --project string Specify the project (by name) in which commits are located. (default &#34;video-to-frame-traces&#34;) --timeout duration Set the search duration timeout. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl find\t- Find a file addition, modification, or deletion in a commit. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9b8140513c62a6e380896a74dfa40413"
    },
    {
      "title": "pachctl finish",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_finish command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_finish/",
      "relURI": "/latest/run-commands/pachctl_finish/",
      "body": " pachctl finish # Finish a Pachyderm resource.\nSynopsis # Finish a Pachyderm resource.\nOptions # -h, --help help for finish Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl finish commit\t- Finish a started commit. pachctl finish transaction\t- Execute and clear the currently active transaction. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "536afd3d59e7667fce182b851be34ad6"
    },
    {
      "title": "pachctl finish commit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_finish_commit command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_finish_commit/",
      "relURI": "/latest/run-commands/pachctl_finish_commit/",
      "body": " pachctl finish commit # Finish a started commit.\nSynopsis # This command finishes a started commit.\nTo force finish a commit, use the --force flag To add a message to the commit, use the --message or --description flag To specify which project the repo is in, use the --project flag pachctl finish commit &lt;repo&gt;@&lt;branch-or-commit&gt; [flags] Examples # pachctl finish commit foo@master pachctl finish commit foo@master --force pachctl finish commit foo@master --project bar pachctl finish commit foo@master --message &#39;my commit message&#39; pachctl finish commit foo@master --description &#39;my commit description&#39; --project bar Options # --description string Set a description of this commit&#39;s contents; overwrites existing commit description (synonym for --message). -f, --force Force finish commit, even if it has provenance, which could break jobs; prefer &#39;pachctl stop stop job&#39; -h, --help help for commit -m, --message string Set a description of this commit&#39;s contents; overwrites existing commit description (synonym for --description). --project string Specify the project (by name) where the repo for this commit is located. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl finish\t- Finish a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "6b42eb169dd78bfd8d89233c3195c211"
    },
    {
      "title": "pachctl finish transaction",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_finish_transaction command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_finish_transaction/",
      "relURI": "/latest/run-commands/pachctl_finish_transaction/",
      "body": " pachctl finish transaction # Execute and clear the currently active transaction.\nSynopsis # Execute and clear the currently active transaction.\npachctl finish transaction [&lt;transaction&gt;] [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl finish\t- Finish a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "73f8f18d13e28a3624ad8a80475caddd"
    },
    {
      "title": "pachctl fsck",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_fsck command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_fsck/",
      "relURI": "/latest/run-commands/pachctl_fsck/",
      "body": " pachctl fsck # Run a file system consistency check on PFS.\nSynopsis # This command runs a file system consistency check on the Pachyderm file system, ensuring the correct provenance relationships are satisfied.\npachctl fsck [flags] Examples # pachctl fsck pachctl fsck --fix pachctl fsck --zombie-all pachctl fsck --zombie foo@bar pachctl fsck --zombie foo@bar --project projectContainingFoo Options # -f, --fix Attempt to fix as many issues as possible. -h, --help help for fsck --project string Specify the project (by name) where the repo of the branch/commit is located. (default &#34;video-to-frame-traces&#34;) --zombie string Set a single commit (by id) to check for zombie files --zombie-all Check all pipelines for zombie files: files corresponding to old inputs that were not properly deleted. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "150fe558cb02a337e717952a8631054b"
    },
    {
      "title": "pachctl get",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_get command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_get/",
      "relURI": "/latest/run-commands/pachctl_get/",
      "body": " pachctl get # Get the raw data represented by a Pachyderm resource.\nSynopsis # Get the raw data represented by a Pachyderm resource.\nOptions # -h, --help help for get Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl get file\t- Return the contents of a file. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "abd77bda77bfe116f757a0625777ecec"
    },
    {
      "title": "pachctl get file",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_get_file command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_get_file/",
      "relURI": "/latest/run-commands/pachctl_get_file/",
      "body": " pachctl get file # Return the contents of a file.\nSynopsis # This command returns the contents of a file. While using this command, take special note of how you can use ancestry syntax (e.g., appending^2 or .-1 to repo@branch) to retrieve the contents of a file from a previous commit.\nTo specify the project where the repo is located, use the &ndash;project flag To specify the output path, use the &ndash;output flag To specify the number of bytes to offset the read by, use the &ndash;offset-bytes flag To retry the operation if it fails, use the &ndash;retry flag pachctl get file &lt;repo&gt;@&lt;branch-or-commit&gt;:&lt;path/in/pfs&gt; [flags] Examples # pachctl get file foo@master:image.png pachctl get file foo@0001a0100b1c10d01111e001fg00h00i:image.png pachctl get file foo@master:/directory -r pachctl get file foo@master:image.png --output /path/to/image.png pachctl get file foo@master:/logs/log.txt--offset-bytes 100 pachctl get file foo@master:image.png --retry pachctl get file foo@master:/logs/log.txt --output /path/to/image.png --offset-bytes 100 --retry pachctl get file foo@master^:chart.png pachctl get file foo@master^2:chart.png pachctl get file foo@master.1:chart.png pachctl get file foo@master.-1:chart.png pachctl get file &#39;foo@master:/test\\[\\].txt&#39; Options # -h, --help help for file --offset int Set the number of bytes in the file to skip ahead when reading. -o, --output string Set the path where data will be downloaded. --progress {true|false} Whether or not to print the progress bars. (default true) --project string Specify the project (by name) where the file&#39;s repo is located. (default &#34;video-to-frame-traces&#34;) -r, --recursive Download multiple files, or recursively download a directory. --retry {true|false} Whether to append the missing bytes to an existing file. No-op if the file doesn&#39;t exist. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl get\t- Get the raw data represented by a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "af98be44cdbdfadae2f8f1ba297a87d7"
    },
    {
      "title": "pachctl glob",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_glob command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_glob/",
      "relURI": "/latest/run-commands/pachctl_glob/",
      "body": " pachctl glob # Print a list of Pachyderm resources matching a glob pattern.\nSynopsis # Print a list of Pachyderm resources matching a glob pattern.\nOptions # -h, --help help for glob Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl glob file\t- Return files that match a glob pattern in a commit. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "dccc9f8d37956e7500a21b9bfdb2bb51"
    },
    {
      "title": "pachctl glob file",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_glob_file command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_glob_file/",
      "relURI": "/latest/run-commands/pachctl_glob_file/",
      "body": " pachctl glob file # Return files that match a glob pattern in a commit.\nSynopsis # This command returns files that match a glob pattern in a commit (that is, match a glob pattern in a repo at the state represented by a commit). Glob patterns are documented here.\nTo specify the project where the repo is located, use the --project flag pachctl glob file &#34;&lt;repo&gt;@&lt;branch-or-commit&gt;:&lt;pattern&gt;&#34; [flags] Examples # pachctl glob file &#34;foo@master:A*&#34; pachctl glob file &#34;foo@0001a0100b1c10d01111e001fg00h00i:data/*&#34; pachctl glob file &#34;foo@master:data/*&#34; Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for file -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) where the repo with potential matching file(s) is located. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl glob\t- Print a list of Pachyderm resources matching a glob pattern. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "8dbcd36201472cbd5f3dae2acfe0c8b1"
    },
    {
      "title": "pachctl idp",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp/",
      "relURI": "/latest/run-commands/pachctl_idp/",
      "body": " pachctl idp # Commands to manage identity provider integrations\nSynopsis # Commands to manage identity provider integrations\nOptions # -h, --help help for idp Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl idp create-client\t- Create a new OIDC client. pachctl idp create-connector\t- Create a new identity provider connector. pachctl idp delete-client\t- Delete an OIDC client. pachctl idp delete-connector\t- Delete an identity provider connector pachctl idp get-client\t- Get an OIDC client. pachctl idp get-config\t- Get the identity server config pachctl idp get-connector\t- Get the config for an identity provider connector. pachctl idp list-client\t- List OIDC clients. pachctl idp list-connector\t- List identity provider connectors pachctl idp set-config\t- Set the identity server config pachctl idp update-client\t- Update an OIDC client. pachctl idp update-connector\t- Update an existing identity provider connector. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "27764c5ec7435a10b577d936593e0e89"
    },
    {
      "title": "pachctl idp create-client",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_create-client command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_create-client/",
      "relURI": "/latest/run-commands/pachctl_idp_create-client/",
      "body": " pachctl idp create-client # Create a new OIDC client.\nSynopsis # Create a new OIDC client.\npachctl idp create-client [flags] Options # --config string The file to read the YAML-encoded client configuration from, or &#39;-&#39; for stdin. (default &#34;-&#34;) -h, --help help for create-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "d5f3f1c96f9a5ffd29c9b4a484805654"
    },
    {
      "title": "pachctl idp create-connector",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_create-connector command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_create-connector/",
      "relURI": "/latest/run-commands/pachctl_idp_create-connector/",
      "body": " pachctl idp create-connector # Create a new identity provider connector.\nSynopsis # Create a new identity provider connector.\npachctl idp create-connector [flags] Options # --config string The file to read the YAML-encoded connector configuration from, or &#39;-&#39; for stdin. (default &#34;-&#34;) -h, --help help for create-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "2db2efd36787c3a154a229bf7b7eb5d9"
    },
    {
      "title": "pachctl idp delete-client",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_delete-client command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_delete-client/",
      "relURI": "/latest/run-commands/pachctl_idp_delete-client/",
      "body": " pachctl idp delete-client # Delete an OIDC client.\nSynopsis # Delete an OIDC client.\npachctl idp delete-client &lt;client ID&gt; [flags] Options # -h, --help help for delete-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9520fce746d34d4aa84f1646bc0f63ab"
    },
    {
      "title": "pachctl idp delete-connector",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_delete-connector command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_delete-connector/",
      "relURI": "/latest/run-commands/pachctl_idp_delete-connector/",
      "body": " pachctl idp delete-connector # Delete an identity provider connector\nSynopsis # Delete an identity provider connector\npachctl idp delete-connector [flags] Options # -h, --help help for delete-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a0cefc0425aa3243c58ac47cf9f351e7"
    },
    {
      "title": "pachctl idp get-client",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_get-client command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_get-client/",
      "relURI": "/latest/run-commands/pachctl_idp_get-client/",
      "body": " pachctl idp get-client # Get an OIDC client.\nSynopsis # Get an OIDC client.\npachctl idp get-client &lt;client ID&gt; [flags] Options # -h, --help help for get-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "c879f22a3fc3ca5ede422e28c825becb"
    },
    {
      "title": "pachctl idp get-config",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_get-config command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_get-config/",
      "relURI": "/latest/run-commands/pachctl_idp_get-config/",
      "body": " pachctl idp get-config # Get the identity server config\nSynopsis # Get the identity server config\npachctl idp get-config [flags] Options # -h, --help help for get-config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "1b2521dbf33d545df8985e57b4cd28be"
    },
    {
      "title": "pachctl idp get-connector",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_get-connector command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_get-connector/",
      "relURI": "/latest/run-commands/pachctl_idp_get-connector/",
      "body": " pachctl idp get-connector # Get the config for an identity provider connector.\nSynopsis # Get the config for an identity provider connector.\npachctl idp get-connector &lt;connector id&gt; [flags] Options # -h, --help help for get-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "dd1fb7e61b3ea5670c26d62a96689950"
    },
    {
      "title": "pachctl idp list-client",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_list-client command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_list-client/",
      "relURI": "/latest/run-commands/pachctl_idp_list-client/",
      "body": " pachctl idp list-client # List OIDC clients.\nSynopsis # List OIDC clients.\npachctl idp list-client [flags] Options # -h, --help help for list-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "cef806d0db12f8186d73a0f13c88c74f"
    },
    {
      "title": "pachctl idp list-connector",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_list-connector command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_list-connector/",
      "relURI": "/latest/run-commands/pachctl_idp_list-connector/",
      "body": " pachctl idp list-connector # List identity provider connectors\nSynopsis # List identity provider connectors\npachctl idp list-connector [flags] Options # -h, --help help for list-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e07b2754ec8c71d589b0a08051508158"
    },
    {
      "title": "pachctl idp set-config",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_set-config command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_set-config/",
      "relURI": "/latest/run-commands/pachctl_idp_set-config/",
      "body": " pachctl idp set-config # Set the identity server config\nSynopsis # Set the identity server config\npachctl idp set-config [flags] Options # --config string The file to read the YAML-encoded configuration from, or &#39;-&#39; for stdin. (default &#34;-&#34;) -h, --help help for set-config Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "37967dbd4d91c7897332d7680b2b0dac"
    },
    {
      "title": "pachctl idp update-client",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_update-client command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_update-client/",
      "relURI": "/latest/run-commands/pachctl_idp_update-client/",
      "body": " pachctl idp update-client # Update an OIDC client.\nSynopsis # Update an OIDC client.\npachctl idp update-client [flags] Options # --config string The file to read the YAML-encoded client configuration from, or &#39;-&#39; for stdin. (default &#34;-&#34;) -h, --help help for update-client Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "d3d5c6142cb5af76fd7ae5ffc53b936e"
    },
    {
      "title": "pachctl idp update-connector",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_idp_update-connector command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_idp_update-connector/",
      "relURI": "/latest/run-commands/pachctl_idp_update-connector/",
      "body": " pachctl idp update-connector # Update an existing identity provider connector.\nSynopsis # Update an existing identity provider connector. Only fields which are specified are updated.\npachctl idp update-connector [flags] Options # --config string The file to read the YAML-encoded connector configuration from, or &#39;-&#39; for stdin. (default &#34;-&#34;) -h, --help help for update-connector Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl idp\t- Commands to manage identity provider integrations ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "0e0e4ce55e7a2a8b9654a7ec4618d8e4"
    },
    {
      "title": "pachctl inspect",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect/",
      "relURI": "/latest/run-commands/pachctl_inspect/",
      "body": " pachctl inspect # Show detailed information about a Pachyderm resource.\nSynopsis # Show detailed information about a Pachyderm resource.\nOptions # -h, --help help for inspect Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl inspect branch\t- Return info about a branch. pachctl inspect cluster\t- Returns info about the pachyderm cluster pachctl inspect commit\t- Return info about a commit. pachctl inspect datum\t- Display detailed info about a single datum. pachctl inspect file\t- Return info about a file. pachctl inspect job\t- Return info about a job. pachctl inspect pipeline\t- Return info about a pipeline. pachctl inspect project\t- Inspect a project. pachctl inspect repo\t- Return info about a repo. pachctl inspect secret\t- Inspect a secret from the cluster. pachctl inspect transaction\t- Print information about an open transaction. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a1bb2a3a4daa5f12b9332f114230bb16"
    },
    {
      "title": "pachctl inspect branch",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect_branch command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect_branch/",
      "relURI": "/latest/run-commands/pachctl_inspect_branch/",
      "body": " pachctl inspect branch # Return info about a branch.\nSynopsis # This command returns info about a branch, such as its Name, Head Commit, and Trigger.\nTo inspect a branch from a repo in another project, use the --project flag To get additional details about the branch, use the --raw flag pachctl inspect branch &lt;repo&gt;@&lt;branch&gt; [flags] Examples # pachctl inspect branch foo@master pachctl inspect branch foo@master --project bar pachctl inspect branch foo@master --raw Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for branch -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) containing branch&#39;s repo. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl inspect\t- Show detailed information about a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "87e4a74b57d8b274d26e4c3f447919c3"
    },
    {
      "title": "pachctl inspect cluster",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect_cluster command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect_cluster/",
      "relURI": "/latest/run-commands/pachctl_inspect_cluster/",
      "body": " pachctl inspect cluster # Returns info about the pachyderm cluster\nSynopsis # Returns info about the pachyderm cluster\npachctl inspect cluster [flags] Options # -h, --help help for cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl inspect\t- Show detailed information about a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9bcd4786b4a5ee4a9e8f2a272a9a7d7b"
    },
    {
      "title": "pachctl inspect commit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect_commit command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect_commit/",
      "relURI": "/latest/run-commands/pachctl_inspect_commit/",
      "body": " pachctl inspect commit # Return info about a commit.\nSynopsis # This command returns information about the commit, such as the commit location (branch@commit-id), originating branch, start/finish times, and size.\nTo view the raw details of the commit in JSON format, use the --raw flag To specify which project the repo is in, use the --project flag pachctl inspect commit &lt;repo&gt;@&lt;branch-or-commit&gt; [flags] Examples # pachctl inspect commit foo@master pachctl inspect commit foo@master --project bar pachctl inspect commit foo@master --raw pachctl inspect commit foo@0001a0100b1c10d01111e001fg00h00i --project bar --raw Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) where the repo for this commit is located. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl inspect\t- Show detailed information about a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "c54e405950c417ed79942aa73ac90914"
    },
    {
      "title": "pachctl inspect datum",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect_datum command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect_datum/",
      "relURI": "/latest/run-commands/pachctl_inspect_datum/",
      "body": " pachctl inspect datum # Display detailed info about a single datum.\nSynopsis # This command displays detailed info about a single datum; requires the pipeline to have stats enabled.\npachctl inspect datum &lt;pipeline&gt;@&lt;job&gt; &lt;datum&gt; [flags] Examples # pachctl inspect datum foo@5f93d03b65fa421996185e53f7f8b1e4 7f3cd988429894000bdad549dfe2d09b5ca7bfc5083b79fec0e6bda3db8cc705 pachctl inspect datum foo@5f93d03b65fa421996185e53f7f8b1e4 7f3cd988429894000bdad549dfe2d09b5ca7bfc5083b79fec0e6bda3db8cc705 --project foo Options # -h, --help help for datum -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Project containing the job (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl inspect\t- Show detailed information about a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9771b17623f42fd7f3330407eda5fee6"
    },
    {
      "title": "pachctl inspect file",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect_file command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect_file/",
      "relURI": "/latest/run-commands/pachctl_inspect_file/",
      "body": " pachctl inspect file # Return info about a file.\nSynopsis # This command returns info about a file.While using this command, take special note of how you can use ancestry syntax (e.g., appending^2 or .-1 to repo@branch) to inspect the contents of a file from a previous commit.\nTo specify the project where the repo is located, use the &ndash;project flag pachctl inspect file &lt;repo&gt;@&lt;branch-or-commit&gt;:&lt;path/in/pfs&gt; [flags] Examples # pachctl inspect file repo@master:/logs/log.txt pachctl inspect file repo@0001a0100b1c10d01111e001fg00h00i:/logs/log.txt pachctl inspect file repo@master:/logs/log.txt^2 pachctl inspect file repo@master:/logs/log.txt.-1 pachctl inspect file repo@master:/logs/log.txt^2 --project foo Options # -h, --help help for file -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) where the file&#39;s repo is located. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl inspect\t- Show detailed information about a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "b198ecc792eb3379f8e647ac5fb0eeb1"
    },
    {
      "title": "pachctl inspect job",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect_job command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect_job/",
      "relURI": "/latest/run-commands/pachctl_inspect_job/",
      "body": " pachctl inspect job # Return info about a job.\nSynopsis # This command returns detailed info about a job, including processing stats, inputs, and transformation configuration (the image and commands used).\nIf you pass in a job set ID (without the pipeline@), it will defer you to using the pachctl list job &lt;id&gt; command. See examples for proper use.\nTo specify the project where the parent pipeline lives, use the --project flag To specify the output should be raw JSON or YAML, use the --raw flag along with --output pachctl inspect job &lt;pipeline&gt;@&lt;job&gt; [flags] Examples # pachctl inspect job foo@e0f68a2fcda7458880c9e2e2dae9e678 pachctl inspect job foo@e0f68a2fcda7458880c9e2e2dae9e678 --project bar pachctl inspect job foo@e0f68a2fcda7458880c9e2e2dae9e678 --project bar --raw --output yaml Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for job -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) containing the parent pipeline for this job. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl inspect\t- Show detailed information about a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "d8315d79dc6d726b15fad614896856e8"
    },
    {
      "title": "pachctl inspect pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect_pipeline command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect_pipeline/",
      "relURI": "/latest/run-commands/pachctl_inspect_pipeline/",
      "body": " pachctl inspect pipeline # Return info about a pipeline.\nSynopsis # This command returns info about a pipeline.\npachctl inspect pipeline &lt;pipeline&gt; [flags] Examples # pachctl inspect pipeline foo pachctl inspect pipeline foo --project bar pachctl inspect pipeline foo --project bar --raw -o yaml Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for pipeline -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) containing the inspected pipeline. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl inspect\t- Show detailed information about a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "d8406e44822751c5c02bfdc2590d04f2"
    },
    {
      "title": "pachctl inspect project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect_project command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect_project/",
      "relURI": "/latest/run-commands/pachctl_inspect_project/",
      "body": " pachctl inspect project # Inspect a project.\nSynopsis # This command inspects a project and returns information like its Name and Created at time.\nTo return additional details, use the --raw flag pachctl inspect project &lt;project&gt; [flags] Examples # pachctl inspect project foo-project pachctl inspect project foo-project --raw pachctl inspect project foo-project --output=yaml Options # -h, --help help for project -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl inspect\t- Show detailed information about a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "2f64bf4c3000f805f8216e2f4b46dbf8"
    },
    {
      "title": "pachctl inspect repo",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect_repo command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect_repo/",
      "relURI": "/latest/run-commands/pachctl_inspect_repo/",
      "body": " pachctl inspect repo # Return info about a repo.\nSynopsis # This command returns details of the repo such as: Name, Description, Created, and Size of HEAD on Master. By default, PachCTL checks for a matching repo in the project that is set to your active context (initially the default project).\nTo specify the project containing the repo you want to inspect, use the --project flag pachctl inspect repo &lt;repo&gt; [flags] Examples # pachctl inspect repo foo pachctl inspect repo foo --project myproject Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for repo -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) where the repo is located. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl inspect\t- Show detailed information about a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "582504cd2dc4f9be3d5df1f3eddff64b"
    },
    {
      "title": "pachctl inspect secret",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect_secret command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect_secret/",
      "relURI": "/latest/run-commands/pachctl_inspect_secret/",
      "body": " pachctl inspect secret # Inspect a secret from the cluster.\nSynopsis # This command inspects a secret from the cluster.\npachctl inspect secret [flags] Examples # pachctl inspect secret my-secret Options # -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl inspect\t- Show detailed information about a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "51698971bd5f6e5d4c32cbc083d9d38c"
    },
    {
      "title": "pachctl inspect transaction",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_inspect_transaction command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_inspect_transaction/",
      "relURI": "/latest/run-commands/pachctl_inspect_transaction/",
      "body": " pachctl inspect transaction # Print information about an open transaction.\nSynopsis # Print information about an open transaction.\npachctl inspect transaction [&lt;transaction&gt;] [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for transaction -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl inspect\t- Show detailed information about a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "31ea6fc49d3fb25de30a380a1979d86d"
    },
    {
      "title": "pachctl kube-events",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_kube-events command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_kube-events/",
      "relURI": "/latest/run-commands/pachctl_kube-events/",
      "body": " pachctl kube-events # Return the kubernetes events.\nSynopsis # This command returns the kubernetes events.\nTo return results starting from a certain amount of time before now, use the --since flag To return the raw events, use the --raw flag pachctl kube-events [flags] Examples # pachctl kube-events --raw pachctl kube-events --since 100s pachctl kube-events --raw --since 1h Options # -h, --help help for kube-events --raw Specify results should return log messages verbatim from server. --since string Specify results should return log messages more recent than &#34;since&#34;. (default &#34;0&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "28deffbcc38859ef46f2eba06692f741"
    },
    {
      "title": "pachctl license",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_license command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_license/",
      "relURI": "/latest/run-commands/pachctl_license/",
      "body": " pachctl license # License commmands manage the Enterprise License service\nSynopsis # License commands manage the Enterprise License service\nOptions # -h, --help help for license Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl license activate\t- Activate the license server with an activation code pachctl license add-cluster\t- Register a new cluster with the license server. pachctl license delete-all\t- Delete all data from the license server pachctl license delete-cluster\t- Delete a cluster registered with the license server. pachctl license get-state\t- Get the configuration of the license service. pachctl license list-clusters\t- List clusters registered with the license server. pachctl license update-cluster\t- Update an existing cluster registered with the license server. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "373d2a6c1a8be1eca78b1fe3861c4b8f"
    },
    {
      "title": "pachctl license activate",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_license_activate command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_license_activate/",
      "relURI": "/latest/run-commands/pachctl_license_activate/",
      "body": " pachctl license activate # Activate the license server with an activation code\nSynopsis # Activate the license server with an activation code\npachctl license activate [flags] Options # -h, --help help for activate --no-register Activate auth on the active enterprise context Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl license\t- License commmands manage the Enterprise License service ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "ede40421e54aa3213647767c9852e6bc"
    },
    {
      "title": "pachctl license add-cluster",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_license_add-cluster command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_license_add-cluster/",
      "relURI": "/latest/run-commands/pachctl_license_add-cluster/",
      "body": " pachctl license add-cluster # Register a new cluster with the license server.\nSynopsis # Register a new cluster with the license server.\npachctl license add-cluster [flags] Options # --address string The host and port where the cluster can be reached -h, --help help for add-cluster --id string The id for the cluster to register --secret string The shared secret to use to authenticate this cluster Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl license\t- License commmands manage the Enterprise License service ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "393455aa9dcea6590a6345ef494fd75e"
    },
    {
      "title": "pachctl license delete-all",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_license_delete-all command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_license_delete-all/",
      "relURI": "/latest/run-commands/pachctl_license_delete-all/",
      "body": " pachctl license delete-all # Delete all data from the license server\nSynopsis # Delete all data from the license server\npachctl license delete-all [flags] Options # -h, --help help for delete-all Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl license\t- License commmands manage the Enterprise License service ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "38f654bbc07cbda8f830077e906a29f5"
    },
    {
      "title": "pachctl license delete-cluster",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_license_delete-cluster command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_license_delete-cluster/",
      "relURI": "/latest/run-commands/pachctl_license_delete-cluster/",
      "body": " pachctl license delete-cluster # Delete a cluster registered with the license server.\nSynopsis # Delete a cluster registered with the license server.\npachctl license delete-cluster [flags] Options # -h, --help help for delete-cluster --id string The id for the cluster to delete Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl license\t- License commmands manage the Enterprise License service ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e28510f7eddf35155e16ecdbfee4df1f"
    },
    {
      "title": "pachctl license get-state",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_license_get-state command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_license_get-state/",
      "relURI": "/latest/run-commands/pachctl_license_get-state/",
      "body": " pachctl license get-state # Get the configuration of the license service.\nSynopsis # Get the configuration of the license service.\npachctl license get-state [flags] Options # -h, --help help for get-state Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl license\t- License commmands manage the Enterprise License service ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "3ae73c2bf1061397a51c926aa609106a"
    },
    {
      "title": "pachctl license list-clusters",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_license_list-clusters command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_license_list-clusters/",
      "relURI": "/latest/run-commands/pachctl_license_list-clusters/",
      "body": " pachctl license list-clusters # List clusters registered with the license server.\nSynopsis # List clusters registered with the license server.\npachctl license list-clusters [flags] Options # -h, --help help for list-clusters Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl license\t- License commmands manage the Enterprise License service ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "1387a6f0b29a5e41c2a5c0606f6529ec"
    },
    {
      "title": "pachctl license update-cluster",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_license_update-cluster command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_license_update-cluster/",
      "relURI": "/latest/run-commands/pachctl_license_update-cluster/",
      "body": " pachctl license update-cluster # Update an existing cluster registered with the license server.\nSynopsis # Update an existing cluster registered with the license server.\npachctl license update-cluster [flags] Options # --address string The host and port where the cluster can be reached by the enterprise server --cluster-deployment-id string The deployment id of the updated cluster -h, --help help for update-cluster --id string The id for the cluster to update --user-address string The host and port where the cluster can be reached by a user Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl license\t- License commmands manage the Enterprise License service ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "57d7d2050d937c27ce23ad1d4d5374cf"
    },
    {
      "title": "pachctl list",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_list command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_list/",
      "relURI": "/latest/run-commands/pachctl_list/",
      "body": " pachctl list # Print a list of Pachyderm resources of a specific type.\nSynopsis # Print a list of Pachyderm resources of a specific type.\nOptions # -h, --help help for list Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl list branch\t- Return all branches on a repo. pachctl list commit\t- Return a list of commits. pachctl list datum\t- Return the datums in a job. pachctl list file\t- Return the files in a directory. pachctl list job\t- Return info about jobs. pachctl list pipeline\t- Return info about all pipelines. pachctl list project\t- Return all projects. pachctl list repo\t- Return a list of repos. pachctl list secret\t- List all secrets from a namespace in the cluster. pachctl list transaction\t- List transactions. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "7c99edbddb63e03ebe0e118f2e7892b1"
    },
    {
      "title": "pachctl list branch",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_list_branch command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_list_branch/",
      "relURI": "/latest/run-commands/pachctl_list_branch/",
      "body": " pachctl list branch # Return all branches on a repo.\nSynopsis # This command returns all branches on a repo.\nTo list branches from a repo in another project, use the --project flag To get additional details about the branches, use the --raw flag pachctl list branch &lt;repo&gt; [flags] Examples # pachctl list branch foo@master pachctl list branch foo@master --project bar pachctl list branch foo@master --raw pachctl list branch foo@master --raw -o yaml Options # -h, --help help for branch -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) containing branch&#39;s repo. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl list\t- Print a list of Pachyderm resources of a specific type. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "763dd9c8c7630eed5d1ac922fd41c905"
    },
    {
      "title": "pachctl list commit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_list_commit command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_list_commit/",
      "relURI": "/latest/run-commands/pachctl_list_commit/",
      "body": " pachctl list commit # Return a list of commits.\nSynopsis # This command returns a list of commits, either across the entire pachyderm cluster or restricted to a single repo.\nTo specify which project the repo is in, use the --project flag To specify the number of commits to return, use the --number flag To list all commits that have come after a certain commit, use the --from flag To specify the origin of the commit, use the --origin flag; options include AUTO, FSCK, &amp; USER. Requires at least repo name in command To expand the commit to include all of its sub-commits, use the --expand flag pachctl list commit [&lt;commit-id&gt;|&lt;repo&gt;[@&lt;branch-or-commit&gt;]] [flags] Examples # pachctl list commit foo ‚ûî returns all commits in repo foo pachctl list commit foo@master ‚ûî returns all commits in repo foo on branch master pachctl list commit foo@master --number 10 ‚ûî returns the last 10 commits in repo foo on branch master pachctl list commit foo@master --from 0001a0100b1c10d01111e001fg00h00i ‚ûî returns all commits in repo foo on branch master since &lt;commit&gt; pachctl list commit foo@master --origin user ‚ûî returns all commits in repo foo on branch master originating from pachctl list commit 0001a0100b1c10d01111e001fg00h00i ‚ûî returns all commits with ID &lt;commit-id&gt; pachctl list commit 0001a0100b1c10d01111e001fg00h00i --expand ‚ûî returns all sub-commits on new lines, along with columns of more information pachctl list commit foo@master --raw -o yaml ‚ûî returns all commits in repo foo on branch master in YAML format Options # --all Specify all types of commits (AUTO, FSCK, USER) should be returned; default only includes USER. -x, --expand Specify results should return one line for each sub-commit and include more columns. -f, --from string Set the starting point of the commit range to list. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit -n, --number int Set the limit of returned results; if zero, list all commits. --origin string Specify the type of commit to scope returned results by; options include AUTO, FSCK, &amp; USER. -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) containing the commit. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl list\t- Print a list of Pachyderm resources of a specific type. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "6ebc5456be4444bf0ea94a459fcb3ab5"
    },
    {
      "title": "pachctl list datum",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_list_datum command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_list_datum/",
      "relURI": "/latest/run-commands/pachctl_list_datum/",
      "body": " pachctl list datum # Return the datums in a job.\nSynopsis # This command returns the datums in a job.\nTo pass in a JSON pipeline spec instead of pipeline@job, use the --file flag To specify the project where the parent pipeline lives, use the --project flag pachctl list datum &lt;pipeline&gt;@&lt;job&gt; [flags] Examples # pachctl list datum foo@5f93d03b65fa421996185e53f7f8b1e4 pachctl list datum foo@5f93d03b65fa421996185e53f7f8b1e4 --project bar pachctl list datum --file pipeline.json Options # -f, --file string Set the JSON file containing the pipeline to list datums from; the pipeline need not exist. -h, --help help for datum -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) containing parent pipeline for the job. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl list\t- Print a list of Pachyderm resources of a specific type. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "12f5fa3ae8a853a59e61ae076e448bfb"
    },
    {
      "title": "pachctl list file",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_list_file command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_list_file/",
      "relURI": "/latest/run-commands/pachctl_list_file/",
      "body": " pachctl list file # Return the files in a directory.\nSynopsis # This command returns the files in a directory. While using this command, take special note of how you can use ancestry syntax (e.g., appending^2 or .-1 to repo@branch) to inspect the contents of a file from a previous commit.\nTo specify the project where the repo is located, use the &ndash;project flag pachctl list file &lt;repo&gt;@&lt;branch-or-commit&gt;[:&lt;path/in/pfs&gt;] [flags] Examples # pachctl list file foo@master pachctl list file foo@master:dir pachctl list file foo@master^ pachctl list file foo@master^2 pachctl list file repo@master.-2 --project foo pachctl list file &#39;foo@master:dir\\[1\\]&#39; Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for file -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) where repo is located. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl list\t- Print a list of Pachyderm resources of a specific type. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "b2bd6e8097ca1214ffbbeb88f99de837"
    },
    {
      "title": "pachctl list job",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_list_job command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_list_job/",
      "relURI": "/latest/run-commands/pachctl_list_job/",
      "body": " pachctl list job # Return info about jobs.\nSynopsis # This command returns info about a list of jobs. You can pass in the command with or without a job ID.\nWithout an ID, this command returns a global list of top-level job sets which contain their own sub-jobs; With an ID, it returns a list of sub-jobs within the specified job set.\nTo return a list of sub-jobs across all job sets, use the --expand flag without passing an ID To return only the sub-jobs from the most recent version of a pipeline, use the --pipeline flag To return all sub-jobs from all versions of a pipeline, use the --history flag To return all sub-jobs whose input commits include data from a particular repo branch/commit, use the --input flag To turn only sub-jobs with a particular state, use the --state flag; options: CREATED, STARTING, UNRUNNABLE, RUNNING, EGRESS, FINISHING, FAILURE, KILLED, SUCCESS pachctl list job [&lt;job-id&gt;] [flags] Examples # pachctl list job pachctl list job --state starting pachctl list job --pipeline foo pachctl list job --expand pachctl list job --expand --pipeline foo pachctl list job --expand --pipeline foo --state failure --state unrunnable pachctl list job 5f93d03b65fa421996185e53f7f8b1e4 pachctl list job 5f93d03b65fa421996185e53f7f8b1e4 --state running pachctl list job --input foo-repo@staging pachctl list job --input foo-repo@5f93d03b65fa421996185e53f7f8b1e4 pachctl list job --pipeline foo --input bar-repo@staging pachctl list job --pipeline foo --input bar-repo@5f93d03b65fa421996185e53f7f8b1e4 Options # -A, --all-projects Specify results should return jobs from all projects. -x, --expand Specify results return as one line for each sub-job and include more columns; not needed if ID is passed. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for job --history string Specify results returned include jobs from historical versions of pipelines. (default &#34;none&#34;) -i, --input strings Specify results should only return jobs with a specific set of input commits; format: &lt;repo&gt;@&lt;branch-or-commit&gt; --no-pager Don&#39;t pipe output into a pager (i.e. less). -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) -p, --pipeline string Specify results should only return jobs created by a given pipeline. --project string Specify the project (by name) containing the parent pipeline for returned jobs. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml --state stringArray Specify results return only sub-jobs with the specified state; can be repeated to include multiple states. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl list\t- Print a list of Pachyderm resources of a specific type. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "eda93f6442a181e94b065f0efb6bce6d"
    },
    {
      "title": "pachctl list pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_list_pipeline command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_list_pipeline/",
      "relURI": "/latest/run-commands/pachctl_list_pipeline/",
      "body": " pachctl list pipeline # Return info about all pipelines.\nSynopsis # This command returns information about all pipelines.\nTo return pipelines with a specific state, use the --state flag To return pipelines as they existed at a specific commit, use the --commit flag To return a history of pipeline revisions, use the --history flag pachctl list pipeline [&lt;pipeline&gt;] [flags] Examples # pachctl list pipeline pachctl list pipeline --spec --output yaml pachctl list pipeline --commit 5f93d03b65fa421996185e53f7f8b1e4 pachctl list pipeline --state crashing pachctl list pipeline --project foo pachctl list pipeline --project foo --state restarting Options # -A, --all-projects Show pipelines form all projects. -c, --commit string List the pipelines as they existed at this commit. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for pipeline --history string Specify results should include revision history for pipelines. (default &#34;none&#34;) -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) containing the pipelines. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml -s, --spec Output &#39;create pipeline&#39; compatibility specs. --state stringArray Specify results should include only pipelines with the specified state (starting, running, restarting, failure, paused, standby, crashing); can be repeated for multiple states. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl list\t- Print a list of Pachyderm resources of a specific type. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a2503cc361b061b5e5e881f06bbdcf50"
    },
    {
      "title": "pachctl list project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_list_project command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_list_project/",
      "relURI": "/latest/run-commands/pachctl_list_project/",
      "body": " pachctl list project # Return all projects.\nSynopsis # This command returns all projects.\npachctl list project &lt;repo&gt; [flags] Examples # pachctl list project pachctl list project --raw Options # -h, --help help for project -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl list\t- Print a list of Pachyderm resources of a specific type. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "9ce4f171d956f3cff6c2a9fac3924532"
    },
    {
      "title": "pachctl list repo",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_list_repo command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_list_repo/",
      "relURI": "/latest/run-commands/pachctl_list_repo/",
      "body": " pachctl list repo # Return a list of repos.\nSynopsis # This command returns a list of repos that you have permissions to view. By default, it does not show system repos like pipeline metadata.\nTo view all input repos across projects, use -A flag To view all repos in a specific project, including system repos, use the --all flag To view repos of a specific type, use the --type flag; options include USER, META, &amp; SPEC To view repos of a specific type across projects, use the --type and -A flags To view repos of a specific type in a specific project, use the --type and --project flags For information on roles and permissions, see the documentation: https://docs.pachyderm.com/latest/set-up/authorization/permissions/\npachctl list repo [flags] Examples # pachctl list repos pachctl list repo -A pachctl list repo --all pachctl list repo --type user pachctl list repo --type user --all pachctl list repo --type user --all --project default pachctl list repo --type user --all --project default --raw Options # --all Specify all repo types should be returned, including hidden system repos. -A, --all-projects Specify all repos across all projects should be returned. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for repo -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) where the repos are located. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml --type string Set a repo type for scoped results. (default &#34;user&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl list\t- Print a list of Pachyderm resources of a specific type. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a37593e9484a71dca80d282b8f909c8f"
    },
    {
      "title": "pachctl list secret",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_list_secret command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_list_secret/",
      "relURI": "/latest/run-commands/pachctl_list_secret/",
      "body": " pachctl list secret # List all secrets from a namespace in the cluster.\nSynopsis # This command lists all secrets from a namespace in the cluster.\npachctl list secret [flags] Examples # pachctl list secret Options # -h, --help help for secret Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl list\t- Print a list of Pachyderm resources of a specific type. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e729b2ac490ddf627d83b106eeb45c36"
    },
    {
      "title": "pachctl list transaction",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_list_transaction command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_list_transaction/",
      "relURI": "/latest/run-commands/pachctl_list_transaction/",
      "body": " pachctl list transaction # List transactions.\nSynopsis # List transactions.\npachctl list transaction [flags] Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for transaction -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl list\t- Print a list of Pachyderm resources of a specific type. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "1c4b47995703940d4a8091cf0d409cf4"
    },
    {
      "title": "pachctl logs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_logs command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_logs/",
      "relURI": "/latest/run-commands/pachctl_logs/",
      "body": " pachctl logs # Return logs from a job.\nSynopsis # This command returns logs from a job.\nTo filter your logs by pipeline, use the --pipeline flag To filter your logs by job, use the --job flag To filter your logs by datum, use the --datum flag To filter your logs by the master process, use the --master flag with the --pipeline flag To filter your logs by the worker process, use the --worker flag To follow the logs as more are created, use the --follow flag To set the number of lines to return, use the --tail flag To return results starting from a certain amount of time before now, use the --since flag pachctl logs [--pipeline=&lt;pipeline&gt;|--job=&lt;pipeline&gt;@&lt;job&gt;] [--datum=&lt;datum&gt;] [flags] Examples # pachctl logs --pipeline foo pachctl logs --job foo@5f93d03b65fa421996185e53f7f8b1e4 pachctl logs --job foo@5f93d03b65fa421996185e53f7f8b1e4 --tail 10 pachctl logs --job foo@5f93d03b65fa421996185e53f7f8b1e4 --follow pachctl logs --job foo@5f93d03b65fa421996185e53f7f8b1e4 --datum 7f3c[...] pachctl logs --pipeline foo --datum 7f3c[...] --master pachctl logs --pipeline foo --datum 7f3c[...] --worker pachctl logs --pipeline foo --datum 7f3c[...] --master --tail 10 pachctl logs --pipeline foo --datum 7f3c[...] --worker --follow Options # --datum string Specify results should only return logs for a given datum ID. -f, --follow Follow logs as more are created. -h, --help help for logs --inputs string Filter for log lines generated while processing these files (accepts PFS paths or file hashes) -j, --job string Specify results should only return logs for a given job ID. --master Specify results should only return logs from the master process; --pipeline must be set. -p, --pipeline string Specify results should only return logs for a given pipeline. --project string Specify the project (by name) containing parent pipeline for the job. (default &#34;video-to-frame-traces&#34;) --raw Specify results should only return log messages verbatim from server. --since string Specify results should return log messages more recent than &#34;since&#34;. (default &#34;24h&#34;) -t, --tail int Set the number of lines to return of the most recent logs. --worker Specify results should only return logs from the worker process. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "ab8a797657c016054cab0c3e17fb72d0"
    },
    {
      "title": "pachctl loki",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_loki command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_loki/",
      "relURI": "/latest/run-commands/pachctl_loki/",
      "body": " pachctl loki # Query the loki logs.\nSynopsis # This command queries the loki logs.\npachctl loki &lt;query&gt; [flags] Examples # pachctl loki &lt;query&gt; --since 100s pachctl loki &lt;query&gt; --since 1h Options # -h, --help help for loki --since string Specify results should return log messages more recent than &#34;since&#34;. (default &#34;0&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "05f8653a9773d00155e70e5da6f0dd8d"
    },
    {
      "title": "pachctl mount",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_mount command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_mount/",
      "relURI": "/latest/run-commands/pachctl_mount/",
      "body": " pachctl mount # Mount pfs locally. This command blocks.\nSynopsis # Mount pfs locally. This command blocks.\npachctl mount &lt;path/to/mount/point&gt; [flags] Options # -d, --debug Turn on debug messages. -h, --help help for mount --project string Project in which repo is located. (default &#34;default&#34;) -r, --repos []string Repos and branches / commits to mount, arguments should be of the form &#34;repo[@branch=commit][+w]&#34;, where the trailing flag &#34;+w&#34; indicates write. You can omit the branch when specifying a commit unless the same commit ID is on multiple branches in the repo. (default []) -w, --write Allow writing to pfs through the mount. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f609ba8066f81059f7c84aa2c86be08b"
    },
    {
      "title": "pachctl port-forward",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_port-forward command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_port-forward/",
      "relURI": "/latest/run-commands/pachctl_port-forward/",
      "body": " pachctl port-forward # Forward a port on the local machine to pachd. This command blocks.\nSynopsis # Forward a port on the local machine to pachd. This command blocks.\npachctl port-forward [flags] Options # --console-port uint16 The local port to bind the console service to. (default 4000) --dex-port uint16 The local port to bind the identity service to. (default 30658) -h, --help help for port-forward --namespace string Kubernetes namespace Pachyderm is deployed in. --oidc-port uint16 The local port to bind pachd&#39;s OIDC callback to. (default 30657) -p, --port uint16 The local port to bind pachd to. (default 30650) --remote-console-port uint16 The remote port to bind the console service to. (default 4000) --remote-dex-port uint16 The local port to bind the identity service to. (default 1658) --remote-oidc-port uint16 The remote port that OIDC callback is bound to in the cluster. (default 1657) --remote-port uint16 The remote port that pachd is bound to in the cluster. (default 1650) --remote-s3gateway-port uint16 The remote port that the s3 gateway is bound to. (default 1600) -s, --s3gateway-port uint16 The local port to bind the s3gateway to. (default 30600) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e13a510a6f8e38577d071c48448e52d6"
    },
    {
      "title": "pachctl put",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_put command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_put/",
      "relURI": "/latest/run-commands/pachctl_put/",
      "body": " pachctl put # Insert data into Pachyderm.\nSynopsis # Insert data into Pachyderm.\nOptions # -h, --help help for put Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl put file\t- Put a file into the filesystem. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "116b2968f2dc5b110f1d06795331c799"
    },
    {
      "title": "pachctl put file",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_put_file command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_put_file/",
      "relURI": "/latest/run-commands/pachctl_put_file/",
      "body": " pachctl put file # Put a file into the filesystem.\nSynopsis # This command puts a file into the filesystem. This command supports a number of ways to insert data into PFS.\nFiles, Directories, &amp; URLs:\nTo upload via local filesystem, use the -f flag To upload via URL, use the -f flag with a URL as the argument To upload via filepaths &amp; urls within a file, use the i flag To upload to a specific path in the repo, use the -f flag and add the path to the repo@branch:/path To upload recursively from a directory, use the -r flag To upload tar files and have them automatically untarred, use the -untar flag Compression, Parallelization, Appends:\nTo compress files before uploading, use the -c flag To define the maximum number of files that can be uploaded in parallel, use the -p flag To append to an existing file, use the -a flag Other: To enable progress bars, use the -P flag\npachctl put file &lt;repo&gt;@&lt;branch-or-commit&gt;[:&lt;path/to/file&gt;] [flags] Examples # pachctl put file repo@master-f image.png pachctl put file repo@master:/logs/log-1.txt pachctl put file -r repo@master -f my-directory pachctl put file -r repo@branch:/path -f my-directory pachctl put file repo@branch -f http://host/example.png pachctl put file repo@branch:/dir -f http://host/example.png pachctl put file repo@branch -r -f s3://my_bucket pachctl put file repo@branch -i file pachctl put file repo@branch -i http://host/path pachctl put file repo@branch -f -untar dir.tar pachctl put file repo@branch -f -c image.png Options # -a, --append Specify file contents should be appended to existing content from previous commits or previous calls to &#39;pachctl put file&#39; within this commit. --compress Specify data should be compressed during upload. This parameter might help you upload your uncompressed data, such as CSV files, to Pachyderm faster. Use &#39;compress&#39; with caution, because if your data is already compressed, this parameter might slow down the upload speed instead of increasing. -f, --file strings Specify the file to be put; it can be a local file or a URL. (default [-]) --full-path Specify entire path provided to -f should be the target filename in PFS; by default only the base of the path is used. -h, --help help for file -i, --input-file string Specify file provided contains a list of files to be put (as paths or URLs). -p, --parallelism int Set the maximum number of files that can be uploaded in parallel. (default 10) --progress Print progress bars. (default true) --project string Specify the project (by name) where the repo for uploading this file is located. (default &#34;video-to-frame-traces&#34;) -r, --recursive Specify files should be recursively put into a directory. --untar Specify file(s) with the extension .tar should be untarred and put as a separate file for each file within the tar stream(s); gzipped (.tar.gz or .tgz) tar file(s) are handled as well Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl put\t- Insert data into Pachyderm. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "fda6cd57a043304efb7ba42197b1b163"
    },
    {
      "title": "pachctl restart",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_restart command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_restart/",
      "relURI": "/latest/run-commands/pachctl_restart/",
      "body": " pachctl restart # Cancel and restart an ongoing task.\nSynopsis # Cancel and restart an ongoing task.\nOptions # -h, --help help for restart Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl restart datum\t- Restart a stuck datum during a currently running job. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "8a43eef08cf416bc58e0e48f0dc24f8e"
    },
    {
      "title": "pachctl restart datum",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_restart_datum command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_restart_datum/",
      "relURI": "/latest/run-commands/pachctl_restart_datum/",
      "body": " pachctl restart datum # Restart a stuck datum during a currently running job.\nSynopsis # This command restarts a stuck datum during a currently running job; it does not solve failed datums.\nYou can configure a job to skip failed datums via the transform.err_cmd setting of your pipeline spec.\nTo specify the project where the parent pipeline lives, use the --project flag pachctl restart datum &lt;pipeline&gt;@&lt;job&gt; &lt;datum-path1&gt;,&lt;datum-path2&gt;,... [flags] Examples # pachctl restart datum foo@5f93d03b65fa421996185e53f7f8b1e4 /logs/logs.txt pachctl restart datum foo@5f93d03b65fa421996185e53f7f8b1e4 /logs/logs-a.txt, /logs/logs-b.txt pachctl restart datum foo@5f93d03b65fa421996185e53f7f8b1e4 /logs/logs-a.txt, /logs/logs-b.txt --project bar Options # -h, --help help for datum --project string Specify the project (by name) containing parent pipeline for the datum&#39;s job (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl restart\t- Cancel and restart an ongoing task. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "27dbe4f9651976a3440c0cd7e92dac8c"
    },
    {
      "title": "pachctl resume",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_resume command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_resume/",
      "relURI": "/latest/run-commands/pachctl_resume/",
      "body": " pachctl resume # Resume a stopped task.\nSynopsis # Resume a stopped task.\nOptions # -h, --help help for resume Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl resume transaction\t- Set an existing transaction as active. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "bc7318ce08ff7e82cfb7de03ad17dd53"
    },
    {
      "title": "pachctl resume transaction",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_resume_transaction command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_resume_transaction/",
      "relURI": "/latest/run-commands/pachctl_resume_transaction/",
      "body": " pachctl resume transaction # Set an existing transaction as active.\nSynopsis # Set an existing transaction as active.\npachctl resume transaction &lt;transaction&gt; [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl resume\t- Resume a stopped task. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "6f05b41851288e696768879f922e26d6"
    },
    {
      "title": "pachctl run",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_run command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_run/",
      "relURI": "/latest/run-commands/pachctl_run/",
      "body": " pachctl run # Manually run a Pachyderm resource.\nSynopsis # Manually run a Pachyderm resource.\nOptions # -h, --help help for run Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl run cron\t- Run an existing Pachyderm cron pipeline now pachctl run pfs-load-test\t- Run a PFS load test. pachctl run pps-load-test\t- Run a PPS load test. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "c51c3e995f1c52d8b13f7109970a8bd3"
    },
    {
      "title": "pachctl run cron",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_run_cron command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_run_cron/",
      "relURI": "/latest/run-commands/pachctl_run_cron/",
      "body": " pachctl run cron # Run an existing Pachyderm cron pipeline now\nSynopsis # This command runs an existing Pachyderm cron pipeline immediately.\npachctl run cron &lt;pipeline&gt; [flags] Examples # pachctl run cron foo pachctl run cron foo --project bar Options # -h, --help help for cron --project string Specify the project (by name) containing the cron pipeline. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl run\t- Manually run a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "00735f11bb49abb7c657eec48b241663"
    },
    {
      "title": "pachctl run pfs-load-test",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_run_pfs-load-test command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_run_pfs-load-test/",
      "relURI": "/latest/run-commands/pachctl_run_pfs-load-test/",
      "body": " pachctl run pfs-load-test # Run a PFS load test.\nSynopsis # This command runs a PFS load test.\npachctl run pfs-load-test &lt;spec-file&gt; [flags] Examples # Specification: -- CommitSpec -- count: int modifications: [ ModificationSpec ] fileSources: [ FileSourceSpec ] validator: ValidatorSpec -- ModificationSpec -- count: int putFile: PutFileSpec -- PutFileSpec -- count: int source: string -- FileSourceSpec -- name: string random: RandomFileSourceSpec -- RandomFileSourceSpec -- directory: RandomDirectorySpec sizes: [ SizeSpec ] incrementPath: bool -- RandomDirectorySpec -- depth: SizeSpec run: int -- SizeSpec -- min: int max: int prob: int [0, 100] -- ValidatorSpec -- frequency: FrequencySpec -- FrequencySpec -- count: int prob: int [0, 100] Example: count: 5 modifications: - count: 5 putFile: count: 5 source: &#34;random&#34; fileSources: - name: &#34;random&#34; random: directory: depth: 3 run: 3 size: - min: 1000 max: 10000 prob: 30 - min: 10000 max: 100000 prob: 30 - min: 1000000 max: 10000000 prob: 30 - min: 10000000 max: 100000000 prob: 10 validator: {} Options # -b, --branch string Specify the branch to use for generating the load. -h, --help help for pfs-load-test --project string Specify the project (by name) where the repo is located. (default &#34;video-to-frame-traces&#34;) -s, --seed int Set the seed to use for generating the load. --state-id string Set the ID of the base state to use for the load. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl run\t- Manually run a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "cddc454f21e4f3609475e02eae2dab11"
    },
    {
      "title": "pachctl run pps-load-test",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_run_pps-load-test command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_run_pps-load-test/",
      "relURI": "/latest/run-commands/pachctl_run_pps-load-test/",
      "body": " pachctl run pps-load-test # Run a PPS load test.\nSynopsis # This command runs a PPS load test for a specified pipeline specification file.\nTo run a load test with a specific seed, use the --seed flag To run a load test with a specific parallelism count, use the --parallelism flag To run a load test with a specific pod patch, use the --pod-patch flag pachctl run pps-load-test &lt;spec-file&gt; [flags] Examples # pachctl run pps-load-test --dag myspec.json pachctl run pps-load-test --dag myspec.json --seed 1 pachctl run pps-load-test --dag myspec.json --parallelism 3 pachctl run pps-load-test --dag myspec.json --pod-patch patch.json pachctl run pps-load-test --dag myspec.json --state-id xyz Options # -d, --dag string Provide DAG specification file to use for the load test -h, --help help for pps-load-test -p, --parallelism int Set the parallelism count to use for the pipelines. --pod-patch string Provide pod patch file to use for the pipelines. -s, --seed int Specify the seed to use for generating the load. --state-id string Provide the ID of the base state to use for the load. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl run\t- Manually run a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f05cc3fa58a14b3a7fe26c318f74bf38"
    },
    {
      "title": "pachctl shell",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_shell command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_shell/",
      "relURI": "/latest/run-commands/pachctl_shell/",
      "body": " pachctl shell # Run the pachyderm shell.\nSynopsis # Run the pachyderm shell.\npachctl shell [flags] Options # -h, --help help for shell --max-completions int The maximum number of completions to show in the shell, defaults to 64. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "84ff2968f05482492d302e389227b1a4"
    },
    {
      "title": "pachctl squash",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_squash command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_squash/",
      "relURI": "/latest/run-commands/pachctl_squash/",
      "body": " pachctl squash # Squash an existing Pachyderm resource.\nSynopsis # Squash an existing Pachyderm resource.\nOptions # -h, --help help for squash Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl squash commit\t- Squash the sub-commits of a commit. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "aa3c22774621b4eb0817a44685bc0043"
    },
    {
      "title": "pachctl squash commit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_squash_commit command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_squash_commit/",
      "relURI": "/latest/run-commands/pachctl_squash_commit/",
      "body": " pachctl squash commit # Squash the sub-commits of a commit.\nSynopsis # This command squashes the sub-commits of a commit. The data in the sub-commits will remain in their child commits. The squash will fail if it includes a commit with no children\npachctl squash commit &lt;commit-id&gt; [flags] Examples # pachctl squash commit 0001a0100b1c10d01111e001fg00h00i Options # -h, --help help for commit Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl squash\t- Squash an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "38fbc209767dab230481742be8c90b35"
    },
    {
      "title": "pachctl start",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_start command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_start/",
      "relURI": "/latest/run-commands/pachctl_start/",
      "body": " pachctl start # Start a Pachyderm resource.\nSynopsis # Start a Pachyderm resource.\nOptions # -h, --help help for start Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl start commit\t- Start a new commit. pachctl start pipeline\t- Restart a stopped pipeline. pachctl start transaction\t- Start a new transaction. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "987d4576e6d7660f0fb20f1653526370"
    },
    {
      "title": "pachctl start commit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_start_commit command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_start_commit/",
      "relURI": "/latest/run-commands/pachctl_start_commit/",
      "body": " pachctl start commit # Start a new commit.\nSynopsis # This command starts a new commit with parent-commit as the parent on the given branch; if the branch does not exist, it will be created.\nTo specify a parent commit, use the --parent flag To add a message to the commit, use the --message or --description flag To specify which project the repo is in, use the --project flag pachctl start commit &lt;repo&gt;@&lt;branch&gt; [flags] Examples # pachctl start commit foo@master pachctl start commit foo@master -p 0001a0100b1c10d01111e001fg00h00i pachctl start commit foo@master --message &#39;my commit message&#39; pachctl start commit foo@master --description &#39;my commit description&#39; pachctl start commit foo@master --project bar Options # --description string Set a description of this commit&#39;s contents (synonym for --message). (default &#34;d&#34;) -h, --help help for commit -m, --message string Set a description for the commit&#39;s contents (synonym for --description). -p, --parent string Set the parent (by id) of the new commit; only needed when branch is not specified using the @ syntax. --project string Specify the project (by name) where the repo for this commit is located. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl start\t- Start a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "2d9bb47cc9b134c1c797bd5f6a84208f"
    },
    {
      "title": "pachctl start pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_start_pipeline command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_start_pipeline/",
      "relURI": "/latest/run-commands/pachctl_start_pipeline/",
      "body": " pachctl start pipeline # Restart a stopped pipeline.\nSynopsis # This command restarts a stopped pipeline.\npachctl start pipeline &lt;pipeline&gt; [flags] Examples # pachctl start pipeline foo pachctl start pipeline foo --project bar Options # -h, --help help for pipeline --project string Project containing pipeline. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl start\t- Start a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f3c0e3f0d5f5122a7033a4986dc9a9a2"
    },
    {
      "title": "pachctl start transaction",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_start_transaction command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_start_transaction/",
      "relURI": "/latest/run-commands/pachctl_start_transaction/",
      "body": " pachctl start transaction # Start a new transaction.\nSynopsis # Start a new transaction.\npachctl start transaction [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl start\t- Start a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "0bbd5748bde35ce1570faf6f0603f23e"
    },
    {
      "title": "pachctl stop",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_stop command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_stop/",
      "relURI": "/latest/run-commands/pachctl_stop/",
      "body": " pachctl stop # Cancel an ongoing task.\nSynopsis # Cancel an ongoing task.\nOptions # -h, --help help for stop Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl stop job\t- Stop a job. pachctl stop pipeline\t- Stop a running pipeline. pachctl stop transaction\t- Stop modifying the current transaction. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "57602926c5caf6a8d54f6d67470a4946"
    },
    {
      "title": "pachctl stop job",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_stop_job command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_stop_job/",
      "relURI": "/latest/run-commands/pachctl_stop_job/",
      "body": " pachctl stop job # Stop a job.\nSynopsis # This command stops a job immediately. To specify the project where the parent pipeline lives, use the --project flag\npachctl stop job &lt;pipeline&gt;@&lt;job&gt; [flags] Options # -h, --help help for job --project string Specify the project (by name) containing the parent pipeline for the job. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl stop\t- Cancel an ongoing task. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "26112fcd2168a572e8df856ba57332de"
    },
    {
      "title": "pachctl stop pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_stop_pipeline command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_stop_pipeline/",
      "relURI": "/latest/run-commands/pachctl_stop_pipeline/",
      "body": " pachctl stop pipeline # Stop a running pipeline.\nSynopsis # This command stops a running pipeline.\npachctl stop pipeline &lt;pipeline&gt; [flags] Examples # pachctl stop pipeline foo pachctl stop pipeline foo --project bar Options # -h, --help help for pipeline --project string Project containing pipeline. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl stop\t- Cancel an ongoing task. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "062f29ad69d19c795fe67d2a5d267ef8"
    },
    {
      "title": "pachctl stop transaction",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_stop_transaction command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_stop_transaction/",
      "relURI": "/latest/run-commands/pachctl_stop_transaction/",
      "body": " pachctl stop transaction # Stop modifying the current transaction.\nSynopsis # Stop modifying the current transaction.\npachctl stop transaction [flags] Options # -h, --help help for transaction Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl stop\t- Cancel an ongoing task. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "077c43dc0290088d954d465fb63904a3"
    },
    {
      "title": "pachctl subscribe",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_subscribe command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_subscribe/",
      "relURI": "/latest/run-commands/pachctl_subscribe/",
      "body": " pachctl subscribe # Wait for notifications of changes to a Pachyderm resource.\nSynopsis # Wait for notifications of changes to a Pachyderm resource.\nOptions # -h, --help help for subscribe Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl subscribe commit\t- Print commits as they are created (finished). ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "710fed30b1322b77a8d6db37d0b9681c"
    },
    {
      "title": "pachctl subscribe commit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_subscribe_commit command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_subscribe_commit/",
      "relURI": "/latest/run-commands/pachctl_subscribe_commit/",
      "body": " pachctl subscribe commit # Print commits as they are created (finished).\nSynopsis # This command prints commits as they are created in the specified repo and branch. By default, all existing commits on the specified branch are returned first. A commit is only considered created when it&rsquo;s been finished.\nTo only see commits created after a certain commit, use the --from flag. To only see new commits created from now on, use the --new flag. To see all commit types, use the --all flag. To only see commits of a specific type, use the --origin flag. pachctl subscribe commit &lt;repo&gt;[@&lt;branch&gt;] [flags] Examples # pachctl subscribe commit foo@master ‚ûî subscribe to commits in the foo repo on the master branch pachctl subscribe commit foo@bar --from 0001a0100b1c10d01111e001fg00h00i ‚ûî starting at &lt;commit-id&gt;, subscribe to commits in the foo repo on the master branch pachctl subscribe commit foo@bar --new ‚ûî subscribe to commits in the foo repo on the master branch, but only for new commits created from now on Options # --all Specify results should return all types of commits (AUTO, FSCK, USER) --from string Subscribe to and return all commits since the specified commit. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit --new Subscribe to and return only new commits created from now on. --origin string Specify results should only return commits of a specific type; options include AUTO, FSCK, &amp; USER. -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) containing the commit. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl subscribe\t- Wait for notifications of changes to a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "ea05c6d559f530df2feaa6a913f412b4"
    },
    {
      "title": "pachctl unmount",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_unmount command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_unmount/",
      "relURI": "/latest/run-commands/pachctl_unmount/",
      "body": " pachctl unmount # Unmount pfs.\nSynopsis # Unmount pfs.\npachctl unmount &lt;path/to/mount/point&gt; [flags] Options # -a, --all unmount all pfs mounts -h, --help help for unmount Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e8d3ba779aa41ce2ea7a97f53421ec1a"
    },
    {
      "title": "pachctl update",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_update command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_update/",
      "relURI": "/latest/run-commands/pachctl_update/",
      "body": " pachctl update # Change the properties of an existing Pachyderm resource.\nSynopsis # Change the properties of an existing Pachyderm resource.\nOptions # -h, --help help for update Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl update pipeline\t- Update an existing Pachyderm pipeline. pachctl update project\t- Update a project. pachctl update repo\t- Update a repo. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "67169f66f6a616143d3578dec786e875"
    },
    {
      "title": "pachctl update pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_update_pipeline command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_update_pipeline/",
      "relURI": "/latest/run-commands/pachctl_update_pipeline/",
      "body": " pachctl update pipeline # Update an existing Pachyderm pipeline.\nSynopsis # This command updates a Pachyderm pipeline with a new pipeline specification. For details on the format, see https://docs.pachyderm.com/latest/reference/pipeline-spec/\nTo update a pipeline from a JSON/YAML file, use the --file flag To update a pipeline from a jsonnet template file, use the --jsonnet flag. You can optionally pay multiple arguments separately using --arg To reprocess all data in the pipeline, use the --reprocess flag To push your local images to docker registry, use the --push-images and --username flags To push your local images to custom registry, use the --push-images, --registry, and --username flags pachctl update pipeline [flags] Examples # pachctl update pipeline -file regression.json pachctl update pipeline -file foo.json --project bar pachctl update pipeline -file foo.json --push-images --username lbliii pachctl update pipeline --jsonnet /templates/foo.jsonnet --arg myimage=bar --arg src=image Options # --arg stringArray Provide a top-level argument in the form of &#39;param=value&#39; passed to the Jsonnet template; requires --jsonnet. For multiple args, --arg may be set more than once. --dry-run If true, pipeline will not actually be updated. -f, --file string Provide a JSON/YAML file (url or filepath) for one or more pipelines. &#34;-&#34; reads from stdin (the default behavior). Exactly one of --file and --jsonnet must be set. -h, --help help for pipeline --jsonnet string Provide a Jsonnet template file (url or filepath) for one or more pipelines. &#34;-&#34; reads from stdin. Exactly one of --file and --jsonnet must be set. Jsonnet templates must contain a top-level function; strings can be passed to this function with --arg (below) -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) in which to create the pipeline. (default &#34;video-to-frame-traces&#34;) -p, --push-images Specify that the local docker images should be pushed into the registry (docker by default). --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml -r, --registry string Specify an alternative registry to push images to. (default &#34;index.docker.io&#34;) --reprocess Reprocess all datums that were already processed by previous version of the pipeline. -u, --username string Specify the username to push images as. Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl update\t- Change the properties of an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "f272a69cae55afeb68177ca6e9e7d4b5"
    },
    {
      "title": "pachctl update project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_update_project command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_update_project/",
      "relURI": "/latest/run-commands/pachctl_update_project/",
      "body": " pachctl update project # Update a project.\nSynopsis # This command updates a project&rsquo;s description.\npachctl update project &lt;project&gt; [flags] Examples # pachctl update project foo-project --description &#39;This is a project for foo.&#39; Options # -d, --description string Set a new description of the updated project. -h, --help help for project Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl update\t- Change the properties of an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "ffc2b4c765be413a2199831f3ba8e5de"
    },
    {
      "title": "pachctl update repo",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_update_repo command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_update_repo/",
      "relURI": "/latest/run-commands/pachctl_update_repo/",
      "body": " pachctl update repo # Update a repo.\nSynopsis # This command enables you to update the description of an existing repo.\nTo specify which project to update the repo in, use the --project flag To update the description of a repo, use the --description flag If you are looking to update the pipelines in your repo, see pachctl update pipeline instead.\npachctl update repo &lt;repo&gt; [flags] Examples # pachctl update repo foo --description &#39;my updated repo description&#39; pachctl update repo foo --project bar --description &#39;my updated repo description&#39; Options # -d, --description string Set a repo description. -h, --help help for repo --project string Specify the project (by name) where the repo is located. (default &#34;video-to-frame-traces&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl update\t- Change the properties of an existing Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "c87a5bcf84d679f46dc986266bbda682"
    },
    {
      "title": "pachctl validate",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_validate command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_validate/",
      "relURI": "/latest/run-commands/pachctl_validate/",
      "body": " pachctl validate # Validate the specification of a Pachyderm resource.\nSynopsis # Validate the specification of a Pachyderm resource. Client-side only.\nOptions # -h, --help help for validate Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl validate pipeline\t- Validate pipeline spec. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a41caa4374384088c8b9a37e0d271717"
    },
    {
      "title": "pachctl validate pipeline",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_validate_pipeline command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_validate_pipeline/",
      "relURI": "/latest/run-commands/pachctl_validate_pipeline/",
      "body": " pachctl validate pipeline # Validate pipeline spec.\nSynopsis # This command validates a pipeline spec. Client-side only; does not check that repos, images, etc exist on the server.\npachctl validate pipeline [flags] Examples # pachctl validate pipeline --file spec.json Options # -f, --file string A JSON file (url or filepath) containing one or more pipelines. &#34;-&#34; reads from stdin (the default behavior). Exactly one of --file and --jsonnet must be set. -h, --help help for pipeline Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl validate\t- Validate the specification of a Pachyderm resource. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "434613f177b00046c807b3528c184357"
    },
    {
      "title": "pachctl version",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_version command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_version/",
      "relURI": "/latest/run-commands/pachctl_version/",
      "body": " pachctl version # Print Pachyderm version information.\nSynopsis # Print Pachyderm version information.\npachctl version [flags] Options # --client-only If set, only print pachctl&#39;s version, but don&#39;t make any RPCs to pachd. Useful if pachd is unavailable --enterprise If set, &#39;pachctl version&#39; will run on the active enterprise context. -h, --help help for version -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml --timeout string If set, &#39;pachctl version&#39; will timeout after the given duration (formatted as a golang time duration--a number followed by ns, us, ms, s, m, or h). If --client-only is set, this flag is ignored. If unset, pachctl will use a default timeout; if set to 0s, the call will never time out. (default &#34;default&#34;) Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "de22870b1732a2616f8e8558b3b81889"
    },
    {
      "title": "pachctl wait",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_wait command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_wait/",
      "relURI": "/latest/run-commands/pachctl_wait/",
      "body": " pachctl wait # Wait for the side-effects of a Pachyderm resource to propagate.\nSynopsis # Wait for the side-effects of a Pachyderm resource to propagate.\nOptions # -h, --help help for wait Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl\t- pachctl wait commit\t- Wait for the specified commit to finish and return it. pachctl wait job\t- Wait for a job to finish then return info about the job. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "3757254573ebb9b5faa73203697a9632"
    },
    {
      "title": "pachctl wait commit",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_wait_commit command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_wait_commit/",
      "relURI": "/latest/run-commands/pachctl_wait_commit/",
      "body": " pachctl wait commit # Wait for the specified commit to finish and return it.\nSynopsis # This command waits for the specified commit to finish before returning it, allowing you to track your commits downstream as they are produced. Each line is printed as soon as a new (sub) commit of your global commit finishes.\npachctl wait commit &lt;repo&gt;@&lt;branch-or-commit&gt; [flags] Examples # pachctl wait commit foo@0001a0100b1c10d01111e001fg00h00i pachctl wait commit foo@0001a0100b1c10d01111e001fg00h00i --project bar pachctl wait commit foo@0001a0100b1c10d01111e001fg00h00i --project bar --raw -o yaml Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for commit -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) containing the commit. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl wait\t- Wait for the side-effects of a Pachyderm resource to propagate. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "8336dba1d8b8621c81ca2336a7b161ef"
    },
    {
      "title": "pachctl wait job",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Run Commands",
      "description": "Learn about the pachctl_wait_job command",
      "date": "September 7, 2023",
      "uri": "https://mldm.pachyderm.com/latest/run-commands/pachctl_wait_job/",
      "relURI": "/latest/run-commands/pachctl_wait_job/",
      "body": " pachctl wait job # Wait for a job to finish then return info about the job.\nSynopsis # This command waits for a job to finish then return info about the job.\npachctl wait job &lt;job&gt;|&lt;pipeline&gt;@&lt;job&gt; [flags] Examples # pachctl wait job e0f68a2fcda7458880c9e2e2dae9e678 pachctl wait job foo@e0f68a2fcda7458880c9e2e2dae9e678 pachctl wait job foo@e0f68a2fcda7458880c9e2e2dae9e678 --project bar pachctl wait job foo@e0f68a2fcda7458880c9e2e2dae9e678 --project bar --raw --output yaml Options # --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -h, --help help for job -o, --output string Output format when --raw is set: &#34;json&#34; or &#34;yaml&#34; (default &#34;json&#34;) --project string Specify the project (by name) containing the parent pipeline for this job. (default &#34;video-to-frame-traces&#34;) --raw Disable pretty printing; serialize data structures to an encoding such as json or yaml Options inherited from parent commands # --no-color Turn off colors. -v, --verbose Output verbose logs SEE ALSO # pachctl wait\t- Wait for the side-effects of a Pachyderm resource to propagate. ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e14aaf055de930e3f6d2296faffc14da"
    },
    {
      "title": "Debug",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "Troubleshoot pipelines using PachCTL to explore logged user events.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/debug/",
      "relURI": "/latest/debug/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "eda3519b692857e59df55b2901c44c61"
    },
    {
      "title": "Common Issues",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Debug",
      "description": "Learn how to troubleshoot common issues.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/debug/common-issues/",
      "relURI": "/latest/debug/common-issues/",
      "body": " Cannot Connect via PachCTL # Context Deadline Exceeded # Symptom # You may be using the pachd address config value or environment variable to specify how pachctl talks to your HPE ML Data Management cluster, or you may be forwarding the HPE ML Data Managementport. In any event, you might see something similar to:\npachctl version COMPONENT VERSION pachctl 2.7.3 context deadline exceeded Also, you might get this message if pachd is not running.\nRecourse # It&rsquo;s possible that the connection is just taking a while. Occasionally this can happen if your cluster is far away (deployed in a region across the country). Check your internet connection.\nIt&rsquo;s also possible that you haven&rsquo;t poked a hole in the firewall to access the node on this port. Usually to do that you adjust a security rule (in AWS parlance a security group). For example, on AWS, if you find your node in the web console and click on it, you should see a link to the associated security group. Inspect that group. There should be a way to &ldquo;add a rule&rdquo; to the group. You&rsquo;ll want to enable TCP access (ingress) on port 30650. You&rsquo;ll usually be asked which incoming IPs should be whitelisted. You can choose to use your own, or enable it for everyone (0.0.0.0/0).\nCould not get Cluster ID + Could Not Inspect Project # Symptom # You are trying to connect to a HPE ML Data Management cluster using pachctl and you see the following error:\ncould not get cluster ID: failed to inspect cluster: rpc error: code = NotFound desc = could not inspect project &#34;foo&#34;: error getting project &#34;foo&#34;: project &#34;foo&#34; not found Recourse # Check your ./pachyderm/config.json file to see if an entry for your active context has an existing conflicting contexts with other details, such as a cluster_deployment_id saved. This can happen if you have reinstalled HPE ML Data Management and left the old configurations in place. Uninstalling HPE ML Data Management does not remove these old configurations. üí° The following example is an output for a ./pachyderm/config.json file with duplicate entries for the same context.\n{ &#34;user_id&#34;: &#34;7b2f2427f72f418ea4a14287c76e63ea&#34;, &#34;v2&#34;: { &#34;active_context&#34;: &#34;http://localhost:80&#34;, &#34;contexts&#34;: { &#34;grpc://localhost:80&#34;: { &#34;pachd_address&#34;: &#34;grpc://localhost:80&#34;, &#34;cluster_deployment_id&#34;: &#34;oO29fP0PC1Q3O39MSDgpVRgdpXraLJfw&#34;, &#34;project&#34;: &#34;foo&#34; }, &#34;http://localhost:80&#34;: { &#34;pachd_address&#34;: &#34;grpc://localhost:80&#34;, &#34;session_token&#34;: &#34;672e166adf994b7ebdd7f32bffa44375&#34;, &#34;cluster_deployment_id&#34;: &#34;GaM2j0EknVSQIcfEHzNof25Z525QAf7S&#34;, &#34;project&#34;: &#34;standard-ml-tutorial&#34; }, &#34;https://localhost:80&#34;: { &#34;pachd_address&#34;: &#34;grpcs://localhost:80&#34; }, } } } Delete the conflicting contexts from the ./pachyderm/config.json file. When you run pachctl connect, a new context will be created.\nReset your default project if the conflicting contexts had an old project set as the default.\npachctl config update context --project default Run pachctl connect again to connect to your HPE ML Data Management cluster.\nRun pachctl version to verify that you are connected to your HPE ML Data Management cluster.\nCertificate Error When Using Kubectl # Symptom # This can happen on any request using kubectl (e.g. kubectl get all). In particular you&rsquo;ll see:\nkubectl version Client Version: version.Info{Major:&#34;1&#34;, Minor:&#34;6&#34;, GitVersion:&#34;v1.6.4&#34;, GitCommit:&#34;d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae&#34;, GitTreeState:&#34;clean&#34;, BuildDate:&#34;2017-05-19T20:41:24Z&#34;, GoVersion:&#34;go1.8.1&#34;, Compiler:&#34;gc&#34;, Platform:&#34;darwin/amd64&#34;} Unable to connect to the server: x509: certificate signed by unknown authority Recourse # Check if you&rsquo;re on any sort of VPN or other egress proxy that would break SSL. Also, there is a possibility that your credentials have expired. In the case where you&rsquo;re using GKE and gcloud, renew your credentials via:\nkubectl get all Unable to connect to the server: x509: certificate signed by unknown authority gcloud container clusters get-credentials my-cluster-name-dev Fetching cluster endpoint and auth data. kubeconfig entry generated for my-cluster-name-dev. kubectl config current-context gke_my-org_us-east1-b_my-cluster-name-dev Uploads and Downloads are Slow # Symptom # Any pachctl put file or pachctl get file commands are slow.\nRecourse # If you do not explicitly set the pachd address config value, pachctl will default to using port forwarding, which throttles traffic to ~1MB/s. If you need to do large downloads/uploads you should consider using pachd address config value. You&rsquo;ll also want to make sure you&rsquo;ve allowed ingress access through any firewalls to your k8s cluster.\nNaming a Repo with an Unsupported Symbol # Symptom # A HPE ML Data Management repo was accidentally named starting with a dash (-) and the repository is treated as a command flag instead of a repository.\nRecourse # HPE ML Data Management supports standard bash utilities that you can use to resolve this and similar problems. For example, in this case, you can specify double dashes (--) to delete the repository. Double dashes signify the end of options and tell the shell to process the rest arguments as filenames and objects.\nFor more information, see man bash.\nFailed Uploads # Symptom # A file upload, particularly a recursive one of many files, fails. You may see log messages containing the following in either pipeline logs, pachd logs, or from the pachctl command locally:\npachctl errror: an error occurred forwarding XXXXX -&gt; 650: error forwarding port 650 pachctl error: EOF pachd or worker: all SubConns are in TransientFailure, latest connection error: connection error: desc = \\&quot;transport: Error while dialing dial tcp 127.0.0.1:653: connect: connection refused\\&quot;; retrying in XXXX.XXXXXs&quot;} Recourse # Either pachd or your pipeline&rsquo;s worker sidecar may be getting OOM killed as it grows while getting data from object storage.\nYou can give the storage container more resources by increasing the cache_size parameter in your pipeline spec. Increase it to what you can afford; its default is 64M.(If you‚Äôre using a release prior to 1.10.0 and you have cluster-wide or namepace policies on resource limits, you may need to manually edit the pipeline RC.)\nIf it still gets OOM killed by k8s, there are a couple of environment variables you can set in your pachd deployment to limit the amount of memory the sidecar and pachd use.\nSTORAGE_UPLOAD_CONCURRENCY_LIMIT limits the parallelism to put files into the storage backend. Default is 100. STORAGE_PUT_FILE_CONCURRENCY_LIMIT limits the number of parallel downloads pachd will initiate. Default is also 100. You may use a binary search technique to hone in on a value appropriate for a production pipeline:\nfor cache_size, max it out. If it works, halve it. If its OOM killed, increase the value by 50%. and so on for the CONCURRENCY_LIMITS, halve and increase by 50% until you get a value that works.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "14f44a3437763b0ab1431ab7f617c737"
    },
    {
      "title": "Debug Pipelines",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Debug",
      "description": "Learn how to troubleshoot pipelines.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/debug/pipelines/",
      "relURI": "/latest/debug/pipelines/",
      "body": " Introduction # Job failures can occur for a variety of reasons, but they generally categorize into 4 failure types:\nYou hit one of the HPE ML Data Management Community Edition Scaling Limits. User-code-related: An error in the user code running inside the container or the json pipeline config. Data-related: A problem with the input data such as incorrect file type or file name. System- or infrastructure-related: An error in HPE ML Data Management or Kubernetes such as missing credentials, transient network errors, or resource constraints (for example, out-of-memory&ndash;OOM&ndash;killed). In this document, we&rsquo;ll show you the tools for determining what kind of failure it is. For each of the failure modes, we‚Äôll describe HPE ML Data Management‚Äôs and Kubernetes‚Äôs specific retry and error-reporting behaviors as well as typical user triaging methodologies.\nFailed jobs in a pipeline will propagate information to downstream pipelines with empty commits to preserve provenance and make tracing the failed job easier. A failed job is no longer running.\nIn this document, we&rsquo;ll describe what you&rsquo;ll see, how HPE ML Data Management will respond, and techniques for triaging each of those three categories of failure.\nAt the bottom of the document, we&rsquo;ll provide specific troubleshooting steps for specific scenarios.\nPipeline exists but never runs All your pods or jobs get evicted Determining the kind of failure # First off, you can see the status of HPE ML Data Management&rsquo;s jobs with pachctl list job --expand, which will show you the status of all jobs. For a failed job, use pachctl inspect job &lt;job-id&gt; to find out more about the failure. The different categories of failures are addressed below.\nCommunity Edition Scaling Limits # If you are running on the Community Edition, you might have hit the limit set on the number of pipelines and/or parallel workers.\nThat scenario is quite easy to troubleshoot:\nCheck your number of pipelines and parallelism settings (&quot;parallelism_spec&quot; attribute in your pipeline specification files) against our limits.\nAdditionally, your stderr and pipeline logs (pachctl log -p &lt;pipeline name&gt; --master or pachctl log -p &lt;pipeline name&gt; --worker) should contain one or both of those messages:\nnumber of pipelines limit exceeded: HPE ML Data Management Community Edition requires an activation key to create more than 16 total pipelines (you have X). Use the command pachctl license activate to enter your key.\nHPE ML Data Management offers readily available activation keys for proofs-of-concept, startups, academic, nonprofit, or open-source projects. Tell us about your project to get one.\nmax number of workers exceeded: This pipeline will only create a total of 8 workers (you specified X). HPE ML Data Management Community Edition requires an activation key to create pipelines with constant parallelism greater than 8. Use the command pachctl license activate to enter your key.\nHPE ML Data Management offers readily available activation keys for proofs-of-concept, startups, academic, nonprofit, or open-source projects. Tell us about your project to get one.\nTo lift those limitations, Request an Enterprise Edition trial token. Check out our Enterprise features for more details on our Enterprise Offer.\nUser Code Failures # When there‚Äôs an error in user code, the typical error message you‚Äôll see is\nfailed to process datum &lt;UUID&gt; with error: &lt;user code error&gt; This means HPE ML Data Management successfully got to the point where it was running user code, but that code exited with a non-zero error code. If any datum in a pipeline fails, the entire job will be marked as failed, but datums that did not fail will not need to be reprocessed on future jobs. You can use pachctl inspect datum &lt;job-id&gt; &lt;datum-id&gt; or pachctl logs with the --pipeline, --job or --datum flags to get more details.\nThere are some cases where users may want mark a datum as successful even for a non-zero error code by setting the transform.accept_return_code field in the pipeline config .\nRetries # HPE ML Data Management will automatically retry user code three (3) times before marking the datum as failed. This mitigates datums failing for transient connection reasons.\nTriage # pachctl logs --job=&lt;job_ID&gt; or pachctl logs --pipeline=&lt;pipeline_name&gt; will print out any logs from your user code to help you triage the issue. Kubernetes will rotate logs occasionally so if nothing is being returned, you‚Äôll need to make sure that you have a persistent log collection tool running in your cluster.\nIn cases where user code is failing, changes first need to be made to the code and followed by updating the HPE ML Data Management pipeline. This involves building a new docker container with the corrected code, modifying the HPE ML Data Management pipeline config to use the new image, and then calling pachctl update pipeline -f updated_pipeline_config.json. Depending on the issue/error, user may or may not want to also include the --reprocess flag with update pipeline.\nData Failures # When there‚Äôs an error in the data, this will typically manifest in a user code error such as\nfailed to process datum &lt;UUID&gt; with error: &lt;user code error&gt; This means HPE ML Data Management successfully got to the point where it was running user code, but that code exited with a non-zero error code, usually due to being unable to find a file or a path, a misformatted file, or incorrect fields/data within a file. If any datum in a pipeline fails, the entire job will be marked as failed. Datums that did not fail will not need to be reprocessed on future jobs.\nRetries # Just like with user code failures, HPE ML Data Management will automatically retry running a datum 3 times before marking the datum as failed. This mitigates datums failing for transient connection reasons.\nTriage # Data failures can be triaged in a few different way depending on the nature of the failure and design of the pipeline.\nIn some cases, where malformed datums are expected to happen occasionally, they can be ‚Äúswallowed‚Äù (e.g. marked as successful using transform.accept_return_codes or written out to a ‚Äúfailed_datums‚Äù directory and handled within user code). This would simply require the necessary updates to the user code and pipeline config as described above. For cases where your code detects bad input data, a &ldquo;dead letter queue&rdquo; design pattern may be needed. Many HPE ML Data Management developers use a special directory in each output repo for &ldquo;bad data&rdquo; and pipelines with globs for detecting bad data direct that data for automated and manual intervention.\nIf a few files as part of the input commit are causing the failure, they can simply be removed from the HEAD commit with start commit, delete file, finish commit. The files can also be corrected in this manner as well. This method is similar to a revert in Git &ndash; the ‚Äúbad‚Äù data will still live in the older commits in HPE ML Data Management, but will not be part of the HEAD commit and therefore not processed by the pipeline.\nSystem-level Failures # System-level failures are the most varied and often hardest to debug. We‚Äôll outline a few common patterns and triage steps. Generally, you‚Äôll need to look at deeper logs to find these errors using pachctl logs --pipeline=&lt;pipeline_name&gt; --raw and/or --master and kubectl logs pod &lt;pod_name&gt;.\nHere are some of the most common system-level failures:\nMalformed or missing credentials such that a pipeline cannot connect to object storage, registry, or other external service. In the best case, you‚Äôll see permission denied errors, but in some cases you‚Äôll only see ‚Äúdoes not exist‚Äù errors (this is common reading from object stores) Out-of-memory (OOM) killed or other resource constraint issues such as not being able to schedule pods on available cluster resources. Network issues trying to connect Pachd, etcd, or other internal or external resources Failure to find or pull a docker image from the registry Retries # For system-level failures, HPE ML Data Management or Kubernetes will generally continually retry the operation with exponential backoff. If a job is stuck in a given state (e.g. starting, merging) or a pod is in CrashLoopBackoff, those are common signs of a system-level failure mode.\nTriage # Triaging system failures varies as widely as the issues do themselves. Here are options for the common issues mentioned previously.\nCredentials: check your secrets in k8s, make sure they‚Äôre added correctly to the pipeline config, and double check your roles/perms within the cluster OOM: Increase the memory limit/request or node size for your pipeline. If you are very resource constrained, making your datums smaller to require less resources may be necessary. Network: Check to make sure etcd and pachd are up and running, that k8s DNS is correctly configured for pods to resolve each other and outside resources, firewalls and other networking configurations allow k8s components to reach each other, and ingress controllers are configured correctly Check your container image name in the pipeline config and image_pull_secret. Specific scenarios # All pods or jobs get evicted # Symptom # After creating a pipeline, a job starts but never progresses through any datums.\nRecourse # Run kubectl get pods and see if the command returns pods that are marked Evicted. If you run kubectl describe &lt;pod-name&gt; with one of those evicted pods, you might get an error saying that it was evicted due to disk pressure. This means that your nodes are not configured with a big enough root volume size. You need to make sure that each node&rsquo;s root volume is big enough to store the biggest datum you expect to process anywhere on your DAG plus the size of the output files that will be written for that datum.\nLet&rsquo;s say you have a repo with 100 folders. You have a single pipeline with this repo as an input, and the glob pattern is /*. That means each folder will be processed as a single datum. If the biggest folder is 50GB and your pipeline&rsquo;s output is about three times as big, then your root volume size needs to be bigger than:\n50 GB (to accommodate the input) + 50 GB x 3 (to accommodate the output) = 200GB In this case we would recommend 250GB to be safe. If your root volume size is less than 50GB (many defaults are 20GB), this pipeline will fail when downloading the input. The pod may get evicted and rescheduled to a different node, where the same thing will happen.\nPipeline exists but never runs # Symptom # You can see the pipeline via pachctl list pipeline, but if you look at the job via pachctl list job --expand, it&rsquo;s marked as running with 0/0 datums having been processed.\nIf you inspect the job via pachctl inspect job &lt;pipeline_name&gt;@&lt;jobID&gt;, you don&rsquo;t see any worker set.\nE.g:\nWorker Status: WORKER JOB DATUM STARTED ... If you do kubectl get pod you see the worker pod for your pipeline, e.g:\npo/pipeline-foo-5-v1-273zc But it&rsquo;s state is Pending or CrashLoopBackoff.\nRecourse # First make sure that there is no parent job still running. Do pachctl list job --expand| grep yourPipelineName to see if there are pending jobs on this pipeline that were kicked off prior to your job. A parent job is the job that corresponds to the parent output commit of this pipeline. A job will block until all parent jobs complete.\nIf there are no parent jobs that are still running, then continue debugging:\nDescribe the pod via:\nkubectl describe po/pipeline-foo-5-v1-273zc If the state is CrashLoopBackoff, you&rsquo;re looking for a descriptive error message. One such cause for this behavior might be if you specified an image for your pipeline that does not exist.\nIf the state is Pending it&rsquo;s likely the cluster doesn&rsquo;t have enough resources. In this case, you&rsquo;ll see a could not schedule type of error message which should describe which resource you&rsquo;re low on. This is more likely to happen if you&rsquo;ve set resource requests (cpu/mem/gpu) for your pipelines. In this case, you&rsquo;ll just need to scale up your resources. You can use your cloud provider&rsquo;s auto scaling groups to increase the size of your instance group. It can take up to 10 minutes for the changes to go into effect.\nCannot Delete Pipelines with an etcd Error # Failed to delete a pipeline with an etcdserver error.\nSymptom # Deleting pipelines fails with the following error:\npachctl delete pipeline pipeline-name etcdserver: too many operations in txn request (XXXXXX comparisons, YYYYYYY writes: hint: set --max-txn-ops on the ETCD cluster to at least the largest of those values) Recourse # When a HPE ML Data Management cluster reaches a certain scale, you need to adjust the default parameters provided for certain etcd flags. Depending on how you deployed HPE ML Data Management, you need to either edit the etcd Deployment or StatefulSet.\nkubectl edit deploy etcd or\nkubectl edit statefulset etcd In the spec/template/containers/command path, set the value for max-txn-ops to a value appropriate for your cluster, in line with the advice in the error above: larger than the greater of XXXXXX or YYYYYYY.\nPipeline is stuck in starting # Symptom # After starting a pipeline, running the pachctl list pipeline command returns the starting status for a very long time. The kubectl get pods command returns the pipeline pods in a pending state indefinitely.\nRecourse # Run the kubectl describe pod &lt;pipeline-pod&gt; and analyze the information in the output of that command. Often, this type of error is associated with insufficient amount of CPU, memory, and GPU resources in your cluster.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "pipelines"
      ],
      "id": "d56b1c64f8a394c690399b97ab01e7f3"
    },
    {
      "title": "Troubleshooting Deployments",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Debug",
      "description": "Learn how to troubleshoot deployments.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/debug/deployment/",
      "relURI": "/latest/debug/deployment/",
      "body": "A common issue related to a deployment: getting a CrashLoopBackoff error.\nPod stuck in CrashLoopBackoff # Symptoms # The pachd pod keeps crashing/restarting:\nkubectl get all NAME READY STATUS RESTARTS AGE po/etcd-281005231-qlkzw 1/1 Running 0 7m po/pachd-1333950811-0sm1p 0/1 CrashLoopBackOff 6 7m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/etcd 100.70.40.162 &lt;nodes&gt; 2379:30938/TCP 7m svc/kubernetes 100.64.0.1 &lt;none&gt; 443/TCP 9m svc/pachd 100.70.227.151 &lt;nodes&gt; 650:30650/TCP,651:30651/TCP 7m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/etcd 1 1 1 1 7m deploy/pachd 1 1 1 0 7m NAME DESIRED CURRENT READY AGE rs/etcd-281005231 1 1 1 7m rs/pachd-1333950811 1 1 0 7m Recourse # First describe the pod:\nkubectl describe po/pachd-1333950811-0sm1p If you see an error including Error attaching EBS volume or similar, see the recourse for that error here under the corresponding section below. If you don&rsquo;t see that error, but do see something like:\n1m 3s 9 {kubelet ip-172-20-48-123.us-west-2.compute.internal} Warning FailedSync Error syncing pod, skipping: failed to &#34;StartContainer&#34; for &#34;pachd&#34; with CrashLoopBackOff: &#34;Back-off 2m40s restarting failed container=pachd pod=pachd-1333950811-0sm1p_default(a92b6665-506a-11e7-8e07-02e3d74c49ac)&#34; it means Kubernetes tried running pachd, but pachd generated an internal error. To see the specifics of this internal error, check the logs for the pachd pod:\nkubectl logs po/pachd-1333950811-0sm1p ‚ÑπÔ∏è If you&rsquo;re using a log aggregator service (e.g. the default in GKE), you won&rsquo;t see any logs when using kubectl logs ... in this way. You will need to look at your logs UI (e.g. in GKE&rsquo;s case the stackdriver console).\nThese logs will most likely reveal the issue directly, or at the very least, a good indicator as to what&rsquo;s causing the problem. For example, you might see, BucketRegionError: incorrect region, the bucket is not in 'us-west-2' region. In that case, your object store bucket in a different region than your HPE ML Data Managementcluster and the fix would be to recreate the bucket in the same region as your pachydermm cluster.\nIf the error / recourse isn&rsquo;t obvious from the error message, post the error as well as the pachd logs in our Slack channel, or open a GitHub Issue and provide the necessary details prompted by the issue template. Please do be sure provide these logs either way as it is extremely helpful in resolving the issue.\nPod stuck in CrashLoopBackoff - with error attaching volume # Symptoms # A pod (could be the pachd pod or a worker pod) fails to startup, and is stuck in CrashLoopBackoff. If you execute kubectl describe po/pachd-xxxx, you&rsquo;ll see an error message like the following at the bottom of the output:\n30s 30s 1 {attachdetach } Warning FailedMount Failed to attach volume &#34;etcd-volume&#34; on node &#34;ip-172-20-44-17.us-west-2.compute.internal&#34; with: Error attaching EBS volume &#34;vol-0c1d403ac05096dfe&#34; to instance &#34;i-0a12e00c0f3fb047d&#34;: VolumeInUse: vol-0c1d403ac05096dfe is already attached to an instance This would indicate that the persistent volume claim is failing to get attached to the node in your kubernetes cluster.\nRecourse # Your best bet is to manually detach the volume and restart the pod.\nFor example, to resolve this issue when HPE ML Data Management is deployed to AWS, pull up your AWS web console and look up the node mentioned in the error message (ip-172-20-44-17.us-west-2.compute.internal in our case). Then on the bottom pane for the attached volume. Follow the link to the attached volume, and detach the volume. You may need to &ldquo;Force Detach&rdquo; it.\nOnce it&rsquo;s detached (and marked as available). Restart the pod by killing it, e.g:\nkubectl delete po/pachd-xxx It will take a moment for a new pod to get scheduled.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "deployment"
      ],
      "id": "38a3ba21eafc244203af33b910fdb653"
    },
    {
      "title": "View Audit Logs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Debug",
      "description": "View a list of user-entered commands using pachctl.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/debug/view-user-logs/",
      "relURI": "/latest/debug/view-user-logs/",
      "body": " How to View Audit Logs # Open a terminal. Input the following command, replacing user command with pachctl terms: pachctl logs | grep &#39;user command&#39; Review logs. pachctl logs | grep &#39;create project&#39; {&#34;log&#34;:&#34;{\\&#34;severity\\&#34;:\\&#34;info\\&#34;,\\&#34;time\\&#34;:\\&#34;2023-01-24T15:46:35.447335088Z\\&#34;,\\&#34;logger\\&#34;:\\&#34;grpc.admin_v2.API/InspectCluster\\&#34;,\\&#34;caller\\&#34;:\\&#34;logging/interceptor.go:529\\&#34;,\\&#34;message\\&#34;:\\&#34;request for admin_v2.API/InspectCluster\\&#34;,\\&#34;service\\&#34;:\\&#34;admin_v2.API\\&#34;,\\&#34;method\\&#34;:\\&#34;InspectCluster\\&#34;,\\&#34;x-request-id\\&#34;:[\\&#34;3384fceb-68a7-4003-92ad-3bc8102d9e72\\&#34;],\\&#34;command\\&#34;:[\\&#34;pachctl create project dogs\\&#34;],\\&#34;peer\\&#34;:\\&#34;10.1.5.2:50784\\&#34;,\\&#34;request\\&#34;:\\&#34;client_version:\\u003cmajor:2 minor:5 additional:\\\\\\&#34;-alpha.4\\\\\\&#34; git_commit:\\\\\\&#34;1a252e4f760513c820353d47227009472213713a\\\\\\&#34; git_tree_modified:\\\\\\&#34;true\\\\\\&#34; build_date:\\\\\\&#34;2023-01-19T22:43:41Z\\\\\\&#34; go_version:\\\\\\&#34;go1.19.5\\\\\\&#34; platform:\\\\\\&#34;arm64\\\\\\&#34; \\u003e \\&#34;}\\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:35.447552838Z&#34;} {&#34;log&#34;:&#34;{\\&#34;severity\\&#34;:\\&#34;info\\&#34;,\\&#34;time\\&#34;:\\&#34;2023-01-24T15:46:35.447439005Z\\&#34;,\\&#34;logger\\&#34;:\\&#34;grpc.admin_v2.API/InspectCluster\\&#34;,\\&#34;caller\\&#34;:\\&#34;logging/interceptor.go:529\\&#34;,\\&#34;message\\&#34;:\\&#34;response for admin_v2.API/InspectCluster\\&#34;,\\&#34;service\\&#34;:\\&#34;admin_v2.API\\&#34;,\\&#34;method\\&#34;:\\&#34;InspectCluster\\&#34;,\\&#34;x-request-id\\&#34;:[\\&#34;3384fceb-68a7-4003-92ad-3bc8102d9e72\\&#34;],\\&#34;command\\&#34;:[\\&#34;pachctl create project dogs\\&#34;],\\&#34;peer\\&#34;:\\&#34;10.1.5.2:50784\\&#34;,\\&#34;response\\&#34;:\\&#34;id:\\\\\\&#34;9d417960076d4f63969254424a25a651\\\\\\&#34; deployment_id:\\\\\\&#34;LImXqySxvSBudquUR0vlohA1NeHnsTrr\\\\\\&#34; version_warnings_ok:true \\&#34;,\\&#34;messagesSent\\&#34;:1,\\&#34;messagesReceived\\&#34;:1,\\&#34;grpc.code\\&#34;:0,\\&#34;duration\\&#34;:0.000194958}\\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:35.448185588Z&#34;} {&#34;log&#34;:&#34;{\\&#34;severity\\&#34;:\\&#34;info\\&#34;,\\&#34;time\\&#34;:\\&#34;2023-01-24T15:46:35.452357963Z\\&#34;,\\&#34;logger\\&#34;:\\&#34;grpc.pfs_v2.API/CreateProject\\&#34;,\\&#34;caller\\&#34;:\\&#34;logging/interceptor.go:529\\&#34;,\\&#34;message\\&#34;:\\&#34;request for pfs_v2.API/CreateProject\\&#34;,\\&#34;service\\&#34;:\\&#34;pfs_v2.API\\&#34;,\\&#34;method\\&#34;:\\&#34;CreateProject\\&#34;,\\&#34;x-request-id\\&#34;:[\\&#34;b1fd3d34-7fd1-4ca1-bcb3-6130b3ece8e8\\&#34;],\\&#34;command\\&#34;:[\\&#34;pachctl create project dogs\\&#34;],\\&#34;peer\\&#34;:\\&#34;10.1.5.2:50784\\&#34;,\\&#34;request\\&#34;:\\&#34;project:\\u003cname:\\\\\\&#34;dogs\\\\\\&#34; \\u003e \\&#34;}\\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:35.452537213Z&#34;} {&#34;log&#34;:&#34;{\\&#34;severity\\&#34;:\\&#34;info\\&#34;,\\&#34;time\\&#34;:\\&#34;2023-01-24T15:46:35.465953046Z\\&#34;,\\&#34;logger\\&#34;:\\&#34;grpc.pfs_v2.API/CreateProject\\&#34;,\\&#34;caller\\&#34;:\\&#34;logging/interceptor.go:529\\&#34;,\\&#34;message\\&#34;:\\&#34;response for pfs_v2.API/CreateProject\\&#34;,\\&#34;service\\&#34;:\\&#34;pfs_v2.API\\&#34;,\\&#34;method\\&#34;:\\&#34;CreateProject\\&#34;,\\&#34;x-request-id\\&#34;:[\\&#34;b1fd3d34-7fd1-4ca1-bcb3-6130b3ece8e8\\&#34;],\\&#34;command\\&#34;:[\\&#34;pachctl create project dogs\\&#34;],\\&#34;peer\\&#34;:\\&#34;10.1.5.2:50784\\&#34;,\\&#34;messagesSent\\&#34;:1,\\&#34;messagesReceived\\&#34;:1,\\&#34;grpc.code\\&#34;:0,\\&#34;duration\\&#34;:0.013663667}\\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:35.46607713Z&#34;} {&#34;log&#34;:&#34;{\\&#34;severity\\&#34;:\\&#34;info\\&#34;,\\&#34;time\\&#34;:\\&#34;2023-01-24T15:46:39.028400256Z\\&#34;,\\&#34;logger\\&#34;:\\&#34;grpc.admin_v2.API/InspectCluster\\&#34;,\\&#34;caller\\&#34;:\\&#34;logging/interceptor.go:529\\&#34;,\\&#34;message\\&#34;:\\&#34;request for admin_v2.API/InspectCluster\\&#34;,\\&#34;service\\&#34;:\\&#34;admin_v2.API\\&#34;,\\&#34;method\\&#34;:\\&#34;InspectCluster\\&#34;,\\&#34;x-request-id\\&#34;:[\\&#34;714c04e7-d180-4182-9230-3ec27e554d97\\&#34;],\\&#34;command\\&#34;:[\\&#34;pachctl create project cats\\&#34;],\\&#34;peer\\&#34;:\\&#34;10.1.5.2:60084\\&#34;,\\&#34;request\\&#34;:\\&#34;client_version:\\u003cmajor:2 minor:5 additional:\\\\\\&#34;-alpha.4\\\\\\&#34; git_commit:\\\\\\&#34;1a252e4f760513c820353d47227009472213713a\\\\\\&#34; git_tree_modified:\\\\\\&#34;true\\\\\\&#34; build_date:\\\\\\&#34;2023-01-19T22:43:41Z\\\\\\&#34; go_version:\\\\\\&#34;go1.19.5\\\\\\&#34; platform:\\\\\\&#34;arm64\\\\\\&#34; \\u003e \\&#34;}\\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:39.028511715Z&#34;} {&#34;log&#34;:&#34;{\\&#34;severity\\&#34;:\\&#34;info\\&#34;,\\&#34;time\\&#34;:\\&#34;2023-01-24T15:46:39.028438673Z\\&#34;,\\&#34;logger\\&#34;:\\&#34;grpc.admin_v2.API/InspectCluster\\&#34;,\\&#34;caller\\&#34;:\\&#34;logging/interceptor.go:529\\&#34;,\\&#34;message\\&#34;:\\&#34;response for admin_v2.API/InspectCluster\\&#34;,\\&#34;service\\&#34;:\\&#34;admin_v2.API\\&#34;,\\&#34;method\\&#34;:\\&#34;InspectCluster\\&#34;,\\&#34;x-request-id\\&#34;:[\\&#34;714c04e7-d180-4182-9230-3ec27e554d97\\&#34;],\\&#34;command\\&#34;:[\\&#34;pachctl create project cats\\&#34;],\\&#34;peer\\&#34;:\\&#34;10.1.5.2:60084\\&#34;,\\&#34;response\\&#34;:\\&#34;id:\\\\\\&#34;9d417960076d4f63969254424a25a651\\\\\\&#34; deployment_id:\\\\\\&#34;LImXqySxvSBudquUR0vlohA1NeHnsTrr\\\\\\&#34; version_warnings_ok:true \\&#34;,\\&#34;messagesSent\\&#34;:1,\\&#34;messagesReceived\\&#34;:1,\\&#34;grpc.code\\&#34;:0,\\&#34;duration\\&#34;:0.000077375}\\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:39.028556673Z&#34;} {&#34;log&#34;:&#34;{\\&#34;severity\\&#34;:\\&#34;info\\&#34;,\\&#34;time\\&#34;:\\&#34;2023-01-24T15:46:39.030396298Z\\&#34;,\\&#34;logger\\&#34;:\\&#34;grpc.pfs_v2.API/CreateProject\\&#34;,\\&#34;caller\\&#34;:\\&#34;logging/interceptor.go:529\\&#34;,\\&#34;message\\&#34;:\\&#34;request for pfs_v2.API/CreateProject\\&#34;,\\&#34;service\\&#34;:\\&#34;pfs_v2.API\\&#34;,\\&#34;method\\&#34;:\\&#34;CreateProject\\&#34;,\\&#34;x-request-id\\&#34;:[\\&#34;a0beac90-f6ad-4c49-8a39-188076e3d03d\\&#34;],\\&#34;command\\&#34;:[\\&#34;pachctl create project cats\\&#34;],\\&#34;peer\\&#34;:\\&#34;10.1.5.2:60084\\&#34;,\\&#34;request\\&#34;:\\&#34;project:\\u003cname:\\\\\\&#34;cats\\\\\\&#34; \\u003e \\&#34;}\\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:39.030474673Z&#34;} {&#34;log&#34;:&#34;{\\&#34;severity\\&#34;:\\&#34;info\\&#34;,\\&#34;time\\&#34;:\\&#34;2023-01-24T15:46:39.031706548Z\\&#34;,\\&#34;logger\\&#34;:\\&#34;grpc.pfs_v2.API/CreateProject\\&#34;,\\&#34;caller\\&#34;:\\&#34;logging/interceptor.go:529\\&#34;,\\&#34;message\\&#34;:\\&#34;response for pfs_v2.API/CreateProject\\&#34;,\\&#34;service\\&#34;:\\&#34;pfs_v2.API\\&#34;,\\&#34;method\\&#34;:\\&#34;CreateProject\\&#34;,\\&#34;x-request-id\\&#34;:[\\&#34;a0beac90-f6ad-4c49-8a39-188076e3d03d\\&#34;],\\&#34;command\\&#34;:[\\&#34;pachctl create project cats\\&#34;],\\&#34;peer\\&#34;:\\&#34;10.1.5.2:60084\\&#34;,\\&#34;messagesSent\\&#34;:1,\\&#34;messagesReceived\\&#34;:1,\\&#34;grpc.code\\&#34;:0,\\&#34;duration\\&#34;:0.001327458}\\n&#34;,&#34;stream&#34;:&#34;stderr&#34;,&#34;time&#34;:&#34;2023-01-24T15:46:39.031799756Z&#34;} Useful Search Terms # Command delete branch delete commit delete file delete job delete pipeline delete repo delete secret delete transaction edit pipeline ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "a12db5668a64ea44030999d400a134dd"
    },
    {
      "title": "View Kubernetes Logs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Debug",
      "description": "View the Kubernetes Log using PachCTL.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/debug/view-k8s-events/",
      "relURI": "/latest/debug/view-k8s-events/",
      "body": "The kube-event-tail pod in your HPE ML Data Management cluster stores Kubernetes logs which are discarded after a certain amount of time. You can view these logs to obtain insights on key events. There are three event types: informational, warning, and error.\nHow to View Kubernetes Logs # Open a terminal. Input the following command, replacing user command with pachctl terms: pachctl kube-events Review logs. LAST SEEN TYPE REASON OBJECT MESSAGE 20 minutes ago ScalingReplicaSet Deployment/pachd Scaled up replica set pachd-84f599bccd to 1 19 minutes ago ScalingReplicaSet Deployment/pachd Scaled down replica set pachd-65fc687687 to 0 from 1 21 minutes ago SandboxChanged Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Container image &#34;pachyderm/kube-event-tail:v0.0.7&#34; already present on machine 21 minutes ago Created Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Created container kube-event-tail 21 minutes ago Started Pod/pachyderm-kube-event-tail-84bdc9977d-zkch9 Started container kube-event-tail 21 minutes ago SandboxChanged Pod/pachyderm-loki-0 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-loki-0 Container image &#34;grafana/loki:2.6.1&#34; already present on machine 21 minutes ago Created Pod/pachyderm-loki-0 Created container loki 21 minutes ago Started Pod/pachyderm-loki-0 Started container loki 20 minutes ago Warning Unhealthy Pod/pachyderm-loki-0 Readiness probe failed: HTTP probe failed with statuscode: 503 20 minutes ago Warning Unhealthy Pod/pachyderm-loki-0 Liveness probe failed: HTTP probe failed with statuscode: 503 21 minutes ago SandboxChanged Pod/pachyderm-promtail-b8plv Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-promtail-b8plv Container image &#34;docker.io/grafana/promtail:2.6.1&#34; already present on machine 21 minutes ago Created Pod/pachyderm-promtail-b8plv Created container promtail 21 minutes ago Started Pod/pachyderm-promtail-b8plv Started container promtail 21 minutes ago SandboxChanged Pod/pachyderm-proxy-7d757c85bb-zp5ht Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pachyderm-proxy-7d757c85bb-zp5ht Container image &#34;envoyproxy/envoy-distroless:v1.24.1&#34; already present on machine 21 minutes ago Created Pod/pachyderm-proxy-7d757c85bb-zp5ht Created container envoy 21 minutes ago Started Pod/pachyderm-proxy-7d757c85bb-zp5ht Started container envoy 21 minutes ago Warning Unhealthy Pod/pachyderm-proxy-7d757c85bb-zp5ht Readiness probe failed: HTTP probe failed with statuscode: 503 21 minutes ago SandboxChanged Pod/pg-bouncer-746bb45867-hgd57 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pg-bouncer-746bb45867-hgd57 Container image &#34;pachyderm/pgbouncer:1.16.2&#34; already present on machine 21 minutes ago Created Pod/pg-bouncer-746bb45867-hgd57 Created container pg-bouncer 21 minutes ago Started Pod/pg-bouncer-746bb45867-hgd57 Started container pg-bouncer 20 minutes ago Warning Unhealthy Pod/pg-bouncer-746bb45867-hgd57 Liveness probe failed: psql: error: FATAL: pgbouncer cannot connect to server 21 minutes ago SandboxChanged Pod/pipeline-joins-inner-join-v1-pfrnh Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pipeline-joins-inner-join-v1-pfrnh Container image &#34;pachyderm/worker:2.5.0-alpha.4&#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-inner-join-v1-pfrnh Created container init 21 minutes ago Started Pod/pipeline-joins-inner-join-v1-pfrnh Started container init 21 minutes ago Pulled Pod/pipeline-joins-inner-join-v1-pfrnh Container image &#34;pachyderm/example-joins-inner-outer:2.1.0&#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-inner-join-v1-pfrnh Created container user 21 minutes ago Started Pod/pipeline-joins-inner-join-v1-pfrnh Started container user 20 minutes ago Pulled Pod/pipeline-joins-inner-join-v1-pfrnh Container image &#34;pachyderm/pachd:2.5.0-alpha.4&#34; already present on machine 20 minutes ago Created Pod/pipeline-joins-inner-join-v1-pfrnh Created container storage 20 minutes ago Started Pod/pipeline-joins-inner-join-v1-pfrnh Started container storage 20 minutes ago Warning BackOff Pod/pipeline-joins-inner-join-v1-pfrnh Back-off restarting failed container 19 minutes ago Killing Pod/pipeline-joins-inner-join-v1-pfrnh Stopping container user 19 minutes ago Killing Pod/pipeline-joins-inner-join-v1-pfrnh Stopping container storage 21 minutes ago SandboxChanged Pod/pipeline-joins-reduce-inner-v1-jjx6w Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/pipeline-joins-reduce-inner-v1-jjx6w Container image &#34;pachyderm/worker:2.5.0-alpha.4&#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-reduce-inner-v1-jjx6w Created container init 21 minutes ago Started Pod/pipeline-joins-reduce-inner-v1-jjx6w Started container init 21 minutes ago Pulled Pod/pipeline-joins-reduce-inner-v1-jjx6w Container image &#34;ubuntu:20.04&#34; already present on machine 21 minutes ago Created Pod/pipeline-joins-reduce-inner-v1-jjx6w Created container user 21 minutes ago Started Pod/pipeline-joins-reduce-inner-v1-jjx6w Started container user 20 minutes ago Pulled Pod/pipeline-joins-reduce-inner-v1-jjx6w Container image &#34;pachyderm/pachd:2.5.0-alpha.4&#34; already present on machine 20 minutes ago Created Pod/pipeline-joins-reduce-inner-v1-jjx6w Created container storage LAST SEEN TYPE REASON OBJECT MESSAGE 20 minutes ago Started Pod/pipeline-joins-reduce-inner-v1-jjx6w Started container storage 20 minutes ago Warning BackOff Pod/pipeline-joins-reduce-inner-v1-jjx6w Back-off restarting failed container 19 minutes ago Killing Pod/pipeline-joins-reduce-inner-v1-jjx6w Stopping container user 19 minutes ago Killing Pod/pipeline-joins-reduce-inner-v1-jjx6w Stopping container storage 21 minutes ago SandboxChanged Pod/postgres-0 Pod sandbox changed, it will be killed and re-created. 21 minutes ago Pulled Pod/postgres-0 Container image &#34;docker.io/pachyderm/postgresql:13.3.0&#34; already present on machine 21 minutes ago Created Pod/postgres-0 Created container postgres 21 minutes ago Started Pod/postgres-0 Started container postgres Key Events # Event Description CrashLoopBackOff A container in a pod keeps crashing and restarting. This typically indicates that there is an issue with the container that needs to be resolved. FailedScheduling The Kubernetes scheduler is unable to schedule a pod on any node. This typically indicates that there are insufficient resources available in the cluster or that there are constraints set that prevent the pod from being scheduled. OutOfMemory A container in a pod ran out of memory. This typically indicates that the container needs to be reconfigured with more memory or that there is an issue with the application running in the container that is causing it to consume too much memory. FailedCreatePodSandBox The Kubernetes API server failed to create a sandbox for a pod. This typically indicates that there is an issue with the node or the network that is preventing the creation of the sandbox. Evicted A pod is evicted from a node due to resource constraints or other reasons. This typically indicates that the node is running low on resources or that there is an issue with the pod that needs to be resolved. NodeNotReady A node is not ready to accept pods. This can occur due to various reasons such as network connectivity issues or insufficient resources. ImagePullBackOff An image pull operation fails. The container runtime is unable to pull the image from the specified registry or repository. FailedMount A mount operation failed, such as when a volume or configMap failed to mount. This can occur due to incorrect configurations, insufficient permissions, or a missing dependency. ‚ÑπÔ∏è These events are just a few of the many events that can occur in Kubernetes. It&rsquo;s important to monitor your cluster for these and other events to ensure the health and stability of your applications.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "e2b1f31194883cd1f3a0a51ce379c4c5"
    },
    {
      "title": "Pachyderm SDK",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "The officially supported library that provides autogenerated gRPC/protobuf code based on betterproto.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/sdk/",
      "relURI": "/latest/sdk/",
      "body": "",
      "beta": "<no value>",
      "hidden": "false",
      "categories": [],
      "tags": [],
      "id": "755494214335a1a6b02e6fc63e635dab"
    },
    {
      "title": "Client Initialization (Start Here)",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pachyderm SDK",
      "description": "Learn how to install the Pachyderm SDK, import a client, and initialize it with your configuration settings.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/sdk/install/",
      "relURI": "/latest/sdk/install/",
      "body": "The Pachyderm SDK enables you to interact with HPE ML Data Management&rsquo;s API, client, and configuration directly in a powerful way.\n1. Installation # Before using the Pachyderm SDK, make sure you have it installed. You can install the SDK using pip:\npip install pachyderm_sdk 2. Import the Client # To use the Client class, you need to import it from the Pachyderm SDK:\nfrom pachyderm_sdk import Client 3. Creating a Client Instance # To interact with a Pachyderm cluster, you need to create an instance of the Client class. The Client class provides multiple ways to create a client instance based on your requirements.\nDefault Settings # client = Client() This creates a client that connects to the local Pachyderm cluster running on localhost:30650 with default authentication settings.\nCustom Settings # You can customize the client settings by providing the relevant parameters to the Client constructor. Here&rsquo;s an example:\nclient = Client( host=&#39;localhost&#39;, port=8080, auth_token=&#39;your-auth-token&#39;, root_certs=None, transaction_id=None, tls=False ) In the above example, the client is configured to connect to the local Pachyderm cluster running on localhost:8080 without TLS encryption.\nThe auth_token parameter allows you to specify an authentication token for accessing the cluster. The root_certs parameter can be used to provide custom root certificates for secure connections. The transaction_id parameter allows you to specify a transaction ID to run operations on. üí° By default, the client will attempt to read the authentication token from the AUTH_TOKEN_ENV environment variable. You can also set the authentication token after creating the client using the auth_token property:\n4. Connect # The Client class provides different methods to connect to a Pachyderm cluster based on your deployment configuration.\nFrom Within a Cluster # If you&rsquo;re running the code within a Pachyderm cluster, you can use the new_in_cluster method to create a client instance that operates within the cluster. This method reads the cluster configuration from the environment and creates a client based on the available configuration.\nclient = Client.new_in_cluster(auth_token=&#39;your-auth-token&#39;, transaction_id=&#39;your-transaction-id&#39;) Via PachD Address # If you have the Pachd address (host:port) of the HPE ML Data Management cluster, you can create a client instance using the from_pachd_address method:\nclient = Client.from_pachd_address(&#39;pachd-address&#39;, auth_token=&#39;your-auth-token&#39;, root_certs=&#39;your-root-certs&#39;, transaction_id=&#39;your-transaction-id&#39;) Referencing a Config File # If you have a HPE ML Data Management configuration file, you can create a client instance using the from_config method:\nclient = Client.from_config(&#39;path-to-config-file&#39;) Test Connection # If you&rsquo;d like to quickly test out working with the Pachyderm SDK on your local machine (e.g., using a locally deployed Docker Desktop instance), try out the following:\nfrom pachyderm_sdk import Client client = Client(host=&#34;localhost&#34;, port=&#34;80&#34;) version = client.get_version() print(version) Example Output\nVersion(major=2, minor=6, micro=4, git_commit=&#39;358bd1229130eb262c22caf82ed87b3cc91ec81c&#39;, git_tree_modified=&#39;false&#39;, build_date=&#39;2023-06-22T14:49:32Z&#39;, go_version=&#39;go1.20.5&#39;, platform=&#39;arm64&#39;) If you see this, you are ready to start working with the SDK.\n",
      "beta": "<no value>",
      "hidden": "false",
      "categories": [],
      "tags": [],
      "id": "10602dcd6419ac46464a5837dabafc9b"
    },
    {
      "title": "Method Mapping",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pachyderm SDK",
      "description": "Use this quick reference guide for mapping methods between Python Pachyderm and the new Pachyderm SDK",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/sdk/mapping/",
      "relURI": "/latest/sdk/mapping/",
      "body": " About # The original python_pachyderm SDK will be deprecated in a future release (spring 2024). The new pachyderm_sdk SDK is a complete rewrite of the original SDK, and it is not backwards compatible. This guide provides a quick reference for mapping methods between the two SDKs.\nMethods # python_pachyderm pachyderm_sdk client.inspect_cluster client.admin.inspect_cluster client.activate_auth client.auth.activate client.authenticate_id_token client.auth.authenticate client.authenticate_oidc client.auth.authenticate client.authorize client.auth.authorize client.deactivate_auth client.auth.deactivate client.extract_auth_tokens client.auth.extract_auth_tokens client.get_auth_configuration client.auth.get_configuration client.get_groups client.auth.get_groups client.get_oidc_login client.auth.get_oidc_login client.get_robot_token client.auth.get_robot_token client.get_role_binding client.auth.get_role_binding client.get_roles_for_permission client.auth.get_roles_for_permission client.get_users client.auth.get_users client.modify_members client.auth.modify_members client.modify_role_binding client.auth.modify_role_binding client.restore_auth_token client.auth.restore_auth_token client.revoke_auth_token client.auth.revoke_auth_token client.set_auth_configuration client.auth.set_configuration client.set_groups_for_user client.auth.set_groups_for_user client.who_am_i client.auth.who_am_i client.binary client.debug.binary client.dump client.debug.dump client.profile_cpu client.debug.profile (w/ profile=Profile(name=&ldquo;cpu&rdquo;)) client.activate_enterprise client.enterprise.activate client.deactivate_enterprise client.enterprise.deactivate client.get_activation_code client.enterprise.get_activation_code client.get_enterprise_state client.enterprise.get_state client.get_pause_status client.enterprise.pause_status client.pause_enterprise client.enterprise.pause client.unpause_enterprise client.enterprise.unpause client.create_idp_connector client.identity.create_idp_connector client.create_oidc_client client.identity.create_oidc_client client.delete_all_identity client.identity.delete_all client.delete_idp_connector client.identity.delete_idp_connector client.delete_oidc_client client.identity.delete_oidc_client client.get_identity_server_config client.identity.get_identity_server_config client.get_idp_connector client.identity.get_idp_connector client.get_oidc_client client.identity.get_oidc_client client.list_idp_connectors client.identity.list_idp_connectors client.list_oidc_clients client.identity.list_oidc_clients client.set_identity_server_config client.identity.set_identity_server_config client.update_idp_connector client.identity.update_idp_connector client.update_oidc_client client.identity.update_oidc_client client.activate_license client.license.activate client.add_cluster client.license.add_cluster client.delete_all_license client.license.delete_all client.delete_cluster client.license.delete_cluster client.get_activation_code client.license.get_activation_code client.list_clusters client.license.list_clusters client.list_user_clusters client.license.list_user_clusters client.update_cluster client.license.update_cluster client.commit client.pfs.commit client.copy_file client.pfs.copy_file client.create_branch client.pfs.create_branch client.create_project client.pfs.create_project client.create_repo client.pfs.create_repo client.delete_all_repos client.pfs.delete_all client.delete_branch client.pfs.delete_branch client.delete_file client.pfs.delete_file client.delete_project client.pfs.delete_project client.delete_repo client.pfs.delete_repo client.diff_file client.pfs.diff_file client.drop_commit client.pfs.drop_commit client.finish_commit client.pfs.finish_commit client.fsck client.pfs.fsck client.get_file client.pfs.get_file or client.pfs.pfs_file client.get_file_tar client.pfs.get_file_tar or client.pfs.pfs_tar_file client.glob_file client.pfs.glob_file client.inspect_branch client.pfs.inspect_branch client.inspect_commit client.pfs.inspect_commit client.inspect_file client.pfs.inspect_file client.inspect_project client.pfs.inspect_project client.inspect_repo client.pfs.inspect_repo client.list_branch client.pfs.list_branch client.list_commit client.pfs.list_commit client.list_file client.pfs.list_file client.list_project client.pfs.list_project client.list_repo client.pfs.list_repo client.modify_file_client &ndash; client.path_exists client.pfs.path_exists client.put_file_bytes client.pfs.put_file_from_bytes client.put_file_url client.pfs.put_file_from_url client.squash_commit client.pfs.squash_commit client.start_commit client.pfs.start_commit client.subscribe_commit client.pfs.subscribe_commit client.wait_commit client.pfs.wait_commit client.walk_file client.pfs.walk_file client.create_pipeline client.pps.create_pipeline client.create_pipeline_from_request &ndash; client.create_secret client.pps.create_secret client.delete_all_pipelines client.pps.delete_all client.delete_job client.pps.delete_job client.delete_pipeline client.pps.delete_pipeline client.delete_secret client.pps.delete_secret client.get_job_logs client.pps.get_logs client.get_kube_events client.pps.get_kube_events client.get_pipeline_logs client.pps.get_logs client.inspect_datum client.pps.inspect_datum client.inspect_job client.pps.inspect_job client.inspect_pipeline client.pps.inspect_pipeline client.inspect_secret client.pps.inspect_secret client.list_datum client.pps.list_datum client.list_job client.pps.list_job client.list_pipeline client.pps.list_pipeline client.list_secret client.pps.list_secret client.restart_datum client.pps.restart_datum client.run_cron client.pps.run_cron client.start_pipeline client.pps.start_pipeline client.stop_job client.pps.stop_job client.stop_pipeline client.pps.stop_pipeline client.batch_transaction client.transaction.batch_transaction client.delete_all_transactions client.transaction.delete_all client.delete_transaction client.transaction.delete_transaction client.finish_transaction client.transaction.finish_transaction client.inspect_transaction client.transaction.inspect_transaction client.list_transaction client.transaction.list_transaction client.start_transaction client.transaction.start_transaction client.transaction client.transaction.transaction ",
      "beta": "<no value>",
      "hidden": "false",
      "categories": [],
      "tags": [
        "python",
        "sdk",
        "methods"
      ],
      "id": "2efbac53007a1b0f06cbc7c973fe4fed"
    },
    {
      "title": "First Project",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pachyderm SDK",
      "description": "Learn how to create your first project, including repos and branches, using the SDK.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/sdk/starter-project/",
      "relURI": "/latest/sdk/starter-project/",
      "body": "This tutorial is based on the Standard ML Pipeline Tutorial found in the Build Pipelines &amp; DAGs section of the documentation. The end result is a simple machine learning pipeline that trains a regression model on housing market data to predict the value of homes in Boston.\n1. Import the dependencies # from pachyderm_sdk import Client from pachyderm_sdk import pfs, pps The imports you&rsquo;ll need for future projects depend on the setup of your instance and the operations you want to perform; see the API submodules reference documentation for more information.\n2. Initiate the Client # client = Client(host=&#34;localhost&#34;, port=&#34;80&#34;) version = client.get_version() print(&#34;Pachyderm Version:&#34;, version) See the Client reference documentation for more information on how to configure and initiate the client in other ways, such as from a PachD Address and from a Config File.\n3. Create a Project, Repo, &amp; Branch # project = pfs.Project(name=&#34;sdk-standard-pipeline&#34;) repo = pfs.Repo(name=&#34;housing_data&#34;, project=project) branch = pfs.Branch.from_uri(f&#34;{repo}@main&#34;) try: client.pfs.create_project(project=project, description=&#39;standard ml pipeline via sdk&#39;) client.pfs.create_repo(repo=repo, description=&#34;my first sdk-created repo&#34;) print(&#34;Project and Repo creation successful.&#34;) except Exception as e: print(&#34;Error creating project or repo:&#34;, e) exit(1) You&rsquo;ll first need to create a class instance for each resource you want to create. In this case, you&rsquo;ll create a project, repo, and branch. You&rsquo;ll then use the create_project and create_repo methods to create the resources in your cluster.\n4. Commit Files # try: with client.pfs.commit(branch=branch) as commit: with open(&#34;../build-dags/tutorials/basic-ml/housing-simplified-1.csv&#34;, &#34;rb&#34;) as source: commit.put_file_from_file(path=&#34;/housing-simplified-1.csv&#34;, file=source) print(&#34;Data loaded into the repo as a commit.&#34;) except Exception as e: print(&#34;Error loading data into the repo:&#34;, e) exit(1) You&rsquo;ll use the commit class to create a commit in your repo. While the commit is open, you can access the OpenCommit subclass to perform file operations (put, copy, delete). In this example, the put_from_file method is used.\nAlternative Commit Methods # The following are just examples with mock filenames.\nCommit As Raw Data # with client.pfs.commit(branch=branch) as commit: file = commit.put_file_from_bytes(path=f&#34;/directory-a/filename-a.md&#34;, data=b&#34;## raw data here \\n this is a **markdown** sentence.&#34;) file = commit.put_file_from_bytes(path=f&#34;/directory-b/filename-b.md&#34;, data=b&#34;## raw data here \\n this is a **markdown** sentence.&#34;) See the OpenCommit&rsquo;s put_file_from_bytes method for more details.\nCommit From URL # with client.pfs.commit(branch=branch) as commit: file = commit.put_file_from_url(path=&#34;/directory-c/locations.csv&#34;, url=&#34;https://edg.epa.gov/EPADataCommons/public/OA/EPA_SmartLocationDatabase_V3_Jan_2021_Final.csv&#34;) See the OpenCommit&rsquo;s put_file_from_url method for more details.\n5. Create a Pipeline # try: input = pps.Input(pfs=pps.PfsInput(project=project.name, branch=&#34;main&#34;, repo=repo.name, glob=&#34;/*&#34;)) transform = pps.Transform( image=&#34;lbliii/housing-prices:latest&#34;, cmd=[&#34;python&#34;, &#34;regression.py&#34;, &#34;--input&#34;, &#34;/pfs/housing_data/&#34;, &#34;--target-col&#34;, &#34;MEDV&#34;, &#34;--output&#34;, &#34;/pfs/out/&#34;], datum_batching=True) pipeline = pps.Pipeline(name=&#34;pipeline-001&#34;, project=project) client.pps.create_pipeline(pipeline=pipeline, input=input, transform=transform) print(&#34;Pipeline created successfully.&#34;) except Exception as e: print(&#34;Error creating the pipeline:&#34;, e) exit(1) Up until this point, you have been working with the pfs (Pachyderm File System) submodule. Now you&rsquo;ll use the pps (Pachyderm Pipeline System) submodule to create a pipeline.\nA basic pipeline requires at least an input, transform, and pipeline class instance. Once those classes have been defined, you can finally create the pipeline by using the create_pipeline method.\nAt this point, you can check the Console UI or use the pachctl list pipelines command to see the pipeline running. To add this functionality to your script, you can use the list_pipeline method.\n",
      "beta": "<no value>",
      "hidden": "false",
      "categories": [],
      "tags": [],
      "id": "d24b2d314a3b1c79a7639a4f16a37260"
    },
    {
      "title": "Examples",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Pachyderm SDK",
      "description": "",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/sdk/examples/",
      "relURI": "/latest/sdk/examples/",
      "body": "",
      "beta": "<no value>",
      "hidden": "false",
      "categories": [],
      "tags": [],
      "id": "dc235d138272723e4ef06388c5d830a1"
    },
    {
      "title": "Breast Cancer Detection",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Examples",
      "description": "Learn how to use Pachyderm and the SDK to set up multiple pipelines that iterate on one another to detect breast cancer from pipeline specs",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/sdk/examples/breast_cancer_detection/",
      "relURI": "/latest/sdk/examples/breast_cancer_detection/",
      "body": " üí° You can download this repo at pachyderm/docs-content and navigate to latest/sdk/examples/breast_cancer_detection to execute the following steps.\nThis is a reproduction of the example found in Pachyderm&rsquo;s examples repository. Rather than using pachctl, the repos and pipelines are created using their Python SDK analogues.\nPrerequisites:\nA running Pachyderm cluster Install the latest package of pachyderm-sdk To run: From within the latest/sdk/examples/breast_cancer_detection directory:\n$ python breast_cancer_detection.py ",
      "beta": "<no value>",
      "hidden": "false",
      "categories": [],
      "tags": [],
      "id": "5ae74104b4e8bed6bb5bcd432bb6ec14"
    },
    {
      "title": "Distributed Image Processing",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Examples",
      "description": "Learn how to use Pachyderm and the SDK to process images in parallel.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/sdk/examples/opencv/",
      "relURI": "/latest/sdk/examples/opencv/",
      "body": " üí° You can download this repo at pachyderm/docs-content and navigate to latest/sdk/examples/opencv to execute the following steps.\nThis is a reproduction of Pachyderm&rsquo;s OpenCV example in Python. This example showcases the pachyderm-sdk analogs of common pachctl commands, such as creating repos and pipelines or getting a file.\nThe image being used to run the pipeline code is built from the Dockerfile located at pachyderm/examples/opencv. You will also find the edges.py script there.\nBefore You Start # You must have a running HPE ML Data Management cluster You must have installed the latest package of pachyderm-sdk How to Create a Distributed Image Processing Pipeline # Create a file named opencv.py with the following code:\nimport shutil import tempfile from pachyderm_sdk import Client from pachyderm_sdk.api import pfs, pps def main(client: Client): # Create a repo called images images = pfs.Repo.from_uri(&#34;images&#34;) client.pfs.create_repo(repo=images) # Create the edges pipeline (and the edges repo automatically). This # pipeline runs when data is committed to the images repo, as indicated # by the input field. edges = pps.Pipeline(name=&#34;edges&#34;) client.pps.create_pipeline( pipeline=edges, transform=pps.Transform( cmd=[&#34;python3&#34;, &#34;/edges.py&#34;], image=&#34;pachyderm/opencv&#34;, ), input=pps.Input(pfs=pps.PfsInput(repo=images.name, glob=&#34;/*&#34;)), ) # Create the montage pipeline (and the montage repo automatically). This # pipeline runs when data is committed to either the images repo or edges # repo, as indicated by the input field. client.pps.create_pipeline( pipeline=pps.Pipeline(name=&#34;montage&#34;), transform=pps.Transform( cmd=[&#34;sh&#34;], image=&#34;v4tech/imagemagick&#34;, stdin=[ &#34;montage -shadow -background SkyBlue -geometry 300x300+2+2 $(find /pfs ! -name .env -type f | sort) /pfs/out/montage.png&#34; ], ), input=pps.Input( cross=[ pps.Input(pfs=pps.PfsInput(glob=&#34;/&#34;, repo=images.name)), pps.Input(pfs=pps.PfsInput(glob=&#34;/&#34;, repo=edges.name)), ] ), ) with client.pfs.commit(branch=pfs.Branch.from_uri(&#34;images@master&#34;)) as commit: # Add some images from urls. # Alternatively, you could use `client.put_file_from_file` or # `client_put_file_bytes`. client.pfs.put_file_from_url(commit=commit, path=&#34;/liberty.jpg&#34;, url=&#34;https://docs.pachyderm.com/images/opencv/liberty.jpg&#34;) client.pfs.put_file_from_url(commit=commit, path=&#34;/kitten.jpg&#34;, url=&#34;https://docs.pachyderm.com/images/opencv/kitten.jpg&#34;) client.pfs.put_file_from_url(commit=commit, path=&#34;/robot.jpg&#34;, url=&#34;https://docs.pachyderm.com/images/opencv/robot.jpg&#34;) # Wait for the commit (and its downstream commits) to finish commit.wait_all() job = pps.Job(pipeline=pps.Pipeline(name=&#34;montage&#34;), id=commit.id) if client.pps.inspect_job(job=job).state != pps.JobState.JOB_SUCCESS: print(&#34;Montage job failed, aborting. Check the pipeline logs for more details:&#34;) print(&#34;pachctl logs --pipeline=montage&#34;) exit(1) # Get the montage source_file = client.pfs.pfs_file(file=pfs.File.from_uri(&#34;montage@master:/montage.png&#34;)) with tempfile.NamedTemporaryFile(suffix=&#34;montage.png&#34;, delete=False) as dest_file: shutil.copyfileobj(source_file, dest_file) print(&#34;montage written to {}&#34;.format(dest_file.name)) def clean(client: Client): client.pps.delete_pipeline(pipeline=pps.Pipeline(name=&#34;montage&#34;)) client.pps.delete_pipeline(pipeline=pps.Pipeline(name=&#34;edges&#34;)) client.pfs.delete_repo(repo=pfs.Repo.from_uri(&#34;images&#34;), force=True) if __name__ == &#34;__main__&#34;: # Connects to a pachyderm cluster using the pachctl config file located # at ~/.pachyderm/config.json. For other setups, you&#39;ll want one of the # alternatives: # 1) To connect to pachyderm when this script is running inside the # cluster, use `Client.new_in_cluster()`. # 2) To connect to pachyderm via a pachd address, use # `Client.new_from_pachd_address`. # 3) To explicitly set the host and port, pass parameters into # `Client()`. # 4) To use a config file located elsewhere, pass in the path to that # config file to Client.from_config() client = Client.from_config() clean(client) main(client) Run the script:\n$ python opencv.py ",
      "beta": "<no value>",
      "hidden": "false",
      "categories": [],
      "tags": [],
      "id": "062bb1319813e36d5087285fb5fb5a6e"
    },
    {
      "title": "Spout Pipelines",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Examples",
      "description": "Learn how to create pipelines using the pachyderm-sdk analogs for creating pipelines.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/sdk/examples/spout/",
      "relURI": "/latest/sdk/examples/spout/",
      "body": "This example is a reproduction of the Spouts101 example from the Pachyderm repo. This example uses the pachyderm-sdk analogs for creating pipelines (spout.py), which was done using pachctl commands in the Spouts101 example. For more information on Spouts, a full walkthrough of the original example, or the pipelines&rsquo; user code, go here.\nBefore You Start # You must have a running HPE ML Data Management cluster You must have installed the latest package of pachyderm-sdk How to Create a Spout Pipeline # üí° You can download this repo at pachyderm/docs-content and navigate to latest/sdk/examples/spout to execute the following steps.\nSave the following code as spout.py:\nfrom pachyderm_sdk import Client from pachyderm_sdk.api import pps def main(client: Client): spout = pps.Pipeline(name=&#34;spout&#34;) client.pps.create_pipeline( pipeline=spout, transform=pps.Transform( cmd=[&#34;python&#34;, &#34;consumer/main.py&#34;], image=&#34;pachyderm/example-spout101:2.0.1&#34;, ), spout=pps.Spout(), description=&#34;A spout pipeline that emulates the reception of data from an external source&#34;, ) processor = pps.Pipeline(name=&#34;processor&#34;) client.pps.create_pipeline( pipeline=processor, transform=pps.Transform( cmd=[&#34;python&#34;, &#34;processor/main.py&#34;], image=&#34;pachyderm/example-spout101:2.0.1&#34;, ), input=pps.Input( pfs=pps.PfsInput(repo=&#34;spout&#34;, branch=&#34;master&#34;, glob=&#34;/*&#34;), ), description=&#34;A pipeline that sorts 1KB vs 2KB files&#34;, ) reducer = pps.Pipeline(name=&#34;reducer&#34;) client.pps.create_pipeline( pipeline=reducer, transform=pps.Transform( cmd=[&#34;bash&#34;], stdin=[ &#34;set -x&#34;, &#34;FILES=/pfs/processor/*/*&#34;, &#34;for f in $FILES&#34;, &#34;do&#34;, &#34;directory=`dirname $f`&#34;, &#34;out=`basename $directory`&#34;, &#34;cat $f &gt;&gt; /pfs/out/${out}.txt&#34;, &#34;done&#34;, ], ), input=pps.Input( pfs=pps.PfsInput(repo=&#34;processor&#34;, branch=&#34;master&#34;, glob=&#34;/*&#34;), ), description=&#34;A pipeline that reduces 1K/ and 2K/ directories&#34;, ) if __name__ == &#34;__main__&#34;: # Connects to a pachyderm cluster using the pachctl config file located # at ~/.pachyderm/config.json. For other setups, you&#39;ll want one of the # alternatives: # 1) To connect to pachyderm when this script is running inside the # cluster, use `Client.new_in_cluster()`. # 2) To connect to pachyderm via a pachd address, use # `Client.new_from_pachd_address`. # 3) To explicitly set the host and port, pass parameters into # `Client()`. # 4) To use a config file located elsewhere, pass in the path to that # config file to Client.from_config() client = Client.from_config() main(client) Run the script:\npython spout.py ",
      "beta": "<no value>",
      "hidden": "false",
      "categories": [],
      "tags": [],
      "id": "ebbdba3598f29a799295ad143f228467"
    },
    {
      "title": "Reference Docs",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Pachyderm SDK",
      "description": "The official python-based Pachyderm SDK for Pachyderm.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/sdk/python/",
      "relURI": "/latest/sdk/python/",
      "body": "",
      "beta": "<no value>",
      "hidden": "false",
      "categories": [],
      "tags": [],
      "id": "8317fae1c45f018ea94b6ff0e92f8536"
    },
    {
      "title": "Contribute",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "Contribute to our open source code and documentation.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/contribute/",
      "relURI": "/latest/contribute/",
      "body": "",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "5e89982b3aa7d4fd9ab59c8e92ddba6e"
    },
    {
      "title": "Coding Conventions",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Contribute",
      "description": "Learn the coding conventions contributors to the code base follow.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/contribute/coding-conventions/",
      "relURI": "/latest/contribute/coding-conventions/",
      "body": "Interested in contributing to HPE ML Data Management&rsquo;s code? Learn the conventions here! For setup instructions, see Setup for Contributors.\nLanguages # The HPE ML Data Management repository is written using Go, Shell, and Make. Exceptions to this are:\n/examples: For showcasing how to use the product in various languages. /doc: For building documentation using a python-based static site generator (MkDocs). Shell # See the Shell Style Guide for standard conventions. Add set -eou pipefail to your scripts. Go # See the Effective Go Style Guide for standard conventions.\nNaming # Consider the package name when naming an interface to avoid redundancy. For example, storage.Interface is better than storage.StorageInterface. Do not use uppercase characters, underscores, or dashes in package names. The package foo line should match the name of the directory in which the .go file exists. Importers can use a different name if they need to disambiguate. When multiple locks are present, give each lock a distinct name following Go conventions (e.g., stateLock, mapLock). Go Modules/Third-Party Code # See the Go Modules Usage and Troubleshooting Guide for managing Go modules. Go dependencies are managed with go modules. Use go get foo to add or update a package; for more specific versions, use go get foo@v1.2.3, go get foo@master, or go get foo@e3702bed2. YAML # See the Helm Best Practices guide series. Review # See the Go Code Review Comments guide for a list of common comments. See the Go Test Comments guide for a list of common test code comments. Make sure CI is passing for your branch. Checks # Run checks using make lint. Testing # All packages and significant functionality must come with test coverage. Local unit tests should pass before pushing to GitHub (make localtest or make integration-tests for integrations). Use short flag for local tests only. Avoid waiting for asynchronous things to happen; If possible, use a method of waiting directly (e.g. &lsquo;flush commit&rsquo; is much better than repeatedly trying to read from a commit). Run single tests or tests from a single package; the Go tool only supports tests that match a regular expression (for example, go test -v ./src/path/to/package -run ^TestMyTest). Documentation # When writing documentation, follow the Style Guide conventions. PRs that have only documentation changes, such as typos, is a great place to start and we welcome your help! ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "developers"
      ],
      "id": "35b72ec736104fa2fa64a947d179925d"
    },
    {
      "title": "Contributor Setup",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Contribute",
      "description": "Learn how to set up your machine to contribute code.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/contribute/setup/",
      "relURI": "/latest/contribute/setup/",
      "body": " General requirements # First, go through the general Local Installation Instructions. Additionally, make sure you have the following installed:\ngolang 1.12+ docker jq pv shellcheck Bash helpers # To stay up to date, we recommend doing the following.\nFirst clone the code: (Note, as of 07/11/19 pachyderm is using go modules and recommends cloning the code outside of the $GOPATH, we use the location ~/workspace as an example, but the code can live anywhere)\ncd ~/workspace git clone git@github.com:pachyderm/pachyderm Then update your ~/.bash_profile by adding the line:\nsource ~/workspace/pachyderm/etc/contributing/bash_helpers And you&rsquo;ll stay up to date!\nSpecial macOS configuration # File descriptor limit # If you&rsquo;re running tests locally, you&rsquo;ll need to up your file descriptor limit. To do this, first setup a LaunchDaemon to up the limit with sudo privileges:\nsudo cp ~/workspace/pachyderm/etc/contributing/com.apple.launchd.limit.plist /Library/LaunchDaemons/ Once you restart, this will take effect. To see the limits, run:\nlaunchctl limit maxfiles Before the change is in place you&rsquo;ll see something like 256 unlimited. After the change you&rsquo;ll see a much bigger number in the first field. This ups the system wide limit, but you&rsquo;ll also need to set a per-process limit.\nSecond, up the per process limit by adding something like this to your ~/.bash_profile :\nulimit -n 12288 Unfortunately, even after setting that limit it never seems to report the updated version. So if you try\nulimit And just see unlimited, don&rsquo;t worry, it took effect.\nTo make sure all of these settings are working, you can test that you have the proper setup by running:\nmake test-pfs-server If this fails with a timeout, you&rsquo;ll probably also see &rsquo;too many files&rsquo; type of errors. If that test passes, you&rsquo;re all good!\nTimeout helper # You&rsquo;ll need the timeout utility to run the make launch task. To install on mac, do:\nbrew install coreutils And then make sure to prepend the following to your path:\nPATH=&#34;/usr/local/opt/coreutils/libexec/gnubin:$PATH&#34; Dev cluster # Now launch the dev cluster: make launch-dev-vm.\nAnd check it&rsquo;s status: kubectl get all.\npachctl # This will install the dev version of pachctl:\ncd ~/workspace/pachyderm make install pachctl version And make sure that $GOPATH/bin is on your $PATH somewhere\nGetting some images in place for local test runs # The following commands will put some images that some of the tests rely on in place in your minikube cluster:\nFor pachyderm_entrypoint container:\nmake docker-build-test-entrypoint ./etc/kube/push-to-minikube.sh pachyderm_entrypoint For pachyderm/python-build container:\n(cd etc/pipeline-build; make push-to-minikube) Running tests # Now that we have a dev cluster, it&rsquo;s nice to be able to run some tests locally as we are developing.\nTo run some specific tests, just use go test directly, e.g:\ngo test -v ./src/server/cmd/pachctl/cmd We don&rsquo;t recommend trying to run all the tests locally, they take a while. Use CI for that.\nFully resetting # Instead of running the makefile targets to re-compile pachctl and redeploy a dev cluster, we have a script that you can use to fully reset your pachyderm environment:\nAll existing cluster data is deleted If possible, the virtual machine that the cluster is running on is wiped out pachctl is recompiled The dev cluster is re-deployed This reset is a bit more time consuming than running one-off Makefile targets, but comprehensively ensures that the cluster is in its expected state, and is especially helpful when you&rsquo;re first getting started with contributions and don&rsquo;t yet have a complete intuition on the various ways a cluster may get in an unexpected state. It&rsquo;s been tested on docker for mac and minikube, but likely works in other kubernetes environments as well.\nTo run it, simply call ./etc/reset.py from the pachyderm repo root.\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "developers"
      ],
      "id": "8ece4f0bc09928027781c3582d8c721b"
    },
    {
      "title": "Developing on Windows with VSCode",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Contribute",
      "description": "Learn how to set up your Windows machine to contribute via VS code.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/contribute/windows/",
      "relURI": "/latest/contribute/windows/",
      "body": " Before You Start # Installation Requirements # You must have all of the following installed before you can start development:\nDocker Desktop Go v1.15.x+ GoReleaser Git HyperV jq kubectl Make minikube ShellCheck VSCode Terminal Settings # Open VS Code. Open your terminal (ctrl+`). Add the following to your settings.json: &#34;terminal.integrated.shell.windows&#34;: &#34;C:\\\\Program Files\\\\Git\\\\bin\\\\bash.exe&#34;, This path may vary depending on where your git bash actually exists.\nGetting started # Open a terminal and navigate to a directory you&rsquo;d like to store HPE ML Data Management. Clone the pachyderm repo using git clone https://github.com/pachyderm/pachyderm. Launch Docker Desktop (with Kubernetes enabled) or start minikube. Provision ~10 GB of memory and ~4CPUs. Via minikube: minikube start --memory=10000mb --cpus=4 --disk-size=40000mb --driver=hyperv Via Docker Desktop: Open Docker Desktop and navigate to Preferences &gt; Resources &gt; Advanced. Build your pachyderm pachd and worker images via the task docker-build. Option 1: Navigate to Terminal &gt; Run Task&hellip; Option 2: Press ctrl+p and input task docker-build Build and install pachctl. Launch a HPE ML Data Management cluster by running the task launch-dev. If the service does not come up promptly (the script never says all the pods are ready), see the Debugging section.\nDebugging # Common Commands # The following commands are used frequently when working with HPE ML Data Management:\nkubectl get all: lists resources in the &lsquo;default&rsquo; namespace, where we deploy locally. kubectl logs -p &lt;pod&gt;: gets the logs from the previous attempt at running a pod; a good place to find errors. minikube logs: gets the logs from minikube itself, useful when a pod runs into a CreateContainerError. docker container ls: lists recently used or in-use docker containers; used to get logs more directly. docker logs &lt;container&gt;: gets the logs from a specific docker container. Gotchas # Docker can get confused by command-line windows-style paths; it reads : as a mode and fails to parse. You may want to export MSYS_NO_PATHCONV=1 to prevent the automated conversion of unix-to-windows paths. Kubernetes resource specs (specifically hostPath) do not work if you use a windows-style path. Instead, you must use a unix-style path where the drive letter is the first directory, e.g. /C/path/to/file. Etcd may fail to mmap files when in a directory shared with the host system. Full Restart # Minikube # If you&rsquo;d like to completely restart, use the following terminal commands:\nminikube delete kubectl delete pvc -l suite=pachyderm minikube start --memory=10000mb --cpus=4 --disk-size=40000mb ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "developers",
        "windows"
      ],
      "id": "bf36ef8333665fe141c410ade65ccbc5"
    },
    {
      "title": "Documentation Style Guide",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Contribute",
      "description": "Learn how to contribute content to the documentation.",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/contribute/docs-style-guide/",
      "relURI": "/latest/contribute/docs-style-guide/",
      "body": "Thank you for taking an interest in contributing to HPE ML Data Management&rsquo;s docs! üêò üìñ\nThis style guide provides editorial guidelines for writing clear and consistent HPE ML Data Management product documentation. See our contribution guide for instructions on how to draft and submit changes.\nAudience # HPE ML Data Management has two main audiences:\nMLOPs Engineers: They install and configure HPE ML Data Management to transform data using pipelines. Data Scientists &amp; Data/ML Engineers: They plan the development of pipelines and consume the outputs of HPE ML Data Management&rsquo;s data processing to feed AI/ML models. Be sure to provide links to pre-requisite or contextual materials whenever possible, as everyone&rsquo;s experience level and career journey is different.\nVoice &amp; Tone # HPE ML Data Management&rsquo;s voice in documentation should consistently convey a personality that is friendly, knowledgeable, and empathetic.\nThe tone of voice may vary depending on the type of content being written. For example, a danger notice may use an urgent and serious tone while a tutorial may use an energetic and instructive tone. Make sure the tone of your documentation aligns with the content. If you aren&rsquo;t sure what tone to convey, ask yourself: &ldquo;What is the reader likely feeling when presented this content? Why are they here?&rdquo; and adjust your language to the most appropriate tone.\nLanguage &amp; Grammar # The following guidelines are to be followed loosely; use your best judgment when approaching content &ndash; there are always exceptions.\nUse Active Voice # Write in active voice to enforce clarity and simplicity.\nüëéüö´ Don&rsquo;t üëç‚úÖ Do The update was failing due to a configuration issue. The update failed due to a configuration issue. A POST request is sent and a response is returned. Send a POST request; the endpoint sends a response. You can break this rule to emphasize an object (the image is installed) or de-emphasize a subject (5 errors were found in this article).\nUse Global English # Write documentation using Global English. Global English makes comprehension easier for all audiences by avoiding regional idioms/expressions and standardizing spelling words using the US English variant.\nPut Conditional Clauses First # Order conditional clauses first when drafting sentences; this empowers the reader to skip to the next step when the condition does not apply.\nüëéüö´ Don&rsquo;t üëç‚úÖ Do See this page for more information on how to use this feature. For more information, see How to Use Console. Enable the CustomField setting if you want to map custom fields. To map custom fields, enable the CustomField setting. Write Accessibly # Be mindful of how you describe software behavior and users; in particular, avoid ableist language. Use generic &ldquo;they/them&rdquo; and &ldquo;you&rdquo; when describing users or actors; avoid the use of &ldquo;obviously&rdquo;, &ldquo;simply&rdquo;, &ldquo;easily&rdquo; &ndash; every reader has a different level of expertise and familiarity with key concepts/tools.\nüëéüö´ Don&rsquo;t üëç‚úÖ Do To start, simply enter the following command: To start, enter the following command: Configuring this setting just requires a simple API call. Make an API call to configure this setting. The results, without this setting enabled, might look crazy. Your results may be inconsistent or unreliable without this setting enabled. Formatting &amp; Punctuation # Markdown # All documentation is written using Markdown syntax in .md files. See this official Markdown Cheat Sheet for a quick introduction to the syntax.\nCode Blocks # Use ``` to wrap long code samples into a code block.\nThis is a code block. Headers # Use title casing for all header titles.\nCapitalize the first and last word. Capitalize adjectives, adverbs, nouns, pronouns, and subordinate conjuctions. Lowercase articles (a, an, the) and coordinating conjunctions (and, but,for, nor, or, so, yet). üëéüö´ Don&rsquo;t üëç‚úÖ Do How to develop sentient ai How to Develop Sentient AI How to use the pachctl cli How to Use the Pachctl CLI Links # Use meaningful link descriptions, such as the original article&rsquo;s title or a one-line summarization of its contents.\nExamples:\n- See the [HPE ML Data Management Technical Documentation Style Guide](../docs-style-guide) - Use the [official HPE ML Data Management style guide](../docs-style-guide). Lists # Use numbered lists for sequential items, such as instructions. Use unbulleted lists for all other list types (like this list). Commas # Use the serial or Oxford comma in a list of three or more items to minimize any chance of confusion.\nüëéüö´ Don&rsquo;t üëç‚úÖ Do I like swimming, biking and singing. I like swimming, biking, and singing. I only trust my parents, Madonna and Shakira. I only trust my parents, Madonna, and Shakira. UI Elements # Bold UI elements when mentioned in a set of instructions (a numbered list).\nNavigate to Settings &gt; Desktop. Scroll to Push Notifications. Organization # Use Nested Headers # Remember to use all header sizes (h1-h6) to organize information. Each header should be a subset of topics or examples more narrow in scope than its parents. This enables readers to both gain more context and mentally parse the information at a glance.\nPublish Modular Topics # Avoid mixing objectives or use cases in one article; instead, organize and separate your content so that it is task-based. If there are many substasks (such as in a long-form tutorial), organize your content so that each major step is itself an article.\nExamples:\nThe below outlines are 4 articles, with the parent article linking to each modular sub-topic.\nHow to Locally Deploy HPE ML Data Management MacOS Local Deployment Guide Linux Local Deployment Guide Windows Local Deployment Guide Images &amp; Diagrams # Visualizations are helpful in learning complex workflows and UIs; however, they can expire quickly and take a lot of effort to maintain. Ask yourself the following questions when deciding whether or not to add visualizations:\nIs the user interface complex enough to warrant a screenshot? Can a diagram convey this concept more efficiently than words? How frequently does this visual need to be updated? Also, remember to add alt-text to your visualizations for screen readers.\nMermaid # You can build diagrams using mermaid.js in our documentation by:\nAdding mermaid: true to the frontmatter of an article. Drafting a diagram, like so: ```mermaid graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; ``` graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; Want to make an update to this style guide? Select Edit on Github and leave a suggestion as a pull request!\n",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [
        "developers"
      ],
      "id": "8bd55067daa32c4ab34857641369091d"
    },
    {
      "title": "Shared",
      "version": "latest",
      "isLatest": "",
      "pageKind": "section",
      "parent": "Latest",
      "description": "",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/shared/",
      "relURI": "/latest/shared/",
      "body": "",
      "beta": "<no value>",
      "hidden": "true",
      "categories": [],
      "tags": [],
      "id": "47bd70d8dddba41bf41003de92bb469d"
    },
    {
      "title": "Log in",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Shared",
      "description": "",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/shared/auth/log-in/",
      "relURI": "/latest/shared/auth/log-in/",
      "body": " How to Log in to a Cluster via IdP # Open a terminal. Input the following command: pachctl auth login Select the connector you wish to use. Provide your credentials ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "c56f4b221f1a070f887407ebde497d52"
    },
    {
      "title": "Docker",
      "version": "latest",
      "isLatest": "",
      "pageKind": "page",
      "parent": "Shared",
      "description": "",
      "date": "January 1, 1",
      "uri": "https://mldm.pachyderm.com/latest/shared/setup/docker/",
      "relURI": "/latest/shared/setup/docker/",
      "body": " Before You Start # Operating System: macOS Windows Linux You must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; You must have WSL enabled (wsl --install) and a Linux distribution installed; if Linux does not boot in your WSL terminal after downloading from the Microsoft store, see the manual installation guide. Manual Step Summary:\nOpen a Powershell terminal. Run each of the following: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart Download the latest WSL2 Linux Kernel for x64 machines. Run each of the following: wsl --set-default-version 2 wsl --install -d Ubuntu Restart your machine. Start a WSL terminal and set up your first Ubuntu user. Update Ubuntu. sudo apt update sudo apt upgrade -y Install Homebrew in Ubuntu so you can complete the rest of this guide: /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; All installation steps after 1. Install Docker Desktop must be run through the WSL terminal (Ubuntu) and not in Powershell.\nYou are now ready to continue to Step 1.\nYou must have Homebrew installed. /bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; 1. Install Docker Desktop # Install Docker Desktop for your machine. Navigate to Settings for Mac, Windows, or Linux. Adjust your resources (~4 CPUs and ~12GB Memory) Enable Kubernetes Select Apply &amp; Restart. 2. Install Pachctl CLI # Operating System: MacOs, Windows, &amp; Darwin Debian brew tap pachyderm/tap &amp;&amp; brew install pachyderm/tap/pachctl@2.7 curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v2.7.3/pachctl_2.7.3_amd64.deb &amp;&amp; sudo dpkg -i /tmp/pachctl.deb 3. Install &amp; Configure Helm # Install Helm: brew install helm Add the Pachyderm repo to Helm: helm repo add pachyderm https://helm.pachyderm.com helm repo update Install Pachyderm: Version: Community Edition Enterprise helm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer Are you using an Enterprise trial key? If so, you can set up Enterprise Pachyderm locally by storing your trial key in a license.txt file and passing it into the following Helm command:\nhelm install pachyderm pachyderm/pachyderm --set deployTarget=LOCAL --set proxy.enabled=true --set proxy.service.type=LoadBalancer --set pachd.enterpriseLicenseKey=$(cat license.txt) --set ingress.host=localhost A mock user is created by default to get you started, with the username: admin and password: password.\nThis may take several minutes to complete.\n4. Verify Installation # In a new terminal, run the following command to check the status of your pods: kubectl get pods NAME READY STATUS RESTARTS AGE pod/console-5b67678df6-s4d8c 1/1 Running 0 2m8s pod/etcd-0 1/1 Running 0 2m8s pod/pachd-c5848b5c7-zwb8p 1/1 Running 0 2m8s pod/pg-bouncer-7b855cb797-jqqpx 1/1 Running 0 2m8s pod/postgres-0 1/1 Running 0 2m8s Re-run this command after a few minutes if pachd is not ready. 5. Connect to Cluster # pachctl connect http://localhost:80 ‚ÑπÔ∏è If the connection commands did not work together, run each separately.\nOptionally open your browser and navigate to the Console UI.\nüí° You can check your Pachyderm version and connection to pachd at any time with the following command:\npachctl version COMPONENT VERSION pachctl 2.7.3 pachd 2.7.3 ",
      "beta": "<no value>",
      "hidden": "<no value>",
      "categories": [],
      "tags": [],
      "id": "187a9cb23d07d4bc6db6423274bd7f79"
    }
  ]