{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b72df6-4961-4128-9281-6c9634dd33fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating an Inference Service using MLFlow and KServe\n",
    "\n",
    "Welcome to part two of the tutorial on building a question-answering application over a private document corpus with\n",
    "Large Language Models (LLMs). In the previous Notebook, you embedded the documents into a high-dimensional latent\n",
    "space using a fine-tuned BGE-M3 model and saved these embeddings in a Vector Store using the Chroma database interface\n",
    "from LangChain.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/inference-service.jpg\" alt=\"isvc\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "    Photo by <a href=\"https://unsplash.com/@growtika?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Growtika</a> on <a href=\"https://unsplash.com/photos/GSiEeoHcNTQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "In this Notebook, you delve deeper. You use MLflow to log the Chroma DB files as experiment artifacts. Once logged, you\n",
    "set up an Inference Service (ISVC) that fetches these artifacts and leverages them to provide context to user\n",
    "inquiries. For this task, you work with KServe, a Kubernetes-centric platform that offers a serverless blueprint for\n",
    "scaling Machine Learning (ML) models seamlessly.\n",
    "\n",
    "A crucial point to remember: KServe doesn't support Chroma DB files natively. Because of this, you integrate a custom\n",
    "predictor component. This involves creating a Docker image, which then serves as your ISVC endpoint. This approach\n",
    "grants you a high level of customization, ensuring the service fits your requirements. You can find the necessary code\n",
    "and the Dockerfile for this custom predictor in the `dockerfiles/vectorstore` directory. But for a quicker setup,\n",
    "there's a pre-built option available: `dpoulopoulos/qna-vectorstore-mlde:v0.1.0`.\n",
    "\n",
    "Lastly, you must also deploy the embeddings model. You can accomplish this using KServe with the Triton Inference Service\n",
    "backend. Triton requires the `model-repository` directory to be organized in a specific manner, which we will discuss in\n",
    "more detail later.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Logging the Vector Store as an Artifact](#logging-the-vector-store-as-an-artifact)\n",
    "1. [Creating and Submitting the Inference Service](#creating-and-submitting-the-inference-service)\n",
    "1. [Conclusion and Next Steps](#conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import getpass\n",
    "import requests\n",
    "import subprocess\n",
    "import mlflow\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656fd38-0660-402b-bd83-72e566cd4e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_base64(message: str):\n",
    "    encoded_bytes = base64.b64encode(message.encode('ASCII'))\n",
    "    return encoded_bytes.decode('ASCII')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8b07a-15d8-44a2-89c4-266ee61d1a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logging the Vector Store as an Artifact\n",
    "\n",
    "To begin, you create a new experiment or use an existing one and log the Chroma DB files as an artifact of this\n",
    "experiment. Ultimately, you retrieve the URI that points to this artifact's location and provide it to the custom\n",
    "predictor component. By doing this, the custom predictor component knows how to fetch the artifact and serve it\n",
    "effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27118610-6f87-4bae-ac1d-efbf0b24b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>MLflow Credentials</h2>\")\n",
    "display(heading)\n",
    "\n",
    "domain_input = widgets.Text(description='Username:', placeholder=\"i001ua.tryezmeral.com\")\n",
    "username_input = widgets.Text(description='Username:')\n",
    "password_input = widgets.Password(description='Password:')\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "success_message = widgets.Output()\n",
    "\n",
    "domain = None\n",
    "mlflow_username = None\n",
    "mlflow_password = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global domain, mlflow_username, mlflow_password\n",
    "    domain = domain_input.value\n",
    "    mlflow_username = username_input.value\n",
    "    mlflow_password = password_input.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        print(\"Credentials submitted successfully!\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(domain_input, username_input, password_input, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03cb65-26ac-4ec0-b88b-18bc3aee9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_url = f\"https://keycloak.{domain}/realms/UA/protocol/openid-connect/token\"\n",
    "\n",
    "data = {\n",
    "    \"username\" : mlflow_username,\n",
    "    \"password\" : mlflow_password,\n",
    "    \"grant_type\" : \"password\",\n",
    "    \"client_id\" : \"ua-grant\",\n",
    "}\n",
    "\n",
    "token_responce = requests.post(token_url, data=data, allow_redirects=True, verify=False)\n",
    "\n",
    "token = token_responce.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a05cb-1e9a-47c8-bf85-f169a0ccff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_TOKEN'] = token\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.environ['MLFLOW_TRACKING_TOKEN']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"s3\"\n",
    "os.environ[\"AWS_ENDPOINT_URL\"] = 'http://local-s3-service.ezdata-system.svc.cluster.local:30000'\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = os.environ[\"AWS_ENDPOINT_URL\"]\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"true\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://mlflow.mlflow.svc.cluster.local:5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23d780-57e0-4d5c-a699-aa3046a5b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(exp_name):\n",
    "    \"\"\"Register an experiment in MLFlow.\n",
    "    \n",
    "    args:\n",
    "      exp_name (str): The name of the experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mlflow.set_experiment(exp_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to set the experiment: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f97e2-4c39-4464-b3e8-c778b96d28da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new MLFlow experiment or re-use an existing one\n",
    "get_or_create_experiment('mlde')\n",
    "\n",
    "# Log the Chroma DB files as an artifact of the experiment\n",
    "mlflow.log_artifact(f\"{os.getcwd()}/db\")\n",
    "\n",
    "# Retrieve the URI of the artifact\n",
    "uri = mlflow.get_artifact_uri(\"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6da2b-a621-4837-b202-9a6f854d3999",
   "metadata": {},
   "source": [
    "# Creating the Triton Model Repository\n",
    "\n",
    "Next, you will deploy the embeddings model using KServe alongside the Triton Inference Server backend. For this purpose, organizing the `model-repository` directory is essential, as Triton relies on it to load and deploy your models.\n",
    "\n",
    "The fundamental layout of this directory is pre-arranged for you. You can explore its contents by delving into the `model-repository` directory. Beyond that, your task is simply to relocate the model to a designated spot and replicate the entire `model-repository` directory onto the shared Persistent Volume Claim (PVC). The pod running the Inference Service will then mount this PVC and automatically retrieve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7648a-30a8-4ec9-90aa-c0186fbd5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv bge-m3 model-repository/bge/1/bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5e099-45ca-4ee9-9a99-be6ed5832d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r model-repository/ /mnt/shared/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534dcf6e-58ca-4905-a600-a2ca98e4d32a",
   "metadata": {},
   "source": [
    "Finally, you are ready to define the Inference Service CR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c75e5-8be7-41bf-b2b4-96391cec61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgpt_isvc = \"\"\"\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"bge\"\n",
    "spec:\n",
    "  predictor:\n",
    "    timeout: 600\n",
    "    triton:\n",
    "      image: dpoulopoulos/triton-inference-server:v0.1.0\n",
    "      securityContext:\n",
    "          runAsUser: 0\n",
    "      resources:\n",
    "        limits:\n",
    "          cpu: \"2\"\n",
    "          memory: 8Gi\n",
    "          nvidia.com/gpu: 1\n",
    "        requests:\n",
    "          cpu: \"2\"\n",
    "          memory: 8Gi\n",
    "      storageUri: \"pvc://kubeflow-shared-pvc/model-repository\"\n",
    "\"\"\"\n",
    "\n",
    "with open(\"sgpt-isvc.yaml\", \"w\") as f:\n",
    "    f.write(sgpt_isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396b3ca-afe6-4476-935c-269ddd40b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"sgpt-isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25e4d-4803-4464-80d9-6b62ad2b4d49",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating and Submitting the Inference Service\n",
    "\n",
    "In the final segment of this Notebook, you create and submit an ISVC via a YAML template and a Python subprocess. This\n",
    "process unfolds as follows:\n",
    "\n",
    "1. Drafting the YAML Template: Here, you craft a YAML file that outlines the ISVC's specifics. This captures elements\n",
    "   like the service's name, the chosen Docker image, and additional configurations. After drafting, you save this YAML\n",
    "   to a file for inspection and later submission.\n",
    "1. Applying the YAML Template: With your YAML template prepped, the next step is to present it to KServe for deployment.\n",
    "   You accomplish this by leveraging a Python subprocess to execute a shell command.\n",
    "\n",
    "By the end of this section, you will have a running ISVC that is ready to receive user queries and provide context for\n",
    "answering them using the Vector Store. This marks the completion of your journey, from transforming unstructured text\n",
    "data into structured vector embeddings, to creating a scalable service that can provide context based on those\n",
    "embeddings.\n",
    "\n",
    "In the upcoming cell, input the name of the Docker image you constructed in the initial phase. If you wish to utilize\n",
    "the pre-fabricated one, simply leave the field untouched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd164d7-7bd4-4ae7-b4c2-9e99f1a5f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>Predictor Image</h2>\")\n",
    "display(heading)\n",
    "\n",
    "predictor_image_widget = widgets.Text(\n",
    "    description=\"Image Name:\",\n",
    "    placeholder=\"Default: dpoulopoulos/qna-vectorstore-mlde:v0.1.0\",\n",
    "    layout=widgets.Layout(width='30%'))\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "success_message = widgets.Output()\n",
    "\n",
    "predictor_image = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global predictor_image\n",
    "    predictor_image = predictor_image_widget.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        if not predictor_image:\n",
    "            predictor_image = \"dpoulopoulos/qna-vectorstore-mlde:v0.1.0\"\n",
    "        print(f\"The name of the predictor image will be: '{predictor_image}'\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(predictor_image_widget, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c9d-72e4-491a-ad99-f6f533f0ef94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isvc = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: minio-secret\n",
    "type: Opaque\n",
    "data:\n",
    "  MINIO_ACCESS_KEY: {0}\n",
    "  MINIO_SECRET_KEY: {1}\n",
    "\n",
    "---\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: vectorstore\n",
    "spec:\n",
    "  predictor:\n",
    "    containers:\n",
    "    - name: kserve-container\n",
    "      image: {2}\n",
    "      imagePullPolicy: Always\n",
    "      resources:\n",
    "        requests:\n",
    "          memory: \"2Gi\"\n",
    "          cpu: \"500m\"\n",
    "        limits:\n",
    "          memory: \"2Gi\"\n",
    "          cpu: \"500m\"\n",
    "      args:\n",
    "      - --persist-uri\n",
    "      - {3}\n",
    "      env:\n",
    "      - name: MLFLOW_S3_ENDPOINT_URL\n",
    "        value: {4}\n",
    "      - name: TRANSFORMERS_CACHE\n",
    "        value: /src\n",
    "      - name: SENTENCE_TRANSFORMERS_HOME\n",
    "        value: /src\n",
    "      - name: MINIO_ACCESS_KEY\n",
    "        valueFrom:\n",
    "          secretKeyRef:\n",
    "            key: MINIO_ACCESS_KEY\n",
    "            name: minio-secret\n",
    "      - name: MINIO_SECRET_KEY\n",
    "        valueFrom:\n",
    "          secretKeyRef:\n",
    "            key: MINIO_SECRET_KEY\n",
    "            name: minio-secret\n",
    "\"\"\".format(encode_base64(os.environ[\"AWS_ACCESS_KEY_ID\"]),\n",
    "           encode_base64(os.environ[\"AWS_SECRET_ACCESS_KEY\"]),\n",
    "           predictor_image,\n",
    "           uri,\n",
    "           os.environ[\"MLFLOW_S3_ENDPOINT_URL\"])\n",
    "\n",
    "with open(\"vectorstore-isvc.yaml\", \"w\") as f:\n",
    "    f.write(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631d06-ea76-4159-a08b-57e76850ff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"vectorstore-isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e2d08-8e09-4c9e-826c-0f0dfdc2d3f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully navigated through the process of logging the Chroma DB files as artifacts using\n",
    "MLflow, creating a custom Docker image, and setting up an ISVC with KServe that retrieves these artifacts to serve your\n",
    "Vector Store. This ISVC forms the backbone of your question-answering application, enabling you to efficiently answer\n",
    "queries based on the document embeddings we generated previously.\n",
    "\n",
    "From here, there are two paths you can choose:\n",
    "\n",
    "- **Testing the Vector Store ISVC**: If you'd like to test the Vector Store ISVC that you've just created, you can proceed\n",
    "  to the third (optional) Notebook. This Notebook provides a step-by-step guide on how to invoke the ISVC and validate\n",
    "  its performance.\n",
    "- **Creating the LLM ISVC**: Alternatively, if you're ready to move on to the next stage of the project, you\n",
    "  can jump straight to our fourth Notebook. In this Notebook, you create an ISVC for the Large Language Model (LLM),\n",
    "  which will work in conjunction with the Vector Store ISVC to provide answers to user queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
