{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f891729d-9030-4d9e-a7be-e81386ae820f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating a Large Language Model Inference Service\n",
    "\n",
    "Welcome to the fourth part of the tutorial series on building a question-answering application over a corpus of private\n",
    "documents using Large Language Models (LLMs). The previous Notebooks walked you through the processes of creating\n",
    "vector embeddings of the documents, setting up the Inference Services (ISVCs) for the Vector Store and the Embeddings model,\n",
    "and testing the performance of the information retrieval system.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/llm.jpg\" alt=\"llm\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "      Photo by <a href=\"https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Google DeepMind</a> on <a href=\"https://unsplash.com/photos/LaKwLAmcnBc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "Now, you're moving towards the next crucial step: creating an ISVC for the LLM. This ISVC is the centerpiece of the\n",
    "question-answering system, working in tandem with the Vector Store ISVC to deliver comprehensive and accurate answers to\n",
    "user queries.\n",
    "\n",
    "In this Notebook, you set up this LLM ISVC. You learn how to build a Docker image for the transformer component and its\n",
    "role, define a KServe ISVC YAML file, and deploy the service. By the end of this Notebook, you'll have a fully functioning\n",
    "LLM ISVC that can accept user queries, interact with the Vector Store, and provide insightful responses.\n",
    "\n",
    "To complete this step you need to have access to the Llama 2 model on Hugging Face Hub and the pre-built TensorRT-LLM engines:\n",
    "\n",
    "* The TensorRT-LLM engines can be downloaded from the following link: https://ezmeral-artifacts.s3.us-east-2.amazonaws.com/llama-engines.tar.gz.\n",
    "  We will do this later in the Notebook.\n",
    "* The model repository is available via the Hugging Face hub at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n",
    "  It is not necessary to clone the model weights, thus eliminating the need for Git LFS. You should sign in to Hugging Face and accept the terms\n",
    "  and conditions for using this model.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Architecture](#architecture)\n",
    "1. [Creating the Inference Service](#creating-the-inference-service)\n",
    "1. [Conclusion and Next Steps](#conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0cc08-1c00-49cc-85bd-b792383b26e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Architecture\n",
    "\n",
    "In this setup, an additional component, called a \"transformer\", plays a pivotal role in processing user queries and\n",
    "integrating the Vector Store ISVC with the LLM ISVC. The transformer's role is to intercept the user's request, extract\n",
    "the necessary information, and then communicate with the Vector Store ISVC to retrieve the relevant context. The\n",
    "transformer then takes the response of the Vector Store ISVC (i.e., the context), combines it with the user's query, and\n",
    "forwards the enriched prompt to the LLM predictor.\n",
    "\n",
    "Here's a detailed look at the process:\n",
    "\n",
    "1. **Intercepting the User's Request**: The transformer acts as a gateway between the user and the LLM ISVC. When a user\n",
    "   sends a query, it first reaches the transformer. The transformer extracts the query from the request.\n",
    "1. **Communicating with the Vector Store ISVC**: The transformer then takes the user's query and sends a POST request to the\n",
    "   Vector Store ISVC including the user's query in the payload, just like you did in the previous Notebook.\n",
    "1. **Receiving and Processing the Context**: The Vector Store ISVC responds by sending back the relevant context.\n",
    "1. **Combining the Context with the User's Query**: The transformer then combines the received context with the user's\n",
    "   original query using a prompt template. This creates an enriched prompt that contains both the user's original\n",
    "   question and the relevant context from our documents.\n",
    "1. **Forwarding the Enriched Query to the LLM Predictor**: Finally, the transformer forwards this enriched query to the LLM\n",
    "   predictor. The predictor then processes this query and generates a response, which is sent back to the transformer.\n",
    "   Steps 2 through 5 are transparent to the user.\n",
    "1. **Final response**:The transformer returns the response to the user.\n",
    "\n",
    "As such, you should build one custom Docker image at this point for the transformer component. The\n",
    "source code and the Dockerfile is provided in the corresponding folder: `dockerfiles/transformer`.\n",
    "For your convenience, you can use the image we have pre-built for you: `marketplace.us1.greenlake-hpe.com/ezmeral/ezkf/qna-transformer-gpu:v1.3.0-e658264`\n",
    "\n",
    "Once ready, proceed with the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9f01f-bdcf-4a5b-a466-cb44005f4dab",
   "metadata": {},
   "source": [
    "# Downloading the Artifacts\n",
    "\n",
    "Next, you should download the necessary artifacts for the model. The first step is to clone the model repository. Visit the Llama 2 7B page on Hugging Face Hub and clone the model in this directory, using the following command:\n",
    "\n",
    "```\n",
    "GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "```\n",
    "\n",
    "You should provide your username and the access token for your accound. If you don't have an access token you should create a new one. When everything's ready, run the following command to move the Llama 2 7B directory you pulled in the right location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75f3a7-a403-486b-84b7-66c98e0c2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv Llama-2-7b-chat-hf/ inflight_batcher_llm/preprocessing/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d445f-50f7-497b-a557-869f34f0871b",
   "metadata": {},
   "source": [
    "Next, let's download the pre-built TensorRT-LLM engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f06b8c-1f9e-4eec-a18d-d934a763797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are behind a proxy, do not forget to set your `https_proxy` and `HTTPS_PROXY` environment variable.\n",
    "# os.environ[\"https_proxy\"] = \"\"\n",
    "# os.environ[\"HTTPS_PROXY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40598e5c-2f3c-4ca5-a9af-f98492cd1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ezmeral-artifacts.s3.us-east-2.amazonaws.com/llama-engines.tar.gz  # download the tarball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6d7f1-c567-4e82-b0fd-31d40844e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv llama-engines.tar.gz inflight_batcher_llm/tensorrt_llm/1/  # move the tarball to the right location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3860bb-043d-4b92-9b5a-d2a306234011",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xzf inflight_batcher_llm/tensorrt_llm/1/llama-engines.tar.gz -C inflight_batcher_llm/tensorrt_llm/1/  # extract the tarball"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b51ccb-f1d3-4132-a88b-62b84d90b11a",
   "metadata": {},
   "source": [
    "# Creating the Inference Service\n",
    "\n",
    "As before, you need to provide the name of the transofmer image You can leave any field empty to use the image we\n",
    "provide for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db302a3-e35f-4bb3-a2c5-24ee637c2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>Transformer Image</h2>\")\n",
    "display(heading)\n",
    "\n",
    "transformer_image_widget = widgets.Text(\n",
    "    description=\"Image Name:\",\n",
    "    placeholder=\"Default: marketplace.us1.greenlake-hpe.com/ezmeral/ezkf/qna-transformer-gpu:v1.3.0-e658264\",\n",
    "    layout=widgets.Layout(width='30%'))\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "success_message = widgets.Output()\n",
    "\n",
    "transformer_image = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global transformer_image\n",
    "    transformer_image = transformer_image_widget.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        if not transformer_image:\n",
    "            transformer_image = \"marketplace.us1.greenlake-hpe.com/ezmeral/ezkf/qna-transformer-gpu:v1.3.0-e658264\"\n",
    "        print(f\"The name of the transformer image will be: '{transformer_image}'\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(transformer_image_widget, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3b154-ee58-4f55-b2b3-27c58a5b085c",
   "metadata": {},
   "source": [
    "Copy the `inflight_batcher_llm` directory to the shared PVC. The `inflight_batcher_llm` directory defines the structure or the model repository that the Triton Inference Server expects to serve Llama 2 7B, using the TensorRT-LLM backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8f88b-a7fd-4dd8-b6f7-e72428103528",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r inflight_batcher_llm/ /mnt/shared/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545db2b-c5a1-4dfd-a382-3b4d964e9d79",
   "metadata": {},
   "source": [
    "Define and apply the LLM Inference Service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c9d-72e4-491a-ad99-f6f533f0ef94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_isvc = \"\"\"\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"ensemble\"\n",
    "spec:\n",
    "  predictor:\n",
    "    timeout: 600\n",
    "    triton:\n",
    "      image: nvcr.io/nvidia/tritonserver:24.01-trtllm-python-py3\n",
    "      securityContext:\n",
    "          runAsUser: 0\n",
    "      resources:\n",
    "        limits:\n",
    "          cpu: \"4\"\n",
    "          memory: 32Gi\n",
    "          nvidia.com/gpu: 1\n",
    "        requests:\n",
    "          cpu: \"4\"\n",
    "          memory: 32Gi\n",
    "      storageUri: \"pvc://kubeflow-shared-pvc/inflight_batcher_llm\"\n",
    "  transformer:\n",
    "    timeout: 600\n",
    "    containers:\n",
    "      - image: {0}\n",
    "        imagePullPolicy: Always\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "        name: kserve-container\n",
    "        args: [\"--protocol\", \"v2\"]\n",
    "\"\"\".format(transformer_image)\n",
    "\n",
    "with open(\"llama-isvc.yaml\", \"w\") as f:\n",
    "    f.write(llama_isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631d06-ea76-4159-a08b-57e76850ff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"llama-isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20235af7-7b47-4b68-8f6d-f0b01b0c23e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations on completing this crucial step in this tutorial series! You've successfully built an LLM ISVC, and\n",
    "you've learned about the role of a transformer in enriching user queries with relevant context from our documents.\n",
    "Together with the Vector Store ISVC, these components form the backbone of your question-answering application.\n",
    "\n",
    "However, the journey doesn't stop here. The next and final step is to test the LLM ISVC, ensuring that it's working as\n",
    "expected and delivering accurate responses. This will help you gain confidence in your setup and prepare you for\n",
    "real-world applications. In the next Notebook, you invoke the LLM ISVC. You see how to construct suitable requests,\n",
    "communicate with the service, and interpret the responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
