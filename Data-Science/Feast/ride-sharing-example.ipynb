{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial, we'll use Feast to generate training data and power online model inference for a \n",
    "ride-sharing driver satisfaction prediction model. Feast solves several common issues in this flow:\n",
    "\n",
    "1. **Training-serving skew and complex data joins:** Feature values often exist across multiple tables. Joining \n",
    "   these datasets can be complicated, slow, and error-prone.\n",
    "   * Feast joins these tables with battle-tested logic that ensures _point-in-time_ correctness so future feature \n",
    "     values do not leak to models.\n",
    "2. **Online feature availability:** At inference time, models often need access to features that aren't readily \n",
    "   available and need to be precomputed from other data sources.\n",
    "   * Feast manages deployment to a variety of online stores (e.g. DynamoDB, Redis, Google Cloud Datastore) and \n",
    "     ensures necessary features are consistently _available_ and _freshly computed_ at inference time.\n",
    "3. **Feature and model versioning:** Different teams within an organization are often unable to reuse \n",
    "   features across projects, resulting in duplicate feature creation logic. Models have data dependencies that need \n",
    "   to be versioned, for example when running A/B tests on model versions.\n",
    "   * Feast enables discovery of and collaboration on previously used features and enables versioning of sets of \n",
    "     features (via _feature services_).\n",
    "   * _(Experimental)_ Feast enables light-weight feature transformations so users can re-use transformation logic \n",
    "     across online / offline use cases and across models.\n",
    "\n",
    "We will:\n",
    "1. Deploy a local feature store with a **Parquet file offline store** and **Sqlite online store**.\n",
    "2. Build a training dataset using our time series features from our **Parquet files**.\n",
    "3. Materialize feature values from the offline store into the online store.\n",
    "4. Read the latest features from the online store for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Install Feast\n",
    "\n",
    "Install Feast (and psycopg2-binary) using pip:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Feast \n",
    "!python3 -m pip install feast==0.29.0\n",
    "!python3 -m pip install typeguard==2.13.3\n",
    "!python3 -m pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inspect a feature repository\n",
    "\n",
    "A feature repository is a directory that contains the configuration of the feature store and individual features. This configuration is written as code (Python/YAML) and it's highly recommended that teams track it centrally using git. See [Feature Repository](https://docs.feast.dev/reference/feature-repository) for a detailed explanation of feature repositories.\n",
    "\n",
    "The easiest way to create a new feature repository to use the `feast init` command. This creates a scaffolding with initial demo data.\n",
    "\n",
    "### Demo data scenario \n",
    "- We have surveyed some drivers for how satisfied they are with their experience in a ride-sharing app. \n",
    "- We want to generate predictions for driver satisfaction for the rest of the users so we can reach out to potentially dissatisfied users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a: Inspecting the feature repository\n",
    "\n",
    "Let's take a look at the demo repo itself. It breaks down into\n",
    "\n",
    "\n",
    "* `data/` contains raw demo parquet data\n",
    "* `definition.py` contains demo feature definitions\n",
    "* `feature_store.yaml` contains a demo setup configuring where data sources are\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n",
      "data  definitions.py  online-store.ipynb  __pycache__\n",
      "\n",
      "./data:\n",
      "driver_stats.parquet\n",
      "\n",
      "./__pycache__:\n",
      "definitions.cpython-38.pyc\n"
     ]
    }
   ],
   "source": [
    "!cd /mnt/shared/feast-store\n",
    "!ls -R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b: Inspecting the project configuration\n",
    "Let's inspect the setup of the project in `feature_store.yaml`. \n",
    "\n",
    "The key line defining the overall architecture of the feature store is the **provider**. \n",
    "\n",
    "The provider value sets default offline and online stores. \n",
    "* The offline store provides the compute layer to process historical data (for generating training data & feature \n",
    "  values for serving). \n",
    "* The online store is a low latency store of the latest feature values (for powering real-time inference).\n",
    "\n",
    "Valid values for `provider` in `feature_store.yaml` are:\n",
    "\n",
    "* local: use file source with SQLite/Redis\n",
    "* gcp: use BigQuery/Snowflake with Google Cloud Datastore/Redis\n",
    "* aws: use Redshift/Snowflake with DynamoDB/Redis\n",
    "\n",
    "Note that there are many other offline / online stores Feast works with, including Azure, Hive, Trino, and PostgreSQL via community plugins. See https://docs.feast.dev/roadmap for all supported connectors.\n",
    "\n",
    "A custom setup can also be made by following [Customizing Feast](https://docs.feast.dev/v/master/how-to-guides/customizing-feast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project: ezaf_feast_demo_ride_sharing\n",
      "registry: /mnt/shared/feast-store/data/registry.db\n",
      "provider: local\n",
      "offline_store:\n",
      "  type: file\n",
      "online_store:\n",
      "  type: sqlite\n",
      "entity_key_serialization_version: 2\n"
     ]
    }
   ],
   "source": [
    "!cat /mnt/shared/feast-store/feature_store.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnMlk4zshywp"
   },
   "source": [
    "### Inspecting the raw data\n",
    "\n",
    "The raw feature data we have in this demo is stored in a local parquet file. The dataset captures hourly stats of a driver in a ride-sharing app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>conv_rate</th>\n",
       "      <th>acc_rate</th>\n",
       "      <th>avg_daily_trips</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-15 08:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.718795</td>\n",
       "      <td>0.758693</td>\n",
       "      <td>679</td>\n",
       "      <td>2023-03-02 08:21:05.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-15 09:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.315331</td>\n",
       "      <td>0.682747</td>\n",
       "      <td>313</td>\n",
       "      <td>2023-03-02 08:21:05.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-15 10:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.288372</td>\n",
       "      <td>0.934601</td>\n",
       "      <td>783</td>\n",
       "      <td>2023-03-02 08:21:05.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-15 11:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.279908</td>\n",
       "      <td>0.104038</td>\n",
       "      <td>445</td>\n",
       "      <td>2023-03-02 08:21:05.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-15 12:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.868386</td>\n",
       "      <td>0.416725</td>\n",
       "      <td>430</td>\n",
       "      <td>2023-03-02 08:21:05.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>2023-03-02 06:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.294989</td>\n",
       "      <td>0.557884</td>\n",
       "      <td>194</td>\n",
       "      <td>2023-03-02 08:21:05.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>2023-03-02 07:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>0.841543</td>\n",
       "      <td>132</td>\n",
       "      <td>2023-03-02 08:21:05.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>2021-04-12 07:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.323702</td>\n",
       "      <td>0.801096</td>\n",
       "      <td>624</td>\n",
       "      <td>2023-03-02 08:21:05.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>2023-02-22 20:00:00+00:00</td>\n",
       "      <td>1003</td>\n",
       "      <td>0.593804</td>\n",
       "      <td>0.182048</td>\n",
       "      <td>417</td>\n",
       "      <td>2023-03-02 08:21:05.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>2023-02-22 20:00:00+00:00</td>\n",
       "      <td>1003</td>\n",
       "      <td>0.593804</td>\n",
       "      <td>0.182048</td>\n",
       "      <td>417</td>\n",
       "      <td>2023-03-02 08:21:05.729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1807 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               event_timestamp  driver_id  conv_rate  acc_rate  \\\n",
       "0    2023-02-15 08:00:00+00:00       1005   0.718795  0.758693   \n",
       "1    2023-02-15 09:00:00+00:00       1005   0.315331  0.682747   \n",
       "2    2023-02-15 10:00:00+00:00       1005   0.288372  0.934601   \n",
       "3    2023-02-15 11:00:00+00:00       1005   0.279908  0.104038   \n",
       "4    2023-02-15 12:00:00+00:00       1005   0.868386  0.416725   \n",
       "...                        ...        ...        ...       ...   \n",
       "1802 2023-03-02 06:00:00+00:00       1001   0.294989  0.557884   \n",
       "1803 2023-03-02 07:00:00+00:00       1001   0.009235  0.841543   \n",
       "1804 2021-04-12 07:00:00+00:00       1001   0.323702  0.801096   \n",
       "1805 2023-02-22 20:00:00+00:00       1003   0.593804  0.182048   \n",
       "1806 2023-02-22 20:00:00+00:00       1003   0.593804  0.182048   \n",
       "\n",
       "      avg_daily_trips                 created  \n",
       "0                 679 2023-03-02 08:21:05.729  \n",
       "1                 313 2023-03-02 08:21:05.729  \n",
       "2                 783 2023-03-02 08:21:05.729  \n",
       "3                 445 2023-03-02 08:21:05.729  \n",
       "4                 430 2023-03-02 08:21:05.729  \n",
       "...               ...                     ...  \n",
       "1802              194 2023-03-02 08:21:05.729  \n",
       "1803              132 2023-03-02 08:21:05.729  \n",
       "1804              624 2023-03-02 08:21:05.729  \n",
       "1805              417 2023-03-02 08:21:05.729  \n",
       "1806              417 2023-03-02 08:21:05.729  \n",
       "\n",
       "[1807 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_parquet(\"/home/hpedemouser01/feast/data/driver_stats.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Register feature definitions and deploy your feature store\n",
    "\n",
    "`feast apply` scans python files in the current directory for feature/entity definitions and deploys infrastructure according to `feature_store.yaml`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3a: Inspecting feature definitions\n",
    "Now we run `feast apply` to register the feature views and entities defined in `definition.py`, and sets up SQLite online store tables. Note that we had previously specified SQLite as the online store in `feature_store.yaml` by specifying a `local` provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This is an example feature definition file\n",
      "\n",
      "from datetime import timedelta\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "from feast import (\n",
      "    Entity,\n",
      "    FeatureService,\n",
      "    FeatureView,\n",
      "    Field,\n",
      "    FileSource,\n",
      "    PushSource,\n",
      "    RequestSource,\n",
      ")\n",
      "from feast.on_demand_feature_view import on_demand_feature_view\n",
      "from feast.types import Float32, Float64, Int64\n",
      "\n",
      "# Define an entity for the driver. You can think of an entity as a primary key used to\n",
      "# fetch features.\n",
      "driver = Entity(name=\"driver\", join_keys=[\"driver_id\"])\n",
      "\n",
      "# Read data from parquet files. Parquet is convenient for local development mode. For\n",
      "# production, you can use your favorite DWH, such as BigQuery. See Feast documentation\n",
      "# for more info.\n",
      "driver_stats_source = FileSource(\n",
      "    name=\"driver_hourly_stats_source\",\n",
      "    path=\"/home/hpedemouser01/feast/data/driver_stats.parquet\",\n",
      "    timestamp_field=\"event_timestamp\",\n",
      "    created_timestamp_column=\"created\",\n",
      ")\n",
      "\n",
      "# Our parquet files contain sample data that includes a driver_id column, timestamps and\n",
      "# three feature column. Here we define a Feature View that will allow us to serve this\n",
      "# data to our model online.\n",
      "driver_stats_fv = FeatureView(\n",
      "    # The unique name of this feature view. Two feature views in a single\n",
      "    # project cannot have the same name\n",
      "    name=\"driver_hourly_stats\",\n",
      "    entities=[driver],\n",
      "    ttl=timedelta(days=1),\n",
      "    # The list of features defined below act as a schema to both define features\n",
      "    # for both materialization of features into a store, and are used as references\n",
      "    # during retrieval for building a training dataset or serving features\n",
      "    schema=[\n",
      "        Field(name=\"conv_rate\", dtype=Float32),\n",
      "        Field(name=\"acc_rate\", dtype=Float32),\n",
      "        Field(name=\"avg_daily_trips\", dtype=Int64, description=\"Average daily trips\"),\n",
      "    ],\n",
      "    online=True,\n",
      "    source=driver_stats_source,\n",
      "    # Tags are user defined key/value pairs that are attached to each\n",
      "    # feature view\n",
      "    tags={\"team\": \"driver_performance\"},\n",
      ")\n",
      "\n",
      "# Define a request data source which encodes features / information only\n",
      "# available at request time (e.g. part of the user initiated HTTP request)\n",
      "input_request = RequestSource(\n",
      "    name=\"vals_to_add\",\n",
      "    schema=[\n",
      "        Field(name=\"val_to_add\", dtype=Int64),\n",
      "        Field(name=\"val_to_add_2\", dtype=Int64),\n",
      "    ],\n",
      ")\n",
      "\n",
      "\n",
      "# Define an on demand feature view which can generate new features based on\n",
      "# existing feature views and RequestSource features\n",
      "@on_demand_feature_view(\n",
      "    sources=[driver_stats_fv, input_request],\n",
      "    schema=[\n",
      "        Field(name=\"conv_rate_plus_val1\", dtype=Float64),\n",
      "        Field(name=\"conv_rate_plus_val2\", dtype=Float64),\n",
      "    ],\n",
      ")\n",
      "def transformed_conv_rate(inputs: pd.DataFrame) -> pd.DataFrame:\n",
      "    df = pd.DataFrame()\n",
      "    df[\"conv_rate_plus_val1\"] = inputs[\"conv_rate\"] + inputs[\"val_to_add\"]\n",
      "    df[\"conv_rate_plus_val2\"] = inputs[\"conv_rate\"] + inputs[\"val_to_add_2\"]\n",
      "    return df\n",
      "\n",
      "\n",
      "# This groups features into a model version\n",
      "driver_activity_v1 = FeatureService(\n",
      "    name=\"driver_activity_v1\",\n",
      "    features=[\n",
      "        driver_stats_fv[[\"conv_rate\"]],  # Sub-selects a feature from a feature view\n",
      "        transformed_conv_rate,  # Selects all features from the feature view\n",
      "    ],\n",
      ")\n",
      "driver_activity_v2 = FeatureService(\n",
      "    name=\"driver_activity_v2\", features=[driver_stats_fv, transformed_conv_rate]\n",
      ")\n",
      "\n",
      "# Defines a way to push data (to be available offline, online or both) into Feast.\n",
      "driver_stats_push_source = PushSource(\n",
      "    name=\"driver_stats_push_source\",\n",
      "    batch_source=driver_stats_source,\n",
      ")\n",
      "\n",
      "# Defines a slightly modified version of the feature view from above, where the source\n",
      "# has been changed to the push source. This allows fresh features to be directly pushed\n",
      "# to the online store for this feature view.\n",
      "driver_stats_fresh_fv = FeatureView(\n",
      "    name=\"driver_hourly_stats_fresh\",\n",
      "    entities=[driver],\n",
      "    ttl=timedelta(days=1),\n",
      "    schema=[\n",
      "        Field(name=\"conv_rate\", dtype=Float32),\n",
      "        Field(name=\"acc_rate\", dtype=Float32),\n",
      "        Field(name=\"avg_daily_trips\", dtype=Int64),\n",
      "    ],\n",
      "    online=True,\n",
      "    source=driver_stats_push_source,  # Changed from above\n",
      "    tags={\"team\": \"driver_performance\"},\n",
      ")\n",
      "\n",
      "\n",
      "# Define an on demand feature view which can generate new features based on\n",
      "# existing feature views and RequestSource features\n",
      "@on_demand_feature_view(\n",
      "    sources=[driver_stats_fresh_fv, input_request],  # relies on fresh version of FV\n",
      "    schema=[\n",
      "        Field(name=\"conv_rate_plus_val1\", dtype=Float64),\n",
      "        Field(name=\"conv_rate_plus_val2\", dtype=Float64),\n",
      "    ],\n",
      ")\n",
      "def transformed_conv_rate_fresh(inputs: pd.DataFrame) -> pd.DataFrame:\n",
      "    df = pd.DataFrame()\n",
      "    df[\"conv_rate_plus_val1\"] = inputs[\"conv_rate\"] + inputs[\"val_to_add\"]\n",
      "    df[\"conv_rate_plus_val2\"] = inputs[\"conv_rate\"] + inputs[\"val_to_add_2\"]\n",
      "    return df\n",
      "\n",
      "\n",
      "driver_activity_v3 = FeatureService(\n",
      "    name=\"driver_activity_v3\",\n",
      "    features=[driver_stats_fresh_fv, transformed_conv_rate_fresh],\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Step 3a: Inspecting feature definitions\n",
    "!cat definitions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3b: Applying feature definitions\n",
    "Now we run `feast apply` to register the feature views and entities defined in `definitions.py`, and sets up SQLite online store tables. Note that we had previously specified SQLite as the online store in `feature_store.yaml` by specifying a `local` provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/home/hpedemouser01/.local/lib/python3.8/site-packages/feast/feature_store.py:563: RuntimeWarning: On demand feature view is an experimental feature. This API is stable, but the functionality does not scale well for offline retrieval\n",
      "  warnings.warn(\n",
      "/home/hpedemouser01/.local/lib/python3.8/site-packages/feast/infra/offline_stores/file_source.py:161: FutureWarning: 'ParquetDataset.schema' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.schema' attribute instead (which will return an Arrow schema instead of a Parquet schema).\n",
      "  schema = ParquetDataset(path).schema.to_arrow_schema()\n"
     ]
    }
   ],
   "source": [
    "from definitions import driver, driver_stats_source, driver_stats_fv, driver_stats_push_source, input_request, driver_activity_v1, driver_activity_v2, driver_activity_v3, driver_stats_fresh_fv\n",
    "from definitions import transformed_conv_rate\n",
    "from pprint import pprint\n",
    "from feast import FeatureStore, Entity, FeatureView, Feature, ValueType, FileSource, RepoConfig\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "Store = \"/mnt/shared/feast-store\"\n",
    "fs = FeatureStore(repo_path=Store)\n",
    "\n",
    "fs.apply([driver, driver_stats_source, driver_stats_fv, driver_stats_push_source, input_request, driver_activity_v1, driver_activity_v2, transformed_conv_rate, driver_activity_v3, driver_stats_fresh_fv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generating training data or powering batch scoring models\n",
    "\n",
    "To train a model, we need features and labels. Often, this label data is stored separately (e.g. you have one table storing user survey results and another set of tables with feature values). Feast can help generate the features that map to these labels.\n",
    "\n",
    "Feast needs a list of **entities** (e.g. driver ids) and **timestamps**. Feast will intelligently join relevant \n",
    "tables to create the relevant feature vectors. There are two ways to generate this list:\n",
    "1. The user can query that table of labels with timestamps and pass that into Feast as an _entity dataframe_ for \n",
    "training data generation. \n",
    "2. The user can also query that table with a *SQL query* which pulls entities. See the documentation on [feature retrieval](https://docs.feast.dev/getting-started/concepts/feature-retrieval) for details    \n",
    "\n",
    "* Note that we include timestamps because we want the features for the same driver at various timestamps to be used in a model.\n",
    "\n",
    "### Step 4a: Generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Feature schema -----\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 10 columns):\n",
      " #   Column                              Non-Null Count  Dtype              \n",
      "---  ------                              --------------  -----              \n",
      " 0   driver_id                           3 non-null      int64              \n",
      " 1   event_timestamp                     3 non-null      datetime64[ns, UTC]\n",
      " 2   label_driver_reported_satisfaction  3 non-null      int64              \n",
      " 3   val_to_add                          3 non-null      int64              \n",
      " 4   val_to_add_2                        3 non-null      int64              \n",
      " 5   conv_rate                           3 non-null      float32            \n",
      " 6   acc_rate                            3 non-null      float32            \n",
      " 7   avg_daily_trips                     3 non-null      int32              \n",
      " 8   conv_rate_plus_val1                 3 non-null      float64            \n",
      " 9   conv_rate_plus_val2                 3 non-null      float64            \n",
      "dtypes: datetime64[ns, UTC](1), float32(2), float64(2), int32(1), int64(4)\n",
      "memory usage: 332.0 bytes\n",
      "None\n",
      "\n",
      "----- Example features -----\n",
      "\n",
      "   driver_id           event_timestamp  label_driver_reported_satisfaction  \\\n",
      "0       1001 2021-04-12 10:59:42+00:00                                   1   \n",
      "1       1002 2021-04-12 08:12:10+00:00                                   5   \n",
      "2       1003 2021-04-12 16:40:26+00:00                                   3   \n",
      "\n",
      "   val_to_add  val_to_add_2  conv_rate  acc_rate  avg_daily_trips  \\\n",
      "0           1            10   0.323702  0.801096              624   \n",
      "1           2            20   0.963921  0.071397              767   \n",
      "2           3            30   0.712921  0.167478              590   \n",
      "\n",
      "   conv_rate_plus_val1  conv_rate_plus_val2  \n",
      "0             1.323702            10.323702  \n",
      "1             2.963921            20.963921  \n",
      "2             3.712921            30.712921  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1722: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  if dataset.partitions is not None:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1750: FutureWarning: 'ParquetDataset.metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  if dataset.metadata:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1769: FutureWarning: 'ParquetDataset.schema' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.schema' attribute instead (which will return an Arrow schema instead of a Parquet schema).\n",
      "  if dataset.schema is not None:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1797: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  if len(dataset.pieces) > 1:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1809: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  for piece, fn in zip(dataset.pieces, fns):\n",
      "/opt/conda/lib/python3.8/site-packages/dask/array/percentile.py:27: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they. (Deprecated NumPy 1.22)\n",
      "  result = np.percentile(a2, q, interpolation=interpolation).astype(a.dtype)\n",
      "/opt/conda/lib/python3.8/site-packages/dask/array/percentile.py:27: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they. (Deprecated NumPy 1.22)\n",
      "  result = np.percentile(a2, q, interpolation=interpolation).astype(a.dtype)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from feast import FeatureStore\n",
    "\n",
    "# The entity dataframe is the dataframe we want to enrich with feature values\n",
    "# Note: see https://docs.feast.dev/getting-started/concepts/feature-retrieval for more details on how to retrieve\n",
    "# for all entities in the offline store instead\n",
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        # entity's join key -> entity values\n",
    "        \"driver_id\": [1001, 1002, 1003],\n",
    "        # \"event_timestamp\" (reserved key) -> timestamps\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "        ],\n",
    "        # (optional) label name -> label values. Feast does not process these\n",
    "        \"label_driver_reported_satisfaction\": [1, 5, 3],\n",
    "        # values we're using for an on-demand transformation\n",
    "        \"val_to_add\": [1, 2, 3],\n",
    "        \"val_to_add_2\": [10, 20, 30],\n",
    "    }\n",
    ")\n",
    "\n",
    "training_df = fs.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_hourly_stats:avg_daily_trips\",\n",
    "        \"transformed_conv_rate:conv_rate_plus_val1\",\n",
    "        \"transformed_conv_rate:conv_rate_plus_val2\",\n",
    "    ],\n",
    ").to_df()\n",
    "\n",
    "print(\"----- Feature schema -----\\n\")\n",
    "print(training_df.info())\n",
    "\n",
    "print()\n",
    "print(\"----- Example features -----\\n\")\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4b: Run offline inference (batch scoring)\n",
    "To power a batch model, we primarily need to generate features with the `get_historical_features` call, but using the current timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Example features -----\n",
      "\n",
      "   driver_id           event_timestamp  label_driver_reported_satisfaction  \\\n",
      "0       1001 2021-04-12 10:59:42+00:00                                   1   \n",
      "1       1002 2021-04-12 08:12:10+00:00                                   5   \n",
      "2       1003 2021-04-12 16:40:26+00:00                                   3   \n",
      "\n",
      "   val_to_add  val_to_add_2  conv_rate  acc_rate  avg_daily_trips  \\\n",
      "0           1            10   0.323702  0.801096              624   \n",
      "1           2            20   0.963921  0.071397              767   \n",
      "2           3            30   0.712921  0.167478              590   \n",
      "\n",
      "   conv_rate_plus_val1  conv_rate_plus_val2  \n",
      "0             1.323702            10.323702  \n",
      "1             2.963921            20.963921  \n",
      "2             3.712921            30.712921  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1722: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  if dataset.partitions is not None:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1750: FutureWarning: 'ParquetDataset.metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  if dataset.metadata:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1769: FutureWarning: 'ParquetDataset.schema' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.schema' attribute instead (which will return an Arrow schema instead of a Parquet schema).\n",
      "  if dataset.schema is not None:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1797: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  if len(dataset.pieces) > 1:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1809: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  for piece, fn in zip(dataset.pieces, fns):\n",
      "/opt/conda/lib/python3.8/site-packages/dask/array/percentile.py:27: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they. (Deprecated NumPy 1.22)\n",
      "  result = np.percentile(a2, q, interpolation=interpolation).astype(a.dtype)\n",
      "/opt/conda/lib/python3.8/site-packages/dask/array/percentile.py:27: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they. (Deprecated NumPy 1.22)\n",
      "  result = np.percentile(a2, q, interpolation=interpolation).astype(a.dtype)\n"
     ]
    }
   ],
   "source": [
    "# entity_df[\"event_timestamp\"] = pd.to_datetime(\"now\", utc=True)\n",
    "training_df = fs.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_hourly_stats:avg_daily_trips\",\n",
    "        \"transformed_conv_rate:conv_rate_plus_val1\",\n",
    "        \"transformed_conv_rate:conv_rate_plus_val2\",\n",
    "    ],\n",
    ").to_df()\n",
    "\n",
    "print(\"\\n----- Example features -----\\n\")\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load features into your online store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5a: Using `materialize_incremental`\n",
    "\n",
    "We now serialize the latest values of features since the beginning of time to prepare for serving (note: `materialize_incremental` serializes all new features since the last `materialize` call).\n",
    "\n",
    "An alternative to using the CLI command is to use Python:\n",
    "\n",
    "```bash\n",
    "CURRENT_TIME=$(date -u +\"%Y-%m-%dT%H:%M:%S\")\n",
    "feast materialize-incremental $CURRENT_TIME\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1722: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  if dataset.partitions is not None:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1750: FutureWarning: 'ParquetDataset.metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  if dataset.metadata:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1769: FutureWarning: 'ParquetDataset.schema' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.schema' attribute instead (which will return an Arrow schema instead of a Parquet schema).\n",
      "  if dataset.schema is not None:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1797: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  if len(dataset.pieces) > 1:\n",
      "/opt/conda/lib/python3.8/site-packages/dask/dataframe/io/parquet/arrow.py:1809: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  for piece, fn in zip(dataset.pieces, fns):\n",
      "/opt/conda/lib/python3.8/site-packages/dask/array/percentile.py:27: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they. (Deprecated NumPy 1.22)\n",
      "  result = np.percentile(a2, q, interpolation=interpolation).astype(a.dtype)\n",
      "/opt/conda/lib/python3.8/site-packages/dask/array/percentile.py:27: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they. (Deprecated NumPy 1.22)\n",
      "  result = np.percentile(a2, q, interpolation=interpolation).astype(a.dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views to \u001b[1m\u001b[32m2023-03-25 06:27:02+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m from \u001b[1m\u001b[32m2023-03-24 06:27:02+00:00\u001b[0m to \u001b[1m\u001b[32m2023-03-25 06:27:02+00:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "fs.materialize_incremental(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5b: Inspect materialized features\n",
    "\n",
    "Note that now there are `online_store.db` and `registry.db`, which store the materialized features and schema information, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data directory ---\n",
      "online.db  registry.db\n",
      "\n",
      "--- Schema of online store ---\n",
      "['entity_key', 'feature_name', 'value', 'event_ts', 'created_ts']\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Data directory ---\")\n",
    "!ls /mnt/shared/feast-store/data\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "con = sqlite3.connect(\"/mnt/shared/feast-store/data/online.db\")\n",
    "print(\"\\n--- Schema of online store ---\")\n",
    "print(\n",
    "    pd.read_sql_query(\n",
    "        \"SELECT * FROM ezaf_feast_repo_driver_hourly_stats\", con).columns.tolist())\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Fetching real-time feature vectors for online inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At inference time, we need to quickly read the latest feature values for different drivers (which otherwise might have existed only in batch sources) from the online feature store using `get_online_features()`. These feature vectors can then be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from feast import FeatureStore\n",
    "\n",
    "feature_vector = fs.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_hourly_stats:avg_daily_trips\",\n",
    "        \"transformed_conv_rate:conv_rate_plus_val1\",\n",
    "        \"transformed_conv_rate:conv_rate_plus_val2\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        # {join_key: entity_value}\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "            \"val_to_add\": 1000,\n",
    "            \"val_to_add_2\": 2000,\n",
    "        },\n",
    "        {\n",
    "            \"driver_id\": 1002,\n",
    "            \"val_to_add\": 1001,\n",
    "            \"val_to_add_2\": 2002,\n",
    "        },\n",
    "    ],\n",
    ").to_dict()\n",
    "\n",
    "pprint(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching features using feature services\n",
    "You can also use feature services to manage multiple features, and decouple feature view definitions and the features needed by end applications. The feature store can also be used to fetch either online or historical features using the same api below. More information can be found [here](https://docs.feast.dev/getting-started/concepts/feature-retrieval).\n",
    "\n",
    " The `driver_activity_v1` feature service pulls all features from the `driver_hourly_stats` feature view:\n",
    "\n",
    "```python\n",
    "driver_stats_fs = FeatureService(\n",
    "    name=\"driver_activity_v1\", features=[driver_hourly_stats_view]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "feature_service = fs.get_feature_service(\"driver_activity_v1\")\n",
    "feature_vector = fs.get_online_features(\n",
    "    features=feature_service,\n",
    "    entity_rows=[\n",
    "        # {join_key: entity_value}\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "            \"val_to_add\": 1000,\n",
    "            \"val_to_add_2\": 2000,\n",
    "        },\n",
    "        {\n",
    "            \"driver_id\": 1002,\n",
    "            \"val_to_add\": 1001,\n",
    "            \"val_to_add_2\": 2002,\n",
    "        },\n",
    "    ],\n",
    ").to_dict()\n",
    "pprint(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Making streaming features available in Feast\n",
    "Feast does not directly ingest from streaming sources. Instead, Feast relies on a push-based model to push features into Feast. You can write a streaming pipeline that generates features, which can then be pushed to the offline store, the online store, or both (depending on your needs).\n",
    "\n",
    "This relies on the `PushSource` defined above. Pushing to this source will populate all dependent feature views with the pushed feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast.data_source import PushMode\n",
    "\n",
    "print(\"\\n--- Simulate a stream event ingestion of the hourly stats df ---\")\n",
    "event_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"driver_id\": [1001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 5, 13, 10, 59, 42),\n",
    "        ],\n",
    "        \"created\": [\n",
    "            datetime(2021, 5, 13, 10, 59, 42),\n",
    "        ],\n",
    "        \"conv_rate\": [1.0],\n",
    "        \"acc_rate\": [1.0],\n",
    "        \"avg_daily_trips\": [1000],\n",
    "    }\n",
    ")\n",
    "print(event_df)\n",
    "fs.push(\"driver_stats_push_source\", event_df, to=PushMode.ONLINE_AND_OFFLINE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
