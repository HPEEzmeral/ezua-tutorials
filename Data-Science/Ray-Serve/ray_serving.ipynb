{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36d84a44",
   "metadata": {
    "id": "36d84a44"
   },
   "source": [
    "# Maximizing Online Inference: Exploring Ray Serve and Diverse Methodologies\n",
    "\n",
    "Ray Serve is an adaptable model deployment framework designed for constructing real-time inference APIs. It's framework-agnostic, allowing you to use a single toolkit to serve a wide range of tools,models and services."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ht0JbCn0bgER",
   "metadata": {
    "id": "ht0JbCn0bgER"
   },
   "source": [
    "![Serve Positioning](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_08/serve_positioning.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22WTQr9xbgER",
   "metadata": {
    "id": "22WTQr9xbgER"
   },
   "source": [
    "![Serve Architecture](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_08/serve_arch.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "071f0c79-917f-4e6f-b19b-01b9cc8c845a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A Deep Dive into Ray Serve with the Agile FastAPI Web Framework Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87a35e4b-1f1b-4978-a822-f09bfae1dc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-29 14:56:41,666\tINFO scripts.py:407 -- Running import path: 'check:app'.\n",
      "2023-10-29 14:56:42,469\tINFO client_builder.py:237 -- Passing the following kwargs to ray.init() on the server: ignore_reinit_error\n",
      "The new client HTTP config differs from the existing one in the following fields: ['location']. The new HTTP config is ignored.\n",
      "2023-10-29 14:56:55,097\tINFO router.py:853 -- Using PowerOfTwoChoicesReplicaScheduler.\n",
      "2023-10-29 14:56:55,106\tINFO router.py:329 -- Got updated replicas for deployment default_RAYFastAPIDeployment: {'default_RAYFastAPIDeployment#WOWWhw', 'default_RAYFastAPIDeployment#PstZJC'}.\n",
      "2023-10-29 14:56:55,110\tSUCC scripts.py:448 -- \u001b[32mDeployed Serve app successfully.\u001b[39m\n",
      "2023-10-29 14:56:55,429\tWARNING long_poll.py:150 -- LongPollClient connection failed, shutting down.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Run this in a separate process to avoid any blocking:\n",
    "! serve run --non-blocking check:app --working-dir=\"./\" --host \"0.0.0.0\" --port 8000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "YO1r_YalbgER",
   "metadata": {
    "id": "YO1r_YalbgER"
   },
   "source": [
    "![NLP API Architecture](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_08/nlp_api_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43200bc7-f001-42a1-9303-f1be1c3d1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''\n",
    "The Blob has long captivated my imagination, emerging as the quintessential cinematic nightmare:\n",
    "an insatiable, amoebic entity with the eerie ability to breach virtually any defense, \n",
    "ominously described by a fated scientist as  assimilating flesh on contact. \n",
    "Mocking parallels to gelatin are futile for this concept embodies the gravest of implications, akin to the cataclysmic \n",
    "gray goo scenario envisioned by technophiles haunted by the specter of runaway artificial intelligence '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042a810c",
   "metadata": {
    "id": "042a810c",
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_content': b'[-0.18333333333333335, -0.6]', '_content_consumed': True, '_next': None, 'status_code': 200, 'headers': {'date': 'Sun, 29 Oct 2023 14:56:48 GMT', 'server': 'uvicorn', 'content-type': 'application/json', 'ray_serve_request_id': 'bhhwtNlXRi', 'Transfer-Encoding': 'chunked'}, 'raw': <urllib3.response.HTTPResponse object at 0x7fbc648647c0>, 'url': 'http://10.224.1.8:8000/check?input_text=%0AThe+Blob+has+long+captivated+my+imagination%2C+emerging+as+the+quintessential+cinematic+nightmare%3A%0Aan+insatiable%2C+amoebic+entity+with+the+eerie+ability+to+breach+virtually+any+defense%2C+%0Aominously+described+by+a+fated+scientist+as++assimilating+flesh+on+contact.+%0AMocking+parallels+to+gelatin+are+futile+for+this+concept+embodies+the+gravest+of+implications%2C+akin+to+the+cataclysmic+%0Agray+goo+scenario+envisioned+by+technophiles+haunted+by+the+specter+of+runaway+artificial+intelligence+', 'encoding': 'utf-8', 'history': [], 'reason': 'OK', 'cookies': <RequestsCookieJar[]>, 'elapsed': datetime.timedelta(seconds=3, microseconds=291072), 'request': <PreparedRequest [GET]>, 'connection': <requests.adapters.HTTPAdapter object at 0x7fbc640285e0>}\n",
      "200\n",
      "[-0.18333333333333335, -0.6]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__)\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__.get('status_code'))\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__.get('_content').decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eae74fc-6e4a-41cd-abb3-b0e89cd2da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''Greetings, World! The Desired Request Handling: Executed as Anticipated!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b83dea8-3e79-4e57-9382-aa1030976ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_content': b'', '_content_consumed': True, '_next': None, 'status_code': 200, 'headers': {'date': 'Sun, 29 Oct 2023 14:57:25 GMT', 'server': 'uvicorn', 'content-type': 'text/plain', 'ray_serve_request_id': 'GoVcnGRvZG', 'Transfer-Encoding': 'chunked'}, 'raw': <urllib3.response.HTTPResponse object at 0x7fbc6486eaf0>, 'url': 'http://10.224.1.8:8000/check?input_text=Greetings%2C+World%21+The+Desired+Request+Handling%3A+Executed+as+Anticipated%21', 'encoding': 'ISO-8859-1', 'history': [], 'reason': 'OK', 'cookies': <RequestsCookieJar[]>, 'elapsed': datetime.timedelta(microseconds=19417), 'request': <PreparedRequest [GET]>, 'connection': <requests.adapters.HTTPAdapter object at 0x7fbc64028c70>}\n",
      "200\n",
      "[0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__)\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__.get('status_code'))\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__.get('_content').decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff10495b-64d8-4368-9711-755d5a29bfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_content': b'[0.0, 0.0]', '_content_consumed': True, '_next': None, 'status_code': 200, 'headers': {'date': 'Sun, 29 Oct 2023 14:57:27 GMT', 'server': 'uvicorn', 'content-type': 'application/json', 'ray_serve_request_id': 'JJHsrKwHaQ', 'Transfer-Encoding': 'chunked'}, 'raw': <urllib3.response.HTTPResponse object at 0x7fbc6486e130>, 'url': 'http://10.224.1.8:8000/check?input_text=Greetings%2C+World%21+The+Desired+Request+Handling%3A+Executed+as+Anticipated%21', 'encoding': 'utf-8', 'history': [], 'reason': 'OK', 'cookies': <RequestsCookieJar[]>, 'elapsed': datetime.timedelta(microseconds=18812), 'request': <PreparedRequest [GET]>, 'connection': <requests.adapters.HTTPAdapter object at 0x7fbc6486e8b0>}\n",
      "200\n",
      "[0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__)\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__.get('status_code'))\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__.get('_content').decode('utf8'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4bd4d9d-46b2-4513-82cb-9f67a53716c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Navigating Ray Serve through RayServeSyncHandle's Methodological Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1339281-753f-4aca-82b7-e959c3545a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-29 15:13:12,948\tINFO scripts.py:407 -- Running import path: 'snycheck:app'.\n",
      "2023-10-29 15:13:13,673\tINFO client_builder.py:237 -- Passing the following kwargs to ray.init() on the server: ignore_reinit_error\n",
      "The new client HTTP config differs from the existing one in the following fields: ['location']. The new HTTP config is ignored.\n",
      "2023-10-29 15:13:26,256\tINFO router.py:853 -- Using PowerOfTwoChoicesReplicaScheduler.\n",
      "2023-10-29 15:13:26,266\tINFO router.py:329 -- Got updated replicas for deployment default_RAYFastAPIDeployment: {'default_RAYFastAPIDeployment#DRDeIb', 'default_RAYFastAPIDeployment#LytUUR'}.\n",
      "2023-10-29 15:13:27,619\tERROR dataclient.py:312 -- Callback error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/site-packages/ray/util/client/dataclient.py\", line 301, in _process_response\n",
      "    can_remove = callback(response)\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/site-packages/ray/util/client/dataclient.py\", line 179, in __call__\n",
      "    self.callback(self.data)\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/site-packages/ray/util/client/common.py\", line 180, in deserialize_obj\n",
      "    py_callback(data)\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/site-packages/ray/util/client/common.py\", line 148, in set_future\n",
      "    fut.set_result(data)\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/concurrent/futures/_base.py\", line 532, in set_result\n",
      "    raise InvalidStateError('{}: {!r}'.format(self._state, self))\n",
      "concurrent.futures._base.InvalidStateError: CANCELLED: <Future at 0x7f04285b6fd0 state=cancelled>\n",
      "None\n",
      "The new client HTTP config differs from the existing one in the following fields: ['host']. The new HTTP config is ignored.\n",
      "The new client HTTP config differs from the existing one in the following fields: ['host']. The new HTTP config is ignored.\n",
      "2023-10-29 15:13:28,445\tINFO router.py:329 -- Got updated replicas for deployment default_RAYFastAPIDeployment: {'default_RAYFastAPIDeployment#LytUUR'}.\n",
      "2023-10-29 15:13:32,088\tINFO router.py:329 -- Got updated replicas for deployment default_RAYFastAPIDeployment: {'default_RAYFastAPIDeployment#NUowjO', 'default_RAYFastAPIDeployment#LytUUR'}.\n",
      "2023-10-29 15:13:32,191\tINFO router.py:329 -- Got updated replicas for deployment default_RAYFastAPIDeployment: {'default_RAYFastAPIDeployment#NUowjO'}.\n",
      "2023-10-29 15:13:35,916\tINFO router.py:329 -- Got updated replicas for deployment default_RAYFastAPIDeployment: {'default_RAYFastAPIDeployment#HWRmZX', 'default_RAYFastAPIDeployment#NUowjO'}.\n",
      "2023-10-29 15:13:36,353\tSUCC scripts.py:448 -- \u001b[32mDeployed Serve app successfully.\u001b[39m\n",
      "Exception in thread ray_client_streaming_rpc:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/site-packages/ray/util/client/dataclient.py\", line 286, in _data_main\n",
      "    self._shutdown()\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/site-packages/ray/util/client/dataclient.py\", line 361, in _shutdown\n",
      "    callback(err)\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/site-packages/ray/util/client/dataclient.py\", line 135, in __call__\n",
      "    self.callback(response)\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/site-packages/ray/util/client/common.py\", line 180, in deserialize_obj\n",
      "    py_callback(data)\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/site-packages/ray/util/client/common.py\", line 146, in set_future\n",
      "    fut.set_exception(data)\n",
      "  File \"/opt/conda/envs/ray/lib/python3.8/concurrent/futures/_base.py\", line 547, in set_exception\n",
      "    raise InvalidStateError('{}: {!r}'.format(self._state, self))\n",
      "concurrent.futures._base.InvalidStateError: CANCELLED: <Future at 0x7f04285b6fd0 state=cancelled>\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Run this in a separate process to avoid any blocking:\n",
    "! serve run --non-blocking snycheck:app --working-dir=\"./\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "040ad3c9",
   "metadata": {},
   "source": [
    "![NLP API Architecture](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_08/nlp_api_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc0335f6-fb45-4096-8cb8-400b21336ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''\n",
    "The Blob has long captivated my imagination, emerging as the quintessential cinematic nightmare:\n",
    "an insatiable, amoebic entity with the eerie ability to breach virtually any defense, \n",
    "ominously described by a fated scientist as  assimilating flesh on contact. \n",
    "Mocking parallels to gelatin are futile for this concept embodies the gravest of implications, akin to the cataclysmic \n",
    "gray goo scenario envisioned by technophiles haunted by the specter of runaway artificial intelligence '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faee3b22-bbd9-4e96-b67c-d88fbd441f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_content': b'[-0.18333333333333335, -0.6]', '_content_consumed': True, '_next': None, 'status_code': 200, 'headers': {'date': 'Sun, 29 Oct 2023 15:13:41 GMT', 'server': 'uvicorn', 'content-type': 'application/json', 'ray_serve_request_id': 'gihfIuAZga', 'Transfer-Encoding': 'chunked'}, 'raw': <urllib3.response.HTTPResponse object at 0x7f30b820ab50>, 'url': 'http://10.224.1.8:8000/check?input_text=%0AThe+Blob+has+long+captivated+my+imagination%2C+emerging+as+the+quintessential+cinematic+nightmare%3A%0Aan+insatiable%2C+amoebic+entity+with+the+eerie+ability+to+breach+virtually+any+defense%2C+%0Aominously+described+by+a+fated+scientist+as++assimilating+flesh+on+contact.+%0AMocking+parallels+to+gelatin+are+futile+for+this+concept+embodies+the+gravest+of+implications%2C+akin+to+the+cataclysmic+%0Agray+goo+scenario+envisioned+by+technophiles+haunted+by+the+specter+of+runaway+artificial+intelligence+', 'encoding': 'utf-8', 'history': [], 'reason': 'OK', 'cookies': <RequestsCookieJar[]>, 'elapsed': datetime.timedelta(seconds=3, microseconds=747787), 'request': <PreparedRequest [GET]>, 'connection': <requests.adapters.HTTPAdapter object at 0x7f30b8243940>}\n",
      "200\n",
      "[-0.18333333333333335, -0.6]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__)\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__.get('status_code'))\n",
    "print(requests.get(\"http://10.224.1.8:8000/check\", params={\"input_text\": input_text}).__dict__.get('_content').decode('utf8'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81dbeb94",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "\n",
    "Ray Serve is an adaptable model deployment framework designed for constructing real-time inference APIs. It's framework-agnostic, allowing you to use a single toolkit to serve a wide range of models, including deep learning models created with popular frameworks like PyTorch, TensorFlow, and Keras, as well as Scikit-Learn models and custom Python business logic. This versatile tool boasts an array of features and performance enhancements, such as response streaming, dynamic request batching, and multi-node/multi-GPU support, making it well-suited for handling Large Language Models and other demanding tasks.\n",
    "What sets Ray Serve apart is its proficiency in orchestrating the composition of multiple machine learning models and business logic components within a single Python-based inference service. Leveraging the power of Ray, it seamlessly scales across multiple machines and offers flexible scheduling capabilities, including fractional GPU allocation, which optimisation resource sharing and enables cost-effective deployment of a multitude of machine learning models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Ray",
   "language": "python",
   "name": "ray"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
